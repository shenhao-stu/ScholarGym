{"query": "Can you tell me some papers about hybrid architectures in reconstruction-based techniques?", "cited_paper": [{"arxiv_id": "2009.02040", "title": "Multivariate Time-series Anomaly Detection via Graph Attention Network", "year": 2020}], "gt_label": [1], "date": "2020-09", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_0", "valid": true}
{"query": "Are there any studies that analysed the use of target networks for Deep Q-learning?", "cited_paper": [{"arxiv_id": "1901.00137", "title": "A Theoretical Analysis of Deep Q-Learning", "year": 2019}], "gt_label": [1], "date": "2019-01", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_1", "valid": true}
{"query": "Any resources providing information about attempts to detect or calibrate biases automatically in peer reviews?", "cited_paper": [{"arxiv_id": "2010.15300", "title": "Uncovering Latent Biases in Text: Method and Application to Peer Review", "year": 2020}, {"arxiv_id": "2110.14802", "title": "You Are the Best Reviewer of Your Own Papers: An Owner-Assisted Scoring Mechanism", "year": 2021}, {"arxiv_id": "1806.05085", "title": "Your 2 is My 1, Your 3 is My 9: Handling Arbitrary Miscalibrations in Ratings", "year": 2018}, {"arxiv_id": "2110.12607", "title": "Least Square Calibration for Peer Review", "year": 2021}], "gt_label": [1, 1, 1, 1], "date": "2021-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_2", "valid": true}
{"query": "What papers are the foundation models for the Natural Language Processing (NLP) field based on?", "cited_paper": [{"arxiv_id": "1810.04805", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "year": 2018}, {"arxiv_id": "2005.14165", "title": "Language Models are Few-Shot Learners", "year": 2020}, {"arxiv_id": "1910.10683", "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer", "year": 2019}, {"arxiv_id": "2204.02311", "title": "PaLM: Scaling Language Modeling with Pathways", "year": 2022}, {"arxiv_id": "2302.13971", "title": "LLaMA: Open and Efficient Foundation Language Models", "year": 2023}], "gt_label": [1, 1, 1, 1, 1], "date": "2023-02", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_3", "valid": true}
{"query": "Could you list the papers that explored identifying optimal interventions through sequential experimentation in causal bandits and causal reinforcement learning?", "cited_paper": [{"arxiv_id": "1606.03203", "title": "Causal Bandits: Learning Good Interventions via Causal Inference", "year": 2016}], "gt_label": [1], "date": "2016-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_4", "valid": true}
{"query": "Could you provide me some studies that focused on white-box scenarios for cyber-security in machine learning?", "cited_paper": [{"arxiv_id": "1908.07125", "title": "Universal Adversarial Triggers for Attacking and Analyzing NLP", "year": 2019}], "gt_label": [1], "date": "2019-08", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_5", "valid": true}
{"query": "Which papers generalize the coordinate definition of the field to cases where the parameters of a viewing ray are used?", "cited_paper": [{"arxiv_id": "2106.02634", "title": "Light Field Networks: Neural Scene Representations with Single-Evaluation Rendering", "year": 2021}, {"arxiv_id": "2111.13152", "title": "Scene Representation Transformer: Geometry-Free Novel View Synthesis Through Set-Latent Scene Representations", "year": 2021}], "gt_label": [1, 1], "date": "2021-11", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_6", "valid": true}
{"query": "Which works develop suitable approximations of the predictive distribution or parts of the integral for uncertainties in deep learning?", "cited_paper": [{"arxiv_id": "1806.05034", "title": "A Probabilistic U-Net for Segmentation of Ambiguous Images", "year": 2018}, {"arxiv_id": "2006.06015", "title": "Stochastic Segmentation Networks: Modelling Spatially Correlated Aleatoric Uncertainty", "year": 2020}, {"arxiv_id": "1905.13077", "title": "A Hierarchical Probabilistic U-Net for Modeling Multi-Scale Ambiguities", "year": 2019}, {"arxiv_id": "1906.04045", "title": "PHiSeg: Capturing Uncertainty in Medical Image Segmentation", "year": 2019}], "gt_label": [1, 1, 1, 1], "date": "2020-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_7", "valid": true}
{"query": "Which studies have proposed using voxel for spatial geometry and texture modeling in 3D scene representation?", "cited_paper": [{"arxiv_id": "1406.5670", "title": "3D ShapeNets: A Deep Representation for Volumetric Shapes", "year": 2014}, {"arxiv_id": "1604.00449", "title": "3D-R2N2: A Unified Approach for Single and Multi-view 3D Object Reconstruction", "year": 2016}], "gt_label": [1, 1], "date": "2016-04", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_8", "valid": true}
{"query": "Which studies present issues about the stationary distribution of rewards over contexts?", "cited_paper": [{"arxiv_id": "1801.01750", "title": "Nonparametric Stochastic Contextual Bandits", "year": 2018}, {"arxiv_id": "1909.02553", "title": "Smooth Contextual Bandits: Bridging the Parametric and Non-differentiable Regret Regimes", "year": 2019}, {"arxiv_id": "1902.00819", "title": "Randomized Allocation with Nonparametric Estimation for Contextual Multi-Armed Bandits with Delayed Rewards", "year": 2019}, {"arxiv_id": "2007.08584", "title": "Self-Tuning Bandits over Unknown Covariate-Shifts", "year": 2020}, {"arxiv_id": "1910.09714", "title": "Smoothness-Adaptive Contextual Bandits", "year": 2019}, {"arxiv_id": "2211.12612", "title": "Transfer Learning for Contextual Multi-armed Bandits", "year": 2022}], "gt_label": [1, 1, 1, 1, 1, 1], "date": "2022-11", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_9", "valid": true}
{"query": "Which work first implemented token-level edit operation prediction in Seq2Edit methods?", "cited_paper": [{"arxiv_id": "1909.01187", "title": "Encode, Tag, Realize: High-Precision Text Editing", "year": 2019}], "gt_label": [1], "date": "2019-09", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_10", "valid": true}
{"query": "Could you provide me a study about generating sign pose sequences from gloss sequences by employing VQ-VAE?", "cited_paper": [{"arxiv_id": "2208.09141", "title": "G2P-DDM: Generating Sign Pose Sequence from Gloss Sequence with Discrete Diffusion Model", "year": 2022}], "gt_label": [1], "date": "2022-08", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_11", "valid": true}
{"query": "Who proposed source-free universal domain adaptation (SF-UniDA)?", "cited_paper": [{"arxiv_id": "2112.08553", "title": "UMAD: Universal Model Adaptation under Domain and Category Shift", "year": 2021}, {"arxiv_id": "2303.07110", "title": "Upcycling Models under Domain and Category Shift", "year": 2023}], "gt_label": [1, 1], "date": "2023-03", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_12", "valid": true}
{"query": "What works aim to study the policies or features that remain stable across the different training tasks?", "cited_paper": [{"arxiv_id": "2006.01096", "title": "Invariant Policy Optimization: Towards Stronger Generalization in Reinforcement Learning", "year": 2020}, {"arxiv_id": "2011.01089", "title": "Instance based Generalization in Reinforcement Learning", "year": 2020}, {"arxiv_id": "2102.07097", "title": "Domain Adversarial Reinforcement Learning", "year": 2021}, {"arxiv_id": "1910.12911", "title": "Generalization in Reinforcement Learning with Selective Noise Injection and Information Bottleneck", "year": 2019}, {"arxiv_id": "2009.08319", "title": "Decoupling Representation Learning from Reinforcement Learning", "year": 2020}, {"arxiv_id": "2006.07217", "title": "Deep Reinforcement and InfoMax Learning", "year": 2020}], "gt_label": [1, 1, 1, 1, 1, 1], "date": "2021-02", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_13", "valid": true}
{"query": "Could you provide me some works about fine-tuning LLMs to better response to visual instructions?", "cited_paper": [{"arxiv_id": "2304.14178", "title": "mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality", "year": 2023}, {"arxiv_id": "2310.03744", "title": "Improved Baselines with Visual Instruction Tuning", "year": 2023}], "gt_label": [1, 1], "date": "2023-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_14", "valid": true}
{"query": "Could you mention some works that classify unsupervised segmentation into two categories: clustering based on invariance and clustering using pre-trained models?", "cited_paper": [{"arxiv_id": "2103.17070", "title": "PiCIE: Unsupervised Semantic Segmentation using Invariance and Equivariance in Clustering", "year": 2021}, {"arxiv_id": "1807.06653", "title": "Invariant Information Clustering for Unsupervised Image Classification and Segmentation", "year": 2018}, {"arxiv_id": "2207.05027", "title": "Unsupervised Semantic Segmentation with Self-supervised Object-centric Representations", "year": 2022}, {"arxiv_id": "2210.05944", "title": "ACSeg: Adaptive Conceptualization for Unsupervised Semantic Segmentation", "year": 2022}, {"arxiv_id": "2203.08414", "title": "Unsupervised Semantic Segmentation by Distilling Feature Correspondences", "year": 2022}, {"arxiv_id": "2209.11228", "title": "NamedMask: Distilling Segmenters from Complementary Foundation Models", "year": 2022}], "gt_label": [1, 1, 1, 1, 1, 1], "date": "2022-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_15", "valid": true}
{"query": "Could you provide me examples of the development of more sophisticated feature extractors that enhance Point Cloud processing?", "cited_paper": [{"arxiv_id": "1811.07246", "title": "PointConv: Deep Convolutional Networks on 3D Point Clouds", "year": 2018}, {"arxiv_id": "2012.09164", "title": "Point Transformer", "year": 2020}, {"arxiv_id": "2202.07123", "title": "Rethinking Network Design and Local Geometry in Point Cloud: A Simple Residual MLP Framework", "year": 2022}, {"arxiv_id": "2010.11929", "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale", "year": 2020}], "gt_label": [1, 1, 1, 1], "date": "2022-02", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_16", "valid": true}
{"query": "What are the papers related to face reenactment, specifically aimed at transferring facial expressions and movements?", "cited_paper": [{"arxiv_id": "2204.05018", "title": "Structure-Aware Motion Transfer with Deformable Anchor Model", "year": 2022}, {"arxiv_id": "2203.14367", "title": "Thin-Plate Spline Motion Model for Image Animation", "year": 2022}, {"arxiv_id": "2203.09043", "title": "Latent Image Animator: Learning to Animate Images via Latent Space Navigation", "year": 2022}, {"arxiv_id": "2301.06281", "title": "DPE: Disentanglement of Pose and Expression for General Video Portrait Editing", "year": 2023}], "gt_label": [1, 1, 1, 1], "date": "2023-01", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_17", "valid": true}
{"query": "What papers propose the use of spatiotemporal transformer for BEV generation?", "cited_paper": [{"arxiv_id": "2203.17270", "title": "BEVFormer: Learning Bird's-Eye-View Representation from Multi-Camera Images via Spatiotemporal Transformers", "year": 2022}], "gt_label": [1], "date": "2022-03", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_18", "valid": true}
{"query": "Can you name some works that extend Global Descent for deep learning architectures?", "cited_paper": [{"arxiv_id": "1811.03962", "title": "A Convergence Theory for Deep Learning via Over-Parameterization", "year": 2018}, {"arxiv_id": "1811.03804", "title": "Gradient Descent Finds Global Minima of Deep Neural Networks", "year": 2018}, {"arxiv_id": "1906.04688", "title": "An Improved Analysis of Training Over-parameterized Deep Neural Networks", "year": 2019}], "gt_label": [1, 1, 1], "date": "2019-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_19", "valid": true}
{"query": "Could you provide me large multimodal models (LMMs) references?", "cited_paper": [{"arxiv_id": "2304.08485", "title": "Visual Instruction Tuning", "year": 2023}, {"arxiv_id": "2301.12597", "title": "BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models", "year": 2023}, {"arxiv_id": "2306.15195", "title": "Shikra: Unleashing Multimodal LLM's Referential Dialogue Magic", "year": 2023}, {"arxiv_id": "2304.10592", "title": "MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models", "year": 2023}], "gt_label": [1, 1, 1, 1], "date": "2023-09", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_20", "valid": true}
{"query": "Could you provide me studies about achieving local editing by involving semantic masks as intermediate representations?", "cited_paper": [{"arxiv_id": "2111.15490", "title": "FENeRF: Face Editing in Neural Radiance Fields", "year": 2021}, {"arxiv_id": "2205.15517", "title": "IDE-3D: Interactive Disentangled Editing for High-Resolution 3D-aware Portrait Synthesis", "year": 2022}], "gt_label": [1, 1], "date": "2022-05", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_21", "valid": true}
{"query": "Which works propose methods for feature matching by detecting and describing keypoints on images?", "cited_paper": [{"arxiv_id": "1712.07629", "title": "SuperPoint: Self-Supervised Interest Point Detection and Description", "year": 2017}, {"arxiv_id": "1905.03561", "title": "D2-Net: A Trainable CNN for Joint Detection and Description of Local Features", "year": 2019}, {"arxiv_id": "1906.06195", "title": "R2D2: Repeatable and Reliable Detector and Descriptor", "year": 2019}], "gt_label": [1, 1, 1], "date": "2019-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_22", "valid": true}
{"query": "Which work first demonstrated the possibility of reconstructing accurate 3D full-body motion using only six IMUs?", "cited_paper": [{"arxiv_id": "1703.08014", "title": "Sparse Inertial Poser: Automatic 3D Human Pose Estimation from Sparse IMUs", "year": 2017}], "gt_label": [1], "date": "2017-03", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_23", "valid": true}
{"query": "Could you provide me a work that extended the minimax method to deep neural networks?", "cited_paper": [{"arxiv_id": "1908.10831", "title": "Stochastic AUC Maximization with Deep Neural Networks", "year": 2019}], "gt_label": [1], "date": "2019-08", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_24", "valid": true}
{"query": "Any works that have commented on the challenge of training the PRM due to expensive human-annotated datasets?", "cited_paper": [{"arxiv_id": "2211.14275", "title": "Solving math word problems with processand outcome-based feedback", "year": 2022}, {"arxiv_id": "2305.20050", "title": "Let's Verify Step by Step", "year": 2023}], "gt_label": [1, 1], "date": "2023-05", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_25", "valid": true}
{"query": "Which papers are known for initially representing 3D scenes with a set of 3D Gaussians?", "cited_paper": [{"arxiv_id": "2308.04079", "title": "3D Gaussian Splatting for Real-Time Radiance Field Rendering", "year": 2023}], "gt_label": [1], "date": "2023-08", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_26", "valid": true}
{"query": "Which works proposed architectures for group equivariance in image classification?", "cited_paper": [{"arxiv_id": "1602.07576", "title": "Group Equivariant Convolutional Networks", "year": 2016}, {"arxiv_id": "1612.08498", "title": "Steerable CNNs", "year": 2016}], "gt_label": [1, 1], "date": "2020-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_27", "valid": true}
{"query": "What papers mention the increased computational complexity and decreased utility due to DPSGD?", "cited_paper": [{"arxiv_id": "1607.00133", "title": "Deep Learning with Differential Privacy", "year": 2016}, {"arxiv_id": "2011.11660", "title": "Differentially Private Learning Needs Better Features (or Much More Data)", "year": 2020}], "gt_label": [1, 1], "date": "2020-11", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_28", "valid": true}
{"query": "In which studies has it been demonstrated that multi-modal models are vulnerable to adversarial attacks?", "cited_paper": [{"arxiv_id": "2304.14614", "title": "Fusion is Not Enough: Single Modal Attacks on Fusion Models for 3D Object Detection", "year": 2023}, {"arxiv_id": "2206.09391", "title": "Towards Adversarial Attack on Vision-Language Pre-training Models", "year": 2022}, {"arxiv_id": "2104.02000", "title": "Can audio-visual integration strengthen robustness under multimodal attacks?", "year": 2021}, {"arxiv_id": "1709.08693", "title": "Fooling Vision and Language Models Despite Localization and Attention Mechanism", "year": 2017}, {"arxiv_id": "1902.05660", "title": "Cycle-Consistency for Robust Visual Question Answering", "year": 2019}, {"arxiv_id": "1412.6572", "title": "Explaining and Harnessing Adversarial Examples", "year": 2014}], "gt_label": [1, 1, 1, 1, 1, 1], "date": "2023-04", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_29", "valid": true}
{"query": "Could you provide me studies that expound the impossibility of identifying latent factors for i.i.d. nonlinearly-dependent data without labels or assumptions about the data generating process?", "cited_paper": [{"arxiv_id": "1811.12359", "title": "Challenging Common Assumptions in the Unsupervised Learning of Disentangled Representations", "year": 2018}, {"arxiv_id": "1907.04809", "title": "Variational Autoencoders and Nonlinear ICA: A Unifying Framework", "year": 2019}], "gt_label": [1, 1], "date": "2019-07", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_30", "valid": true}
{"query": "Could you provide me an example where an open-source model was introduced for input-output unsafety detection for LLMs?", "cited_paper": [{"arxiv_id": "2312.06674", "title": "Llama Guard: LLM-based Input-Output Safeguard for Human-AI Conversations", "year": 2023}], "gt_label": [1], "date": "2023-12", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_31", "valid": true}
{"query": "What papers focused on source data estimation or self-training for pinhole images in the context of SFUDA?", "cited_paper": [{"arxiv_id": "2108.11249", "title": "Generalize then Adapt: Source-Free Domain Adaptive Semantic Segmentation", "year": 2021}, {"arxiv_id": "2106.03422", "title": "Source-Free Open Compound Domain Adaptation in Semantic Segmentation", "year": 2021}, {"arxiv_id": "2103.16372", "title": "Source-Free Domain Adaptation for Semantic Segmentation", "year": 2021}, {"arxiv_id": "2108.03152", "title": "Source-Free Domain Adaptation for Image Segmentation", "year": 2021}], "gt_label": [1, 1, 1, 1], "date": "2021-08", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_32", "valid": true}
{"query": "Can you provide some works about predicting the contact map, the distance map and/or the torsion angles between protein residues?", "cited_paper": [], "gt_label": [], "date": "2016-09", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_33", "valid": true}
{"query": "What paper explored the application of VLMs, specifically CLIP, for BEV retrieval tasks?", "cited_paper": [{"arxiv_id": "2401.01065", "title": "BEV-TSR: Text-Scene Retrieval in BEV Space for Autonomous Driving", "year": 2024}], "gt_label": [1], "date": "2024-01", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_34", "valid": true}
{"query": "Could you list research that demonstrated the advantages of Quantization-Aware Training (QAT), which can enable the model to learn better representations for low-bit weights?", "cited_paper": [{"arxiv_id": "2308.13137", "title": "OmniQuant: Omnidirectionally Calibrated Quantization for Large Language Models", "year": 2023}, {"arxiv_id": "2310.00034", "title": "PB-LLM: Partially Binarized Large Language Models", "year": 2023}, {"arxiv_id": "2310.11453", "title": "BitNet: Scaling 1-bit Transformers for Large Language Models", "year": 2023}], "gt_label": [1, 1, 1], "date": "2023-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_35", "valid": true}
{"query": "What are the researches that have explored the application of Crypto-based Private Learning in privacy-preserving machine learning?", "cited_paper": [{"arxiv_id": "2106.07229", "title": "Privacy-Preserving Machine Learning with Fully Homomorphic Encryption for Deep Neural Network", "year": 2021}], "gt_label": [1], "date": "2021-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_36", "valid": true}
{"query": "Any works that focus on augmenting sparse inputs with synthetically generated views?", "cited_paper": [{"arxiv_id": "2205.05922", "title": "Ray Priors through Reprojection: Improving Neural Radiance Fields for Novel View Extrapolation", "year": 2022}, {"arxiv_id": "2210.04214", "title": "VM-NeRF: Tackling Sparsity in NeRF with View Morphing", "year": 2022}, {"arxiv_id": "2301.10941", "title": "GeCoNeRF: Few-shot Neural Radiance Fields via Geometric Consistency", "year": 2023}], "gt_label": [1, 1, 1], "date": "2023-01", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_37", "valid": true}
{"query": "Which work introduces Point-E, a language-guided DM?", "cited_paper": [{"arxiv_id": "2212.08751", "title": "Point-E: A System for Generating 3D Point Clouds from Complex Prompts", "year": 2022}], "gt_label": [1], "date": "2022-12", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_38", "valid": true}
{"query": "Which papers discuss the practical applicability of black-box and transfer-based threat model, and the related security and safety risks?", "cited_paper": [{"arxiv_id": "1602.02697", "title": "Practical Black-Box Attacks against Machine Learning", "year": 2016}, {"arxiv_id": "1710.06081", "title": "Boosting Adversarial Attacks with Momentum", "year": 2017}], "gt_label": [1, 1], "date": "2017-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_39", "valid": true}
{"query": "What studies develop hierarchical models in relation to diffusion models?", "cited_paper": [{"arxiv_id": "2205.11487", "title": "Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding", "year": 2022}, {"arxiv_id": "2204.06125", "title": "Hierarchical Text-Conditional Image Generation with CLIP Latents", "year": 2022}, {"arxiv_id": "2106.15282", "title": "Cascaded Diffusion Models for High Fidelity Image Generation", "year": 2021}], "gt_label": [1, 1, 1], "date": "2022-05", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_40", "valid": true}
{"query": "What are the papers that analyze the limitations of simple random walks on the clique expansion of the hypergraph?", "cited_paper": [{"arxiv_id": "1911.02613", "title": "Hyper-SAGNN: a self-attention based graph neural network for hypergraphs", "year": 2019}, {"arxiv_id": "2106.06039", "title": "Neural Predicting Higher-order Patterns in Temporal Networks", "year": 2021}], "gt_label": [1, 1], "date": "2021-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_41", "valid": true}
{"query": "Which study explicitly determines and measures the faithfulness of explanations in LLMs?", "cited_paper": [{"arxiv_id": "2307.11768", "title": "Question Decomposition Improves the Faithfulness of Model-Generated Reasoning", "year": 2023}], "gt_label": [1], "date": "2023-07", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_42", "valid": true}
{"query": "Which study argued on the difficulties of implementing a GAN-like procedure using the dual form of UOT?", "cited_paper": [{"arxiv_id": "2010.05862", "title": "Robust Optimal Transport with Applications in Generative Modeling and Domain Adaptation", "year": 2020}], "gt_label": [1], "date": "2020-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_43", "valid": true}
{"query": "What studies deal with standard feature selection that selects the same subset of features for each data sample?", "cited_paper": [{"arxiv_id": "1601.07996", "title": "Feature Selection: A Data Perspective", "year": 2016}], "gt_label": [1], "date": "2016-01", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_44", "valid": true}
{"query": "What works are related to the use of commonsense knowledge in Knowledge Graphs?", "cited_paper": [{"arxiv_id": "1612.03975", "title": "ConceptNet 5.5: An Open Multilingual Graph of General Knowledge", "year": 2016}], "gt_label": [1], "date": "2016-12", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_45", "valid": true}
{"query": "What works discuss the lack of robustness in NLP benchmarks?", "cited_paper": [{"arxiv_id": "2402.01781", "title": "When Benchmarks are Targets: Revealing the Sensitivity of Large Language Model Leaderboards", "year": 2024}], "gt_label": [1], "date": "2024-02", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_46", "valid": true}
{"query": "Which papers examined pretraining on scientific text corpora?", "cited_paper": [{"arxiv_id": "1903.10676", "title": "SciBERT: A Pretrained Language Model for Scientific Text", "year": 2019}, {"arxiv_id": "2007.15779", "title": "Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing", "year": 2020}, {"arxiv_id": "1904.05342", "title": "ClinicalBERT: Modeling Clinical Notes and Predicting Hospital Readmission", "year": 2019}], "gt_label": [1, 1, 1], "date": "2020-07", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_47", "valid": true}
{"query": "Which studies apply model-agnostic meta learning (MAML) to deep anomaly detector models?", "cited_paper": [{"arxiv_id": "1703.03400", "title": "Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks", "year": 2017}, {"arxiv_id": "2007.04146", "title": "Few-Shot One-Class Classification via Meta-Learning", "year": 2020}, {"arxiv_id": "2007.07843", "title": "Few-shot Scene-adaptive Anomaly Detection", "year": 2020}, {"arxiv_id": "2102.11165", "title": "Few-shot Network Anomaly Detection via Cross-network Meta-learning", "year": 2021}], "gt_label": [1, 1, 1, 1], "date": "2021-02", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_48", "valid": true}
{"query": "What works have proposed guidelines for documenting ML datasets?", "cited_paper": [{"arxiv_id": "1803.09010", "title": "Datasheets for Datasets", "year": 2018}], "gt_label": [1], "date": "2018-03", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_49", "valid": true}
{"query": "Which papers focused on locally aligning fixed patches with textual words?", "cited_paper": [{"arxiv_id": "2102.03334", "title": "ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision", "year": 2021}, {"arxiv_id": "2111.07783", "title": "FILIP: Fine-grained Interactive Language-Image Pre-Training", "year": 2021}, {"arxiv_id": "2109.01949", "title": "Improving Joint Learning of Chest X-Ray and Radiology Report by Word Region Alignment", "year": 2021}, {"arxiv_id": "2210.06044", "title": "Multi-Granularity Cross-modal Alignment for Generalized Medical Visual Representation Learning", "year": 2022}], "gt_label": [1, 1, 1, 1], "date": "2022-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_50", "valid": true}
{"query": "What is the fundamental work on fully convolutional networks (FCNs) used for deep learning-based semantic segmentation?", "cited_paper": [{"arxiv_id": "1411.4038", "title": "Fully Convolutional Networks for Semantic Segmentation", "year": 2014}], "gt_label": [1], "date": "2014-11", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_51", "valid": true}
{"query": "Which works focused on ray-based rendering for novel view synthesis approach?", "cited_paper": [{"arxiv_id": "2104.06935", "title": "Stereo Radiance Fields (SRF): Learning View Synthesis for Sparse Views of Novel Scenes", "year": 2021}, {"arxiv_id": "2102.13090", "title": "IBRNet: Learning Multi-View Image-Based Rendering", "year": 2021}, {"arxiv_id": "2207.10662", "title": "Generalizable Patch-Based Neural Rendering", "year": 2022}, {"arxiv_id": "2207.13298", "title": "Is Attention All That NeRF Needs?", "year": 2022}, {"arxiv_id": "2304.12294", "title": "Explicit Correspondence Matching for Generalizable Neural Radiance Fields", "year": 2023}], "gt_label": [1, 1, 1, 1, 1], "date": "2023-04", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_52", "valid": true}
{"query": "Which papers contribute to the advancement of model-based reinforcement learning through the study of the world model?", "cited_paper": [{"arxiv_id": "1809.01999", "title": "Recurrent World Models Facilitate Policy Evolution", "year": 2018}, {"arxiv_id": "1811.04551", "title": "Learning Latent Dynamics for Planning from Pixels", "year": 2018}, {"arxiv_id": "1912.01603", "title": "Dream to Control: Learning Behaviors by Latent Imagination", "year": 2019}, {"arxiv_id": "2003.08876", "title": "Learning to Fly via Deep Model-Based Reinforcement Learning", "year": 2020}, {"arxiv_id": "2010.02193", "title": "Mastering Atari with Discrete World Models", "year": 2020}, {"arxiv_id": "2301.04104", "title": "Mastering Diverse Domains through World Models", "year": 2023}, {"arxiv_id": "1903.00374", "title": "Model-Based Reinforcement Learning for Atari", "year": 2019}, {"arxiv_id": "1911.08265", "title": "Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model", "year": 2019}], "gt_label": [1, 1, 1, 1, 1, 1, 1, 1], "date": "2023-01", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_53", "valid": true}
{"query": "Could you provide me some studies proposing models for learning latent graphs?", "cited_paper": [{"arxiv_id": "1801.07829", "title": "Dynamic Graph CNN for Learning on Point Clouds", "year": 2018}, {"arxiv_id": "2003.13620", "title": "Latent-Graph Learning for Disease Prediction", "year": 2020}, {"arxiv_id": "2002.04999", "title": "Differentiable Graph Module (DGM) for Graph Convolutional Networks", "year": 2020}], "gt_label": [1, 1, 1], "date": "2020-03", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_54", "valid": true}
{"query": "Which study proposed a method that works only on toy images of up to 333 objects on a black background?", "cited_paper": [{"arxiv_id": "2011.10287", "title": "Learning Object-Centric Video Models by Contrasting Sets", "year": 2020}], "gt_label": [1], "date": "2020-11", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_55", "valid": true}
{"query": "Any work about applying re-reading prompt to improve reasoning tasks of LLM?", "cited_paper": [{"arxiv_id": "2309.06275", "title": "Re-Reading Improves Reasoning in Large Language Models", "year": 2023}], "gt_label": [1], "date": "2023-09", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_56", "valid": true}
{"query": "What studies introduce the unsupervised disentanglement score called Distortion?", "cited_paper": [{"arxiv_id": "2205.13182", "title": "Analyzing the Latent Space of GAN through Local Dimension Estimation", "year": 2022}], "gt_label": [1], "date": "2022-05", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_57", "valid": true}
{"query": "Which research leveraged large language models like GPT-3 to learn a proxy reward function while avoiding the need for many expert demonstrations?", "cited_paper": [{"arxiv_id": "2005.14165", "title": "Language Models are Few-Shot Learners", "year": 2020}], "gt_label": [1], "date": "2020-05", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_58", "valid": true}
{"query": "What papers used a predefined set of names for enhancing cross-style transfer?", "cited_paper": [{"arxiv_id": "2202.12837", "title": "Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?", "year": 2022}, {"arxiv_id": "2201.08904", "title": "Description-Driven Task-Oriented Dialog Modeling", "year": 2022}], "gt_label": [1, 1], "date": "2023-03", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_59", "valid": true}
{"query": "Which studies have recently been working on the integration of visual perception and large language models?", "cited_paper": [{"arxiv_id": "1706.03762", "title": "Attention Is All You Need", "year": 2017}, {"arxiv_id": "2005.14165", "title": "Language Models are Few-Shot Learners", "year": 2020}, {"arxiv_id": "2303.08774", "title": "GPT-4 Technical Report", "year": 2023}, {"arxiv_id": "2302.13971", "title": "LLaMA: Open and Efficient Foundation Language Models", "year": 2023}, {"arxiv_id": "2307.09288", "title": "Llama 2: Open Foundation and Fine-Tuned Chat Models", "year": 2023}], "gt_label": [1, 1, 1, 1, 1], "date": "2023-07", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_60", "valid": true}
{"query": "What papers introduced the fast gradient sign method (FGSM) and the basic iterative method (BIM) for adversarial attacks?", "cited_paper": [{"arxiv_id": "1412.6572", "title": "Explaining and Harnessing Adversarial Examples", "year": 2014}, {"arxiv_id": "1607.02533", "title": "Adversarial examples in the physical world", "year": 2016}], "gt_label": [1, 1], "date": "2016-07", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_61", "valid": true}
{"query": "Any works talked about the use of meta-gradients to learn a combination of hyperparameters?", "cited_paper": [], "gt_label": [], "date": "2021-09", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_62", "valid": true}
{"query": "Are there any works that improve cost-effectiveness, performance, and data generation quality in the prompting framework of large language models?", "cited_paper": [{"arxiv_id": "2305.18323", "title": "ReWOO: Decoupling Reasoning from Observations for Efficient Augmented Language Models", "year": 2023}, {"arxiv_id": "2303.11366", "title": "Reflexion: Language Agents with Verbal Reinforcement Learning", "year": 2023}, {"arxiv_id": "2303.11381", "title": "MM-REACT: Prompting ChatGPT for Multimodal Reasoning and Action", "year": 2023}, {"arxiv_id": "2307.16789", "title": "ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs", "year": 2023}, {"arxiv_id": "2306.05301", "title": "ToolAlpaca: Generalized Tool Learning for Language Models with 3000 Simulated Cases", "year": 2023}], "gt_label": [1, 1, 1, 1, 1], "date": "2023-07", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_63", "valid": true}
{"query": "In which paper the term FPE was formalised for general function approximators?", "cited_paper": [{"arxiv_id": "1903.08738", "title": "Batch Policy Learning under Constraints", "year": 2019}], "gt_label": [1], "date": "2019-03", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_64", "valid": true}
{"query": "Which works focus on modelling the annotator distribution?", "cited_paper": [{"arxiv_id": "1906.04045", "title": "PHiSeg: Capturing Uncertainty in Medical Image Segmentation", "year": 2019}, {"arxiv_id": "1905.13077", "title": "A Hierarchical Probabilistic U-Net for Modeling Multi-Scale Ambiguities", "year": 2019}, {"arxiv_id": "2006.02683", "title": "Uncertainty quantification in medical image segmentation with normalizing flows", "year": 2020}], "gt_label": [1, 1, 1], "date": "2020-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_65", "valid": true}
{"query": "Which studies designed a siamese network framework using AlexNet for feature extraction in visual object tracking?", "cited_paper": [{"arxiv_id": "1606.09549", "title": "Fully-Convolutional Siamese Networks for Object Tracking", "year": 2016}], "gt_label": [1], "date": "2016-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_66", "valid": true}
{"query": "What graph analysis model is tested in the benchmark?", "cited_paper": [{"arxiv_id": "2006.05176", "title": "Explainable Classification of Brain Networks via Contrast Subgraphs", "year": 2020}], "gt_label": [1], "date": "2020-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_67", "valid": true}
{"query": "Any research focused on the memorization risks during the fine-tuning stage?", "cited_paper": [{"arxiv_id": "2205.12506", "title": "Memorization in NLP Fine-tuning Methods", "year": 2022}, {"arxiv_id": "2203.07618", "title": "Do Language Models Plagiarize?", "year": 2022}], "gt_label": [1, 1], "date": "2022-05", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_68", "valid": true}
{"query": "Could you provide me some studies about reducing the gradient misestimation by approximating discrete quantization with a differentiable function?", "cited_paper": [{"arxiv_id": "1908.05033", "title": "Differentiable Soft Quantization: Bridging Full-Precision and Low-Bit Neural Networks", "year": 2019}, {"arxiv_id": "1606.06160", "title": "DoReFa-Net: Training Low Bitwidth Convolutional Neural Networks with Low Bitwidth Gradients", "year": 2016}], "gt_label": [1, 1], "date": "2019-08", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_69", "valid": true}
{"query": "Could you provide me some works about optimizing batch processing for LLMs?", "cited_paper": [{"arxiv_id": "2301.08721", "title": "Batch Prompting: Efficient Inference with Large Language Model APIs", "year": 2023}, {"arxiv_id": "2010.05680", "title": "TurboTransformers: An Efficient GPU Serving System For Transformer Models", "year": 2020}], "gt_label": [1, 1], "date": "2023-01", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_70", "valid": true}
{"query": "Which study extended the capabilities of LLMs to the field of multi-modality?", "cited_paper": [], "gt_label": [], "date": "2023-12", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_71", "valid": true}
{"query": "What research has been done on finding optimal interventions using observational data?", "cited_paper": [{"arxiv_id": "2007.00973", "title": "Learning to search efficiently for causally near-optimal treatments", "year": 2020}], "gt_label": [1], "date": "2020-07", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_72", "valid": true}
{"query": "What papers are about prototypical adaptation methods?", "cited_paper": [{"arxiv_id": "2203.01452", "title": "Bending Reality: Distortion-aware Transformers for Adapting to Panoramic Semantic Segmentation", "year": 2022}, {"arxiv_id": "2207.11860", "title": "Behind Every Domain There is a Shift: Adapting Distortion-aware Vision Transformers for Panoramic Semantic Segmentation", "year": 2022}], "gt_label": [1, 1], "date": "2022-07", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_73", "valid": true}
{"query": "Could you name the works that applied CLIP for zero-shot AD, scoring the anomalies by comparing the alignment of test images with the correct text of normal samples?", "cited_paper": [{"arxiv_id": "2205.11474", "title": "Exposing Outlier Exposure: What Can Be Learned From Few, One, and Zero Outlier Images", "year": 2022}], "gt_label": [1], "date": "2022-05", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_74", "valid": true}
{"query": "What papers illustrate recent neural scene representations methods that try to optimize poses with differentiable rendering in Structure-from-Motion research?", "cited_paper": [{"arxiv_id": "2104.06405", "title": "BARF: Bundle-Adjusting Neural Radiance Fields", "year": 2021}, {"arxiv_id": "2108.13826", "title": "Self-Calibrating Neural Radiance Fields", "year": 2021}], "gt_label": [1, 1], "date": "2021-08", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_75", "valid": true}
{"query": "Could you provide me some works that investigate the interplay between weight loss landscape and adversarial robustness?", "cited_paper": [{"arxiv_id": "2004.05884", "title": "Adversarial Weight Perturbation Helps Robust Generalization", "year": 2020}, {"arxiv_id": "2203.06020", "title": "Enhancing Adversarial Training with Second-Order Statistics of Weights", "year": 2022}], "gt_label": [1, 1], "date": "2022-03", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_76", "valid": true}
{"query": "Which works employed a dynamic weighting transformer for integration in MMEA?", "cited_paper": [{"arxiv_id": "2212.14454", "title": "MEAformer: Multi-modal Entity Alignment Transformer for Meta Modality Hybrid", "year": 2022}], "gt_label": [1], "date": "2022-12", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_77", "valid": true}
{"query": "Which works have been conducted on memory methods for object navigation tasks?", "cited_paper": [{"arxiv_id": "2103.17138", "title": "SOON: Scenario Oriented Object Navigation with Graph-based Exploration", "year": 2021}], "gt_label": [1], "date": "2021-03", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_78", "valid": true}
{"query": "Which study presents the use of synthetic captions for training BLIP and BLIP2 models?", "cited_paper": [{"arxiv_id": "2201.12086", "title": "BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation", "year": 2022}, {"arxiv_id": "2301.12597", "title": "BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models", "year": 2023}], "gt_label": [1, 1], "date": "2023-01", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_79", "valid": true}
{"query": "Could you provide me some works about multi-agent debating frameworks?", "cited_paper": [{"arxiv_id": "2305.14325", "title": "Improving Factuality and Reasoning in Language Models through Multiagent Debate", "year": 2023}, {"arxiv_id": "2305.19118", "title": "Encouraging Divergent Thinking in Large Language Models through Multi-Agent Debate", "year": 2023}, {"arxiv_id": "2308.07201", "title": "ChatEval: Towards Better LLM-based Evaluators through Multi-Agent Debate", "year": 2023}], "gt_label": [1, 1, 1], "date": "2023-08", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_80", "valid": true}
{"query": "Which research provide examples of multimodal-conditional image synthesis systems?", "cited_paper": [{"arxiv_id": "2112.10752", "title": "High-Resolution Image Synthesis with Latent Diffusion Models", "year": 2021}, {"arxiv_id": "2211.01324", "title": "eDiff-I: Text-to-Image Diffusion Models with an Ensemble of Expert Denoisers", "year": 2022}, {"arxiv_id": "2302.05543", "title": "Adding Conditional Control to Text-to-Image Diffusion Models", "year": 2023}, {"arxiv_id": "2301.07093", "title": "GLIGEN: Open-Set Grounded Text-to-Image Generation", "year": 2023}], "gt_label": [1, 1, 1, 1], "date": "2023-02", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_81", "valid": true}
{"query": "Which studies showed successful results using group-level persona variables?", "cited_paper": [{"arxiv_id": "2202.02950", "title": "Jury Learning: Integrating Dissenting Voices into Machine Learning Models", "year": 2022}, {"arxiv_id": "2305.06626", "title": "When the Majority is Wrong: Modeling Annotator Disagreement for Subjective Tasks", "year": 2023}], "gt_label": [1, 1], "date": "2023-05", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_82", "valid": true}
{"query": "Could you provide me with works that discuss the problem of performance degradation when distilling larger LMs, especially when the student is of small scale?", "cited_paper": [{"arxiv_id": "1902.03393", "title": "Improved Knowledge Distillation via Teacher Assistant", "year": 2019}, {"arxiv_id": "1910.01348", "title": "On the Efficacy of Knowledge Distillation", "year": 2019}, {"arxiv_id": "2305.12129", "title": "Lifting the Curse of Capacity Gap in Distilling Language Models", "year": 2023}], "gt_label": [1, 1, 1], "date": "2023-05", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_83", "valid": true}
{"query": "Could you provide me some works about generative methods for transferable adversarial attacks?", "cited_paper": [{"arxiv_id": "1905.11736", "title": "Cross-Domain Transferability of Adversarial Perturbations", "year": 2019}, {"arxiv_id": "2103.14641", "title": "On Generating Transferable Targeted Perturbations", "year": 2021}], "gt_label": [1, 1], "date": "2021-03", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_84", "valid": true}
{"query": "What is the key work on Trust Region Policy Optimization?", "cited_paper": [{"arxiv_id": "1502.05477", "title": "Trust Region Policy Optimization", "year": 2015}], "gt_label": [1], "date": "2015-02", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_85", "valid": true}
{"query": "What works focus on spatial feature transformation for BEV feature generation?", "cited_paper": [{"arxiv_id": "2203.11089", "title": "PersFormer: 3D Lane Detection via Perspective Transformer and the OpenLane Benchmark", "year": 2022}], "gt_label": [1], "date": "2022-03", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_86", "valid": true}
{"query": "What work used a modified VQ-GAN for isolated word sign language video generation?", "cited_paper": [{"arxiv_id": "2103.06982", "title": "Continuous 3D Multi-Channel Sign Language Production via Progressive Transformers and Mixture Density Networks", "year": 2021}], "gt_label": [1], "date": "2021-03", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_87", "valid": true}
{"query": "What papers propose the use of FP8 for accelerated inference?", "cited_paper": [{"arxiv_id": "2208.09225", "title": "FP8 Quantization: The Power of the Exponent", "year": 2022}], "gt_label": [1], "date": "2022-08", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_88", "valid": true}
{"query": "Who analysed the NTK spectrum for shallow ReLU networks under the uniform and nonuniform distributions?", "cited_paper": [{"arxiv_id": "2003.04560", "title": "Frequency Bias in Neural Networks for Input of Non-Uniform Density", "year": 2020}], "gt_label": [1], "date": "2020-03", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_89", "valid": true}
{"query": "Which works explored the theoretical analysis of the NTK spectrum via random matrix theory?", "cited_paper": [{"arxiv_id": "2005.11879", "title": "Spectra of the Conjugate Kernel and Neural Tangent Kernel for linear-width neural networks", "year": 2020}], "gt_label": [1], "date": "2020-05", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_90", "valid": true}
{"query": "Any research work about directly predicting CNN classifier accuracy by deriving distribution distance features between training and test images with a linear regression model?", "cited_paper": [{"arxiv_id": "2007.02915", "title": "Are Labels Always Necessary for Classifier Accuracy Evaluation?", "year": 2020}, {"arxiv_id": "2106.05961", "title": "What Does Rotation Prediction Tell Us about Classifier Accuracy under Varying Testing Environments?", "year": 2021}], "gt_label": [1, 1], "date": "2021-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_91", "valid": true}
{"query": "What works feature insightful discussions on preconditioning?", "cited_paper": [{"arxiv_id": "2006.10732", "title": "When Does Preconditioning Help or Hurt Generalization?", "year": 2020}, {"arxiv_id": "2302.06504", "title": "Preconditioned Score-based Generative Models", "year": 2023}, {"arxiv_id": "1512.03385", "title": "Deep Residual Learning for Image Recognition", "year": 2015}], "gt_label": [1, 1, 1], "date": "2023-02", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_92", "valid": true}
{"query": "Which paper introduced Vector Quantized Variational Autoencoders (VQ-VAE)?", "cited_paper": [{"arxiv_id": "1711.00937", "title": "Neural Discrete Representation Learning", "year": 2017}], "gt_label": [1], "date": "2017-11", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_93", "valid": true}
{"query": "Which research introduced a graph generation method for query structure prediction in parsing?", "cited_paper": [{"arxiv_id": "2109.03614", "title": "Formal Query Building with Query Structure Prediction for Complex Question Answering over Knowledge Base", "year": 2021}], "gt_label": [1], "date": "2021-09", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_94", "valid": true}
{"query": "Could you provide some works about deep AD approaches that employ a self-supervised loss function to train the detector and score anomalies?", "cited_paper": [{"arxiv_id": "1805.10917", "title": "Deep Anomaly Detection Using Geometric Transformations", "year": 2018}, {"arxiv_id": "1906.12340", "title": "Using Self-Supervised Learning Can Improve Model Robustness and Uncertainty", "year": 2019}, {"arxiv_id": "2011.02578", "title": "Learning and Evaluating Representations for Deep One-class Classification", "year": 2020}, {"arxiv_id": "2005.02359", "title": "Classification-Based Anomaly Detection for General Data", "year": 2020}, {"arxiv_id": "2103.16440", "title": "Neural Transformation Learning for Deep Anomaly Detection Beyond Images", "year": 2021}, {"arxiv_id": "2202.03944", "title": "Detecting Anomalies within Time Series using Local Neural Transformations", "year": 2022}], "gt_label": [1, 1, 1, 1, 1, 1], "date": "2023-02", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_95", "valid": true}
{"query": "What studies have proposed methods to facilitate better model and AI service documentation?", "cited_paper": [{"arxiv_id": "1810.03993", "title": "Model Cards for Model Reporting", "year": 2018}], "gt_label": [1], "date": "2018-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_96", "valid": true}
{"query": "Which study offers a lightweight, subject-driven personalization for text-to-image diffusion models?", "cited_paper": [{"arxiv_id": "2307.06949", "title": "HyperDreamBooth: HyperNetworks for Fast Personalization of Text-to-Image Models", "year": 2023}], "gt_label": [1], "date": "2023-07", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_97", "valid": true}
{"query": "What works present operators of tensor decomposition composed of fast Fourier / trigonometric transforms?", "cited_paper": [{"arxiv_id": "1909.04801", "title": "Faster Johnson-Lindenstrauss Transforms via Kronecker Products", "year": 2019}], "gt_label": [1], "date": "2019-09", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_98", "valid": true}
{"query": "What paper describes the dataset MiniWoB++, where sequences of low-level UI commands describe multi-step tasks?", "cited_paper": [{"arxiv_id": "1802.08802", "title": "Reinforcement Learning on Web Interfaces Using Workflow-Guided Exploration", "year": 2018}], "gt_label": [1], "date": "2018-02", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_99", "valid": true}
{"query": "Which paper proposed improving the anomaly score for reconstruction-based techniques via constructing a score that integrates reconstruction error and discriminator loss?", "cited_paper": [{"arxiv_id": "2201.07284", "title": "TranAD: Deep Transformer Networks for Anomaly Detection in Multivariate Time Series Data", "year": 2022}], "gt_label": [1], "date": "2022-01", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_100", "valid": true}
{"query": "What work proposes to model speech based on HuBERT codes or semantic tokens?", "cited_paper": [{"arxiv_id": "2102.01192", "title": "Generative Spoken Language Modeling from Raw Audio", "year": 2021}, {"arxiv_id": "2104.00355", "title": "Speech Resynthesis from Discrete Disentangled Self-Supervised Representations", "year": 2021}], "gt_label": [1, 1], "date": "2021-04", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_101", "valid": true}
{"query": "What research works talk about using Inverse Propensity Score (IPS) and Self-Normalized IPS (SNIPS) methods to tackle selection bias on data?", "cited_paper": [{"arxiv_id": "1602.05352", "title": "Recommendations as Treatments: Debiasing Learning and Evaluation", "year": 2016}], "gt_label": [1], "date": "2016-02", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_102", "valid": true}
{"query": "What paper introduced the Segment Anything Model (SAM) which is a foundational model for image segmentation?", "cited_paper": [{"arxiv_id": "2401.14159", "title": "Grounded SAM: Assembling Open-World Models for Diverse Visual Tasks", "year": 2024}], "gt_label": [1], "date": "2024-01", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_103", "valid": true}
{"query": "Which publications contain the principal components of latent space obtained by performing PCA as global meaningful perturbations?", "cited_paper": [{"arxiv_id": "2004.02546", "title": "GANSpace: Discovering Interpretable GAN Controls", "year": 2020}], "gt_label": [1], "date": "2020-04", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_104", "valid": true}
{"query": "What studies introduced mask classification-based methods for instance-level segmentation?", "cited_paper": [{"arxiv_id": "2005.12872", "title": "End-to-End Object Detection with Transformers", "year": 2020}, {"arxiv_id": "2012.00759", "title": "MaX-DeepLab: End-to-End Panoptic Segmentation with Mask Transformers", "year": 2020}], "gt_label": [1, 1], "date": "2020-12", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_105", "valid": true}
{"query": "What studies extensively examined the MMD two-sample test?", "cited_paper": [{"arxiv_id": "1811.08357", "title": "Learning deep kernels for exponential family densities", "year": 2018}, {"arxiv_id": "1611.04488", "title": "Generative Models and Model Criticism via Optimized Maximum Mean Discrepancy", "year": 2016}, {"arxiv_id": "1605.06796", "title": "Interpretable Distribution Features with Maximum Testing Power", "year": 2016}], "gt_label": [1, 1, 1], "date": "2018-11", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_106", "valid": true}
{"query": "What works are related to 3D Gaussian Splatting for 3D reconstruction?", "cited_paper": [{"arxiv_id": "2308.04079", "title": "3D Gaussian Splatting for Real-Time Radiance Field Rendering", "year": 2023}, {"arxiv_id": "2308.09713", "title": "Dynamic 3D Gaussians: Tracking by Persistent Dynamic View Synthesis", "year": 2023}], "gt_label": [1, 1], "date": "2023-08", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_107", "valid": true}
{"query": "Can you specify the studies about using prompt and fine-tuning techniques for adapting VLMs to a new downstream task?", "cited_paper": [{"arxiv_id": "2109.01134", "title": "Learning to Prompt for Vision-Language Models", "year": 2021}, {"arxiv_id": "2110.04544", "title": "CLIP-Adapter: Better Vision-Language Models with Feature Adapters", "year": 2021}], "gt_label": [1, 1], "date": "2021-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_108", "valid": true}
{"query": "What works propose 2D-to-3D pose lifting task and regress the 3D keypoints based on a convolutional neural network from 2D keypoints?", "cited_paper": [{"arxiv_id": "1705.03098", "title": "A simple yet effective baseline for 3d human pose estimation", "year": 2017}], "gt_label": [1], "date": "2017-05", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_109", "valid": true}
{"query": "What researches have created evaluation data for individual tasks in Indic languages?", "cited_paper": [{"arxiv_id": "2311.17743", "title": "Mukhyansh: A Headline Generation Dataset for Indic Languages", "year": 2023}, {"arxiv_id": "2305.08828", "title": "PMIndiaSum: Multilingual and Cross-lingual Headline Summarization for Languages in India", "year": 2023}], "gt_label": [1, 1], "date": "2023-11", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_110", "valid": true}
{"query": "What are some classic methods for learning latent graphs?", "cited_paper": [{"arxiv_id": "1903.11960", "title": "Learning Discrete Structures for Graph Neural Networks", "year": 2019}, {"arxiv_id": "2006.13009", "title": "Iterative Deep Graph Learning for Graph Neural Networks: Better and Robust Node Embeddings", "year": 2020}, {"arxiv_id": "2005.10203", "title": "Graph Structure Learning for Robust Graph Neural Networks", "year": 2020}], "gt_label": [1, 1, 1], "date": "2020-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_111", "valid": true}
{"query": "Which paper introduced the method known as CoT prompting?", "cited_paper": [{"arxiv_id": "2201.11903", "title": "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models", "year": 2022}], "gt_label": [1], "date": "2022-01", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_112", "valid": true}
{"query": "Which works assumed Gaussian noise in RGB space for pixel-wise uncertainty in the context of NeRF?", "cited_paper": [{"arxiv_id": "2008.02268", "title": "NeRF in the Wild: Neural Radiance Fields for Unconstrained Photo Collections", "year": 2020}, {"arxiv_id": "2209.08546", "title": "ActiveNeRF: Learning where to See with Uncertainty Estimation", "year": 2022}], "gt_label": [1, 1], "date": "2022-09", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_113", "valid": true}
{"query": "What papers explored using diffusion models within the realm of reinforcement learning?", "cited_paper": [{"arxiv_id": "2205.09991", "title": "Planning with Diffusion for Flexible Behavior Synthesis", "year": 2022}], "gt_label": [1], "date": "2022-05", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_114", "valid": true}
{"query": "Could you provide me works that touch upon one-class classification approaches for video anomaly detection?", "cited_paper": [{"arxiv_id": "2004.07657", "title": "Old is Gold: Redefining the Adversarially Learned One-Class Classifier Training Paradigm", "year": 2020}, {"arxiv_id": "1802.09088", "title": "Adversarially Learned One-Class Classifier for Novelty Detection", "year": 2018}, {"arxiv_id": "1904.02639", "title": "Memorizing Normality to Detect Anomaly: Memory-augmented Deep Autoencoder for Unsupervised Anomaly Detection", "year": 2019}, {"arxiv_id": "1812.04960", "title": "Object-centric Auto-encoders and Dummy Anomalies for Abnormal Event Detection in Video", "year": 2018}], "gt_label": [1, 1, 1, 1], "date": "2020-04", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_115", "valid": true}
{"query": "Which studies have focused on the protein docking technique?", "cited_paper": [{"arxiv_id": "2111.07786", "title": "Independent SE(3)-Equivariant Models for End-to-End Rigid Protein Docking", "year": 2021}], "gt_label": [1], "date": "2021-11", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_116", "valid": true}
{"query": "What are some works in vision that stress the importance of data selection in supervised or semi-supervised setting?", "cited_paper": [{"arxiv_id": "2206.07137", "title": "Prioritized Training on Points that are Learnable, Worth Learning, and Not Yet Learnt", "year": 2022}, {"arxiv_id": "1901.01151", "title": "Learning From Less Data: A Unified Data Subset Selection and Active Learning Framework for Computer Vision", "year": 2019}, {"arxiv_id": "2012.10630", "title": "GLISTER: Generalization based Data Subset Selection for Efficient and Robust Learning", "year": 2020}, {"arxiv_id": "2103.00123", "title": "GRAD-MATCH: Gradient Matching based Data Subset Selection for Efficient Deep Model Training", "year": 2021}, {"arxiv_id": "1911.10088", "title": "Optimizing Data Usage via Differentiable Rewards", "year": 2019}, {"arxiv_id": "2107.07075", "title": "Deep Learning on a Data Diet: Finding Important Examples Early in Training", "year": 2021}, {"arxiv_id": "1906.01827", "title": "Coresets for Data-efficient Training of Machine Learning Models", "year": 2019}, {"arxiv_id": "1906.11829", "title": "Selection via Proxy: Efficient Data Selection for Deep Learning", "year": 2019}, {"arxiv_id": "1708.00489", "title": "Active Learning for Convolutional Neural Networks: A Core-Set Approach", "year": 2017}], "gt_label": [1, 1, 1, 1, 1, 1, 1, 1, 1], "date": "2022-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_117", "valid": true}
{"query": "What works adopted large language models (LLMs) for a cost-effective generation of Counterfactually Augmented Data (CAD)?", "cited_paper": [{"arxiv_id": "2101.00288", "title": "Polyjuice: Generating Counterfactuals for Explaining, Evaluating, and Improving Models", "year": 2021}, {"arxiv_id": "2012.04698", "title": "Generate Your Counterfactuals: Towards Controlled Counterfactual Generation for Text", "year": 2020}, {"arxiv_id": "2211.16202", "title": "AutoCAD: Automatically Generating Counterfactuals for Mitigating Shortcut Learning", "year": 2022}, {"arxiv_id": "2210.04873", "title": "CORE: A Retrieve-then-Edit Framework for Counterfactual Data Generation", "year": 2022}, {"arxiv_id": "2305.03495", "title": "Automatic Prompt Optimization with \"Gradient Descent\" and Beam Search", "year": 2023}, {"arxiv_id": "2212.10534", "title": "DISCO: Distilling Counterfactuals with Large Language Models", "year": 2022}], "gt_label": [1, 1, 1, 1, 1, 1], "date": "2023-05", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_118", "valid": true}
{"query": "Which studies demonstrated that vector arithmetic on latent space leads to the semantic arithmetic on the image space?", "cited_paper": [{"arxiv_id": "1511.06434", "title": "Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks", "year": 2015}, {"arxiv_id": "1611.05507", "title": "Deep Feature Interpolation for Image Content Changes", "year": 2016}], "gt_label": [1, 1], "date": "2016-11", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_119", "valid": true}
{"query": "Which research papers cover the challenges of covariate shift and causal confusion in behavior cloning?", "cited_paper": [{"arxiv_id": "1905.11979", "title": "Causal Confusion in Imitation Learning", "year": 2019}], "gt_label": [1], "date": "2019-05", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_120", "valid": true}
{"query": "What papers focus on the concept of equivariance in Convolutional Neural Networks (CNNs)?", "cited_paper": [{"arxiv_id": "1602.07576", "title": "Group Equivariant Convolutional Networks", "year": 2016}], "gt_label": [1], "date": "2016-02", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_121", "valid": true}
{"query": "What papers discuss the definitions and concepts of Pseudo-Global Stability in regards to Differential Privacy?", "cited_paper": [{"arxiv_id": "2110.11208", "title": "User-Level Private Learning via Correlated Sampling", "year": 2021}], "gt_label": [1], "date": "2021-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_122", "valid": true}
{"query": "Which works focus on predicting model generalization error?", "cited_paper": [{"arxiv_id": "2007.02915", "title": "Are Labels Always Necessary for Classifier Accuracy Evaluation?", "year": 2020}, {"arxiv_id": "2201.04234", "title": "Leveraging Unlabeled Data to Predict Out-of-Distribution Performance", "year": 2022}, {"arxiv_id": "2202.05834", "title": "Predicting Out-of-Distribution Error with the Projection Norm", "year": 2022}, {"arxiv_id": "2207.07065", "title": "On the Strong Correlation Between Model Invariance and Generalization", "year": 2022}, {"arxiv_id": "2107.03315", "title": "Predicting with Confidence on Unseen Distributions", "year": 2021}, {"arxiv_id": "2106.05961", "title": "What Does Rotation Prediction Tell Us about Classifier Accuracy under Varying Testing Environments?", "year": 2021}], "gt_label": [1, 1, 1, 1, 1, 1], "date": "2022-07", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_123", "valid": true}
{"query": "Which study proposed the Subformer model with shared middle layers and embedding factorization?", "cited_paper": [{"arxiv_id": "2101.00234", "title": "Subformer: Exploring Weight Sharing for Parameter Efficiency in Generative Transformers", "year": 2021}], "gt_label": [1], "date": "2021-01", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_124", "valid": true}
{"query": "What studies have proven Generation-Augmented Retrieval effective in question answering and passage retrieval?", "cited_paper": [{"arxiv_id": "2009.08553", "title": "Generation-Augmented Retrieval for Open-domain Question Answering", "year": 2020}, {"arxiv_id": "2212.10496", "title": "Precise Zero-Shot Dense Retrieval without Relevance Labels", "year": 2022}, {"arxiv_id": "2303.07678", "title": "Query2doc: Query Expansion with Large Language Models", "year": 2023}], "gt_label": [1, 1, 1], "date": "2023-03", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_125", "valid": true}
{"query": "What paper introduces the Mirror Descent Modified Policy Iteration (MD-MPI) framework?", "cited_paper": [{"arxiv_id": "1901.11275", "title": "A Theory of Regularized Markov Decision Processes", "year": 2019}], "gt_label": [1], "date": "2019-01", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_126", "valid": true}
{"query": "What papers provide linear convergence guarantees of NPG and PMD in softmax tabular policy settings by adding regularization?", "cited_paper": [{"arxiv_id": "2007.06558", "title": "Fast Global Convergence of Natural Policy Gradient Methods with Entropy Regularization", "year": 2020}, {"arxiv_id": "2105.11066", "title": "Policy Mirror Descent for Regularized Reinforcement Learning: A Generalized Framework with Linear Convergence", "year": 2021}, {"arxiv_id": "2102.00135", "title": "Policy Mirror Descent for Reinforcement Learning: Linear Convergence, New Sampling Complexity, and Generalized Problem Classes", "year": 2021}], "gt_label": [1, 1, 1], "date": "2022-01", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_127", "valid": true}
{"query": "Which papers established a basis of ARA using language models based on deep neural networks?", "cited_paper": [{"arxiv_id": "1907.11779", "title": "Supervised and Unsupervised Neural Approaches to Text Readability", "year": 2019}], "gt_label": [1], "date": "2019-07", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_128", "valid": true}
{"query": "Which works proposed a simple approach based on the statistics of the dataset for response length prediction?", "cited_paper": [{"arxiv_id": "1910.11555", "title": "Fast Structured Decoding for Sequence Models", "year": 2019}], "gt_label": [1], "date": "2019-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_129", "valid": true}
{"query": "Can you mention publications that implemented transformers in 2D-to-3D pose lifting?", "cited_paper": [{"arxiv_id": "2103.10455", "title": "3D Human Pose Estimation with Spatial and Temporal Transformers", "year": 2021}, {"arxiv_id": "2203.00859", "title": "MixSTE: Seq2seq Mixed Spatio-Temporal Encoder for 3D Human Pose Estimation in Video", "year": 2022}, {"arxiv_id": "2303.17472", "title": "PoseFormerV2: Exploring Frequency Domain for Efficient and Robust 3D Human Pose Estimation", "year": 2023}], "gt_label": [1, 1, 1], "date": "2023-03", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_130", "valid": true}
{"query": "Can you provide an example of a paper that presents self-tuning algorithms?", "cited_paper": [{"arxiv_id": "2002.12928", "title": "A Self-Tuning Actor-Critic Algorithm", "year": 2020}], "gt_label": [1], "date": "2020-02", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_131", "valid": true}
{"query": "Which papers talk about using LLMs to interpret themselves or other ML models by providing numerical importances for their inputs?", "cited_paper": [{"arxiv_id": "2310.05797", "title": "In-Context Explainers: Harnessing LLMs for Explaining Black Box Models", "year": 2023}], "gt_label": [1], "date": "2023-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_132", "valid": true}
{"query": "Could you provide me studies about knowledge distillation methods for semantic segmentation focusing on preserving structural semantic relations?", "cited_paper": [{"arxiv_id": "2204.06986", "title": "Cross-Image Relational Knowledge Distillation for Semantic Segmentation", "year": 2022}], "gt_label": [1], "date": "2022-04", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_133", "valid": true}
{"query": "What works have focused on evaluating API-use scenarios based on a separate LLM to assess the quality of examples?", "cited_paper": [{"arxiv_id": "2306.05301", "title": "ToolAlpaca: Generalized Tool Learning for Language Models with 3000 Simulated Cases", "year": 2023}, {"arxiv_id": "2307.16789", "title": "ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs", "year": 2023}], "gt_label": [1, 1], "date": "2023-07", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_134", "valid": true}
{"query": "Any works about user-annotations based image animation?", "cited_paper": [{"arxiv_id": "2107.02790", "title": "iPOKE: Poking a Still Image for Controlled Stochastic Video Synthesis", "year": 2021}, {"arxiv_id": "2002.09219", "title": "Stochastic Latent Residual Video Prediction", "year": 2020}, {"arxiv_id": "2308.08089", "title": "DragNUWA: Fine-grained Control in Video Generation by Integrating Text, Image, and Trajectory", "year": 2023}, {"arxiv_id": "2306.02018", "title": "VideoComposer: Compositional Video Synthesis with Motion Controllability", "year": 2023}, {"arxiv_id": "2305.13077", "title": "ControlVideo: Training-free Controllable Text-to-Video Generation", "year": 2023}, {"arxiv_id": "2304.14404", "title": "Motion-Conditioned Diffusion Model for Controllable Video Synthesis", "year": 2023}], "gt_label": [1, 1, 1, 1, 1, 1], "date": "2023-08", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_135", "valid": true}
{"query": "What works conducted studies on LegalBERT and CaseLaw-BERT that focused on the legal domain?", "cited_paper": [{"arxiv_id": "2010.02559", "title": "LEGAL-BERT: The Muppets straight out of Law School", "year": 2020}, {"arxiv_id": "2104.08671", "title": "When Does Pretraining Help? Assessing Self-Supervised Learning for Law and the CaseHOLD Dataset", "year": 2021}], "gt_label": [1, 1], "date": "2021-04", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_136", "valid": true}
{"query": "Are there any works that created datasets and systems for tasks related to information time-tracking?", "cited_paper": [{"arxiv_id": "1606.05699", "title": "Socially-Informed Timeline Generation for Complex Events", "year": 2016}, {"arxiv_id": "2005.10107", "title": "Examining the State-of-the-Art in News Timeline Summarization", "year": 2020}], "gt_label": [1, 1], "date": "2020-05", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_137", "valid": true}
{"query": "Could you provide me studies about anomaly detection in federated learning particularly related to network security?", "cited_paper": [{"arxiv_id": "2005.05752", "title": "A Secure Federated Learning Framework for 5G Networks", "year": 2020}], "gt_label": [1], "date": "2020-05", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_138", "valid": true}
{"query": "What research focused on 3D instance segmentation with only 3D box annotation requirements?", "cited_paper": [{"arxiv_id": "2206.01203", "title": "Box2Mask: Weakly Supervised 3D Semantic Instance Segmentation Using Bounding Boxes", "year": 2022}], "gt_label": [1], "date": "2022-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_139", "valid": true}
{"query": "Are there any papers that investigate the need for an initially annotated fraction of data to bootstrap an active learning method?", "cited_paper": [{"arxiv_id": "2210.02442", "title": "Making Your First Choice: To Address Cold Start Problem in Vision Active Learning", "year": 2022}, {"arxiv_id": "2010.09535", "title": "Cold-start Active Learning through Self-supervised Language Modeling", "year": 2020}, {"arxiv_id": "1805.09023", "title": "Addressing the Item Cold-start Problem by Attribute-driven Active Learning", "year": 2018}], "gt_label": [1, 1, 1], "date": "2023-04", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_140", "valid": true}
{"query": "Can you provide works that evaluate RL agents by changing the initial states in the same environment?", "cited_paper": [{"arxiv_id": "1910.07224", "title": "Teacher algorithms for curriculum learning of Deep RL in continuously parameterized environments", "year": 2019}, {"arxiv_id": "1710.06537", "title": "Sim-to-Real Transfer of Robotic Control with Dynamics Randomization", "year": 2017}, {"arxiv_id": "1802.07239", "title": "Continual Reinforcement Learning with Complex Synapses", "year": 2018}], "gt_label": [1, 1, 1], "date": "2019-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_141", "valid": true}
{"query": "Which research paper introduced the DenseCL method that applies contrastive learning on patches with highest similarity?", "cited_paper": [{"arxiv_id": "2011.09157", "title": "Dense Contrastive Learning for Self-Supervised Visual Pre-Training", "year": 2020}], "gt_label": [1], "date": "2020-11", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_142", "valid": true}
{"query": "Which work outlines a way to calculate adversarial perturbations during training by first randomly perturbing the initial point then applying a single step of projected gradient descent?", "cited_paper": [{"arxiv_id": "2001.03994", "title": "Fast is better than free: Revisiting adversarial training", "year": 2020}], "gt_label": [1], "date": "2020-01", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_143", "valid": true}
{"query": "Could you provide some works that discuss multimodal prompting methods?", "cited_paper": [{"arxiv_id": "2205.11916", "title": "Large Language Models are Zero-Shot Reasoners", "year": 2022}, {"arxiv_id": "2305.04091", "title": "Plan-and-Solve Prompting: Improving Zero-Shot Chain-of-Thought Reasoning by Large Language Models", "year": 2023}, {"arxiv_id": "2305.14106", "title": "Better Zero-Shot Reasoning with Self-Adaptive Prompting", "year": 2023}, {"arxiv_id": "2005.14165", "title": "Language Models are Few-Shot Learners", "year": 2020}, {"arxiv_id": "2202.12837", "title": "Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?", "year": 2022}, {"arxiv_id": "2301.00234", "title": "A Survey on In-context Learning", "year": 2023}, {"arxiv_id": "2303.13217", "title": "Fairness-guided Few-shot Prompting for Large Language Models", "year": 2023}, {"arxiv_id": "2305.14688", "title": "ExpertPrompting: Instructing Large Language Models to be Distinguished Experts", "year": 2023}, {"arxiv_id": "2201.11903", "title": "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models", "year": 2022}, {"arxiv_id": "2210.03493", "title": "Automatic Chain of Thought Prompting in Large Language Models", "year": 2022}, {"arxiv_id": "2203.11171", "title": "Self-Consistency Improves Chain of Thought Reasoning in Language Models", "year": 2022}, {"arxiv_id": "2305.10601", "title": "Tree of Thoughts: Deliberate Problem Solving with Large Language Models", "year": 2023}, {"arxiv_id": "2308.09687", "title": "Graph of Thoughts: Solving Elaborate Problems with Large Language Models", "year": 2023}, {"arxiv_id": "2305.16582", "title": "Beyond Chain-of-Thought, Effective Graph-of-Thought Reasoning in Language Models", "year": 2023}, {"arxiv_id": "2308.08614", "title": "Boosting Logical Reasoning in Large Language Models through a New Framework: The Graph of Thought", "year": 2023}], "gt_label": [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], "date": "2023-08", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_144", "valid": true}
{"query": "Which work proposes reweighting different activation channels based on the global context of the input samples?", "cited_paper": [{"arxiv_id": "1709.01507", "title": "Squeeze-and-Excitation Networks", "year": 2017}], "gt_label": [1], "date": "2017-09", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_145", "valid": true}
{"query": "What works focused on MAML and its variants?", "cited_paper": [{"arxiv_id": "1703.03400", "title": "Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks", "year": 2017}, {"arxiv_id": "2206.03996", "title": "Sharp-MAML: Sharpness-Aware Model-Agnostic Meta Learning", "year": 2022}, {"arxiv_id": "1909.09157", "title": "Rapid Learning or Feature Reuse? Towards Understanding the Effectiveness of MAML", "year": 2019}, {"arxiv_id": "1905.07435", "title": "Alpha MAML: Adaptive Model-Agnostic Meta-Learning", "year": 2019}, {"arxiv_id": "1909.04630", "title": "Meta-Learning with Implicit Gradients", "year": 2019}], "gt_label": [1, 1, 1, 1, 1], "date": "2022-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_146", "valid": true}
{"query": "What research papers have addressed issues of equivariance by customizing kernel designs in the context of kernel methods and Gaussian processes (GPs)?", "cited_paper": [{"arxiv_id": "1809.11165", "title": "GPyTorch: Blackbox Matrix-Matrix Gaussian Process Inference with GPU Acceleration", "year": 2018}, {"arxiv_id": "1511.02222", "title": "Deep Kernel Learning", "year": 2015}, {"arxiv_id": "2106.08185", "title": "Kernel Identification Through Transformers", "year": 2021}], "gt_label": [1, 1, 1], "date": "2021-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_147", "valid": true}
{"query": "What studies have leveraged extensive image-text pair datasets to broaden the detection vocabulary in Open-vocabulary detection?", "cited_paper": [{"arxiv_id": "2011.10678", "title": "Open-Vocabulary Object Detection Using Captions", "year": 2020}, {"arxiv_id": "2112.09106", "title": "RegionCLIP: Region-based Language-Image Pretraining", "year": 2021}, {"arxiv_id": "2203.16513", "title": "PromptDet: Towards Open-vocabulary Detection using Uncurated Images", "year": 2022}, {"arxiv_id": "2112.03857", "title": "Grounded Language-Image Pre-training", "year": 2021}, {"arxiv_id": "2211.14843", "title": "Learning Object-Language Alignments for Open-Vocabulary Object Detection", "year": 2022}, {"arxiv_id": "2304.04514", "title": "DetCLIPv2: Scalable Open-Vocabulary Object Detection Pre-training via Word-Region Alignment", "year": 2023}], "gt_label": [1, 1, 1, 1, 1, 1], "date": "2023-04", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_148", "valid": true}
{"query": "Could you mention research that addresses MMS approximations for suodular and subadditive valuations?", "cited_paper": [{"arxiv_id": "1703.01851", "title": "Approximation Algorithms for Maximin Fair Division", "year": 2017}], "gt_label": [1], "date": "2017-03", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_149", "valid": true}
{"query": "Which works can you provide that are focused on creating evaluation data on Indic languages?", "cited_paper": [{"arxiv_id": "2212.05409", "title": "Towards Leaving No Indic Language Behind: Building Monolingual Corpora, Benchmark and Models for Indic Languages", "year": 2022}, {"arxiv_id": "2212.10168", "title": "Naamapadam: A Large-Scale Named Entity Annotated Data for Indic Languages", "year": 2022}, {"arxiv_id": "2204.08582", "title": "MASSIVE: A 1M-Example Multilingual Natural Language Understanding Dataset with 51 Typologically-Diverse Languages", "year": 2022}, {"arxiv_id": "2004.12376", "title": "GLUECoS : An Evaluation Benchmark for Code-Switched NLP", "year": 2020}, {"arxiv_id": "2308.16884", "title": "The Belebele Benchmark: a Parallel Reading Comprehension Dataset in 122 Language Variants", "year": 2023}], "gt_label": [1, 1, 1, 1, 1], "date": "2023-08", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_150", "valid": true}
{"query": "What work highlights the disconnect between benchmark results and real world impacts in NLP?", "cited_paper": [], "gt_label": [], "date": "2012-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_151", "valid": true}
{"query": "What work applied CLIP to dense prediction tasks?", "cited_paper": [{"arxiv_id": "2112.01518", "title": "DenseCLIP: Language-Guided Dense Prediction with Context-Aware Prompting", "year": 2021}], "gt_label": [1], "date": "2021-12", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_152", "valid": true}
{"query": "Which papers have proposed for extracting the specific style from reference images?", "cited_paper": [{"arxiv_id": "2309.01770", "title": "StyleAdapter: A Unified Stylized Image Generation Model", "year": 2023}, {"arxiv_id": "2205.09542", "title": "Domain Enhanced Arbitrary Image Style Transfer via Contrastive Learning", "year": 2022}, {"arxiv_id": "2308.07863", "title": "StyleDiffusion: Controllable Disentangled Style Transfer via Diffusion Models", "year": 2023}, {"arxiv_id": "2211.13203", "title": "Inversion-Based Style Transfer with Diffusion Models", "year": 2022}, {"arxiv_id": "2306.00983", "title": "StyleDrop: Text-to-Image Generation in Any Style", "year": 2023}], "gt_label": [1, 1, 1, 1, 1], "date": "2023-09", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_153", "valid": true}
{"query": "What papers studied the construction of diffusion models for discrete categorical data using a categorical noise process?", "cited_paper": [{"arxiv_id": "2102.05379", "title": "Argmax Flows and Multinomial Diffusion: Learning Categorical Distributions", "year": 2021}], "gt_label": [1], "date": "2021-07", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_154", "valid": true}
{"query": "Which works combine external knowledge from KGs into LLMs during the prompting stage?", "cited_paper": [{"arxiv_id": "2308.13259", "title": "Knowledge-Driven CoT: Exploring Faithful Reasoning in LLMs for Knowledge-intensive Question Answering", "year": 2023}, {"arxiv_id": "2308.09729", "title": "MindMap: Knowledge Graph Prompting Sparks Graph of Thoughts in Large Language Models", "year": 2023}, {"arxiv_id": "2310.01061", "title": "Reasoning on Graphs: Faithful and Interpretable Large Language Model Reasoning", "year": 2023}, {"arxiv_id": "2307.07697", "title": "Think-on-Graph: Deep and Responsible Reasoning of Large Language Model on Knowledge Graph", "year": 2023}], "gt_label": [1, 1, 1, 1], "date": "2023-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_155", "valid": true}
{"query": "What works have concentrated on generating free-form natural language explanations (NLEs) for justifying model predictions?", "cited_paper": [{"arxiv_id": "1812.01193", "title": "e-SNLI: Natural Language Inference with Natural Language Explanations", "year": 2018}, {"arxiv_id": "2004.14546", "title": "WT5?! Training Text-to-Text Models to Explain their Predictions", "year": 2020}, {"arxiv_id": "2102.12060", "title": "Teach Me to Explain: A Review of Datasets for Explainable Natural Language Processing", "year": 2021}], "gt_label": [1, 1, 1], "date": "2021-02", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_156", "valid": true}
{"query": "What work proposes the intrinsic local dimension estimation scheme for the latent manifold through the robust rank estimate?", "cited_paper": [{"arxiv_id": "2205.13182", "title": "Analyzing the Latent Space of GAN through Local Dimension Estimation", "year": 2022}], "gt_label": [1], "date": "2022-05", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_157", "valid": true}
{"query": "Which paper first used the term hypercolumn in the context of neural network features?", "cited_paper": [{"arxiv_id": "1411.5752", "title": "Hypercolumns for Object Segmentation and Fine-grained Localization", "year": 2014}], "gt_label": [1], "date": "2014-11", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_158", "valid": true}
{"query": "Could you provide me some studies that used NeRFs for novel view synthesis and 3D scene reconstruction?", "cited_paper": [{"arxiv_id": "2003.08934", "title": "NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis", "year": 2020}, {"arxiv_id": "2111.12077", "title": "Mip-NeRF 360: Unbounded Anti-Aliased Neural Radiance Fields", "year": 2021}, {"arxiv_id": "2201.05989", "title": "Instant Neural Graphics Primitives with a Multiresolution Hash Encoding", "year": 2022}], "gt_label": [1, 1, 1], "date": "2022-01", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_159", "valid": true}
{"query": "Could you provide me some works that incorporate ideas from multicalibration to improve conditional coverage?", "cited_paper": [{"arxiv_id": "2206.01067", "title": "Practical Adversarial Multivalid Conformal Prediction", "year": 2022}], "gt_label": [1], "date": "2022-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_160", "valid": true}
{"query": "Which papers have achieved progress in the field of graph contrastive learning?", "cited_paper": [{"arxiv_id": "2010.13902", "title": "Graph Contrastive Learning with Augmentations", "year": 2020}, {"arxiv_id": "2106.05819", "title": "Adversarial Graph Augmentation to Improve Graph Contrastive Learning", "year": 2021}, {"arxiv_id": "2106.07594", "title": "Graph Contrastive Learning Automated", "year": 2021}, {"arxiv_id": "2202.06491", "title": "Adversarial Graph Contrastive Learning with Information Regularization", "year": 2022}, {"arxiv_id": "2201.01702", "title": "Bringing Your Own View: Graph Contrastive Learning without Prefabricated Data Augmentations", "year": 2022}], "gt_label": [1, 1, 1, 1, 1], "date": "2022-02", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_161", "valid": true}
{"query": "Can you provide existing research that used contrastive methods in representation learning?", "cited_paper": [{"arxiv_id": "1809.10341", "title": "Deep Graph Infomax", "year": 2018}], "gt_label": [1], "date": "2018-09", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_162", "valid": true}
{"query": "What works introduced methodologies involving the learning of kernels to detect illuminant chromaticity?", "cited_paper": [{"arxiv_id": "1507.00410", "title": "Convolutional Color Constancy", "year": 2015}, {"arxiv_id": "1611.07596", "title": "Fast Fourier Color Constancy", "year": 2016}], "gt_label": [1, 1], "date": "2016-11", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_163", "valid": true}
{"query": "Could you tell me what studies propose to bridge vision and language modalities through visual prompt generators?", "cited_paper": [{"arxiv_id": "2204.14198", "title": "Flamingo: a Visual Language Model for Few-Shot Learning", "year": 2022}, {"arxiv_id": "2301.12597", "title": "BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models", "year": 2023}, {"arxiv_id": "2305.06500", "title": "InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning", "year": 2023}, {"arxiv_id": "2304.08485", "title": "Visual Instruction Tuning", "year": 2023}, {"arxiv_id": "2302.14045", "title": "Language Is Not All You Need: Aligning Perception with Language Models", "year": 2023}], "gt_label": [1, 1, 1, 1, 1], "date": "2023-05", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_164", "valid": true}
{"query": "Could you provide me some studies about sign language translation and production using RWTH-Phoenix dataset?", "cited_paper": [{"arxiv_id": "2003.13830", "title": "Sign Language Transformers: Joint End-to-end Sign Language Recognition and Translation", "year": 2020}, {"arxiv_id": "2004.14874", "title": "Progressive Transformers for End-to-End Sign Language Production", "year": 2020}], "gt_label": [1, 1], "date": "2020-04", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_165", "valid": true}
{"query": "Which work talked about ODIN, a method where two networks jointly optimize the segmentation masks and representation of objects?", "cited_paper": [{"arxiv_id": "2203.08777", "title": "Object discovery and representation networks", "year": 2022}], "gt_label": [1], "date": "2022-03", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_166", "valid": true}
{"query": "Which works proposed modifications for discrete-time models, such as recurrent neural networks to handle irregular time series data?", "cited_paper": [{"arxiv_id": "1409.1259", "title": "On the Properties of Neural Machine Translation: Encoder-Decoder Approaches", "year": 2014}], "gt_label": [1], "date": "2014-09", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_167", "valid": true}
{"query": "Could you mention a study that generates diverse reasoning paths using CoT?", "cited_paper": [{"arxiv_id": "2203.11171", "title": "Self-Consistency Improves Chain of Thought Reasoning in Language Models", "year": 2022}], "gt_label": [1], "date": "2022-03", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_168", "valid": true}
{"query": "Could you provide me studies where the solution of the reverse-time SDE is theoretically derived?", "cited_paper": [{"arxiv_id": "2011.13456", "title": "Score-Based Generative Modeling through Stochastic Differential Equations", "year": 2020}], "gt_label": [1], "date": "2020-11", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_169", "valid": true}
{"query": "Could you provide me some researches about extensions for population-based training?", "cited_paper": [{"arxiv_id": "1711.09846", "title": "Population Based Training of Neural Networks", "year": 2017}, {"arxiv_id": "1902.01894", "title": "A Generalized Framework for Population Based Training", "year": 2019}], "gt_label": [1, 1], "date": "2019-02", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_170", "valid": true}
{"query": "Any studies that focused on the problem of proving structured queries to knowledge base?", "cited_paper": [{"arxiv_id": "1705.11040", "title": "End-to-End Differentiable Proving", "year": 2017}, {"arxiv_id": "1912.10824", "title": "Differentiable Reasoning on Large Knowledge Bases and Natural Language", "year": 2019}], "gt_label": [1, 1], "date": "2019-12", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_171", "valid": true}
{"query": "Which paper improved the scene text editing performance by incorporating stroke-level information?", "cited_paper": [{"arxiv_id": "2212.01982", "title": "Exploring Stroke-Level Modifications for Scene Text Editing", "year": 2022}], "gt_label": [1], "date": "2022-12", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_172", "valid": true}
{"query": "Which works implement the conceptual design of complex-valued activations for object binding in synchrony-based models?", "cited_paper": [{"arxiv_id": "1312.6115", "title": "Neuronal Synchrony in Complex-Valued Deep Networks", "year": 2013}, {"arxiv_id": "2204.02075", "title": "Complex-Valued Autoencoders for Object Discovery", "year": 2022}], "gt_label": [1, 1], "date": "2022-04", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_173", "valid": true}
{"query": "Which works have implemented different kernels at various regions of the DP pair for single-task based deblurring?", "cited_paper": [{"arxiv_id": "2005.00305", "title": "Defocus Deblurring Using Dual-Pixel Data", "year": 2020}, {"arxiv_id": "2012.03255", "title": "Learning to Reduce Defocus Blur by Realistically Modeling Dual-Pixel Data", "year": 2020}], "gt_label": [1, 1], "date": "2021-08", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_174", "valid": true}
{"query": "Where does the ControlNet, a finetuning model for synthetic face generation, discussed?", "cited_paper": [{"arxiv_id": "2302.05543", "title": "Adding Conditional Control to Text-to-Image Diffusion Models", "year": 2023}], "gt_label": [1], "date": "2023-02", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_175", "valid": true}
{"query": "Which works considered the physical world and social norms for affordance inference?", "cited_paper": [{"arxiv_id": "1712.07576", "title": "Learning to Act Properly: Predicting and Explaining Affordances from Images", "year": 2017}], "gt_label": [1], "date": "2017-12", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_176", "valid": true}
{"query": "Can you mention some studies that have explored quantization as a means of compressing the parameters of LLMs for efficient inference?", "cited_paper": [{"arxiv_id": "2208.07339", "title": "LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale", "year": 2022}], "gt_label": [1], "date": "2022-08", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_177", "valid": true}
{"query": "Could you provide me with some works on memorization study that involve finding training dataset documents related to the output?", "cited_paper": [{"arxiv_id": "2203.07618", "title": "Do Language Models Plagiarize?", "year": 2022}, {"arxiv_id": "2211.08411", "title": "Large Language Models Struggle to Learn Long-Tail Knowledge", "year": 2022}, {"arxiv_id": "2203.08242", "title": "Data Contamination: From Memorization to Exploitation", "year": 2022}], "gt_label": [1, 1, 1], "date": "2022-11", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_178", "valid": true}
{"query": "Which references provided a dataset facilitating entity embedding through linked images?", "cited_paper": [{"arxiv_id": "1903.05485", "title": "MMKG: Multi-Modal Knowledge Graphs", "year": 2019}], "gt_label": [1], "date": "2019-03", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_179", "valid": true}
{"query": "What works have studied sampling for regret minimization in the framework of Unsupervised Environment Design?", "cited_paper": [{"arxiv_id": "2012.02096", "title": "Emergent Complexity and Zero-shot Transfer via Unsupervised Environment Design", "year": 2020}, {"arxiv_id": "2110.02439", "title": "Replay-Guided Adversarial Environment Design", "year": 2021}], "gt_label": [1, 1], "date": "2021-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_180", "valid": true}
{"query": "Which works used middle-layer LLM outputs and inserted them into the VPG to identify differences between images?", "cited_paper": [{"arxiv_id": "2308.04152", "title": "Fine-tuning Multimodal LLMs to Follow Zero-shot Demonstrative Instructions", "year": 2023}], "gt_label": [1], "date": "2023-08", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_181", "valid": true}
{"query": "Which works achieve high identity preservation in personalized generation through massive datasets and expensive hardware resources?", "cited_paper": [{"arxiv_id": "2306.06638", "title": "Face0: Instantaneously Conditioning a Text-to-Image Model on a Face", "year": 2023}, {"arxiv_id": "2307.11410", "title": "Subject-Diffusion:Open Domain Personalized Text-to-Image Generation without Test-time Fine-tuning", "year": 2023}], "gt_label": [1, 1], "date": "2023-07", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_182", "valid": true}
{"query": "Are there any studies about efficient selection of source tasks in NLP?", "cited_paper": [{"arxiv_id": "2111.04130", "title": "NLP From Scratch Without Large-Scale Pretraining: A Simple and Efficient Framework", "year": 2021}, {"arxiv_id": "2210.06277", "title": "Task Compass: Scaling Multi-task Pre-training with Task Prefix", "year": 2022}], "gt_label": [1, 1], "date": "2022-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_183", "valid": true}
{"query": "What works discuss transformer-based methods for 3D instance segmentation?", "cited_paper": [{"arxiv_id": "2210.03105", "title": "Mask3D: Mask Transformer for 3D Semantic Instance Segmentation", "year": 2022}, {"arxiv_id": "2211.15766", "title": "Superpoint Transformer for 3D Scene Instance Segmentation", "year": 2022}], "gt_label": [1, 1], "date": "2022-11", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_184", "valid": true}
{"query": "Which work explores model calibration for Uncertainty Estimation in the context of multiple-choice question answering?", "cited_paper": [{"arxiv_id": "2012.00955", "title": "How Can We Know When Language Models Know? On the Calibration of Language Models for Question Answering", "year": 2020}], "gt_label": [1], "date": "2020-12", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_185", "valid": true}
{"query": "Could you provide me some studies that explored policy optimization via backpropagating through the dynamics model?", "cited_paper": [{"arxiv_id": "2005.08068", "title": "Model-Augmented Actor-Critic: Backpropagating through Paths", "year": 2020}], "gt_label": [1], "date": "2020-05", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_186", "valid": true}
{"query": "Which study proposed a complete graph kernel based on homomorphism counts in the context of graph learning tasks?", "cited_paper": [{"arxiv_id": "1802.08876", "title": "Lovsz Meets Weisfeiler and Leman", "year": 2018}], "gt_label": [1], "date": "2018-02", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_187", "valid": true}
{"query": "Which study developed a Jacobi diffusion process for discrete data diffusion models?", "cited_paper": [{"arxiv_id": "2305.10699", "title": "Dirichlet Diffusion Score Model for Biological Sequence Generation", "year": 2023}], "gt_label": [1], "date": "2023-05", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_188", "valid": true}
{"query": "What research introduced DPMs and linked the generative model to a denoising diffusion model?", "cited_paper": [{"arxiv_id": "1312.6114", "title": "Auto-Encoding Variational Bayes", "year": 2013}, {"arxiv_id": "1406.2661", "title": "Generative Adversarial Networks", "year": 2014}, {"arxiv_id": "2102.10303", "title": "Towards Building A Group-based Unsupervised Representation Disentanglement Framework", "year": 2021}, {"arxiv_id": "2102.10543", "title": "Learning Disentangled Representation by Exploiting Pretrained Generative Models: A Contrastive Learning View", "year": 2021}, {"arxiv_id": "1503.03585", "title": "Deep Unsupervised Learning using Nonequilibrium Thermodynamics", "year": 2015}, {"arxiv_id": "2006.11239", "title": "Denoising Diffusion Probabilistic Models", "year": 2020}, {"arxiv_id": "2102.09672", "title": "Improved Denoising Diffusion Probabilistic Models", "year": 2021}, {"arxiv_id": "2010.02502", "title": "Denoising Diffusion Implicit Models", "year": 2020}, {"arxiv_id": "2202.05830", "title": "Learning Fast Samplers for Diffusion Models by Differentiating Through Sample Quality", "year": 2022}, {"arxiv_id": "2105.14080", "title": "Gotta Go Fast When Generating Data with Score-Based Models", "year": 2021}, {"arxiv_id": "2201.06503", "title": "Analytic-DPM: an Analytic Estimate of the Optimal Reverse Variance in Diffusion Probabilistic Models", "year": 2022}, {"arxiv_id": "2106.05931", "title": "Score-based Generative Modeling in Latent Space", "year": 2021}], "gt_label": [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], "date": "2022-02", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_189", "valid": true}
{"query": "Which studies focused on structured neural networks embedding principles like equivariance, Euclidean symmetry and periodicity?", "cited_paper": [{"arxiv_id": "1602.07576", "title": "Group Equivariant Convolutional Networks", "year": 2016}, {"arxiv_id": "2112.05124", "title": "Neural Descriptor Fields: SE(3)-Equivariant Object Representations for Manipulation", "year": 2021}, {"arxiv_id": "2110.10510", "title": "Periodic DMP formulation for Quaternion Trajectories", "year": 2021}], "gt_label": [1, 1, 1], "date": "2021-12", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_190", "valid": true}
{"query": "Which study proposed the use of memorization as a metric in CoreSet selection?", "cited_paper": [{"arxiv_id": "2008.03703", "title": "What Neural Networks Memorize and Why: Discovering the Long Tail via Influence Estimation", "year": 2020}], "gt_label": [1], "date": "2020-08", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_191", "valid": true}
{"query": "Which papers discussed Parameter-efficient fine-tuning (PEFT) methods for reducing storage requirements in large-scale PLMs?", "cited_paper": [{"arxiv_id": "1902.00751", "title": "Parameter-Efficient Transfer Learning for NLP", "year": 2019}, {"arxiv_id": "2104.08691", "title": "The Power of Scale for Parameter-Efficient Prompt Tuning", "year": 2021}, {"arxiv_id": "2101.00190", "title": "Prefix-Tuning: Optimizing Continuous Prompts for Generation", "year": 2021}], "gt_label": [1, 1, 1], "date": "2021-04", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_192", "valid": true}
{"query": "What studies compare using a large web corpus versus Wikipedia?", "cited_paper": [{"arxiv_id": "1903.07785", "title": "Cloze-driven Pretraining of Self-attention Networks", "year": 2019}, {"arxiv_id": "1906.08237", "title": "XLNet: Generalized Autoregressive Pretraining for Language Understanding", "year": 2019}], "gt_label": [1, 1], "date": "2019-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_193", "valid": true}
{"query": "Are there any studies that discuss about learning 'subspace juntas', functions of an unknown low-dimensional subspace?", "cited_paper": [], "gt_label": [], "date": "2011-08", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_194", "valid": true}
{"query": "Which work attempted to explore the inner modeling of hierarchical Transformer-based backbone?", "cited_paper": [{"arxiv_id": "2207.10448", "title": "An Efficient Spatio-Temporal Pyramid Transformer for Action Detection", "year": 2022}], "gt_label": [1], "date": "2022-07", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_195", "valid": true}
{"query": "Which works on deep learning are primarily used as losses to train generative models?", "cited_paper": [{"arxiv_id": "1701.04862", "title": "Towards Principled Methods for Training Generative Adversarial Networks", "year": 2017}], "gt_label": [1], "date": "2017-01", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_196", "valid": true}
{"query": "Which references discussed the empirical observation of oversmoothing issue in attention-based GNNs such as GATs or transformers?", "cited_paper": [{"arxiv_id": "2303.10993", "title": "A Survey on Oversmoothing in Graph Neural Networks", "year": 2023}, {"arxiv_id": "2202.08625", "title": "Revisiting Over-smoothing in BERT from the Perspective of Graph", "year": 2022}], "gt_label": [1, 1], "date": "2023-03", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_197", "valid": true}
{"query": "Which researchers proposed altering the memory-computation trade-off of the neural architecture for improving computational speed in neural scene representations?", "cited_paper": [{"arxiv_id": "2011.12490", "title": "DeRF: Decomposed Radiance Fields", "year": 2020}, {"arxiv_id": "2103.13744", "title": "KiloNeRF: Speeding up Neural Radiance Fields with Thousands of Tiny MLPs", "year": 2021}, {"arxiv_id": "2103.10380", "title": "FastNeRF: High-Fidelity Neural Rendering at 200FPS", "year": 2021}, {"arxiv_id": "2112.05131", "title": "Plenoxels: Radiance Fields without Neural Networks", "year": 2021}, {"arxiv_id": "2111.11215", "title": "Direct Voxel Grid Optimization: Super-fast Convergence for Radiance Fields Reconstruction", "year": 2021}], "gt_label": [1, 1, 1, 1, 1], "date": "2021-12", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_198", "valid": true}
{"query": "What are the recent works that employed clustering for personalized Federated Learning?", "cited_paper": [{"arxiv_id": "1910.01991", "title": "Clustered Federated Learning: Model-Agnostic Distributed Multi-Task Optimization under Privacy Constraints", "year": 2019}, {"arxiv_id": "2001.09249", "title": "TiFL: A Tier-based Federated Learning System", "year": 2020}], "gt_label": [1, 1], "date": "2020-04", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_199", "valid": true}
{"query": "What research demonstrated that the Felzenswalb algorithm can generate useful geometric segment clusters in the ScanNet dataset?", "cited_paper": [{"arxiv_id": "1702.04405", "title": "ScanNet: Richly-annotated 3D Reconstructions of Indoor Scenes", "year": 2017}], "gt_label": [1], "date": "2017-02", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_200", "valid": true}
{"query": "Which studies showed the success of Diffusion Models in the field of image synthesis?", "cited_paper": [{"arxiv_id": "2105.05233", "title": "Diffusion Models Beat GANs on Image Synthesis", "year": 2021}, {"arxiv_id": "2107.00630", "title": "Variational Diffusion Models", "year": 2021}, {"arxiv_id": "2006.11239", "title": "Denoising Diffusion Probabilistic Models", "year": 2020}], "gt_label": [1, 1, 1], "date": "2021-07", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_201", "valid": true}
{"query": "Which researches focused on batchifying queries in few-shot settings for LLMs?", "cited_paper": [{"arxiv_id": "2301.08721", "title": "Batch Prompting: Efficient Inference with Large Language Model APIs", "year": 2023}], "gt_label": [1], "date": "2023-01", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_202", "valid": true}
{"query": "What are the papers that applied zero/few-shot learning using large language models?", "cited_paper": [{"arxiv_id": "2005.14165", "title": "Language Models are Few-Shot Learners", "year": 2020}, {"arxiv_id": "2204.02311", "title": "PaLM: Scaling Language Modeling with Pathways", "year": 2022}], "gt_label": [1, 1], "date": "2022-04", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_203", "valid": true}
{"query": "Which works have achieved impressive results in the field of image generation using Diffusion Probabilistic Models?", "cited_paper": [{"arxiv_id": "2006.11239", "title": "Denoising Diffusion Probabilistic Models", "year": 2020}, {"arxiv_id": "2209.04747", "title": "Diffusion Models in Vision: A Survey", "year": 2022}], "gt_label": [1, 1], "date": "2022-09", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_204", "valid": true}
{"query": "Which work resulted in the creation of the BLOOM model and ROOTS corpus as part of open science community initiatives?", "cited_paper": [{"arxiv_id": "2211.05100", "title": "BLOOM: A 176B-Parameter Open-Access Multilingual Language Model", "year": 2022}, {"arxiv_id": "2303.03915", "title": "The BigScience ROOTS Corpus: A 1.6TB Composite Multilingual Dataset", "year": 2023}], "gt_label": [1, 1], "date": "2023-03", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_205", "valid": true}
{"query": "Could you detail any works about answering questions given counterfactual conditions?", "cited_paper": [{"arxiv_id": "2305.14010", "title": "IfQA: A Dataset for Open-domain Question Answering under Counterfactual Presuppositions", "year": 2023}, {"arxiv_id": "1909.04739", "title": "WIQA: A dataset for \"What if...\" reasoning over procedural text", "year": 2019}, {"arxiv_id": "2106.04571", "title": "TIMEDIAL: Temporal Commonsense Reasoning in Dialog", "year": 2021}], "gt_label": [1, 1, 1], "date": "2023-05", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_206", "valid": true}
{"query": "Which works focused on leveraging the internal states of LLMs to study hallucinated content?", "cited_paper": [{"arxiv_id": "2307.03987", "title": "A Stitch in Time Saves Nine: Detecting and Mitigating Hallucinations of LLMs by Validating Low-Confidence Generation", "year": 2023}, {"arxiv_id": "2309.15098", "title": "Attention Satisfies: A Constraint-Satisfaction Lens on Factual Errors of Language Models", "year": 2023}, {"arxiv_id": "2304.13734", "title": "The Internal State of an LLM Knows When It's Lying", "year": 2023}], "gt_label": [1, 1, 1], "date": "2023-09", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_207", "valid": true}
{"query": "Can you provide references for grouping-based methods of 3D instance segmentation?", "cited_paper": [{"arxiv_id": "2004.01658", "title": "PointGroup: Dual-Set Point Grouping for 3D Instance Segmentation", "year": 2020}, {"arxiv_id": "2108.02350", "title": "Hierarchical Aggregation for 3D Instance Segmentation", "year": 2021}, {"arxiv_id": "2108.07478", "title": "Instance Segmentation in 3D Scenes using Semantic Superpoint Tree Networks", "year": 2021}, {"arxiv_id": "2203.14662", "title": "MaskGroup: Hierarchical Point Grouping and Masking for 3D Instance Segmentation", "year": 2022}, {"arxiv_id": "2203.01509", "title": "SoftGroup for 3D Instance Segmentation on Point Clouds", "year": 2022}, {"arxiv_id": "2207.07372", "title": "3D Instances as 1D Kernels", "year": 2022}, {"arxiv_id": "2303.00246", "title": "ISBNet: a 3D Point Cloud Instance Segmentation Network with Instance-aware Sampling and Box-aware Dynamic Convolution", "year": 2023}], "gt_label": [1, 1, 1, 1, 1, 1, 1], "date": "2023-03", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_208", "valid": true}
{"query": "In which studies has the focus been on translating natural language proofs into formal representations?", "cited_paper": [{"arxiv_id": "2109.00110", "title": "MiniF2F: a cross-system benchmark for formal Olympiad-level mathematics", "year": 2021}, {"arxiv_id": "2104.01112", "title": "NaturalProofs: Mathematical Theorem Proving in Natural Language", "year": 2021}], "gt_label": [1, 1], "date": "2021-09", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_209", "valid": true}
{"query": "Which works extend the concept of estimating a transformation between two point clouds into finding an ideal set of primitives to represent a shape collection?", "cited_paper": [{"arxiv_id": "1908.04725", "title": "Learning elementary structures for 3D shape generation and matching", "year": 2019}], "gt_label": [1], "date": "2019-08", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_210", "valid": true}
{"query": "Which works achieved impressive results in scene understanding using multi-modal models?", "cited_paper": [{"arxiv_id": "1908.08498", "title": "EPIC-Fusion: Audio-Visual Temporal Binding for Egocentric Action Recognition", "year": 2019}], "gt_label": [1], "date": "2019-08", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_211", "valid": true}
{"query": "Who are the researchers that attempted to close the gap between QM calculations and ML potentials?", "cited_paper": [{"arxiv_id": "2110.05064", "title": "Ab-Initio Potential Energy Surfaces by Pairing GNNs with Neural Wave Functions", "year": 2021}, {"arxiv_id": "2205.14962", "title": "Sampling-free Inference for Ab-Initio Potential Energy Surface Networks", "year": 2022}], "gt_label": [1, 1], "date": "2023-02", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_212", "valid": true}
{"query": "Any references that applied DDPM based on PVCNNs on the point-voxel representation of 3D shapes?", "cited_paper": [{"arxiv_id": "2104.03670", "title": "3D Shape Generation and Completion through Point-Voxel Diffusion", "year": 2021}], "gt_label": [1], "date": "2021-04", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_213", "valid": true}
{"query": "What papers discussed multi-modal models?", "cited_paper": [{"arxiv_id": "1908.08498", "title": "EPIC-Fusion: Audio-Visual Temporal Binding for Egocentric Action Recognition", "year": 2019}, {"arxiv_id": "2104.09224", "title": "Multi-Modal Fusion Transformer for End-to-End Autonomous Driving", "year": 2021}, {"arxiv_id": "2008.11351", "title": "SNE-RoadSeg: Incorporating Surface Normal Information into Semantic Segmentation for Accurate Freespace Detection", "year": 2020}], "gt_label": [1, 1, 1], "date": "2021-04", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_214", "valid": true}
{"query": "Which paper provides a thorough discussion on quantisation-aware training?", "cited_paper": [{"arxiv_id": "2106.08295", "title": "A White Paper on Neural Network Quantization", "year": 2021}], "gt_label": [1], "date": "2021-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_215", "valid": true}
{"query": "What works propose the usage of vector-quantized variational autoencoder and point-based diffusion model in the context of LiDAR scene generation?", "cited_paper": [{"arxiv_id": "2311.01448", "title": "UltraLiDAR: Learning Compact Representations for LiDAR Completion and Generation", "year": 2023}, {"arxiv_id": "2209.03954", "title": "Learning to Generate Realistic LiDAR Point Clouds", "year": 2022}], "gt_label": [1, 1], "date": "2023-11", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_216", "valid": true}
{"query": "What papers proposed IL+RL methods that initialize policy search methods with policies trained via behavioral cloning?", "cited_paper": [{"arxiv_id": "1709.10087", "title": "Learning Complex Dexterous Manipulation with Deep Reinforcement Learning and Demonstrations", "year": 2017}], "gt_label": [1], "date": "2017-09", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_217", "valid": true}
{"query": "Could you provide some studies about LiDAR perception techniques using active learning and domain adaptation methods?", "cited_paper": [{"arxiv_id": "2101.06931", "title": "Label-Efficient Point Cloud Semantic Segmentation: An Active Learning Approach", "year": 2021}, {"arxiv_id": "2106.02377", "title": "A Survey on Deep Domain Adaptation for LiDAR Perception", "year": 2021}], "gt_label": [1, 1], "date": "2021-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_218", "valid": true}
{"query": "Which studies used deep learning in vision tasks like classification and object detection?", "cited_paper": [{"arxiv_id": "1901.10233", "title": "Reconstruction of 3D Porous Media From 2D Slices", "year": 2019}], "gt_label": [1], "date": "2019-01", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_219", "valid": true}
{"query": "Which works construct a generalized decoding model capable of predicting pixel-level segmentation and language tokens?", "cited_paper": [{"arxiv_id": "2212.11270", "title": "Generalized Decoding for Pixel, Image, and Language", "year": 2022}, {"arxiv_id": "2304.06718", "title": "Segment Everything Everywhere All at Once", "year": 2023}], "gt_label": [1, 1], "date": "2023-04", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_220", "valid": true}
{"query": "What works showed that tighter dynamic regret rates are possible in non-stationary multi-armed bandits?", "cited_paper": [{"arxiv_id": "2201.06532", "title": "A New Look at Dynamic Regret for Non-Stationary Stochastic Bandits", "year": 2022}, {"arxiv_id": "2112.13838", "title": "Tracking Most Significant Arm Switches in Bandits", "year": 2021}], "gt_label": [1, 1], "date": "2022-01", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_221", "valid": true}
{"query": "What work combines an original NeRF with an edited NeRF generated using text-based SDS?", "cited_paper": [{"arxiv_id": "2303.12048", "title": "Vox-E: Text-guided Voxel Editing of 3D Objects", "year": 2023}], "gt_label": [1], "date": "2023-03", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_222", "valid": true}
{"query": "What works establish the first connection between replicability and differential privacy in the context of PAC learning?", "cited_paper": [{"arxiv_id": "2110.11208", "title": "User-Level Private Learning via Correlated Sampling", "year": 2021}], "gt_label": [1], "date": "2021-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_223", "valid": true}
{"query": "Could you provide me some works that focus on specific applications such as dialogue, structured knowledge grounding, or chain-of-thought reasoning?", "cited_paper": [{"arxiv_id": "2304.07327", "title": "OpenAssistant Conversations -- Democratizing Large Language Model Alignment", "year": 2023}, {"arxiv_id": "2201.05966", "title": "UnifiedSKG: Unifying and Multi-Tasking Structured Knowledge Grounding with Text-to-Text Language Models", "year": 2022}, {"arxiv_id": "2201.11903", "title": "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models", "year": 2022}, {"arxiv_id": "2305.14045", "title": "The CoT Collection: Improving Zero-shot and Few-shot Learning of Language Models via Chain-of-Thought Fine-Tuning", "year": 2023}], "gt_label": [1, 1, 1, 1], "date": "2023-05", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_224", "valid": true}
{"query": "What papers are about updating text generation metrics?", "cited_paper": [{"arxiv_id": "2102.01672", "title": "The GEM Benchmark: Natural Language Generation, its Evaluation and Metrics", "year": 2021}, {"arxiv_id": "2106.05532", "title": "How Robust are Model Rankings: A Leaderboard Customization Approach for Equitable Evaluation", "year": 2021}, {"arxiv_id": "2112.04139", "title": "Bidimensional Leaderboards: Generate and Evaluate Language Hand in Hand", "year": 2021}], "gt_label": [1, 1, 1], "date": "2021-12", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_225", "valid": true}
{"query": "Which work provided upper and lower bounds on optimal regret based on a variant of the DEC?", "cited_paper": [], "gt_label": [], "date": "2023-01", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_226", "valid": true}
{"query": "Which papers propose graph-based approaches for capturing longer-term dependencies in 3D human pose forecasting?", "cited_paper": [{"arxiv_id": "1908.05436", "title": "Learning Trajectory Dependencies for Human Motion Prediction", "year": 2019}, {"arxiv_id": "2108.07152", "title": "MSR-GCN: Multi-Scale Residual Graph Convolution Networks for Human Motion Prediction", "year": 2021}, {"arxiv_id": "2003.08802", "title": "Dynamic Multiscale Graph Neural Networks for 3D Skeleton-Based Human Motion Prediction", "year": 2020}, {"arxiv_id": "2110.04573", "title": "Space-Time-Separable Graph Convolutional Network for Pose Forecasting", "year": 2021}, {"arxiv_id": "2203.01474", "title": "Spatio-Temporal Gating-Adjacency GCN for Human Motion Prediction", "year": 2022}, {"arxiv_id": "2007.06426", "title": "Multitask Non-Autoregressive Model for Human Motion Prediction", "year": 2020}, {"arxiv_id": "2302.04860", "title": "Diverse Human Motion Prediction Guided by Multi-Level Spatial-Temporal Anchors", "year": 2023}], "gt_label": [1, 1, 1, 1, 1, 1, 1], "date": "2023-02", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_227", "valid": true}
{"query": "Could you provide me with the works using unified maximum likelihood estimation and object retrieval to support various tasks?", "cited_paper": [{"arxiv_id": "2211.09808", "title": "Uni-Perceiver v2: A Generalist Model for Large-Scale Vision and Vision-Language Tasks", "year": 2022}, {"arxiv_id": "2303.06674", "title": "Universal Instance Perception as Object Discovery and Retrieval", "year": 2023}], "gt_label": [1, 1], "date": "2023-03", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_228", "valid": true}
{"query": "Which works classify samples as hard based on the presence of large gradient norm and large norm of error vectors?", "cited_paper": [{"arxiv_id": "2107.07075", "title": "Deep Learning on a Data Diet: Finding Important Examples Early in Training", "year": 2021}], "gt_label": [1], "date": "2021-07", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_229", "valid": true}
{"query": "What work uses meta-learning methods in the field of knowledge editing?", "cited_paper": [{"arxiv_id": "2104.08164", "title": "Editing Factual Knowledge in Language Models", "year": 2021}, {"arxiv_id": "2110.11309", "title": "Fast Model Editing at Scale", "year": 2021}], "gt_label": [1, 1], "date": "2021-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_230", "valid": true}
{"query": "What are some studies that discuss non-linear front-door adjustment?", "cited_paper": [{"arxiv_id": "2010.04855", "title": "Kernel Methods for Causal Functions: Dose, Heterogeneous, and Incremental Response Curves", "year": 2020}], "gt_label": [1], "date": "2020-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_231", "valid": true}
{"query": "What works apply the mentioned technique to the first-order methods?", "cited_paper": [], "gt_label": [], "date": "2016-09", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_232", "valid": true}
{"query": "What papers explored the combination of GANs and implicit neural representation for 3D-aware image synthesis?", "cited_paper": [{"arxiv_id": "2011.12100", "title": "GIRAFFE: Representing Scenes as Compositional Generative Neural Feature Fields", "year": 2020}, {"arxiv_id": "2112.11427", "title": "StyleSDF: High-Resolution 3D-Consistent Image and Geometry Generation", "year": 2021}, {"arxiv_id": "2112.07945", "title": "Efficient Geometry-aware 3D Generative Adversarial Networks", "year": 2021}, {"arxiv_id": "2012.00926", "title": "pi-GAN: Periodic Implicit Generative Adversarial Networks for 3D-Aware Image Synthesis", "year": 2020}], "gt_label": [1, 1, 1, 1], "date": "2021-12", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_233", "valid": true}
{"query": "Could you provide me some research that demonstrated zero-shot cross-lingual transfer?", "cited_paper": [{"arxiv_id": "2203.07450", "title": "A Neural Pairwise Ranking Model for Readability Assessment", "year": 2022}], "gt_label": [1], "date": "2022-03", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_234", "valid": true}
{"query": "What papers discuss the application of non-Euclidean diffusion equation leading to a scheme with adaptive spatial derivatives?", "cited_paper": [{"arxiv_id": "2110.09443", "title": "Beltrami Flow and Neural Diffusion on Graphs", "year": 2021}], "gt_label": [1], "date": "2021-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_235", "valid": true}
{"query": "Which paper used mutual information for invariant representation learning outside of RL?", "cited_paper": [{"arxiv_id": "2208.02656", "title": "Invariant Representations with Stochastically Quantized Neural Networks", "year": 2022}], "gt_label": [1], "date": "2022-08", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_236", "valid": true}
{"query": "Which paper introduces DataComp, a benchmark for designing better pre-training datasets for CLIP?", "cited_paper": [{"arxiv_id": "2304.14108", "title": "DataComp: In search of the next generation of multimodal datasets", "year": 2023}], "gt_label": [1], "date": "2023-04", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_237", "valid": true}
{"query": "What paper advances the representative Seq2Edit model, GECToR, by improving its multi-round correction?", "cited_paper": [{"arxiv_id": "2203.09136", "title": "Type-Driven Multi-Turn Corrections for Grammatical Error Correction", "year": 2022}], "gt_label": [1], "date": "2022-03", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_238", "valid": true}
{"query": "Which works discuss provably efficient algorithms in the context of function approximation in linear mixture MDPs?", "cited_paper": [{"arxiv_id": "1912.05830", "title": "Provably Efficient Exploration in Policy Optimization", "year": 2019}, {"arxiv_id": "2012.08507", "title": "Nearly Minimax Optimal Reinforcement Learning for Linear Mixture Markov Decision Processes", "year": 2020}, {"arxiv_id": "2006.13165", "title": "Provably Efficient Reinforcement Learning for Discounted MDPs with Feature Mapping", "year": 2020}], "gt_label": [1, 1, 1], "date": "2020-12", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_239", "valid": true}
{"query": "Could you provide me some critiques on benchmarks in NLP about annotation artifacts?", "cited_paper": [{"arxiv_id": "1803.02324", "title": "Annotation Artifacts in Natural Language Inference Data", "year": 2018}], "gt_label": [1], "date": "2018-03", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_240", "valid": true}
{"query": "What studies presented method for selecting single target-state pair with stochastic batch acquisition in a BOED setting?", "cited_paper": [{"arxiv_id": "2203.02016", "title": "Interventions, Where and How? Experimental Design for Causal Models at Scale", "year": 2022}], "gt_label": [1], "date": "2022-03", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_241", "valid": true}
{"query": "Which work introduced API specially designed for content moderation in vast amounts of data?", "cited_paper": [{"arxiv_id": "2208.03274", "title": "A Holistic Approach to Undesired Content Detection in the Real World", "year": 2022}], "gt_label": [1], "date": "2022-08", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_242", "valid": true}
{"query": "What is the research paper that discusses instruction tuning?", "cited_paper": [{"arxiv_id": "2203.02155", "title": "Training language models to follow instructions with human feedback", "year": 2022}], "gt_label": [1], "date": "2022-03", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_243", "valid": true}
{"query": "Which work proposed the method of calculating the inner integral of Sampled Policy Gradients (SPG) using a Q-network?", "cited_paper": [{"arxiv_id": "1709.00503", "title": "Mean Actor Critic", "year": 2017}], "gt_label": [1], "date": "2017-09", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_244", "valid": true}
{"query": "Which work applied perturbations to word embeddings in the NLP domain?", "cited_paper": [{"arxiv_id": "1605.07725", "title": "Adversarial Training Methods for Semi-Supervised Text Classification", "year": 2016}], "gt_label": [1], "date": "2016-05", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_245", "valid": true}
{"query": "What studies have considered extending beta diffusion to encompass the exponential family?", "cited_paper": [{"arxiv_id": "1601.00670", "title": "Variational Inference: A Review for Statisticians", "year": 2016}], "gt_label": [1], "date": "2016-01", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_246", "valid": true}
{"query": "What papers explored metrics that either emphasize a single dimension or lack human relevance?", "cited_paper": [{"arxiv_id": "2004.04228", "title": "Asking and Answering Questions to Evaluate the Factual Consistency of Summaries", "year": 2020}, {"arxiv_id": "2010.03994", "title": "GRADE: Automatic Graph-Enhanced Coherence Metric for Evaluating Open-Domain Dialogue Systems", "year": 2020}, {"arxiv_id": "2005.00456", "title": "USR: An Unsupervised and Reference Free Evaluation Metric for Dialog Generation", "year": 2020}, {"arxiv_id": "2210.07197", "title": "Towards a Unified Multi-Dimensional Evaluator for Text Generation", "year": 2022}], "gt_label": [1, 1, 1, 1], "date": "2022-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_247", "valid": true}
{"query": "Are there any studies that provide convergence guarantees for PFL with the bounded gradients assumption on heterogeneous data?", "cited_paper": [{"arxiv_id": "1907.02189", "title": "On the Convergence of FedAvg on Non-IID Data", "year": 2019}], "gt_label": [1], "date": "2019-07", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_248", "valid": true}
{"query": "Can you give examples of research that altered the parameter of the transition function (like gravity strength or the weight of the ball) for RL agent evaluation?", "cited_paper": [{"arxiv_id": "1810.12282", "title": "Assessing Generalization in Deep Reinforcement Learning", "year": 2018}, {"arxiv_id": "1703.02702", "title": "Robust Adversarial Reinforcement Learning", "year": 2017}], "gt_label": [1, 1], "date": "2018-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_249", "valid": true}
{"query": "What subsequent work improved the complexities of the method developed by bib.bib60?", "cited_paper": [], "gt_label": [], "date": "2016-07", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_250", "valid": true}
{"query": "What works proposed methods based on the primary form of the Optimal Transport problem?", "cited_paper": [{"arxiv_id": "1905.00158", "title": "On Scalable and Efficient Computation of Large Scale Optimal Transport", "year": 2019}, {"arxiv_id": "2003.06635", "title": "Large-Scale Optimal Transport via Adversarial Training with Cycle-Consistency", "year": 2020}], "gt_label": [1, 1], "date": "2020-03", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_251", "valid": true}
{"query": "Could you provide me some works that focus on aligning the learning trajectories for emulating long-range training dynamics of real data when performing dataset condensation?", "cited_paper": [{"arxiv_id": "2203.11932", "title": "Dataset Distillation by Matching Training Trajectories", "year": 2022}], "gt_label": [1], "date": "2022-03", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_252", "valid": true}
{"query": "Which studies succeeded in addressing regression tasks through deep learning?", "cited_paper": [{"arxiv_id": "1711.09017", "title": "MPIIGaze: Real-World Dataset and Deep Appearance-Based Gaze Estimation", "year": 2017}, {"arxiv_id": "1611.08860", "title": "It's Written All Over Your Face: Full-Face Appearance-Based Gaze Estimation", "year": 2016}], "gt_label": [1, 1], "date": "2017-11", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_253", "valid": true}
{"query": "What works exist that provide corrective measures for issues concerning the RP gradient?", "cited_paper": [{"arxiv_id": "2111.05803", "title": "Gradients are Not All You Need", "year": 2021}], "gt_label": [1], "date": "2021-11", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_254", "valid": true}
{"query": "What works have been done to extract answers from question-specific subgraphs generated with text corpora?", "cited_paper": [{"arxiv_id": "1809.00782", "title": "Open Domain Question Answering Using Early Fusion of Knowledge Bases and Text", "year": 2018}, {"arxiv_id": "1904.09537", "title": "PullNet: Open Domain Question Answering with Iterative Retrieval on Knowledge Bases and Text", "year": 2019}], "gt_label": [1, 1], "date": "2019-04", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_255", "valid": true}
{"query": "Which works propose methods to handle continuous treatments in the back-door adjustment?", "cited_paper": [{"arxiv_id": "1902.00981", "title": "Learning Counterfactual Representations for Estimating Individual Dose-Response Curves", "year": 2019}], "gt_label": [1], "date": "2020-04", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_256", "valid": true}
{"query": "Can you identify any papers that analysed the use of target networks with linear function approximation, needed in theoretical properties of target networks?", "cited_paper": [{"arxiv_id": "2101.08862", "title": "Breaking the Deadly Triad with a Target Network", "year": 2021}], "gt_label": [1], "date": "2021-01", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_257", "valid": true}
{"query": "Which works adapt diffusion models to condition generation on protein pockets?", "cited_paper": [{"arxiv_id": "2210.05274", "title": "Equivariant 3D-Conditional Diffusion Models for Molecular Linker Design", "year": 2022}, {"arxiv_id": "2210.13695", "title": "Structure-based Drug Design with Equivariant Diffusion Models", "year": 2022}, {"arxiv_id": "2210.01776", "title": "DiffDock: Diffusion Steps, Twists, and Turns for Molecular Docking", "year": 2022}], "gt_label": [1, 1, 1], "date": "2022-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_258", "valid": true}
{"query": "What are some of the works that provide a detailed analysis of the stationary distribution of SGD iterations with a constant learning rate?", "cited_paper": [{"arxiv_id": "1704.04289", "title": "Stochastic Gradient Descent as Approximate Bayesian Inference", "year": 2017}], "gt_label": [1], "date": "2017-04", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_259", "valid": true}
{"query": "What works measure bias based on the difference in ground-truth object-group co-occurrences in the training set and test set co-occurrences predicted by a model?", "cited_paper": [{"arxiv_id": "1707.09457", "title": "Men Also Like Shopping: Reducing Gender Bias Amplification using Corpus-level Constraints", "year": 2017}], "gt_label": [1], "date": "2017-07", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_260", "valid": true}
{"query": "What works are there on incorporating interactive discussions and human collaboration into LLM-based evaluation methodologies?", "cited_paper": [{"arxiv_id": "2308.07201", "title": "ChatEval: Towards Better LLM-based Evaluators through Multi-Agent Debate", "year": 2023}, {"arxiv_id": "2310.19740", "title": "Exploring the Reliability of Large Language Models as Customized Evaluators for Diverse NLP Tasks", "year": 2023}], "gt_label": [1, 1], "date": "2023-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_261", "valid": true}
{"query": "What works made advancements in achieving certified robustness with state-of-the-art randomized ablation?", "cited_paper": [{"arxiv_id": "1911.09272", "title": "Robustness Certificates for Sparse Adversarial Attacks by Randomized Ablation", "year": 2019}, {"arxiv_id": "2011.07633", "title": "Almost Tight L0-norm Certified Robustness of Top-k Predictions against Adversarial Perturbations", "year": 2020}], "gt_label": [1, 1], "date": "2020-11", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_262", "valid": true}
{"query": "Which works perform multi-view refinement with flow or dense features in Structure-from-Motion (SfM)?", "cited_paper": [{"arxiv_id": "2003.08348", "title": "Multi-View Optimization of Local Feature Geometry", "year": 2020}, {"arxiv_id": "2108.08291", "title": "Pixel-Perfect Structure-from-Motion with Featuremetric Refinement", "year": 2021}], "gt_label": [1, 1], "date": "2021-08", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_263", "valid": true}
{"query": "Could you give me examples of research achieving progress in sentiment analysis using multi-modal models?", "cited_paper": [{"arxiv_id": "2003.01043", "title": "Gated Mechanism for Attention Based Multimodal Sentiment Analysis", "year": 2020}], "gt_label": [1], "date": "2022-01", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_264", "valid": true}
{"query": "Which works propose techniques for improving the quality of image-text datasets in multimodal networks?", "cited_paper": [{"arxiv_id": "2301.02280", "title": "Filtering, Distillation, and Hard Negatives for Vision-Language Pre-Training", "year": 2023}, {"arxiv_id": "2305.05095", "title": "Less is More: Removing Text-regions Improves CLIP Training Efficiency and Robustness", "year": 2023}, {"arxiv_id": "2303.09540", "title": "SemDeDup: Data-efficient learning at web-scale through semantic deduplication", "year": 2023}], "gt_label": [1, 1, 1], "date": "2023-07", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_265", "valid": true}
{"query": "In what works can I find large-scale unsupervised pre-training on unstructured text for multilingual corpora?", "cited_paper": [{"arxiv_id": "2211.05100", "title": "BLOOM: A 176B-Parameter Open-Access Multilingual Language Model", "year": 2022}, {"arxiv_id": "2210.15424", "title": "What Language Model to Train if You Have One Million GPU Hours?", "year": 2022}, {"arxiv_id": "2303.03915", "title": "The BigScience ROOTS Corpus: A 1.6TB Composite Multilingual Dataset", "year": 2023}, {"arxiv_id": "2309.04662", "title": "MADLAD-400: A Multilingual And Document-Level Large Audited Dataset", "year": 2023}, {"arxiv_id": "2305.14288", "title": "LLM-powered Data Augmentation for Enhanced Cross-lingual Performance", "year": 2023}], "gt_label": [1, 1, 1, 1, 1], "date": "2023-09", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_266", "valid": true}
{"query": "Which papers focus on broader applications of NeRF, including generative modeling, video synthesis, and scene editing?", "cited_paper": [{"arxiv_id": "2209.11163", "title": "GET3D: A Generative Model of High Quality 3D Textured Shapes Learned from Images", "year": 2022}, {"arxiv_id": "2201.04873", "title": "VoLux-GAN: A Generative Model for 3D Face Synthesis with HDRI Relighting", "year": 2022}, {"arxiv_id": "2011.13084", "title": "Neural Scene Flow Fields for Space-Time View Synthesis of Dynamic Scenes", "year": 2020}, {"arxiv_id": "2211.11610", "title": "Tensor4D : Efficient Neural 4D Decomposition for High-fidelity Dynamic Reconstruction and Rendering", "year": 2022}, {"arxiv_id": "2012.09790", "title": "Neural Radiance Flow for 4D View Synthesis and Video Processing", "year": 2020}, {"arxiv_id": "2105.06466", "title": "Editing Conditional Radiance Fields", "year": 2021}, {"arxiv_id": "2205.04978", "title": "NeRF-Editing: Geometry Editing of Neural Radiance Fields", "year": 2022}], "gt_label": [1, 1, 1, 1, 1, 1, 1], "date": "2022-11", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_267", "valid": true}
{"query": "What studies considered to initialise the vertices of the graphs with random labels in graph embedding?", "cited_paper": [{"arxiv_id": "2002.03155", "title": "Random Features Strengthen Graph Neural Networks", "year": 2020}], "gt_label": [1], "date": "2020-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_268", "valid": true}
{"query": "What studies removed the strong assumptions about knowledge of non-stationarity in non-contextual bandits?", "cited_paper": [], "gt_label": [], "date": "2019-02", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_269", "valid": true}
{"query": "What is the standout example of point-based methodologies that transformed the direct processing of Point Cloud?", "cited_paper": [{"arxiv_id": "1612.00593", "title": "PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation", "year": 2016}], "gt_label": [1], "date": "2016-12", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_270", "valid": true}
{"query": "Could you provide me some works about studying a special case of fractionally subadditive valuations?", "cited_paper": [{"arxiv_id": "1812.09561", "title": "The Fair Division of Hereditary Set Systems", "year": 2018}], "gt_label": [1], "date": "2018-12", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_271", "valid": true}
{"query": "Which works explored ad-hoc teamwork and zero-shot coordination in AI?", "cited_paper": [{"arxiv_id": "2003.02979", "title": "\"Other-Play\" for Zero-Shot Coordination", "year": 2020}, {"arxiv_id": "2106.06613", "title": "A New Formalism, Method and Open Issues for Zero-Shot Coordination", "year": 2021}, {"arxiv_id": "2210.12124", "title": "Equivariant Networks for Zero-Shot Coordination", "year": 2022}], "gt_label": [1, 1, 1], "date": "2022-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_272", "valid": true}
{"query": "What works explored the constraint of Neural Differential Equations by expensive training and prediction times?", "cited_paper": [{"arxiv_id": "1904.01681", "title": "Augmented Neural ODEs", "year": 2019}, {"arxiv_id": "2007.04504", "title": "Learning Differential Equations that are Easy to Solve", "year": 2020}, {"arxiv_id": "2006.10711", "title": "STEER: Simple Temporal Regularization For Neural ODEs", "year": 2020}, {"arxiv_id": "2105.03918", "title": "Opening the Blackbox: Accelerating Neural Differential Equations by Regularizing Internal Solver Heuristics", "year": 2021}], "gt_label": [1, 1, 1, 1], "date": "2021-05", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_273", "valid": true}
{"query": "What work is the only public simulator that supports differentiable simulation for in-graph acceleration?", "cited_paper": [{"arxiv_id": "2104.11212", "title": "Imagining The Road Ahead: Multi-Agent Trajectory Prediction via Differentiable Simulation", "year": 2021}], "gt_label": [1], "date": "2021-04", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_274", "valid": true}
{"query": "What studies work on body motion conditioned on text descriptions?", "cited_paper": [{"arxiv_id": "2209.00349", "title": "FLAME: Free-form Language-based Motion Synthesis & Editing", "year": 2022}, {"arxiv_id": "2104.05670", "title": "Action-Conditioned 3D Human Motion Synthesis with Transformer VAE", "year": 2021}, {"arxiv_id": "2308.01850", "title": "Synthesizing Long-Term Human Motions with Diffusion Models via Coherent Sampling", "year": 2023}, {"arxiv_id": "2204.14109", "title": "TEMOS: Generating diverse human motions from textual descriptions", "year": 2022}, {"arxiv_id": "2103.14675", "title": "Synthesis of Compositional Animations from Textual Descriptions", "year": 2021}], "gt_label": [1, 1, 1, 1, 1], "date": "2023-08", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_275", "valid": true}
{"query": "Which study examines users' discomfort or concern due to the lack of responsibility in LLMs' recommendations for emotional support response?", "cited_paper": [{"arxiv_id": "2401.14362", "title": "The Typing Cure: Experiences with Large Language Model Chatbots for Mental Health Support", "year": 2024}], "gt_label": [1], "date": "2024-01", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_276", "valid": true}
{"query": "Which reference is about the TorchGeo, a Python library for the integration of remote sensing datasets into the PyTorch deep learning ecosystem?", "cited_paper": [{"arxiv_id": "2111.08872", "title": "TorchGeo: Deep Learning With Geospatial Data", "year": 2021}], "gt_label": [1], "date": "2021-11", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_277", "valid": true}
{"query": "Which studies have investigated factors like loss function, surrogate gradient estimation, and batch normalization that affect the learning behavior in direct training of SNNs?", "cited_paper": [{"arxiv_id": "2202.11946", "title": "Temporal Efficient Training of Spiking Neural Network via Gradient Re-weighting", "year": 2022}, {"arxiv_id": "2308.08359", "title": "Membrane Potential Batch Normalization for Spiking Neural Networks", "year": 2023}], "gt_label": [1, 1], "date": "2023-08", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_278", "valid": true}
{"query": "Could you provide me works that exemplified an asymmetric discussion mechanism with different LLMs and using a weighted voting mechanism?", "cited_paper": [{"arxiv_id": "2309.13007", "title": "ReConcile: Round-Table Conference Improves Reasoning via Consensus among Diverse LLMs", "year": 2023}], "gt_label": [1], "date": "2023-09", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_279", "valid": true}
{"query": "What references proposed methods to discover the global semantic structure underlying the whole dataset, shed light on graph contrastive learning?", "cited_paper": [{"arxiv_id": "2106.04113", "title": "Self-supervised Graph-level Representation Learning with Local and Global Structure", "year": 2021}, {"arxiv_id": "2205.15746", "title": "Omni-Granular Ego-Semantic Propagation for Self-Supervised Graph Representation Learning", "year": 2022}], "gt_label": [1, 1], "date": "2022-05", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_280", "valid": true}
{"query": "Could you mention research studies that utilized features for minimizing the surrogate models in dataset distillation?", "cited_paper": [{"arxiv_id": "2203.01531", "title": "CAFE: Learning to Condense Dataset by Aligning Features", "year": 2022}], "gt_label": [1], "date": "2022-03", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_281", "valid": true}
{"query": "Could you name the papers where the research focus on the L2 convergence rate for KRR, and it can be easily extended to [],0 convergence rate?", "cited_paper": [{"arxiv_id": "1801.06720", "title": "Optimal Rates for Spectral Algorithms with Least-Squares Regression over Hilbert Spaces", "year": 2018}], "gt_label": [1], "date": "2018-01", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_282", "valid": true}
{"query": "What work used MCP when evaluating InstructGPT?", "cited_paper": [{"arxiv_id": "2207.08143", "title": "Can large language models reason about medical questions?", "year": 2022}], "gt_label": [1], "date": "2022-07", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_283", "valid": true}
{"query": "What studies in medical VLMs use diffusion-based methods in report-to-CXR generation task?", "cited_paper": [{"arxiv_id": "2211.12737", "title": "RoentGen: Vision-Language Foundation Model for Chest X-ray Generation", "year": 2022}], "gt_label": [1], "date": "2022-11", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_284", "valid": true}
{"query": "Which works initially proposed generating test cases and corresponding accurate assert statements with Transformer models?", "cited_paper": [{"arxiv_id": "2009.05617", "title": "Unit Test Case Generation with Transformers and Focal Context", "year": 2020}, {"arxiv_id": "2009.05634", "title": "Generating Accurate Assert Statements for Unit Test Cases using Pretrained Transformers", "year": 2020}], "gt_label": [1, 1], "date": "2020-09", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_285", "valid": true}
{"query": "Which paper carried out an extensive empirical study which shows significant improvement in model performance by implementing SWAG with multiple randomly initialized models?", "cited_paper": [{"arxiv_id": "2002.08791", "title": "Bayesian Deep Learning and a Probabilistic Perspective of Generalization", "year": 2020}], "gt_label": [1], "date": "2020-02", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_286", "valid": true}
{"query": "What works have developed algorithms to solve the sparse coding problem?", "cited_paper": [{"arxiv_id": "0908.0050", "title": "Online Learning for Matrix Factorization and Sparse Coding", "year": 2009}, {"arxiv_id": "1308.6273", "title": "New Algorithms for Learning Incoherent and Overcomplete Dictionaries", "year": 2013}, {"arxiv_id": "1401.0579", "title": "More Algorithms for Provable Dictionary Learning", "year": 2014}, {"arxiv_id": "1503.00778", "title": "Simple, Efficient, and Neural Algorithms for Sparse Coding", "year": 2015}], "gt_label": [1, 1, 1, 1], "date": "2015-03", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_287", "valid": true}
{"query": "What studies used concept activation vectors and multimodal models to annotate concepts for CBMs?", "cited_paper": [{"arxiv_id": "2205.15480", "title": "Post-hoc Concept Bottleneck Models", "year": 2022}, {"arxiv_id": "2304.06129", "title": "Label-Free Concept Bottleneck Models", "year": 2023}, {"arxiv_id": "2103.00020", "title": "Learning Transferable Visual Models From Natural Language Supervision", "year": 2021}], "gt_label": [1, 1, 1], "date": "2023-04", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_288", "valid": true}
{"query": "Any works that discuss learning the GAN fingerprints towards image attribution?", "cited_paper": [{"arxiv_id": "1811.08180", "title": "Attributing Fake Images to GANs: Learning and Analyzing GAN Fingerprints", "year": 2018}], "gt_label": [1], "date": "2018-11", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_289", "valid": true}
{"query": "What works used pretrained GAN generators and text encoders to optimize images based on textual prompts?", "cited_paper": [{"arxiv_id": "2103.10951", "title": "Paint by Word", "year": 2021}, {"arxiv_id": "2108.00946", "title": "StyleGAN-NADA: CLIP-Guided Domain Adaptation of Image Generators", "year": 2021}], "gt_label": [1, 1], "date": "2021-08", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_290", "valid": true}
{"query": "What research proposed GroundingSAM, a combination of GroundingDINO and SAM for generating segmentation masks?", "cited_paper": [{"arxiv_id": "2303.05499", "title": "Grounding DINO: Marrying DINO with Grounded Pre-Training for Open-Set Object Detection", "year": 2023}, {"arxiv_id": "2401.14159", "title": "Grounded SAM: Assembling Open-World Models for Diverse Visual Tasks", "year": 2024}], "gt_label": [1, 1], "date": "2024-01", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_291", "valid": true}
{"query": "What research is done about 3D diffusion models involving meshes?", "cited_paper": [{"arxiv_id": "2303.05371", "title": "3DGen: Triplane Latent Diffusion for Textured Mesh Generation", "year": 2023}, {"arxiv_id": "2303.08133", "title": "MeshDiffusion: Score-based Generative 3D Mesh Modeling", "year": 2023}, {"arxiv_id": "2303.07938", "title": "Controllable Mesh Generation Through Sparse Latent Point Diffusion Models", "year": 2023}], "gt_label": [1, 1, 1], "date": "2023-03", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_292", "valid": true}
{"query": "Which papers deal with the utilization of context to rewrite the conversation into a standalone query in CQR models?", "cited_paper": [{"arxiv_id": "2004.01909", "title": "Conversational Question Reformulation via Sequence-to-Sequence Architectures and Pretrained Language Models", "year": 2020}, {"arxiv_id": "2305.15645", "title": "ConvGQR: Generative Query Reformulation for Conversational Search", "year": 2023}], "gt_label": [1, 1], "date": "2023-05", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_293", "valid": true}
{"query": "In what work was PoseNets innovative transfer learning first introduced?", "cited_paper": [{"arxiv_id": "1505.07427", "title": "PoseNet: A Convolutional Network for Real-Time 6-DOF Camera Relocalization", "year": 2015}], "gt_label": [1], "date": "2015-05", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_294", "valid": true}
{"query": "Which studies describe model structures that implicitly generate reasoning processes?", "cited_paper": [{"arxiv_id": "1705.04146", "title": "Program Induction by Rationale Generation : Learning to Solve and Explain Algebraic Word Problems", "year": 2017}, {"arxiv_id": "2105.07624", "title": "TAT-QA: A Question Answering Benchmark on a Hybrid of Tabular and Textual Content in Finance", "year": 2021}, {"arxiv_id": "2209.07692", "title": "Answering Numerical Reasoning Questions in Table-Text Hybrid Contents with Graph-based Encoder and Tree-based Decoder", "year": 2022}, {"arxiv_id": "2211.16482", "title": "Chaining Simultaneous Thoughts for Numerical Reasoning", "year": 2022}, {"arxiv_id": "2210.10105", "title": "ELASTIC: Numerical Reasoning with Adaptive Symbolic Compiler", "year": 2022}], "gt_label": [1, 1, 1, 1, 1], "date": "2022-11", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_295", "valid": true}
{"query": "What studies investigated phenomenon on invariant representations in the context of algorithmic fairness?", "cited_paper": [{"arxiv_id": "1511.05897", "title": "Censoring Representations with an Adversary", "year": 2015}, {"arxiv_id": "1801.07593", "title": "Mitigating Unwanted Biases with Adversarial Learning", "year": 2018}, {"arxiv_id": "1910.07162", "title": "Conditional Learning of Fair Representations", "year": 2019}], "gt_label": [1, 1, 1], "date": "2019-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_296", "valid": true}
{"query": "Any studies showcase the potential of non-attention architectures in language modeling?", "cited_paper": [{"arxiv_id": "2212.14052", "title": "Hungry Hungry Hippos: Towards Language Modeling with State Space Models", "year": 2022}, {"arxiv_id": "2305.13048", "title": "RWKV: Reinventing RNNs for the Transformer Era", "year": 2023}, {"arxiv_id": "2311.04823", "title": "Hierarchically Gated Recurrent Neural Network for Sequence Modeling", "year": 2023}], "gt_label": [1, 1, 1], "date": "2023-11", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_297", "valid": true}
{"query": "What papers proposed iterative methods for transferable adversarial attacks?", "cited_paper": [{"arxiv_id": "1412.6572", "title": "Explaining and Harnessing Adversarial Examples", "year": 2014}, {"arxiv_id": "1710.06081", "title": "Boosting Adversarial Attacks with Momentum", "year": 2017}, {"arxiv_id": "1908.06281", "title": "Nesterov Accelerated Gradient and Scale Invariance for Adversarial Attacks", "year": 2019}, {"arxiv_id": "2103.15571", "title": "Enhancing the Transferability of Adversarial Attacks through Variance Tuning", "year": 2021}, {"arxiv_id": "1803.06978", "title": "Improving Transferability of Adversarial Examples with Input Diversity", "year": 2018}, {"arxiv_id": "2106.04169", "title": "On Improving Adversarial Transferability of Vision Transformers", "year": 2021}, {"arxiv_id": "2112.05379", "title": "Cross-Modal Transferable Adversarial Attacks from Images to Videos", "year": 2021}], "gt_label": [1, 1, 1, 1, 1, 1, 1], "date": "2021-12", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_298", "valid": true}
{"query": "Which papers proposed datasets for open domain question answering (QA) for English and other languages?", "cited_paper": [{"arxiv_id": "2207.00758", "title": "MIA 2022 Shared Task: Evaluating Cross-lingual Open-Retrieval Question Answering for 16 Diverse Languages", "year": 2022}, {"arxiv_id": "2007.15207", "title": "MKQA: A Linguistically Diverse Benchmark for Multilingual Open Domain Question Answering", "year": 2020}, {"arxiv_id": "2108.08787", "title": "Mr. TyDi: A Multi-lingual Benchmark for Dense Retrieval", "year": 2021}], "gt_label": [1, 1, 1], "date": "2022-07", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_299", "valid": true}
{"query": "What research work focuses on a criterion, named difference of confidences (DoC), which estimates and reflects model accuracy?", "cited_paper": [{"arxiv_id": "2107.03315", "title": "Predicting with Confidence on Unseen Distributions", "year": 2021}], "gt_label": [1], "date": "2021-07", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_300", "valid": true}
{"query": "What studies introduced the mask-reconstruction paradigm for unimodal pretraining?", "cited_paper": [{"arxiv_id": "1810.04805", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "year": 2018}], "gt_label": [1], "date": "2018-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_301", "valid": true}
{"query": "Are there any studies that suggest the algorithm they propose cannot be directly applied to model-free reinforcement learning settings?", "cited_paper": [{"arxiv_id": "2209.11745", "title": "Unified Algorithms for RL with Decision-Estimation Coefficients: PAC, Reward-Free, Preference-Based Learning, and Beyond", "year": 2022}], "gt_label": [1], "date": "2022-09", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_302", "valid": true}
{"query": "Are there any studies that used meta-learning for updating the knowledge in LLMs through varying their parameters?", "cited_paper": [{"arxiv_id": "2004.00345", "title": "Editable Neural Networks", "year": 2020}, {"arxiv_id": "2110.11309", "title": "Fast Model Editing at Scale", "year": 2021}, {"arxiv_id": "2104.08164", "title": "Editing Factual Knowledge in Language Models", "year": 2021}], "gt_label": [1, 1, 1], "date": "2021-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_303", "valid": true}
{"query": "Which works were proposed that consider networks of weights matrices with bounded norms?", "cited_paper": [{"arxiv_id": "1706.08498", "title": "Spectrally-normalized margin bounds for neural networks", "year": 2017}, {"arxiv_id": "1605.08254", "title": "Robust Large Margin Deep Neural Networks", "year": 2016}], "gt_label": [1, 1], "date": "2017-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_304", "valid": true}
{"query": "Which study proposed the method PixSfM for feature-metric keypoint adjustment and bundle adjustment in SfM?", "cited_paper": [{"arxiv_id": "2108.08291", "title": "Pixel-Perfect Structure-from-Motion with Featuremetric Refinement", "year": 2021}], "gt_label": [1], "date": "2021-08", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_305", "valid": true}
{"query": "What papers studied unbalanced optimal transport (UOT) using methods that estimate UOT potentials on discrete space?", "cited_paper": [{"arxiv_id": "2209.15621", "title": "Neural Unbalanced Optimal Transport via Cycle-Consistent Semi-Couplings", "year": 2022}, {"arxiv_id": "2002.03293", "title": "On Unbalanced Optimal Transport: An Analysis of Sinkhorn Algorithm", "year": 2020}, {"arxiv_id": "2106.04145", "title": "Unbalanced Optimal Transport through Non-negative Penalized Linear Regression", "year": 2021}, {"arxiv_id": "2103.03606", "title": "Unbalanced minibatch Optimal Transport; applications to Domain Adaptation", "year": 2021}], "gt_label": [1, 1, 1, 1], "date": "2022-09", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_306", "valid": true}
{"query": "What works used open-loop imitation learning for predicting the behavior of the ego vehicle in autonomous driving?", "cited_paper": [{"arxiv_id": "1604.07316", "title": "End to End Learning for Self-Driving Cars", "year": 2016}, {"arxiv_id": "1905.01296", "title": "PRECOG: PREdiction Conditioned On Goals in Visual Multi-Agent Settings", "year": 2019}, {"arxiv_id": "1710.02410", "title": "End-to-end Driving via Conditional Imitation Learning", "year": 2017}, {"arxiv_id": "2109.13602", "title": "SafetyNet: Safe planning for real-world self-driving vehicles using machine-learned policies", "year": 2021}, {"arxiv_id": "1912.12294", "title": "Learning by Cheating", "year": 2019}], "gt_label": [1, 1, 1, 1, 1], "date": "2021-09", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_307", "valid": true}
{"query": "Which works formulates medical VQA datasets based on MIMIC-CXR?", "cited_paper": [{"arxiv_id": "2302.09636", "title": "Interpretable Medical Image Visual Question Answering via Multi-Modal Relationship Graph Learning", "year": 2023}], "gt_label": [1], "date": "2023-02", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_308", "valid": true}
{"query": "Can you name some studies that propose different metrics to prune networks at initialization?", "cited_paper": [{"arxiv_id": "2002.07376", "title": "Picking Winning Tickets Before Training by Preserving Gradient Flow", "year": 2020}, {"arxiv_id": "2006.05467", "title": "Pruning neural networks without any data by iteratively conserving synaptic flow", "year": 2020}, {"arxiv_id": "2006.09081", "title": "Progressive Skeletonization: Trimming more fat from a network at initialization", "year": 2020}, {"arxiv_id": "2010.11354", "title": "PHEW: Constructing Sparse Networks that Learn Fast and Generalize Well without Training Data", "year": 2020}, {"arxiv_id": "2202.08132", "title": "Prospect Pruning: Finding Trainable Weights at Initialization using Meta-Gradients", "year": 2022}], "gt_label": [1, 1, 1, 1, 1], "date": "2022-02", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_309", "valid": true}
{"query": "What papers reviewed when focusing on methods for quantifying aleatoric segmentation uncertainty?", "cited_paper": [{"arxiv_id": "1806.05034", "title": "A Probabilistic U-Net for Segmentation of Ambiguous Images", "year": 2018}, {"arxiv_id": "2006.06015", "title": "Stochastic Segmentation Networks: Modelling Spatially Correlated Aleatoric Uncertainty", "year": 2020}], "gt_label": [1, 1], "date": "2020-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_310", "valid": true}
{"query": "What papers outlined the studies on how to select the firing threshold to cover all the features in an ANN?", "cited_paper": [{"arxiv_id": "1612.04052", "title": "Theory and Tools for the Conversion of Analog to Spiking Convolutional Neural Networks", "year": 2016}], "gt_label": [1], "date": "2016-12", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_311", "valid": true}
{"query": "What papers express key concerns about the validity of performance measurements obtained with various NLP benchmarks?", "cited_paper": [{"arxiv_id": "2104.02145", "title": "What Will it Take to Fix Benchmarking in Natural Language Understanding?", "year": 2021}], "gt_label": [1], "date": "2021-04", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_312", "valid": true}
{"query": "Please provide papers advocating the use of scratchpads in Large Language Models.", "cited_paper": [{"arxiv_id": "2112.00114", "title": "Show Your Work: Scratchpads for Intermediate Computation with Language Models", "year": 2021}], "gt_label": [1], "date": "2021-12", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_313", "valid": true}
{"query": "Which studies look at the convergence rates for PMD-type methods for Lipschitz and smooth policies?", "cited_paper": [{"arxiv_id": "2106.12112", "title": "Bregman Gradient Policy Optimization", "year": 2021}], "gt_label": [1], "date": "2021-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_314", "valid": true}
{"query": "What papers discuss mitigation methods for temporal adaptation?", "cited_paper": [{"arxiv_id": "2106.15110", "title": "Time-Aware Language Models as Temporal Knowledge Bases", "year": 2021}, {"arxiv_id": "2204.14211", "title": "TemporalWiki: A Lifelong Benchmark for Training and Evaluating Ever-Evolving Language Models", "year": 2022}, {"arxiv_id": "2110.03215", "title": "Towards Continual Knowledge Learning of Language Models", "year": 2021}, {"arxiv_id": "2204.12785", "title": "Plug-and-Play Adaptation for Continuously-updated QA", "year": 2022}], "gt_label": [1, 1, 1, 1], "date": "2022-04", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_315", "valid": true}
{"query": "What are the papers that introduce various learning objectives to tackle data heterogeneity in Federated Learning?", "cited_paper": [{"arxiv_id": "1812.06127", "title": "Federated Optimization in Heterogeneous Networks", "year": 2018}, {"arxiv_id": "2003.08082", "title": "Federated Visual Classification with Real-World Data Distribution", "year": 2020}, {"arxiv_id": "1910.06378", "title": "SCAFFOLD: Stochastic Controlled Averaging for Federated Learning", "year": 2019}, {"arxiv_id": "2103.16257", "title": "Model-Contrastive Federated Learning", "year": 2021}], "gt_label": [1, 1, 1, 1], "date": "2021-03", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_316", "valid": true}
{"query": "Could you provide me some works that use complex-valued activations with real-valued weights and a gating mechanism?", "cited_paper": [{"arxiv_id": "1312.6115", "title": "Neuronal Synchrony in Complex-Valued Deep Networks", "year": 2013}, {"arxiv_id": "2204.02075", "title": "Complex-Valued Autoencoders for Object Discovery", "year": 2022}], "gt_label": [1, 1], "date": "2022-04", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_317", "valid": true}
{"query": "Which studies highlight the benefit of capturing long-distance relations in Graph Neural Networks (GNNs) by stacking more feature aggregation layers or unrolling various fixed point iterations?", "cited_paper": [{"arxiv_id": "1810.05997", "title": "Predict then Propagate: Graph Neural Networks meet Personalized PageRank", "year": 2018}, {"arxiv_id": "2009.06211", "title": "Implicit Graph Neural Networks", "year": 2020}, {"arxiv_id": "2007.09296", "title": "Towards Deeper Graph Neural Networks", "year": 2020}, {"arxiv_id": "2007.02133", "title": "Simple and Deep Graph Convolutional Networks", "year": 2020}, {"arxiv_id": "2106.07476", "title": "Training Graph Neural Networks with 1000 Layers", "year": 2021}, {"arxiv_id": "2010.01777", "title": "A Unified View on Graph Neural Networks as Graph Signal Denoising", "year": 2020}, {"arxiv_id": "2101.11859", "title": "Interpreting and Unifying Graph Neural Networks with An Optimization Framework", "year": 2021}], "gt_label": [1, 1, 1, 1, 1, 1, 1], "date": "2021-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_318", "valid": true}
{"query": "Could you mention some studies about learning-based MVS approaches?", "cited_paper": [{"arxiv_id": "1911.12012", "title": "Deep Stereo using Adaptive Thin Volume Representation with Uncertainty Awareness", "year": 2019}, {"arxiv_id": "1912.06378", "title": "Cascade Cost Volume for High-Resolution Multi-View Stereo and Stereo Matching", "year": 2019}, {"arxiv_id": "1912.08329", "title": "Cost Volume Pyramid Based Depth Inference for Multi-View Stereo", "year": 2019}], "gt_label": [1, 1, 1], "date": "2019-12", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_319", "valid": true}
{"query": "Which works opted to incorporate the backbone into the network's training process to form an end-to-end TAD framework?", "cited_paper": [{"arxiv_id": "2103.13137", "title": "Learning Salient Boundary Feature for Anchor-free Temporal Action Localization", "year": 2021}, {"arxiv_id": "2204.01680", "title": "TALLFormer: Temporal Action Localization with a Long-memory Transformer", "year": 2022}, {"arxiv_id": "2207.10448", "title": "An Efficient Spatio-Temporal Pyramid Transformer for Action Detection", "year": 2022}, {"arxiv_id": "2211.14053", "title": "Re^2TAL: Rewiring Pretrained Video Backbones for Reversible Temporal Action Localization", "year": 2022}], "gt_label": [1, 1, 1, 1], "date": "2022-11", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_320", "valid": true}
{"query": "Which works are referred when discussing previous benchmarks for solving math word problems?", "cited_paper": [{"arxiv_id": "1705.04146", "title": "Program Induction by Rationale Generation : Learning to Solve and Explain Algebraic Word Problems", "year": 2017}, {"arxiv_id": "1905.13319", "title": "MathQA: Towards Interpretable Math Word Problem Solving with Operation-Based Formalisms", "year": 2019}, {"arxiv_id": "2110.14168", "title": "Training Verifiers to Solve Math Word Problems", "year": 2021}, {"arxiv_id": "2306.16636", "title": "CMATH: Can Your Language Model Pass Chinese Elementary School Math Test?", "year": 2023}], "gt_label": [1, 1, 1, 1], "date": "2023-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_321", "valid": true}
{"query": "Are there works that use image groupings and pairs for disentanglement?", "cited_paper": [{"arxiv_id": "1910.09772", "title": "Weakly Supervised Disentanglement with Guarantees", "year": 2019}, {"arxiv_id": "2002.02886", "title": "Weakly-Supervised Disentanglement Without Compromises", "year": 2020}], "gt_label": [1, 1], "date": "2020-02", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_322", "valid": true}
{"query": "Could you provide me some examples of research that discusses the application of data augmentations in the latent space?", "cited_paper": [{"arxiv_id": "1909.11764", "title": "FreeLB: Enhanced Adversarial Training for Natural Language Understanding", "year": 2019}, {"arxiv_id": "2006.11834", "title": "AdvAug: Robust Adversarial Augmentation for Neural Machine Translation", "year": 2020}, {"arxiv_id": "2209.05297", "title": "DoubleMix: Simple Interpolation-Based Data Augmentation for Text Classification", "year": 2022}, {"arxiv_id": "2202.13840", "title": "Text Smoothing: Enhance Various Data Augmentation Methods on Text Classification Tasks", "year": 2022}, {"arxiv_id": "1910.03487", "title": "Controlled Text Generation for Data Augmentation in Intelligent Artificial Agents", "year": 2019}], "gt_label": [1, 1, 1, 1, 1], "date": "2022-09", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_323", "valid": true}
{"query": "Which works make a formal equivalence between differential privacy and replicability for finite domains?", "cited_paper": [{"arxiv_id": "2303.12921", "title": "Stability is Stable: Connections between Replicability, Privacy, and Adaptive Generalization", "year": 2023}], "gt_label": [1], "date": "2023-03", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_324", "valid": true}
{"query": "Could you find any works that proposed methods applicable to online non-parametric regression tasks?", "cited_paper": [{"arxiv_id": "1603.04190", "title": "Online Isotonic Regression", "year": 2016}, {"arxiv_id": "1906.03364", "title": "Online Forecasting of Total-Variation-bounded Sequences", "year": 2019}], "gt_label": [1, 1], "date": "2019-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_325", "valid": true}
{"query": "Which studies introduced text-to-SQL datasets associated with MIMIC-III and eICU?", "cited_paper": [{"arxiv_id": "2301.07695", "title": "EHRSQL: A Practical Text-to-SQL Benchmark for Electronic Health Records", "year": 2023}], "gt_label": [1], "date": "2023-01", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_326", "valid": true}
{"query": "Are there any works that used hypernetworks in Meta-SL?", "cited_paper": [{"arxiv_id": "1807.05960", "title": "Meta-Learning with Latent Embedding Optimization", "year": 2018}, {"arxiv_id": "1703.00837", "title": "Meta Networks", "year": 2017}], "gt_label": [1, 1], "date": "2018-07", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_327", "valid": true}
{"query": "Which researches have covered the attributes of non-functional requirements in terms of time/space performance?", "cited_paper": [{"arxiv_id": "2302.07867", "title": "Learning Performance-Improving Code Edits", "year": 2023}, {"arxiv_id": "2306.08568", "title": "WizardCoder: Empowering Code Large Language Models with Evol-Instruct", "year": 2023}], "gt_label": [1, 1], "date": "2023-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_328", "valid": true}
{"query": "Could you provide examples of works about certified defenses focused on unimodal models?", "cited_paper": [{"arxiv_id": "1902.02918", "title": "Certified Adversarial Robustness via Randomized Smoothing", "year": 2019}, {"arxiv_id": "1912.09899", "title": "Certified Robustness for Top-k Predictions against Adversarial Perturbations via Randomized Smoothing", "year": 2019}, {"arxiv_id": "1911.09272", "title": "Robustness Certificates for Sparse Adversarial Attacks by Randomized Ablation", "year": 2019}, {"arxiv_id": "2003.06693", "title": "Certified Defenses for Adversarial Patches", "year": 2020}, {"arxiv_id": "2005.14424", "title": "SAFER: A Structure-free Approach for Certified Robustness to Adversarial Word Substitutions", "year": 2020}, {"arxiv_id": "1802.03471", "title": "Certified Robustness to Adversarial Examples with Differential Privacy", "year": 2018}, {"arxiv_id": "2103.03046", "title": "PointGuard: Provably Robust 3D Point Cloud Classification", "year": 2021}, {"arxiv_id": "2105.03743", "title": "Certified Robustness to Text Adversarial Attacks by Randomized [MASK]", "year": 2021}, {"arxiv_id": "2108.09135", "title": "PatchCleanser: Certifiably Robust Defense against Adversarial Patches for Any Image Classifier", "year": 2021}, {"arxiv_id": "2210.01111", "title": "MultiGuard: Provably Robust Multi-label Classification against Adversarial Examples", "year": 2022}, {"arxiv_id": "2311.11225", "title": "TextGuard: Provable Defense against Backdoor Attacks on Text Classification", "year": 2023}, {"arxiv_id": "2303.01959", "title": "PointCert: Point Cloud Classification with Deterministic Certified Robustness Guarantees", "year": 2023}], "gt_label": [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], "date": "2023-11", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_329", "valid": true}
{"query": "What are the early efforts that expanded traditional KG representation learning methods for single-modal knowledge graphs?", "cited_paper": [{"arxiv_id": "1609.07028", "title": "Image-embodied Knowledge Representation Learning", "year": 2016}], "gt_label": [1], "date": "2016-09", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_330", "valid": true}
{"query": "Could you provide me the work that incorporated PEFT-based layer adaptation to the shared attention and FFN modules in transformers?", "cited_paper": [{"arxiv_id": "2202.07959", "title": "EdgeFormer: A Parameter-Efficient Transformer for On-Device Seq2seq Generation", "year": 2022}], "gt_label": [1], "date": "2022-02", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_331", "valid": true}
{"query": "What papers proposed domain adaptation (DA) methods?", "cited_paper": [{"arxiv_id": "1505.07818", "title": "Domain-Adversarial Training of Neural Networks", "year": 2015}, {"arxiv_id": "1901.05335", "title": "A review of domain adaptation without target labels", "year": 2019}], "gt_label": [1, 1], "date": "2019-01", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_332", "valid": true}
{"query": "Which studies proposed the use of a seq2seq architecture and a progressive transformer for Sign Language Production (SLP)?", "cited_paper": [{"arxiv_id": "2004.14874", "title": "Progressive Transformers for End-to-End Sign Language Production", "year": 2020}], "gt_label": [1], "date": "2020-04", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_333", "valid": true}
{"query": "What work stored manual edits in a memory module to amend the output of LLMs?", "cited_paper": [{"arxiv_id": "2206.06520", "title": "Memory-Based Model Editing at Scale", "year": 2022}], "gt_label": [1], "date": "2022-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_334", "valid": true}
{"query": "What researches have been conducted about privacy-preserving learning with learning invariant representations?", "cited_paper": [{"arxiv_id": "1610.03577", "title": "Minimax Filter: Learning to Preserve Privacy from Inference Attacks", "year": 2016}, {"arxiv_id": "1808.09408", "title": "Privacy-preserving Neural Representations of Text", "year": 2018}, {"arxiv_id": "1911.10143", "title": "Adversarial Learning of Privacy-Preserving and Task-Oriented Representations", "year": 2019}], "gt_label": [1, 1, 1], "date": "2019-11", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_335", "valid": true}
{"query": "What research papers have explored multimodal tasks within UI contexts?", "cited_paper": [{"arxiv_id": "2112.05692", "title": "VUT: Versatile UI Transformer for Multi-Modal Multi-Task User Interface Modeling", "year": 2021}, {"arxiv_id": "2107.13731", "title": "UIBert: Learning Generic Multimodal Representations for UI Understanding", "year": 2021}, {"arxiv_id": "2012.12350", "title": "ActionBert: Leveraging User Actions for Semantic Understanding of User Interfaces", "year": 2020}], "gt_label": [1, 1, 1], "date": "2021-12", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_336", "valid": true}
{"query": "What studies focus on the different techniques utilized to fine-tune the pre-trained models?", "cited_paper": [{"arxiv_id": "2210.11416", "title": "Scaling Instruction-Finetuned Language Models", "year": 2022}, {"arxiv_id": "2203.02155", "title": "Training language models to follow instructions with human feedback", "year": 2022}, {"arxiv_id": "1902.00751", "title": "Parameter-Efficient Transfer Learning for NLP", "year": 2019}, {"arxiv_id": "2106.09685", "title": "LoRA: Low-Rank Adaptation of Large Language Models", "year": 2021}, {"arxiv_id": "2101.00190", "title": "Prefix-Tuning: Optimizing Continuous Prompts for Generation", "year": 2021}, {"arxiv_id": "2104.08691", "title": "The Power of Scale for Parameter-Efficient Prompt Tuning", "year": 2021}, {"arxiv_id": "2110.07602", "title": "P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks", "year": 2021}], "gt_label": [1, 1, 1, 1, 1, 1, 1], "date": "2022-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_337", "valid": true}
{"query": "Which works discussed the relation between sinusoidal networks and networks with Fourier feature transformations?", "cited_paper": [{"arxiv_id": "2006.10739", "title": "Fourier Features Let Networks Learn High Frequency Functions in Low Dimensional Domains", "year": 2020}], "gt_label": [1], "date": "2020-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_338", "valid": true}
{"query": "Could you provide me some works that enhance unimodal language models with retrieval?", "cited_paper": [{"arxiv_id": "2112.04426", "title": "Improving language models by retrieving from trillions of tokens", "year": 2021}, {"arxiv_id": "2208.03299", "title": "Atlas: Few-shot Learning with Retrieval Augmented Language Models", "year": 2022}], "gt_label": [1, 1], "date": "2022-08", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_339", "valid": true}
{"query": "Which works have utilized instructional videos for online learning, especially in the medical field?", "cited_paper": [{"arxiv_id": "2201.12888", "title": "A Dataset for Medical Instructional Video Classification and Question Answering", "year": 2022}], "gt_label": [1], "date": "2022-01", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_340", "valid": true}
{"query": "Who designed attentional GNNs by discretizing parabolic diffusion-type PDEs?", "cited_paper": [{"arxiv_id": "2106.10934", "title": "GRAND: Graph Neural Diffusion", "year": 2021}], "gt_label": [1], "date": "2021-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_341", "valid": true}
{"query": "Which studies investigated zero-shot NAS methods aiming to reduce search costs?", "cited_paper": [{"arxiv_id": "2006.04647", "title": "Neural Architecture Search without Training", "year": 2020}], "gt_label": [1], "date": "2021-01", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_342", "valid": true}
{"query": "Which papers approach the studies about adversarial attacks?", "cited_paper": [{"arxiv_id": "1312.6199", "title": "Intriguing properties of neural networks", "year": 2013}, {"arxiv_id": "1708.06131", "title": "Evasion Attacks against Machine Learning at Test Time", "year": 2017}, {"arxiv_id": "1608.04644", "title": "Towards Evaluating the Robustness of Neural Networks", "year": 2016}, {"arxiv_id": "1706.06083", "title": "Towards Deep Learning Models Resistant to Adversarial Attacks", "year": 2017}, {"arxiv_id": "2003.01690", "title": "Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks", "year": 2020}, {"arxiv_id": "1802.00420", "title": "Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples", "year": 2018}], "gt_label": [1, 1, 1, 1, 1, 1], "date": "2020-03", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_343", "valid": true}
{"query": "What papers give an account of the datasets developed for the vision-based 3D Semantic Occupancy Prediction?", "cited_paper": [{"arxiv_id": "2303.03991", "title": "OpenOccupancy: A Large Scale Benchmark for Surrounding Semantic Occupancy Perception", "year": 2023}, {"arxiv_id": "2304.14365", "title": "Occ3D: A Large-Scale 3D Occupancy Prediction Benchmark for Autonomous Driving", "year": 2023}], "gt_label": [1, 1], "date": "2023-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_344", "valid": true}
{"query": "Which studies surpassed traditional image compression standards by using neural image compression methods?", "cited_paper": [{"arxiv_id": "2011.09704", "title": "Causal Contextual Prediction for Learned Image Compression", "year": 2020}, {"arxiv_id": "2303.14978", "title": "Learned Image Compression with Mixed Transformer-CNN Architectures", "year": 2023}, {"arxiv_id": "2006.09965", "title": "High-Fidelity Generative Image Compression", "year": 2020}], "gt_label": [1, 1, 1], "date": "2023-03", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_345", "valid": true}
{"query": "Which research works have explored the organizational structures required to facilitate the development of research communities around under-represented languages?", "cited_paper": [{"arxiv_id": "2104.02516", "title": "AI4D -- African Language Program", "year": 2021}, {"arxiv_id": "2103.15475", "title": "NLP for Ghanaian Languages", "year": 2021}], "gt_label": [1, 1], "date": "2021-04", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_346", "valid": true}
{"query": "Which studies proposed a homotopy algorithm in the context of a response vector parameterized with a real value?", "cited_paper": [], "gt_label": [], "date": "2017-08", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_347", "valid": true}
{"query": "What works proposed kernel-based distillation methods?", "cited_paper": [{"arxiv_id": "2206.00719", "title": "Dataset Distillation using Neural Feature Regression", "year": 2022}, {"arxiv_id": "2210.12067", "title": "Efficient Dataset Distillation Using Random Feature Approximation", "year": 2022}], "gt_label": [1, 1], "date": "2022-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_348", "valid": true}
{"query": "What research allows direct NeRF editing by changing a reference image in 2D space?", "cited_paper": [{"arxiv_id": "2303.13277", "title": "SINE: Semantic-driven Image-based NeRF Editing with Prior-guided Editing Field", "year": 2023}], "gt_label": [1], "date": "2023-03", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_349", "valid": true}
{"query": "What works focus on obtaining strong visual representations through self-supervised training on large-scale image data?", "cited_paper": [{"arxiv_id": "2104.14294", "title": "Emerging Properties in Self-Supervised Vision Transformers", "year": 2021}, {"arxiv_id": "2111.06377", "title": "Masked Autoencoders Are Scalable Vision Learners", "year": 2021}, {"arxiv_id": "2211.07636", "title": "EVA: Exploring the Limits of Masked Visual Representation Learning at Scale", "year": 2022}], "gt_label": [1, 1, 1], "date": "2022-11", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_350", "valid": true}
{"query": "Can you refer any works that are used rounding or merging strategies to produce long feature tracks?", "cited_paper": [{"arxiv_id": "2208.14201", "title": "ASpanFormer: Detector-Free Image Matching with Adaptive Span Transformer", "year": 2022}], "gt_label": [1], "date": "2022-08", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_351", "valid": true}
{"query": "What is the research proposing a two-step approach involving foreground-background separation and text spatial alignment for scene text editing?", "cited_paper": [{"arxiv_id": "1908.03047", "title": "Editing Text in the Wild", "year": 2019}], "gt_label": [1], "date": "2019-08", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_352", "valid": true}
{"query": "Could you provide me some works about zero-shot classification?", "cited_paper": [{"arxiv_id": "1707.00600", "title": "Zero-Shot Learning -- A Comprehensive Evaluation of the Good, the Bad and the Ugly", "year": 2017}], "gt_label": [1], "date": "2017-07", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_353", "valid": true}
{"query": "What research extended the work of Zaheer et al. by using hierarchical clustering to obtain fine-grained pseudo-labels?", "cited_paper": [{"arxiv_id": "2310.17650", "title": "A Coarse-to-Fine Pseudo-Labeling (C2FPL) Framework for Unsupervised Video Anomaly Detection", "year": 2023}], "gt_label": [1], "date": "2023-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_354", "valid": true}
{"query": "Which paper discussed about the retrieval augmented generation (RAG) solutions?", "cited_paper": [{"arxiv_id": "2005.11401", "title": "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks", "year": 2020}], "gt_label": [1], "date": "2020-05", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_355", "valid": true}
{"query": "Which paper uses an Iterative Dataset Update(IDU) strategy to edit NeRFs image dataset?", "cited_paper": [{"arxiv_id": "2303.12789", "title": "Instruct-NeRF2NeRF: Editing 3D Scenes with Instructions", "year": 2023}], "gt_label": [1], "date": "2023-03", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_356", "valid": true}
{"query": "What studies showed that maximum distance from a dataset to its subset was an effective measure of risk for instance optimality?", "cited_paper": [{"arxiv_id": "2005.10630", "title": "Near Instance-Optimality in Differential Privacy", "year": 2020}], "gt_label": [1], "date": "2020-05", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_357", "valid": true}
{"query": "Which studies reported that the performance of LLMs declines as the proportion of noise in the retrieval context increases?", "cited_paper": [{"arxiv_id": "2309.01431", "title": "Benchmarking Large Language Models in Retrieval-Augmented Generation", "year": 2023}, {"arxiv_id": "2310.01558", "title": "Making Retrieval-Augmented Language Models Robust to Irrelevant Context", "year": 2023}, {"arxiv_id": "2312.11361", "title": "\"Knowing When You Don't Know\": A Multilingual Relevance Assessment Dataset for Robust Retrieval-Augmented Generation", "year": 2023}], "gt_label": [1, 1, 1], "date": "2023-12", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_358", "valid": true}
{"query": "What works have used hypercolumns for tasks like keypoint detection, segmentation and semantic correspondence?", "cited_paper": [{"arxiv_id": "1411.5752", "title": "Hypercolumns for Object Segmentation and Fine-grained Localization", "year": 2014}, {"arxiv_id": "1707.06484", "title": "Deep Layer Aggregation", "year": 2017}, {"arxiv_id": "1908.06537", "title": "Hyperpixel Flow: Semantic Correspondence with Multi-layer Neural Features", "year": 2019}, {"arxiv_id": "1704.04749", "title": "AnchorNet: A Weakly Supervised Network to Learn Geometry-sensitive Features For Semantic Matching", "year": 2017}, {"arxiv_id": "2007.10587", "title": "Learning to Compose Hypercolumns for Visual Correspondence", "year": 2020}], "gt_label": [1, 1, 1, 1, 1], "date": "2020-07", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_359", "valid": true}
{"query": "Which papers mentioned about automatic learning of prompts, often termed as 'Prompt Learning'?", "cited_paper": [{"arxiv_id": "2104.08691", "title": "The Power of Scale for Parameter-Efficient Prompt Tuning", "year": 2021}], "gt_label": [1], "date": "2021-04", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_360", "valid": true}
{"query": "What paper uses dynamically-updatable tree-sketches in the context of Kronecker regression?", "cited_paper": [], "gt_label": [], "date": "2022-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_361", "valid": true}
{"query": "Which references mentioned that the estimation of a policy gradient in policy-gradient approaches requires more data than in their benchmarks?", "cited_paper": [{"arxiv_id": "1810.03642", "title": "Fast Context Adaptation via Meta-Learning", "year": 2018}, {"arxiv_id": "2301.08028", "title": "A Tutorial on Meta-Reinforcement Learning", "year": 2023}], "gt_label": [1, 1], "date": "2023-01", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_362", "valid": true}
{"query": "Can you provide some examples of studies that used differentiable simulation for efficient co-optimization of soft robots?", "cited_paper": [{"arxiv_id": "2104.00837", "title": "DiffAqua: A Differentiable Computational Design Pipeline for Soft Underwater Swimmers with Shape Interpolation", "year": 2021}], "gt_label": [1], "date": "2021-04", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_363", "valid": true}
{"query": "Which studies have examined the Polyak-Lojasiewicz (PL) inequality, a generalization of strong-convexity?", "cited_paper": [{"arxiv_id": "1608.04636", "title": "Linear Convergence of Gradient and Proximal-Gradient Methods Under the Polyak-ojasiewicz Condition", "year": 2016}], "gt_label": [1], "date": "2016-08", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_364", "valid": true}
{"query": "Which works have explored self-consistency techniques for refining language models in post-hoc correction?", "cited_paper": [{"arxiv_id": "2207.05221", "title": "Language Models (Mostly) Know What They Know", "year": 2022}, {"arxiv_id": "2312.09300", "title": "Self-Evaluation Improves Selective Generation in Large Language Models", "year": 2023}, {"arxiv_id": "2305.14975", "title": "Just Ask for Calibration: Strategies for Eliciting Calibrated Confidence Scores from Language Models Fine-Tuned with Human Feedback", "year": 2023}, {"arxiv_id": "2303.17651", "title": "Self-Refine: Iterative Refinement with Self-Feedback", "year": 2023}, {"arxiv_id": "2309.11495", "title": "Chain-of-Verification Reduces Hallucination in Large Language Models", "year": 2023}, {"arxiv_id": "2203.11171", "title": "Self-Consistency Improves Chain of Thought Reasoning in Language Models", "year": 2022}], "gt_label": [1, 1, 1, 1, 1, 1], "date": "2023-12", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_365", "valid": true}
{"query": "Could you provide me some studies that attempted to mitigate bias found in in-context learning by utilizing outputs distribution obtained from content-free texts?", "cited_paper": [{"arxiv_id": "2102.09690", "title": "Calibrate Before Use: Improving Few-Shot Performance of Language Models", "year": 2021}, {"arxiv_id": "2305.19148", "title": "Mitigating Label Biases for In-context Learning", "year": 2023}], "gt_label": [1, 1], "date": "2023-05", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_366", "valid": true}
{"query": "What papers propose a reduction from list-global stability to pseudo-global stability via correlated sampling in finite domains?", "cited_paper": [{"arxiv_id": "2110.11208", "title": "User-Level Private Learning via Correlated Sampling", "year": 2021}], "gt_label": [1], "date": "2021-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_367", "valid": true}
{"query": "Which works provide a categorization of existing graph OOD generalization methodologies?", "cited_paper": [{"arxiv_id": "2202.07987", "title": "Out-Of-Distribution Generalization on Graphs: A Survey", "year": 2022}], "gt_label": [1], "date": "2022-02", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_368", "valid": true}
{"query": "What studies are about Mixture of Experts, a technique used to improve robustness and overall accuracy in ensemble learning?", "cited_paper": [{"arxiv_id": "1701.06538", "title": "Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer", "year": 2017}, {"arxiv_id": "2112.06905", "title": "GLaM: Efficient Scaling of Language Models with Mixture-of-Experts", "year": 2021}], "gt_label": [1, 1], "date": "2021-12", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_369", "valid": true}
{"query": "Can you indicate which studies have investigated the trade-off between personalization and performance?", "cited_paper": [{"arxiv_id": "2005.13718", "title": "Operationalizing the Legal Principle of Data Minimization for Personalization", "year": 2020}, {"arxiv_id": "2107.08096", "title": "Learning to Limit Data Collection via Scaling Laws: A Computational Interpretation for the Legal Principle of Data Minimization", "year": 2021}], "gt_label": [1, 1], "date": "2021-07", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_370", "valid": true}
{"query": "Which paper proposed the ROAR algorithm for finding the closest and robust counterfactuals?", "cited_paper": [{"arxiv_id": "2102.13620", "title": "Towards Robust and Reliable Algorithmic Recourse", "year": 2021}], "gt_label": [1], "date": "2021-02", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_371", "valid": true}
{"query": "Which studies deal with aligning visual features with pre-trained LLMs for multimodal comprehension tasks?", "cited_paper": [{"arxiv_id": "2304.10592", "title": "MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models", "year": 2023}, {"arxiv_id": "2208.10442", "title": "Image as a Foreign Language: BEiT Pretraining for All Vision and Vision-Language Tasks", "year": 2022}, {"arxiv_id": "2304.08485", "title": "Visual Instruction Tuning", "year": 2023}, {"arxiv_id": "2304.14178", "title": "mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality", "year": 2023}, {"arxiv_id": "2302.14045", "title": "Language Is Not All You Need: Aligning Perception with Language Models", "year": 2023}, {"arxiv_id": "2301.12597", "title": "BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models", "year": 2023}, {"arxiv_id": "2305.11175", "title": "VisionLLM: Large Language Model is also an Open-Ended Decoder for Vision-Centric Tasks", "year": 2023}, {"arxiv_id": "2305.03726", "title": "Otter: A Multi-Modal Model with In-Context Instruction Tuning", "year": 2023}], "gt_label": [1, 1, 1, 1, 1, 1, 1, 1], "date": "2023-05", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_372", "valid": true}
{"query": "Which studies have designed additional supervision signals or training processes for models in context of CoT reasoning?", "cited_paper": [{"arxiv_id": "2311.02805", "title": "Tailoring Self-Rationalizers with Multi-Reward Distillation", "year": 2023}, {"arxiv_id": "2310.04921", "title": "Crystal: Introspective Reasoners Reinforced with Self-Feedback", "year": 2023}], "gt_label": [1, 1], "date": "2023-11", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_373", "valid": true}
{"query": "What works present the intervention technique of steering model output?", "cited_paper": [{"arxiv_id": "1912.02164", "title": "Plug and Play Language Models: A Simple Approach to Controlled Text Generation", "year": 2019}, {"arxiv_id": "2010.05906", "title": "Back to the Future: Unsupervised Backprop-based Decoding for Counterfactual and Abductive Commonsense Reasoning", "year": 2020}, {"arxiv_id": "2009.06367", "title": "GeDi: Generative Discriminator Guided Sequence Generation", "year": 2020}, {"arxiv_id": "2205.14217", "title": "Diffusion-LM Improves Controllable Text Generation", "year": 2022}], "gt_label": [1, 1, 1, 1], "date": "2022-05", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_374", "valid": true}
{"query": "What research papers focus on minimizing the surrogate models learned from synthetic and original datasets in dataset distillation using matching gradients?", "cited_paper": [{"arxiv_id": "2006.05929", "title": "Dataset Condensation with Gradient Matching", "year": 2020}, {"arxiv_id": "2212.06152", "title": "Accelerating Dataset Distillation via Model Augmentation", "year": 2022}], "gt_label": [1, 1], "date": "2022-12", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_375", "valid": true}
{"query": "What works study the geometry of embeddings that decomposes the shifted pointwise mutual information matrix?", "cited_paper": [{"arxiv_id": "1810.04882", "title": "Towards Understanding Linear Word Analogies", "year": 2018}], "gt_label": [1], "date": "2018-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_376", "valid": true}
{"query": "Are there any papers that use MIAs to assess whether a given data point was used to train an LLM?", "cited_paper": [{"arxiv_id": "2112.03570", "title": "Membership Inference Attacks From First Principles", "year": 2021}, {"arxiv_id": "1610.05820", "title": "Membership Inference Attacks against Machine Learning Models", "year": 2016}], "gt_label": [1, 1], "date": "2021-12", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_377", "valid": true}
{"query": "What research directly trains sparse GANs from scratch?", "cited_paper": [{"arxiv_id": "2203.02770", "title": "Don't Be So Dense: Sparse-to-Sparse GAN Training Without Sacrificing Performance", "year": 2022}], "gt_label": [1], "date": "2022-03", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_378", "valid": true}
{"query": "Could you name any studies that focus on report-to-CXR generation?", "cited_paper": [{"arxiv_id": "2112.10752", "title": "High-Resolution Image Synthesis with Latent Diffusion Models", "year": 2021}, {"arxiv_id": "2210.04133", "title": "Adapting Pretrained Vision-Language Foundational Models to Medical Imaging Domains", "year": 2022}, {"arxiv_id": "2211.12737", "title": "RoentGen: Vision-Language Foundation Model for Chest X-ray Generation", "year": 2022}], "gt_label": [1, 1, 1], "date": "2022-11", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_379", "valid": true}
{"query": "What paper provides a connection between the matrix mechanism and the line of work about generating differentially private synthetic data?", "cited_paper": [{"arxiv_id": "1901.09136", "title": "Graphical-model based estimation and inference for differential privacy", "year": 2019}], "gt_label": [1], "date": "2019-01", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_380", "valid": true}
{"query": "What works employed human pose estimation using 3D shape rendering from multiple views?", "cited_paper": [{"arxiv_id": "2104.02273", "title": "Multi-View Multi-Person 3D Pose Estimation with Plane Sweep Stereo", "year": 2021}], "gt_label": [1], "date": "2021-04", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_381", "valid": true}
{"query": "Which studies regarded Kernel ridge regression as a special kind of spectral regularization algorithm?", "cited_paper": [], "gt_label": [], "date": "2010-01", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_382", "valid": true}
{"query": "Which paper first proposed Graph Contrastive Learning (GCL) with random edge dropping and feature masking as data augmentations?", "cited_paper": [{"arxiv_id": "2006.04131", "title": "Deep Graph Contrastive Representation Learning", "year": 2020}], "gt_label": [1], "date": "2020-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_383", "valid": true}
{"query": "What studies have suggested the usage of produced clusters for multi-document summarization?", "cited_paper": [{"arxiv_id": "1906.01749", "title": "Multi-News: a Large-Scale Multi-Document Summarization Dataset and Abstractive Hierarchical Model", "year": 2019}, {"arxiv_id": "2010.14235", "title": "Multi-XScience: A Large-scale Dataset for Extreme Multi-document Summarization of Scientific Articles", "year": 2020}], "gt_label": [1, 1], "date": "2020-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_384", "valid": true}
{"query": "Which research gave regret bounds for a E2D that incorporates randomized estimators, but not optimism?", "cited_paper": [{"arxiv_id": "2209.11745", "title": "Unified Algorithms for RL with Decision-Estimation Coefficients: PAC, Reward-Free, Preference-Based Learning, and Beyond", "year": 2022}], "gt_label": [1], "date": "2022-09", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_385", "valid": true}
{"query": "What studies introduced strategies to improve the computational and parameter efficiency of transformers by refining parameter-sharing mechanisms?", "cited_paper": [{"arxiv_id": "2104.06022", "title": "Lessons on Parameter Sharing across Layers in Transformers", "year": 2021}], "gt_label": [1], "date": "2021-04", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_386", "valid": true}
{"query": "Could you provide me with research papers about learning a disentangled representation for RL?", "cited_paper": [{"arxiv_id": "1707.08475", "title": "DARLA: Improving Zero-Shot Transfer in Reinforcement Learning", "year": 2017}, {"arxiv_id": "2207.05480", "title": "Temporal Disentanglement of Representations for Improved Generalisation in Reinforcement Learning", "year": 2022}], "gt_label": [1, 1], "date": "2022-07", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_387", "valid": true}
{"query": "Which studies treated Transformer-based backbones as a 'black box'? ", "cited_paper": [{"arxiv_id": "2204.02674", "title": "Faster-TAD: Towards Temporal Action Detection with Proposal Generation and Classification in a Unified Network", "year": 2022}, {"arxiv_id": "2204.01680", "title": "TALLFormer: Temporal Action Localization with a Long-memory Transformer", "year": 2022}, {"arxiv_id": "2211.14053", "title": "Re^2TAL: Rewiring Pretrained Video Backbones for Reversible Temporal Action Localization", "year": 2022}], "gt_label": [1, 1, 1], "date": "2022-11", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_388", "valid": true}
{"query": "Which studies first used CLIP to optimize an underlying 3D representation in text-guided 3D generation?", "cited_paper": [{"arxiv_id": "2112.01455", "title": "Zero-Shot Text-Guided Object Generation with Dream Fields", "year": 2021}, {"arxiv_id": "2203.13333", "title": "CLIP-Mesh: Generating textured meshes from text using pretrained image-text models", "year": 2022}, {"arxiv_id": "2103.00020", "title": "Learning Transferable Visual Models From Natural Language Supervision", "year": 2021}], "gt_label": [1, 1, 1], "date": "2022-03", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_389", "valid": true}
{"query": "What papers mention the vulnerability of DNNs to common corruptions, random noises, and adversarial perturbations?", "cited_paper": [{"arxiv_id": "1903.12261", "title": "Benchmarking Neural Network Robustness to Common Corruptions and Perturbations", "year": 2019}, {"arxiv_id": "1705.02498", "title": "A Study and Comparison of Human and Deep Learning Recognition Performance Under Visual Distortions", "year": 2017}, {"arxiv_id": "1708.06131", "title": "Evasion Attacks against Machine Learning at Test Time", "year": 2017}, {"arxiv_id": "1312.6199", "title": "Intriguing properties of neural networks", "year": 2013}], "gt_label": [1, 1, 1, 1], "date": "2019-03", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_390", "valid": true}
{"query": "Are there any studies regarding the creation of evaluation benchmarks for Natural Language Generation (NLG) on Indic languages?", "cited_paper": [], "gt_label": [], "date": "2022-03", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_391", "valid": true}
{"query": "Which work first initiated the efforts to manipulate Neural Radiance Fields (NeRFs)?", "cited_paper": [{"arxiv_id": "2205.04978", "title": "NeRF-Editing: Geometry Editing of Neural Radiance Fields", "year": 2022}], "gt_label": [1], "date": "2022-05", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_392", "valid": true}
{"query": "Could you refer me to some studies that use score-based models for graph generation?", "cited_paper": [{"arxiv_id": "2003.00638", "title": "Permutation Invariant Graph Generation via Score-Based Generative Modeling", "year": 2020}, {"arxiv_id": "2202.02514", "title": "Score-based Generative Modeling of Graphs via the System of Stochastic Differential Equations", "year": 2022}, {"arxiv_id": "2011.13456", "title": "Score-Based Generative Modeling through Stochastic Differential Equations", "year": 2020}, {"arxiv_id": "2209.14734", "title": "DiGress: Discrete Denoising diffusion for graph generation", "year": 2022}, {"arxiv_id": "2210.01549", "title": "Diffusion Models for Graphs Benefit From Discrete State Spaces", "year": 2022}], "gt_label": [1, 1, 1, 1, 1], "date": "2022-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_393", "valid": true}
{"query": "Could you provide some studies about unfaithful hallucination?", "cited_paper": [{"arxiv_id": "2110.06674", "title": "Truthful AI: Developing and governing AI that does not lie", "year": 2021}, {"arxiv_id": "2308.14752", "title": "AI Deception: A Survey of Examples, Risks, and Potential Solutions", "year": 2023}, {"arxiv_id": "2306.03341", "title": "Inference-Time Intervention: Eliciting Truthful Answers from a Language Model", "year": 2023}], "gt_label": [1, 1, 1], "date": "2023-08", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_394", "valid": true}
{"query": "Any works that introduced a method of learning policy by simulating states via the dynamics model?", "cited_paper": [{"arxiv_id": "1903.00374", "title": "Model-Based Reinforcement Learning for Atari", "year": 2019}], "gt_label": [1], "date": "2019-03", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_395", "valid": true}
{"query": "Which research made a claim that oversmoothing is asymptotically inevitable in GATs?", "cited_paper": [{"arxiv_id": "1910.11945", "title": "Improving Graph Attention Networks with Large Margin-based Constraints", "year": 2019}], "gt_label": [1], "date": "2019-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_396", "valid": true}
{"query": "Which work considers a different approach to limiting the amount of predicted information within learning-augmented paging?", "cited_paper": [{"arxiv_id": "2202.04262", "title": "Parsimonious Learning-Augmented Caching", "year": 2022}], "gt_label": [1], "date": "2022-02", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_397", "valid": true}
{"query": "What work demonstrates the use of transformers in 3D pose estimation task?", "cited_paper": [{"arxiv_id": "2005.04551", "title": "Epipolar Transformers", "year": 2020}], "gt_label": [1], "date": "2020-05", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_398", "valid": true}
{"query": "Are there any studies using a sequence-encoding VQ-VAE to learn the discrete codebook of listener motion?", "cited_paper": [{"arxiv_id": "2204.08451", "title": "Learning to Listen: Modeling Non-Deterministic Dyadic Facial Motion", "year": 2022}], "gt_label": [1], "date": "2022-04", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_399", "valid": true}
{"query": "What paper presents a form of data augmentation for training CLIP models?", "cited_paper": [{"arxiv_id": "2305.20088", "title": "Improving CLIP Training with Language Rewrites", "year": 2023}], "gt_label": [1], "date": "2023-05", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_400", "valid": true}
{"query": "Can you indicate some works that use 2D diffusion models to generate multi-view images then use them for 3D reconstruction with NeRF?", "cited_paper": [{"arxiv_id": "2210.04628", "title": "Novel View Synthesis with Diffusion Models", "year": 2022}, {"arxiv_id": "2303.11328", "title": "Zero-1-to-3: Zero-shot One Image to 3D Object", "year": 2023}, {"arxiv_id": "2306.07881", "title": "Viewset Diffusion: (0-)Image-Conditioned 3D Generative Models from 2D Data", "year": 2023}, {"arxiv_id": "2306.16928", "title": "One-2-3-45: Any Single Image to 3D Mesh in 45 Seconds without Per-Shape Optimization", "year": 2023}], "gt_label": [1, 1, 1, 1], "date": "2023-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_401", "valid": true}
{"query": "Which works propose consistency-based methods for detecting non-factual generations in LLM generated content?", "cited_paper": [{"arxiv_id": "2102.01017", "title": "Measuring and Improving Consistency in Pretrained Language Models", "year": 2021}, {"arxiv_id": "2305.15852", "title": "Self-contradictory Hallucinations of Large Language Models: Evaluation, Detection and Mitigation", "year": 2023}, {"arxiv_id": "2309.15840", "title": "How to Catch an AI Liar: Lie Detection in Black-Box LLMs by Asking Unrelated Questions", "year": 2023}, {"arxiv_id": "2303.08896", "title": "SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models", "year": 2023}, {"arxiv_id": "2305.13281", "title": "LM vs LM: Detecting Factual Errors via Cross Examination", "year": 2023}, {"arxiv_id": "2304.13734", "title": "The Internal State of an LLM Knows When It's Lying", "year": 2023}, {"arxiv_id": "2309.11495", "title": "Chain-of-Verification Reduces Hallucination in Large Language Models", "year": 2023}, {"arxiv_id": "2302.09664", "title": "Semantic Uncertainty: Linguistic Invariances for Uncertainty Estimation in Natural Language Generation", "year": 2023}, {"arxiv_id": "2207.05221", "title": "Language Models (Mostly) Know What They Know", "year": 2022}, {"arxiv_id": "2310.01405", "title": "Representation Engineering: A Top-Down Approach to AI Transparency", "year": 2023}, {"arxiv_id": "2311.09000", "title": "Factcheck-Bench: Fine-Grained Evaluation Benchmark for Automatic Fact-checkers", "year": 2023}, {"arxiv_id": "2210.08726", "title": "RARR: Researching and Revising What Language Models Say, Using Language Models", "year": 2022}, {"arxiv_id": "2307.13528", "title": "FacTool: Factuality Detection in Generative AI -- A Tool Augmented Framework for Multi-Task and Multi-Domain Scenarios", "year": 2023}], "gt_label": [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], "date": "2023-11", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_402", "valid": true}
{"query": "Can you list the works that followed the concept of neural fields for 3D scene and object representation?", "cited_paper": [{"arxiv_id": "2003.08934", "title": "NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis", "year": 2020}, {"arxiv_id": "2003.09852", "title": "Multiview Neural Surface Reconstruction by Disentangling Geometry and Appearance", "year": 2020}], "gt_label": [1, 1], "date": "2020-03", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_403", "valid": true}
{"query": "Who uses similarities between word sense definitions to approximate human judgments on semantic proximity?", "cited_paper": [{"arxiv_id": "2305.11993", "title": "Interpretable Word Sense Representations via Definition Generation: The Case of Semantic Change Analysis", "year": 2023}], "gt_label": [1], "date": "2023-05", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_404", "valid": true}
{"query": "Could you provide me some works about human-curated instruction datasets in languages outside of English?", "cited_paper": [{"arxiv_id": "2211.01786", "title": "Crosslingual Generalization through Multitask Finetuning", "year": 2022}, {"arxiv_id": "2306.04387", "title": "M$^3$IT: A Large-Scale Dataset towards Multi-Modal Multilingual Instruction Tuning", "year": 2023}], "gt_label": [1, 1], "date": "2023-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_405", "valid": true}
{"query": "Who are the pioneer researchers in providing different contexts for a single visual concept using multiple images as part of personalized visual content generation?", "cited_paper": [{"arxiv_id": "2208.01618", "title": "An Image is Worth One Word: Personalizing Text-to-Image Generation using Textual Inversion", "year": 2022}, {"arxiv_id": "2208.12242", "title": "DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation", "year": 2022}], "gt_label": [1, 1], "date": "2022-08", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_406", "valid": true}
{"query": "Are there any studies in sports video understanding which involves benchmarks for spatio-temporal reasoning?", "cited_paper": [{"arxiv_id": "1212.0402", "title": "UCF101: A Dataset of 101 Human Actions Classes From Videos in The Wild", "year": 2012}, {"arxiv_id": "2105.07404", "title": "MultiSports: A Multi-Person Video Dataset of Spatio-Temporally Localized Sports Actions", "year": 2021}, {"arxiv_id": "2004.06704", "title": "FineGym: A Hierarchical Video Dataset for Fine-grained Action Understanding", "year": 2020}, {"arxiv_id": "1804.04527", "title": "SoccerNet: A Scalable Dataset for Action Spotting in Soccer Videos", "year": 2018}, {"arxiv_id": "2304.05170", "title": "SportsMOT: A Large Multi-Object Tracking Dataset in Multiple Sports Scenes", "year": 2023}, {"arxiv_id": "2007.09470", "title": "Social Adaptive Module for Weakly-supervised Group Activity Recognition", "year": 2020}, {"arxiv_id": "1511.06040", "title": "A Hierarchical Deep Temporal Model for Group Activity Recognition", "year": 2015}], "gt_label": [1, 1, 1, 1, 1, 1, 1], "date": "2023-04", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_407", "valid": true}
{"query": "Could you provide me some studies of the use of beam search on KGs using LLMs to dynamically extract the most relevant reasoning paths?", "cited_paper": [{"arxiv_id": "2307.07697", "title": "Think-on-Graph: Deep and Responsible Reasoning of Large Language Model on Knowledge Graph", "year": 2023}], "gt_label": [1], "date": "2023-07", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_408", "valid": true}
{"query": "What research observed the detrimental effect of adding irrelevant noise to the context on model performance?", "cited_paper": [{"arxiv_id": "1707.07328", "title": "Adversarial Examples for Evaluating Reading Comprehension Systems", "year": 2017}, {"arxiv_id": "2205.09712", "title": "Selection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning", "year": 2022}], "gt_label": [1, 1], "date": "2022-05", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_409", "valid": true}
{"query": "What papers explored hallucination source and detection in Linguistically-Informed Language Models (LLMs)?", "cited_paper": [{"arxiv_id": "2311.05232", "title": "A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions", "year": 2023}, {"arxiv_id": "2202.03629", "title": "Survey of Hallucination in Natural Language Generation", "year": 2022}, {"arxiv_id": "2309.01219", "title": "Siren's Song in the AI Ocean: A Survey on Hallucination in Large Language Models", "year": 2023}, {"arxiv_id": "2305.11747", "title": "HaluEval: A Large-Scale Hallucination Evaluation Benchmark for Large Language Models", "year": 2023}], "gt_label": [1, 1, 1, 1], "date": "2023-11", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_410", "valid": true}
{"query": "Could you provide me some works that provide convergence guarantees of stochastic methods for solving quasi-strongly monotone Variational inequality problems (VIPs)?", "cited_paper": [{"arxiv_id": "1908.08465", "title": "On the convergence of single-call stochastic extra-gradient methods", "year": 2019}], "gt_label": [1], "date": "2021-11", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_411", "valid": true}
{"query": "What references were made to works related to transformer-based models in hierarchical classification research?", "cited_paper": [{"arxiv_id": "2010.11929", "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale", "year": 2020}, {"arxiv_id": "2103.14030", "title": "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows", "year": 2021}, {"arxiv_id": "2111.09883", "title": "Swin Transformer V2: Scaling Up Capacity and Resolution", "year": 2021}], "gt_label": [1, 1, 1], "date": "2021-11", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_412", "valid": true}
{"query": "Which paper tackles the problem of registration of two scene fragments captured from two 3D viewpoints with low overlap?", "cited_paper": [{"arxiv_id": "2212.01985", "title": "ObjectMatch: Robust Registration using Canonical Object Correspondences", "year": 2022}], "gt_label": [1], "date": "2022-12", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_413", "valid": true}
{"query": "Which studies have leveraged object template information for identifying potential object locations?", "cited_paper": [{"arxiv_id": "1907.10008", "title": "Incremental Class Discovery for Semantic Segmentation with RGBD Sensing", "year": 2019}], "gt_label": [1], "date": "2019-07", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_414", "valid": true}
{"query": "Which paper is about the Transformers architecture that most recent large language models (LLM) are based on?", "cited_paper": [{"arxiv_id": "1706.03762", "title": "Attention Is All You Need", "year": 2017}], "gt_label": [1], "date": "2017-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_415", "valid": true}
{"query": "Which studies highlight that increased model capacity is needed to achieve robustness against adversarial examples?", "cited_paper": [{"arxiv_id": "1901.00532", "title": "Adversarial Robustness May Be at Odds With Simplicity", "year": 2019}], "gt_label": [1], "date": "2019-01", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_416", "valid": true}
{"query": "Could you provide me some works that apply Gaussian corruptions to token-vector embeddings in diffusion models?", "cited_paper": [{"arxiv_id": "2211.04236", "title": "Self-conditioned Embedding Diffusion for Text Generation", "year": 2022}, {"arxiv_id": "2211.15089", "title": "Continuous diffusion for categorical data", "year": 2022}], "gt_label": [1, 1], "date": "2022-11", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_417", "valid": true}
{"query": "Could you list down studies that convert building footprint extraction into roof segmentation and roof-to-footprint offset estimation tasks?", "cited_paper": [{"arxiv_id": "2204.13637", "title": "Learning to Extract Building Footprints from Off-Nadir Aerial Images", "year": 2022}], "gt_label": [1], "date": "2022-04", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_418", "valid": true}
{"query": "Which paper proposed a Mixed Integer Linear Programming based method to relax GNN certification?", "cited_paper": [{"arxiv_id": "2302.02829", "title": "Collective Robustness Certificates: Exploiting Interdependence in Graph Neural Networks", "year": 2023}], "gt_label": [1], "date": "2023-02", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_419", "valid": true}
{"query": "What works discuss initializations from the optimization perspective?", "cited_paper": [{"arxiv_id": "1502.01852", "title": "Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification", "year": 2015}, {"arxiv_id": "1612.05231", "title": "Tunable Efficient Unitary Neural Networks (EUNN) and their application to RNNs", "year": 2016}], "gt_label": [1, 1], "date": "2020-04", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_420", "valid": true}
{"query": "In which work is explained how LLMs have also been trained on instruction datasets, meaning they can generate text using a custom instruction prompt?", "cited_paper": [{"arxiv_id": "2203.02155", "title": "Training language models to follow instructions with human feedback", "year": 2022}], "gt_label": [1], "date": "2022-03", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_421", "valid": true}
{"query": "Could you provide me studies that apply CoT prompting for multiple reasoning traces and diversifying reasoning paths?", "cited_paper": [{"arxiv_id": "2203.11171", "title": "Self-Consistency Improves Chain of Thought Reasoning in Language Models", "year": 2022}, {"arxiv_id": "2310.14799", "title": "Cross-lingual Prompting: Improving Zero-shot Chain-of-Thought Reasoning across Languages", "year": 2023}], "gt_label": [1, 1], "date": "2023-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_422", "valid": true}
{"query": "Which research papers employ recurrent attention for routing but not for inferring slots?", "cited_paper": [{"arxiv_id": "1901.11390", "title": "MONet: Unsupervised Scene Decomposition and Representation", "year": 2019}, {"arxiv_id": "1907.13052", "title": "GENESIS: Generative Scene Inference and Sampling with Object-Centric Latent Representations", "year": 2019}], "gt_label": [1, 1], "date": "2019-07", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_423", "valid": true}
{"query": "Which papers talk about the upper bounds for general activation functions?", "cited_paper": [{"arxiv_id": "1905.08539", "title": "Universal Approximation with Deep Narrow Networks", "year": 2019}, {"arxiv_id": "2006.08859", "title": "Minimum Width for Universal Approximation", "year": 2020}], "gt_label": [1, 1], "date": "2020-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_424", "valid": true}
{"query": "Can you provide the references of studies that have employed diffusion models for text-driven image editing?", "cited_paper": [{"arxiv_id": "2301.12247", "title": "SEGA: Instructing Text-to-Image Models using Semantic Guidance", "year": 2023}, {"arxiv_id": "2211.09800", "title": "InstructPix2Pix: Learning to Follow Image Editing Instructions", "year": 2022}], "gt_label": [1, 1], "date": "2023-01", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_425", "valid": true}
{"query": "Which are the studies that have done efforts via multi-task learning in personalized Federated Learning?", "cited_paper": [{"arxiv_id": "1705.10467", "title": "Federated Multi-Task Learning", "year": 2017}, {"arxiv_id": "2007.03797", "title": "Personalized Cross-Silo Federated Learning on Non-IID Data", "year": 2020}], "gt_label": [1, 1], "date": "2021-08", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_426", "valid": true}
{"query": "What studies discuss the field of 'learning from human feedback'?", "cited_paper": [{"arxiv_id": "1409.0473", "title": "Neural Machine Translation by Jointly Learning to Align and Translate", "year": 2014}, {"arxiv_id": "2112.09332", "title": "WebGPT: Browser-assisted question-answering with human feedback", "year": 2021}, {"arxiv_id": "2203.02155", "title": "Training language models to follow instructions with human feedback", "year": 2022}, {"arxiv_id": "2204.05862", "title": "Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback", "year": 2022}, {"arxiv_id": "2212.08073", "title": "Constitutional AI: Harmlessness from AI Feedback", "year": 2022}, {"arxiv_id": "2304.05977", "title": "ImageReward: Learning and Evaluating Human Preferences for Text-to-Image Generation", "year": 2023}, {"arxiv_id": "2304.09244", "title": "Text-guided Image-and-Shape Editing and Generation: A Short Survey", "year": 2023}, {"arxiv_id": "2302.12192", "title": "Aligning Text-to-Image Models using Human Feedback", "year": 2023}, {"arxiv_id": "2304.06767", "title": "RAFT: Reward rAnked FineTuning for Generative Foundation Model Alignment", "year": 2023}], "gt_label": [1, 1, 1, 1, 1, 1, 1, 1, 1], "date": "2023-04", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_427", "valid": true}
{"query": "What work discusses the limitation of how answer generation can be updated based on retrievd documents?", "cited_paper": [{"arxiv_id": "2109.05052", "title": "Entity-Based Knowledge Conflicts in Question Answering", "year": 2021}], "gt_label": [1], "date": "2021-09", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_428", "valid": true}
{"query": "What study proposed a hierarchical transformer architecture that unifies semantic tokens and stacked hierarchical acoustic tokens within one stage?", "cited_paper": [{"arxiv_id": "2310.00704", "title": "UniAudio: An Audio Foundation Model Toward Universal Audio Generation", "year": 2023}], "gt_label": [1], "date": "2023-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_429", "valid": true}
{"query": "What are some existing methods proposed for Federated Domain Generalization?", "cited_paper": [{"arxiv_id": "2103.06030", "title": "FedDG: Federated Domain Generalization on Medical Image Segmentation via Episodic Learning in Continuous Frequency Space", "year": 2021}, {"arxiv_id": "2210.00912", "title": "Federated Domain Generalization for Image Recognition via Cross-Client Style Transfer", "year": 2022}], "gt_label": [1, 1], "date": "2022-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_430", "valid": true}
{"query": "Which papers present locate-then-edit methods for knowledge editing?", "cited_paper": [{"arxiv_id": "2104.08696", "title": "Knowledge Neurons in Pretrained Transformers", "year": 2021}, {"arxiv_id": "2202.05262", "title": "Locating and Editing Factual Associations in GPT", "year": 2022}, {"arxiv_id": "2210.07229", "title": "Mass-Editing Memory in a Transformer", "year": 2022}], "gt_label": [1, 1, 1], "date": "2022-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_431", "valid": true}
{"query": "What are some works that have focused on how LLMs can be connected to visual foundation models?", "cited_paper": [{"arxiv_id": "2204.14198", "title": "Flamingo: a Visual Language Model for Few-Shot Learning", "year": 2022}, {"arxiv_id": "2307.10922", "title": "Language-based Action Concept Spaces Improve Video Self-Supervised Learning", "year": 2023}, {"arxiv_id": "2301.12597", "title": "BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models", "year": 2023}, {"arxiv_id": "2304.08485", "title": "Visual Instruction Tuning", "year": 2023}, {"arxiv_id": "2306.05424", "title": "Video-ChatGPT: Towards Detailed Video Understanding via Large Vision and Language Models", "year": 2023}], "gt_label": [1, 1, 1, 1, 1], "date": "2023-07", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_432", "valid": true}
{"query": "Could you provide me some works that reveal the limitations of LLMs' ability in providing emotional support?", "cited_paper": [{"arxiv_id": "2311.13857", "title": "Challenges of Large Language Models for Mental Health Counseling", "year": 2023}, {"arxiv_id": "2304.09873", "title": "ChatGPT as a Therapist Assistant: A Suitability Study", "year": 2023}, {"arxiv_id": "2401.14362", "title": "The Typing Cure: Experiences with Large Language Model Chatbots for Mental Health Support", "year": 2024}], "gt_label": [1, 1, 1], "date": "2024-01", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_433", "valid": true}
{"query": "Could you provide me some studies about Neural Radiance Field (NeRF)?", "cited_paper": [{"arxiv_id": "2003.08934", "title": "NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis", "year": 2020}], "gt_label": [1], "date": "2020-03", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_434", "valid": true}
{"query": "Could you provide me some studies about imitation learning methods that doesn't strictly fit into IDM or GAIL based approaches?", "cited_paper": [{"arxiv_id": "1805.07914", "title": "Imitating Latent Policies from Observation", "year": 2018}, {"arxiv_id": "2011.06507", "title": "Reinforcement Learning with Videos: Combining Offline Observations with Interaction", "year": 2020}], "gt_label": [1, 1], "date": "2020-11", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_435", "valid": true}
{"query": "Which research proposed discrete adaptations of the diffusion model in the context of text generation?", "cited_paper": [], "gt_label": [], "date": "2021-07", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_436", "valid": true}
{"query": "Could you provide me some works where human feedback was utilised to finetune large language models?", "cited_paper": [{"arxiv_id": "1409.0473", "title": "Neural Machine Translation by Jointly Learning to Align and Translate", "year": 2014}, {"arxiv_id": "2112.09332", "title": "WebGPT: Browser-assisted question-answering with human feedback", "year": 2021}, {"arxiv_id": "2203.02155", "title": "Training language models to follow instructions with human feedback", "year": 2022}, {"arxiv_id": "2204.05862", "title": "Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback", "year": 2022}, {"arxiv_id": "2212.08073", "title": "Constitutional AI: Harmlessness from AI Feedback", "year": 2022}], "gt_label": [1, 1, 1, 1, 1], "date": "2022-12", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_437", "valid": true}
{"query": "What papers study the integration of visual encoding modules, like ViT, with LLMs such as LLaMa?", "cited_paper": [{"arxiv_id": "2010.11929", "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale", "year": 2020}, {"arxiv_id": "2302.13971", "title": "LLaMA: Open and Efficient Foundation Language Models", "year": 2023}], "gt_label": [1, 1], "date": "2023-02", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_438", "valid": true}
{"query": "Which study was the first to consider the problem of differentially private submodular maximization in the context of CPPP?", "cited_paper": [{"arxiv_id": "0903.4510", "title": "Differentially Private Combinatorial Optimization", "year": 2009}], "gt_label": [1], "date": "2009-03", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_439", "valid": true}
{"query": "What research papers proposed using attention mechanisms to combine optical flow and deformable convolution for feature alignment?", "cited_paper": [{"arxiv_id": "2206.02146", "title": "Recurrent Video Restoration Transformer with Guided Deformable Attention", "year": 2022}, {"arxiv_id": "2207.10852", "title": "Spatio-Temporal Deformable Attention Network for Video Deblurring", "year": 2022}, {"arxiv_id": "2104.13371", "title": "BasicVSR++: Improving Video Super-Resolution with Enhanced Propagation and Alignment", "year": 2021}, {"arxiv_id": "2303.09757", "title": "Video Dehazing via a Multi-Range Temporal Alignment Network with Physical Prior", "year": 2023}], "gt_label": [1, 1, 1, 1], "date": "2023-03", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_440", "valid": true}
{"query": "Which paper suggested that LLMs internally need to infer latent variables for better prediction in the context of in-context learning?", "cited_paper": [{"arxiv_id": "2111.02080", "title": "An Explanation of In-context Learning as Implicit Bayesian Inference", "year": 2021}], "gt_label": [1], "date": "2021-11", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_441", "valid": true}
{"query": "Are there any works on analytical reconstruction attacks which recursively reconstruct activation maps layer per layer?", "cited_paper": [{"arxiv_id": "2010.07733", "title": "R-GAP: Recursive Gradient Attack on Privacy", "year": 2020}], "gt_label": [1], "date": "2020-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_442", "valid": true}
{"query": "What is the first work that proposed a question-answering dataset that requires the model to perform counterfactual reasoning?", "cited_paper": [{"arxiv_id": "2305.14010", "title": "IfQA: A Dataset for Open-domain Question Answering under Counterfactual Presuppositions", "year": 2023}], "gt_label": [1], "date": "2023-05", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_443", "valid": true}
{"query": "Which works focus on neural architecture search (NAS) for discovering optimal network structures?", "cited_paper": [{"arxiv_id": "2203.09137", "title": "Global Convergence of MAML and Theory-Inspired Neural Architecture Search for Few-Shot Learning", "year": 2022}, {"arxiv_id": "1806.09055", "title": "DARTS: Differentiable Architecture Search", "year": 2018}], "gt_label": [1, 1], "date": "2022-03", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_444", "valid": true}
{"query": "Which works have used pre-computing or post-computing methods for feature aggregation in GNN models?", "cited_paper": [{"arxiv_id": "1902.07153", "title": "Simplifying Graph Convolutional Networks", "year": 2019}, {"arxiv_id": "2004.11198", "title": "SIGN: Scalable Inception Graph Neural Networks", "year": 2020}, {"arxiv_id": "2104.09376", "title": "Scalable and Adaptive Graph Neural Networks with Self-Label-Enhanced training", "year": 2021}, {"arxiv_id": "2206.04355", "title": "Graph Attention Multi-Layer Perceptron", "year": 2022}, {"arxiv_id": "2007.01570", "title": "Scaling Graph Neural Networks with Approximate PageRank", "year": 2020}, {"arxiv_id": "2010.13993", "title": "Combining Label Propagation and Simple Models Out-performs Graph Neural Networks", "year": 2020}], "gt_label": [1, 1, 1, 1, 1, 1], "date": "2022-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_445", "valid": true}
{"query": "Could you provide me some works about the problem of exploding variance of the RP gradients due to the chaotic nature of environments?", "cited_paper": [{"arxiv_id": "1902.01240", "title": "PIPPS: Flexible Model-Based Policy Search Robust to the Curse of Chaos", "year": 2019}, {"arxiv_id": "2111.05803", "title": "Gradients are Not All You Need", "year": 2021}], "gt_label": [1, 1], "date": "2021-11", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_446", "valid": true}
{"query": "What research introduced methods that adapt the training procedure of the classifier itself?", "cited_paper": [{"arxiv_id": "1906.12340", "title": "Using Self-Supervised Learning Can Improve Model Robustness and Uncertainty", "year": 2019}, {"arxiv_id": "1506.02142", "title": "Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning", "year": 2015}, {"arxiv_id": "1803.04386", "title": "Flipout: Efficient Pseudo-Independent Weight Perturbations on Mini-Batches", "year": 2018}, {"arxiv_id": "1612.01474", "title": "Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles", "year": 2016}, {"arxiv_id": "1906.02629", "title": "When Does Label Smoothing Help?", "year": 2019}, {"arxiv_id": "2007.08259", "title": "Transferable Calibration with Lower Bias and Variance in Domain Adaptation", "year": 2020}, {"arxiv_id": "1905.11001", "title": "On Mixup Training: Improved Calibration and Predictive Uncertainty for Deep Neural Networks", "year": 2019}, {"arxiv_id": "1710.09412", "title": "mixup: Beyond Empirical Risk Minimization", "year": 2017}, {"arxiv_id": "1806.01768", "title": "Evidential Deep Learning to Quantify Classification Uncertainty", "year": 2018}, {"arxiv_id": "2012.10923", "title": "Towards Trustworthy Predictions from Deep Neural Networks with Fast Adversarial Calibration", "year": 2020}, {"arxiv_id": "2002.06470", "title": "Pitfalls of In-Domain Uncertainty Estimation and Ensembling in Deep Learning", "year": 2020}], "gt_label": [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], "date": "2020-12", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_447", "valid": true}
{"query": "Which works look at achieving -optimal reward in non-convex settings?", "cited_paper": [{"arxiv_id": "2107.04518", "title": "Optimal Gradient-based Algorithms for Non-concave Bandit Optimization", "year": 2021}, {"arxiv_id": "2103.16082", "title": "Optimal Stochastic Nonconvex Optimization with Bandit Feedback", "year": 2021}], "gt_label": [1, 1], "date": "2021-07", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_448", "valid": true}
{"query": "Which work proposed the Mirror Learning algorithm?", "cited_paper": [{"arxiv_id": "2201.02373", "title": "Mirror Learning: A Unifying Framework of Policy Optimisation", "year": 2022}], "gt_label": [1], "date": "2022-01", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_449", "valid": true}
{"query": "What works have explored video human-object interaction detection?", "cited_paper": [{"arxiv_id": "2105.11731", "title": "ST-HOI: A Spatial-Temporal Baseline for Human-Object Interaction Detection in Videos", "year": 2021}], "gt_label": [1], "date": "2021-05", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_450", "valid": true}
{"query": "What are some representative works about graph embedding-based methods?", "cited_paper": [{"arxiv_id": "1902.10197", "title": "RotatE: Knowledge Graph Embedding by Relational Rotation in Complex Space", "year": 2019}, {"arxiv_id": "1707.01476", "title": "Convolutional 2D Knowledge Graph Embeddings", "year": 2017}, {"arxiv_id": "1606.06357", "title": "Complex Embeddings for Simple Link Prediction", "year": 2016}, {"arxiv_id": "1510.04935", "title": "Holographic Embeddings of Knowledge Graphs", "year": 2015}, {"arxiv_id": "1711.04071", "title": "KBGAN: Adversarial Learning for Knowledge Graph Embeddings", "year": 2017}, {"arxiv_id": "1901.09590", "title": "TuckER: Tensor Factorization for Knowledge Graph Completion", "year": 2019}, {"arxiv_id": "1412.6575", "title": "Embedding Entities and Relations for Learning and Inference in Knowledge Bases", "year": 2014}, {"arxiv_id": "2002.05969", "title": "Query2box: Reasoning over Knowledge Graphs in Vector Space using Box Embeddings", "year": 2020}, {"arxiv_id": "2101.00345", "title": "Modeling Fine-Grained Entity Types with Box Embeddings", "year": 2021}], "gt_label": [1, 1, 1, 1, 1, 1, 1, 1, 1], "date": "2021-01", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_451", "valid": true}
{"query": "What are some works related to the Mean Teacher paradigm?", "cited_paper": [{"arxiv_id": "2105.00097", "title": "Self-supervised Augmentation Consistency for Adapting Semantic Segmentation", "year": 2021}, {"arxiv_id": "2111.14887", "title": "DAFormer: Improving Network Architectures and Training Strategies for Domain-Adaptive Semantic Segmentation", "year": 2021}, {"arxiv_id": "2101.10979", "title": "Prototypical Pseudo Label Denoising and Target Structure Learning for Domain Adaptive Semantic Segmentation", "year": 2021}, {"arxiv_id": "2106.09018", "title": "End-to-End Semi-Supervised Object Detection with Soft Teacher", "year": 2021}, {"arxiv_id": "2303.08348", "title": "Active Teacher for Semi-Supervised Object Detection", "year": 2023}, {"arxiv_id": "2203.16089", "title": "Omni-DETR: Omni-Supervised Object Detection with Transformers", "year": 2022}, {"arxiv_id": "2309.07914", "title": "ALWOD: Active Learning for Weakly-Supervised Object Detection", "year": 2023}, {"arxiv_id": "2305.03034", "title": "Contrastive Mean Teacher for Domain Adaptive Object Detectors", "year": 2023}, {"arxiv_id": "2111.13216", "title": "Cross-Domain Adaptive Teacher for Object Detection", "year": 2021}, {"arxiv_id": "2001.01526", "title": "Mutual Mean-Teaching: Pseudo Label Refinery for Unsupervised Domain Adaptation on Person Re-identification", "year": 2020}, {"arxiv_id": "2012.08733", "title": "Exploiting Sample Uncertainty for Domain Adaptive Person Re-Identification", "year": 2020}, {"arxiv_id": "2112.14025", "title": "Delving into Probabilistic Uncertainty for Unsupervised Domain Adaptive Person Re-Identification", "year": 2021}], "gt_label": [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], "date": "2023-09", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_452", "valid": true}
{"query": "Could you provide me studies where they utilized self-supervision and few-shot prompting to address the scalability of large language model tool-use?", "cited_paper": [{"arxiv_id": "2302.04761", "title": "Toolformer: Language Models Can Teach Themselves to Use Tools", "year": 2023}, {"arxiv_id": "2205.12255", "title": "TALM: Tool Augmented Language Models", "year": 2022}], "gt_label": [1, 1], "date": "2023-02", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_453", "valid": true}
{"query": "What papers depict studies of sample complexity of posterior sampling in tabular settings?", "cited_paper": [{"arxiv_id": "1402.0635", "title": "Generalization and Exploration via Randomized Value Functions", "year": 2014}, {"arxiv_id": "1906.02870", "title": "Worst-Case Regret Bounds for Exploration via Randomized Value Functions", "year": 2019}], "gt_label": [1, 1], "date": "2019-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_454", "valid": true}
{"query": "Can you provide the studies that use additional guiding signals to incorporate explicit control in conditional diffusion models?", "cited_paper": [{"arxiv_id": "2211.14305", "title": "SpaText: Spatio-Textual Representation for Controllable Image Generation", "year": 2022}, {"arxiv_id": "2112.10752", "title": "High-Resolution Image Synthesis with Latent Diffusion Models", "year": 2021}, {"arxiv_id": "2211.09800", "title": "InstructPix2Pix: Learning to Follow Image Editing Instructions", "year": 2022}], "gt_label": [1, 1, 1], "date": "2022-11", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_455", "valid": true}
{"query": "What research papers discuss memory-based methods as a knowledge editing paradigm?", "cited_paper": [{"arxiv_id": "2206.06520", "title": "Memory-Based Model Editing at Scale", "year": 2022}, {"arxiv_id": "2301.09785", "title": "Transformer-Patcher: One Mistake worth One Neuron", "year": 2023}, {"arxiv_id": "2210.03329", "title": "Calibrating Factual Knowledge in Pretrained Language Models", "year": 2022}, {"arxiv_id": "2305.12740", "title": "Can We Edit Factual Knowledge by In-Context Learning?", "year": 2023}], "gt_label": [1, 1, 1, 1], "date": "2023-05", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_456", "valid": true}
{"query": "Any studies about generating adversarial examples in textual domains?", "cited_paper": [{"arxiv_id": "1707.07328", "title": "Adversarial Examples for Evaluating Reading Comprehension Systems", "year": 2017}, {"arxiv_id": "1804.07998", "title": "Generating Natural Language Adversarial Examples", "year": 2018}, {"arxiv_id": "2106.00245", "title": "Adversarial VQA: A New Benchmark for Evaluating the Robustness of VQA Models", "year": 2021}, {"arxiv_id": "1712.06751", "title": "HotFlip: White-Box Adversarial Examples for Text Classification", "year": 2017}, {"arxiv_id": "1908.07125", "title": "Universal Adversarial Triggers for Attacking and Analyzing NLP", "year": 2019}], "gt_label": [1, 1, 1, 1, 1], "date": "2021-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_457", "valid": true}
{"query": "What work highlights computational challenges and memory issues in bi-level optimization-based methods?", "cited_paper": [{"arxiv_id": "2211.10586", "title": "Scaling Up Dataset Distillation to ImageNet-1K with Constant Memory", "year": 2022}], "gt_label": [1], "date": "2022-11", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_458", "valid": true}
{"query": "Which works describe FL strategies for handling communication burden issues?", "cited_paper": [], "gt_label": [], "date": "2019-09", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_459", "valid": true}
{"query": "What papers discussed handcrafted prompts for specific tasks?", "cited_paper": [{"arxiv_id": "2010.15980", "title": "AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts", "year": 2020}], "gt_label": [1], "date": "2020-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_460", "valid": true}
{"query": "What are some works that have advanced image editing methods achieving more accurate attribute modification textually?", "cited_paper": [{"arxiv_id": "2208.01626", "title": "Prompt-to-Prompt Image Editing with Cross Attention Control", "year": 2022}, {"arxiv_id": "2211.09800", "title": "InstructPix2Pix: Learning to Follow Image Editing Instructions", "year": 2022}, {"arxiv_id": "2211.09794", "title": "Null-text Inversion for Editing Real Images using Guided Diffusion Models", "year": 2022}, {"arxiv_id": "2305.16807", "title": "Negative-prompt Inversion: Fast Image Inversion for Editing with Text-guided Diffusion Models", "year": 2023}], "gt_label": [1, 1, 1, 1], "date": "2023-05", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_461", "valid": true}
{"query": "Which research papers studied methods based on model mixture for Full Model Personalization?", "cited_paper": [{"arxiv_id": "2002.05516", "title": "Federated Learning of a Mixture of Global and Local Models", "year": 2020}, {"arxiv_id": "2003.13461", "title": "Adaptive Personalized Federated Learning", "year": 2020}, {"arxiv_id": "2002.10619", "title": "Three Approaches for Personalization with Applications to Federated Learning", "year": 2020}], "gt_label": [1, 1, 1], "date": "2020-03", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_462", "valid": true}
{"query": "Can you mention studies that are trained on massive pairs of images and captions, enabling them to generate detailed image content conditioned on textual instruction?", "cited_paper": [{"arxiv_id": "2102.12092", "title": "Zero-Shot Text-to-Image Generation", "year": 2021}, {"arxiv_id": "2204.06125", "title": "Hierarchical Text-Conditional Image Generation with CLIP Latents", "year": 2022}, {"arxiv_id": "2112.10752", "title": "High-Resolution Image Synthesis with Latent Diffusion Models", "year": 2021}], "gt_label": [1, 1, 1], "date": "2022-04", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_463", "valid": true}
{"query": "What studies further propagated the token-level edit operation approach proposed by LaserTagger?", "cited_paper": [{"arxiv_id": "1910.02893", "title": "Parallel Iterative Edit Models for Local Sequence Transduction", "year": 2019}, {"arxiv_id": "2005.12592", "title": "GECToR -- Grammatical Error Correction: Tag, Not Rewrite", "year": 2020}], "gt_label": [1, 1], "date": "2020-05", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_464", "valid": true}
{"query": "Which paper first introduced the concept of fully unsupervised anomaly detection?", "cited_paper": [{"arxiv_id": "2203.03962", "title": "Generative Cooperative Learning for Unsupervised Video Anomaly Detection", "year": 2022}], "gt_label": [1], "date": "2022-03", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_465", "valid": true}
{"query": "Which study introduced a metric based on the sample influence score of the optimal empirical risk in CoreSet selection?", "cited_paper": [{"arxiv_id": "2310.14664", "title": "Data Pruning via Moving-one-Sample-out", "year": 2023}], "gt_label": [1], "date": "2023-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_466", "valid": true}
{"query": "What papers study hallucination issues in MLLMs?", "cited_paper": [{"arxiv_id": "2308.15126", "title": "Evaluation and Analysis of Hallucination in Large Vision-Language Models", "year": 2023}], "gt_label": [1], "date": "2023-08", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_467", "valid": true}
{"query": "What works learn disentangled representations from time-series data?", "cited_paper": [{"arxiv_id": "1605.06336", "title": "Unsupervised Feature Extraction by Time-Contrastive Learning and Nonlinear ICA", "year": 2016}], "gt_label": [1], "date": "2016-05", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_468", "valid": true}
{"query": "Could you provide some examples of diffusion models that involve different number of denoising steps and parameterization of transformation?", "cited_paper": [{"arxiv_id": "1503.03585", "title": "Deep Unsupervised Learning using Nonequilibrium Thermodynamics", "year": 2015}, {"arxiv_id": "2006.11239", "title": "Denoising Diffusion Probabilistic Models", "year": 2020}, {"arxiv_id": "2010.02502", "title": "Denoising Diffusion Implicit Models", "year": 2020}, {"arxiv_id": "2206.00927", "title": "DPM-Solver: A Fast ODE Solver for Diffusion Probabilistic Model Sampling in Around 10 Steps", "year": 2022}, {"arxiv_id": "2211.01095", "title": "DPM-Solver++: Fast Solver for Guided Sampling of Diffusion Probabilistic Models", "year": 2022}, {"arxiv_id": "2202.09778", "title": "Pseudo Numerical Methods for Diffusion Models on Manifolds", "year": 2022}], "gt_label": [1, 1, 1, 1, 1, 1], "date": "2022-11", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_469", "valid": true}
{"query": "Which works use consistency between model generated content and external information for factuality detection in LLM?", "cited_paper": [{"arxiv_id": "2311.09000", "title": "Factcheck-Bench: Fine-Grained Evaluation Benchmark for Automatic Fact-checkers", "year": 2023}, {"arxiv_id": "2210.08726", "title": "RARR: Researching and Revising What Language Models Say, Using Language Models", "year": 2022}, {"arxiv_id": "2307.13528", "title": "FacTool: Factuality Detection in Generative AI -- A Tool Augmented Framework for Multi-Task and Multi-Domain Scenarios", "year": 2023}], "gt_label": [1, 1, 1], "date": "2023-11", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_470", "valid": true}
{"query": "Which papers discussed the use of gradient guidance to increase the occurrence of a desired attribute in discrete generative models?", "cited_paper": [{"arxiv_id": "1612.00005", "title": "Plug & Play Generative Networks: Conditional Iterative Generation of Images in Latent Space", "year": 2016}], "gt_label": [1], "date": "2016-12", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_471", "valid": true}
{"query": "Which works experimentally proved that adding Gaussian noise during training increases adversarial robustness?", "cited_paper": [{"arxiv_id": "1608.08967", "title": "Robustness of classifiers: from adversarial to random noise", "year": 2016}, {"arxiv_id": "1901.10513", "title": "Adversarial Examples Are a Natural Consequence of Test Error in Noise", "year": 2019}], "gt_label": [1, 1], "date": "2019-01", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_472", "valid": true}
{"query": "Could you name some studies that have explored the influence of asking LLMs to respond as a particular person?", "cited_paper": [{"arxiv_id": "2204.10825", "title": "Meet Your Favorite Character: Open-domain Chatbot Mimicking Fictional Characters with only a Few Utterances", "year": 2022}, {"arxiv_id": "1909.05858", "title": "CTRL: A Conditional Transformer Language Model for Controllable Generation", "year": 2019}], "gt_label": [1, 1], "date": "2022-04", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_473", "valid": true}
{"query": "What papers provide a discussion or implementation of the method of 'prompting'?", "cited_paper": [{"arxiv_id": "2109.01134", "title": "Learning to Prompt for Vision-Language Models", "year": 2021}, {"arxiv_id": "2203.05557", "title": "Conditional Prompt Learning for Vision-Language Models", "year": 2022}, {"arxiv_id": "2203.17274", "title": "Exploring Visual Prompts for Adapting Large-Scale Models", "year": 2022}, {"arxiv_id": "2212.04412", "title": "Task Bias in Vision-Language Models", "year": 2022}], "gt_label": [1, 1, 1, 1], "date": "2022-12", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_474", "valid": true}
{"query": "Are there any studies that have applied large-scale pre-trained models to the UCDR task?", "cited_paper": [{"arxiv_id": "1902.00113", "title": "Episodic Training for Domain Generalization", "year": 2019}, {"arxiv_id": "2208.02803", "title": "Semantic Data Augmentation based Distance Metric Learning for Domain Generalization", "year": 2022}, {"arxiv_id": "2108.08356", "title": "Universal Cross-Domain Retrieval: Generalizing Across Classes and Domains", "year": 2021}], "gt_label": [1, 1, 1], "date": "2022-08", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_475", "valid": true}
{"query": "What work is similar to the researcher's use of context distillation in knowledge editing?", "cited_paper": [{"arxiv_id": "2112.00861", "title": "A General Language Assistant as a Laboratory for Alignment", "year": 2021}], "gt_label": [1], "date": "2021-12", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_476", "valid": true}
{"query": "Which studies focused on vision-based GUI navigation using GPT-4V?", "cited_paper": [{"arxiv_id": "2311.07562", "title": "GPT-4V in Wonderland: Large Multimodal Models for Zero-Shot Smartphone GUI Navigation", "year": 2023}, {"arxiv_id": "2312.13108", "title": "ASSISTGUI: Task-Oriented Desktop Graphical User Interface Automation", "year": 2023}, {"arxiv_id": "2312.13771", "title": "AppAgent: Multimodal Agents as Smartphone Users", "year": 2023}], "gt_label": [1, 1, 1], "date": "2023-12", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_477", "valid": true}
{"query": "Could you provide me the works that introduced policy-based methods for performing adaptive experimentation?", "cited_paper": [{"arxiv_id": "2111.02329", "title": "Implicit Deep Adaptive Design: Policy-Based Experimental Design without Likelihoods", "year": 2021}, {"arxiv_id": "2103.02438", "title": "Deep Adaptive Design: Amortizing Sequential Bayesian Experimental Design", "year": 2021}], "gt_label": [1, 1], "date": "2021-11", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_478", "valid": true}
{"query": "Which works focus on developing more effective pretraining losses beyond the autoregressive or masked language modeling objectives?", "cited_paper": [{"arxiv_id": "1906.08237", "title": "XLNet: Generalized Autoregressive Pretraining for Language Understanding", "year": 2019}, {"arxiv_id": "2003.10555", "title": "ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators", "year": 2020}, {"arxiv_id": "2002.12804", "title": "UniLMv2: Pseudo-Masked Language Models for Unified Language Model Pre-Training", "year": 2020}], "gt_label": [1, 1, 1], "date": "2020-03", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_479", "valid": true}
{"query": "Are there any works on multilingual ARA models?", "cited_paper": [{"arxiv_id": "2305.13478", "title": "Automatic Readability Assessment for Closely Related Languages", "year": 2023}], "gt_label": [1], "date": "2023-05", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_480", "valid": true}
{"query": "Are there studies adapting popular uncertainty tools in Bayesian Uncertainty Estimation literature to generative LLMs?", "cited_paper": [{"arxiv_id": "2002.07650", "title": "Uncertainty Estimation in Autoregressive Structured Prediction", "year": 2020}], "gt_label": [1], "date": "2020-02", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_481", "valid": true}
{"query": "Could you provide me some works about knowledge retrieval from prompts?", "cited_paper": [{"arxiv_id": "2305.14160", "title": "Label Words are Anchors: An Information Flow Perspective for Understanding In-Context Learning", "year": 2023}], "gt_label": [1], "date": "2023-05", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_482", "valid": true}
{"query": "What studies have been conducted on multilingual Large Language Models (LLMs) with a focus on hundreds of languages?", "cited_paper": [{"arxiv_id": "2010.11934", "title": "mT5: A massively multilingual pre-trained text-to-text transformer", "year": 2020}, {"arxiv_id": "2303.08774", "title": "GPT-4 Technical Report", "year": 2023}], "gt_label": [1, 1], "date": "2023-12", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_483", "valid": true}
{"query": "Could you provide me some works about visual pretraining that emphasized contrastive learning?", "cited_paper": [{"arxiv_id": "2002.05709", "title": "A Simple Framework for Contrastive Learning of Visual Representations", "year": 2020}, {"arxiv_id": "1911.05722", "title": "Momentum Contrast for Unsupervised Visual Representation Learning", "year": 2019}, {"arxiv_id": "2104.14294", "title": "Emerging Properties in Self-Supervised Vision Transformers", "year": 2021}], "gt_label": [1, 1, 1], "date": "2021-04", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_484", "valid": true}
{"query": "What studies have focused on architectures that exhibit combinatorial generalization such as transformers, graph neural networks, and bilinear models?", "cited_paper": [{"arxiv_id": "1706.03762", "title": "Attention Is All You Need", "year": 2017}, {"arxiv_id": "2102.09544", "title": "Combinatorial optimization and reasoning with graph neural networks", "year": 2021}], "gt_label": [1, 1], "date": "2021-02", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_485", "valid": true}
{"query": "What paper proposed the averaged stochastic approximation (NASA) to obtain a better rate for non-convex objectives?", "cited_paper": [], "gt_label": [], "date": "2018-12", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_486", "valid": true}
{"query": "Which papers concentrate on decomposing complex questions into sub-questions?", "cited_paper": [{"arxiv_id": "2205.10625", "title": "Least-to-Most Prompting Enables Complex Reasoning in Large Language Models", "year": 2022}, {"arxiv_id": "2212.04092", "title": "Successive Prompting for Decomposing Complex Questions", "year": 2022}], "gt_label": [1, 1], "date": "2022-12", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_487", "valid": true}
{"query": "Which paper proposed multi-channel equivariant graph networks?", "cited_paper": [{"arxiv_id": "2208.06073", "title": "Conditional Antibody Design as 3D Equivariant Graph Translation", "year": 2022}], "gt_label": [1], "date": "2022-08", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_488", "valid": true}
{"query": "Can you name the papers where VeRA, the method that reduces the number of parameters with the help of random projections, is mentioned?", "cited_paper": [{"arxiv_id": "2310.11454", "title": "VeRA: Vector-based Random Matrix Adaptation", "year": 2023}], "gt_label": [1], "date": "2023-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_489", "valid": true}
{"query": "Any studies about prompt fine-tuning for vision models to adapt image models from one image task to another?", "cited_paper": [{"arxiv_id": "2203.12119", "title": "Visual Prompt Tuning", "year": 2022}, {"arxiv_id": "1806.11146", "title": "Adversarial Reprogramming of Neural Networks", "year": 2018}], "gt_label": [1, 1], "date": "2022-03", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_490", "valid": true}
{"query": "Can you mention some sequence models proposing the use of multiple time scales, akin to the multi-rate mechanism in this study?", "cited_paper": [{"arxiv_id": "2103.05487", "title": "UnICORNN: A recurrent model for learning very long time dependencies", "year": 2021}, {"arxiv_id": "2110.04744", "title": "Long Expressive Memory for Sequence Modeling", "year": 2021}], "gt_label": [1, 1], "date": "2021-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_491", "valid": true}
{"query": "What works propose strategies for face capture that are more easily accessible and convenient for daily users?", "cited_paper": [{"arxiv_id": "2003.13845", "title": "AvatarMe: Realistically Renderable 3D Facial Reconstruction \"in-the-wild\"", "year": 2020}, {"arxiv_id": "2305.06077", "title": "Relightify: Relightable 3D Faces from a Single Image via Diffusion Models", "year": 2023}, {"arxiv_id": "2303.11686", "title": "Learning a 3D Morphable Face Reflectance Model from Low-cost Data", "year": 2023}, {"arxiv_id": "2004.02711", "title": "A Morphable Face Albedo Model", "year": 2020}, {"arxiv_id": "2004.03458", "title": "Learning Formation of Physically-Based Face Attributes", "year": 2020}, {"arxiv_id": "2305.09641", "title": "FitMe: Deep Photorealistic 3D Morphable Model Avatars", "year": 2023}, {"arxiv_id": "2101.05356", "title": "Practical Face Reconstruction via Differentiable Ray Tracing", "year": 2021}], "gt_label": [1, 1, 1, 1, 1, 1, 1], "date": "2023-05", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_492", "valid": true}
{"query": "Which early works employed ConvNets and inverse perspective mapping (IPM) for mapping features from perspective view to BEV view?", "cited_paper": [{"arxiv_id": "1611.07759", "title": "Multi-View 3D Object Detection Network for Autonomous Driving", "year": 2016}, {"arxiv_id": "2008.05711", "title": "Lift, Splat, Shoot: Encoding Images From Arbitrary Camera Rigs by Implicitly Unprojecting to 3D", "year": 2020}, {"arxiv_id": "1906.03560", "title": "Cross-view Semantic Segmentation for Sensing Surroundings", "year": 2019}], "gt_label": [1, 1, 1], "date": "2020-08", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_493", "valid": true}
{"query": "What researches made use of auxiliary data such as human-annotated attribute information, text description or knowledge graph in Zero-Shot Learning (ZSL)?", "cited_paper": [{"arxiv_id": "1605.05395", "title": "Learning Deep Representations of Fine-grained Visual Descriptions", "year": 2016}, {"arxiv_id": "1711.06526", "title": "Multi-Label Zero-Shot Learning with Structured Knowledge Graphs", "year": 2017}], "gt_label": [1, 1], "date": "2017-11", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_494", "valid": true}
{"query": "What research papers prioritize salient weights for PTQ in LLMs?", "cited_paper": [{"arxiv_id": "2306.03078", "title": "SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression", "year": 2023}, {"arxiv_id": "2306.00978", "title": "AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration", "year": 2023}, {"arxiv_id": "2306.07629", "title": "SqueezeLLM: Dense-and-Sparse Quantization", "year": 2023}], "gt_label": [1, 1, 1], "date": "2023-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_495", "valid": true}
{"query": "Any studies showing that LLM hidden states can effectively represent a task defined by input-output pairs?", "cited_paper": [{"arxiv_id": "2310.15213", "title": "Function Vectors in Large Language Models", "year": 2023}], "gt_label": [1], "date": "2023-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_496", "valid": true}
{"query": "What studies gave rise to prompt-based learning in LLMs?", "cited_paper": [{"arxiv_id": "1810.04805", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "year": 2018}, {"arxiv_id": "2005.14165", "title": "Language Models are Few-Shot Learners", "year": 2020}, {"arxiv_id": "2303.08774", "title": "GPT-4 Technical Report", "year": 2023}, {"arxiv_id": "2205.01068", "title": "OPT: Open Pre-trained Transformer Language Models", "year": 2022}], "gt_label": [1, 1, 1, 1], "date": "2023-03", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_497", "valid": true}
{"query": "What studies apply slot attention for object discovery?", "cited_paper": [{"arxiv_id": "2006.15055", "title": "Object-Centric Learning with Slot Attention", "year": 2020}, {"arxiv_id": "1907.13052", "title": "GENESIS: Generative Scene Inference and Sampling with Object-Centric Latent Representations", "year": 2019}, {"arxiv_id": "2111.12594", "title": "Conditional Object-Centric Learning from Video", "year": 2021}, {"arxiv_id": "2303.17842", "title": "Shepherding Slots to Objects: Towards Stable and Robust Object-Centric Learning", "year": 2023}], "gt_label": [1, 1, 1, 1], "date": "2023-03", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_498", "valid": true}
{"query": "What works proposed method for experimental design for causal discovery in a non-BOED setting in the presence of cycles?", "cited_paper": [{"arxiv_id": "2205.10083", "title": "A Unified Experiment Design Approach for Cyclic and Acyclic Causal Models", "year": 2022}], "gt_label": [1], "date": "2022-05", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_499", "valid": true}
{"query": "What study employs a contextual word retrieval task where the model is tasked with finding corresponding words and sentences across parallel corpora?", "cited_paper": [{"arxiv_id": "2002.03518", "title": "Multilingual Alignment of Contextual Word Representations", "year": 2020}], "gt_label": [1], "date": "2020-02", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_500", "valid": true}
{"query": "Which work leverages graph generation as the training objective in generative self-supervised learning?", "cited_paper": [{"arxiv_id": "2006.15437", "title": "GPT-GNN: Generative Pre-Training of Graph Neural Networks", "year": 2020}], "gt_label": [1], "date": "2020-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_501", "valid": true}
{"query": "Could you provide me some studies that propose non-parametric approaches to post-hoc calibration methods?", "cited_paper": [{"arxiv_id": "2003.07329", "title": "Mix-n-Match: Ensemble and Compositional Methods for Uncertainty Calibration in Deep Learning", "year": 2020}, {"arxiv_id": "1805.10915", "title": "Dirichlet-based Gaussian Processes for Large-scale Calibrated Classification", "year": 2018}], "gt_label": [1, 1], "date": "2020-03", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_502", "valid": true}
{"query": "Could you provide me some studies about object hallucination in MLLMs?", "cited_paper": [{"arxiv_id": "2305.10355", "title": "Evaluating Object Hallucination in Large Vision-Language Models", "year": 2023}], "gt_label": [1], "date": "2023-05", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_503", "valid": true}
{"query": "Which studies proposed methods for learning 3D-aware image and geometry generation with implicit neural radiance fields as generators?", "cited_paper": [{"arxiv_id": "2007.02442", "title": "GRAF: Generative Radiance Fields for 3D-Aware Image Synthesis", "year": 2020}, {"arxiv_id": "2012.00926", "title": "pi-GAN: Periodic Implicit Generative Adversarial Networks for 3D-Aware Image Synthesis", "year": 2020}], "gt_label": [1, 1], "date": "2020-12", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_504", "valid": true}
{"query": "Could you provide me some studies on global stability that contributed to the Probably Eventually Correct (PEC) learning model?", "cited_paper": [{"arxiv_id": "2003.00563", "title": "An Equivalence Between Private Classification and Online Prediction", "year": 2020}, {"arxiv_id": "2012.03893", "title": "Sample-efficient proper PAC learning with approximate differential privacy", "year": 2020}], "gt_label": [1, 1], "date": "2020-12", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_505", "valid": true}
{"query": "Which studies have aimed at generalization to underrepresented languages in LLM-based evaluation methodologies?", "cited_paper": [{"arxiv_id": "2309.07462", "title": "Are Large Language Model-based Evaluators the Solution to Scaling Up Multilingual Evaluation?", "year": 2023}], "gt_label": [1], "date": "2023-09", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_506", "valid": true}
{"query": "What studies have aimed to capture functional requirements in software development by generating code-like outlines using in-context learning?", "cited_paper": [{"arxiv_id": "2305.10679", "title": "Think Outside the Code: Brainstorming Boosts Large Language Models in Code Generation", "year": 2023}], "gt_label": [1], "date": "2023-05", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_507", "valid": true}
{"query": "Could you provide me some references that evaluate the ability of LMs to reason about emerging entities?", "cited_paper": [{"arxiv_id": "2102.01951", "title": "Mind the Gap: Assessing Temporal Generalization in Neural Language Models", "year": 2021}, {"arxiv_id": "2106.15110", "title": "Time-Aware Language Models as Temporal Knowledge Bases", "year": 2021}, {"arxiv_id": "2207.13332", "title": "RealTime QA: What's the Answer Right Now?", "year": 2022}], "gt_label": [1, 1, 1], "date": "2022-07", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_508", "valid": true}
{"query": "What papers are about utilizing machine-learned predictions for designing efficient offline algorithms?", "cited_paper": [{"arxiv_id": "2107.09770", "title": "Faster Matchings via Learned Duals", "year": 2021}, {"arxiv_id": "2204.12055", "title": "Faster Fundamental Graph Algorithms via Learned Predictions", "year": 2022}, {"arxiv_id": "2205.09961", "title": "Discrete-Convex-Analysis-Based Framework for Warm-Starting Algorithms with Predictions", "year": 2022}, {"arxiv_id": "2207.12911", "title": "Learning-Augmented Maximum Flow", "year": 2022}], "gt_label": [1, 1, 1, 1], "date": "2022-07", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_509", "valid": true}
{"query": "Could you provide me a work that introduced EL2N score as a measure of importance in CoreSet selection?", "cited_paper": [{"arxiv_id": "2107.07075", "title": "Deep Learning on a Data Diet: Finding Important Examples Early in Training", "year": 2021}], "gt_label": [1], "date": "2021-07", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_510", "valid": true}
{"query": "Which works proposed reinforcement learning methods to tackle challenges of behavior cloning in autonomous driving?", "cited_paper": [{"arxiv_id": "1807.00412", "title": "Learning to Drive in a Day", "year": 2018}, {"arxiv_id": "1705.01196", "title": "Navigating Occluded Intersections with Autonomous Vehicles using Deep Reinforcement Learning", "year": 2017}, {"arxiv_id": "2212.11419", "title": "Imitation Is Not Enough: Robustifying Imitation with Reinforcement Learning for Challenging Driving Scenarios", "year": 2022}], "gt_label": [1, 1, 1], "date": "2022-12", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_511", "valid": true}
{"query": "Which paper involves the use of the Faster-RCNN model in Vision-Language Pre-training?", "cited_paper": [{"arxiv_id": "1506.01497", "title": "Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks", "year": 2015}], "gt_label": [1], "date": "2015-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_512", "valid": true}
{"query": "What works examined training agents through reinforcement learning on the MiniWob web environment?", "cited_paper": [{"arxiv_id": "1802.08802", "title": "Reinforcement Learning on Web Interfaces Using Workflow-Guided Exploration", "year": 2018}, {"arxiv_id": "1812.09195", "title": "Learning to Navigate the Web", "year": 2018}], "gt_label": [1, 1], "date": "2018-12", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_513", "valid": true}
{"query": "What works proposed hierarchical BERT models designed for extractive summarization?", "cited_paper": [{"arxiv_id": "1905.06566", "title": "HIBERT: Document Level Pre-training of Hierarchical Bidirectional Transformers for Document Summarization", "year": 2019}, {"arxiv_id": "2010.08242", "title": "Unsupervised Extractive Summarization by Pre-training Hierarchical Transformers", "year": 2020}, {"arxiv_id": "2203.09629", "title": "HiStruct+: Improving Extractive Text Summarization with Hierarchical Structure Information", "year": 2022}], "gt_label": [1, 1, 1], "date": "2022-03", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_514", "valid": true}
{"query": "Which work related to the researcher's work examines tasks like distilling a persona-conditioned language model?", "cited_paper": [{"arxiv_id": "2206.11349", "title": "Prompt Injection: Parameterization of Fixed Inputs", "year": 2022}], "gt_label": [1], "date": "2022-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_515", "valid": true}
{"query": "Could you provide me studies focused on the grounding capabilities of LVLMs?", "cited_paper": [{"arxiv_id": "2305.11175", "title": "VisionLLM: Large Language Model is also an Open-Ended Decoder for Vision-Centric Tasks", "year": 2023}, {"arxiv_id": "2310.09478", "title": "MiniGPT-v2: large language model as a unified interface for vision-language multi-task learning", "year": 2023}], "gt_label": [1, 1], "date": "2023-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_516", "valid": true}
{"query": "Which work is related to private prediction?", "cited_paper": [{"arxiv_id": "1803.10266", "title": "Privacy-preserving Prediction", "year": 2018}], "gt_label": [1], "date": "2018-03", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_517", "valid": true}
{"query": "Which studies incorporated the use of diffusion models in their work?", "cited_paper": [{"arxiv_id": "2305.11846", "title": "Any-to-Any Generation via Composable Diffusion", "year": 2023}, {"arxiv_id": "2307.05222", "title": "Emu: Generative Pretraining in Multimodality", "year": 2023}], "gt_label": [1, 1], "date": "2023-07", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_518", "valid": true}
{"query": "What work proposed a method of retraining classifiers on 're-weighting' data to reduce reliance on spurious features?", "cited_paper": [{"arxiv_id": "2204.02937", "title": "Last Layer Re-Training is Sufficient for Robustness to Spurious Correlations", "year": 2022}], "gt_label": [1], "date": "2022-04", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_519", "valid": true}
{"query": "Could you provide me research that allows for controlled text-based scene editing by finetuning an image diffusion model?", "cited_paper": [{"arxiv_id": "2306.13455", "title": "DreamEditor: Text-Driven 3D Scene Editing with Neural Fields", "year": 2023}], "gt_label": [1], "date": "2023-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_520", "valid": true}
{"query": "Could you provide me some work that applied transformer or its variants into TAD head?", "cited_paper": [{"arxiv_id": "2204.02932", "title": "An Empirical Study of End-to-End Temporal Action Detection", "year": 2022}, {"arxiv_id": "2102.01894", "title": "Relaxed Transformer Decoders for Direct Action Proposal Generation", "year": 2021}, {"arxiv_id": "2202.07925", "title": "ActionFormer: Localizing Moments of Actions with Transformers", "year": 2022}, {"arxiv_id": "2207.07097", "title": "ReAct: Temporal Action Detection with Relational Queries", "year": 2022}], "gt_label": [1, 1, 1, 1], "date": "2022-07", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_521", "valid": true}
{"query": "What papers have incorporated the use of NLEs in fields beyond NLP, such as in computer vision, medical field, and self-driving cars?", "cited_paper": [{"arxiv_id": "1807.09685", "title": "Grounding Visual Explanations", "year": 2018}, {"arxiv_id": "1811.10830", "title": "From Recognition to Cognition: Visual Commonsense Reasoning", "year": 2018}, {"arxiv_id": "2106.13876", "title": "Knowledge-Grounded Self-Rationalization via Extractive and Natural Language Explanations", "year": 2021}, {"arxiv_id": "2207.04343", "title": "Explaining Chest X-ray Pathologies in Natural Language", "year": 2022}, {"arxiv_id": "1807.11546", "title": "Textual Explanations for Self-Driving Vehicles", "year": 2018}], "gt_label": [1, 1, 1, 1, 1], "date": "2022-07", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_522", "valid": true}
{"query": "Which research proposes to enhance the reasoning process in LLMs by framing thoughts as graphs?", "cited_paper": [{"arxiv_id": "2305.16582", "title": "Beyond Chain-of-Thought, Effective Graph-of-Thought Reasoning in Language Models", "year": 2023}], "gt_label": [1], "date": "2023-05", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_523", "valid": true}
{"query": "Any studies arguing for centering language models evaluation on how models will be used in practice?", "cited_paper": [{"arxiv_id": "2306.03100", "title": "Rethinking Model Evaluation as Narrowing the Socio-Technical Gap", "year": 2023}], "gt_label": [1], "date": "2023-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_524", "valid": true}
{"query": "Could you name some studies employing distributional semantic models such as count-based or Word2Vec approaches for the LSC task?", "cited_paper": [{"arxiv_id": "1606.02821", "title": "Cultural Shift or Linguistic Drift? Comparing Two Computational Measures of Semantic Change", "year": 2016}], "gt_label": [1], "date": "2016-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_525", "valid": true}
{"query": "Which works addressed the issue of inadequate data quality in large corpora and extended cleaning efforts?", "cited_paper": [{"arxiv_id": "2201.06642", "title": "Towards a Cleaner Document-Oriented Multilingual Crawled Corpus", "year": 2022}], "gt_label": [1], "date": "2022-01", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_526", "valid": true}
{"query": "Can you provide papers that discussed the concept of latent embeddings?", "cited_paper": [{"arxiv_id": "1902.10197", "title": "RotatE: Knowledge Graph Embedding by Relational Rotation in Complex Space", "year": 2019}, {"arxiv_id": "1707.01476", "title": "Convolutional 2D Knowledge Graph Embeddings", "year": 2017}, {"arxiv_id": "1606.06357", "title": "Complex Embeddings for Simple Link Prediction", "year": 2016}, {"arxiv_id": "1510.04935", "title": "Holographic Embeddings of Knowledge Graphs", "year": 2015}, {"arxiv_id": "1711.04071", "title": "KBGAN: Adversarial Learning for Knowledge Graph Embeddings", "year": 2017}, {"arxiv_id": "1901.09590", "title": "TuckER: Tensor Factorization for Knowledge Graph Completion", "year": 2019}, {"arxiv_id": "2002.05969", "title": "Query2box: Reasoning over Knowledge Graphs in Vector Space using Box Embeddings", "year": 2020}, {"arxiv_id": "2101.00345", "title": "Modeling Fine-Grained Entity Types with Box Embeddings", "year": 2021}], "gt_label": [1, 1, 1, 1, 1, 1, 1, 1], "date": "2021-01", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_527", "valid": true}
{"query": "What kinds of researches have been conducted using reinforcement learning to train policies for adaptive experimental design?", "cited_paper": [{"arxiv_id": "2202.00821", "title": "Optimizing Sequential Experimental Design with Deep Reinforcement Learning", "year": 2022}], "gt_label": [1], "date": "2022-03", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_528", "valid": true}
{"query": "Which research investigations in language finetuning consider data selection as crucial?", "cited_paper": [{"arxiv_id": "1906.11829", "title": "Selection via Proxy: Efficient Data Selection for Deep Learning", "year": 2019}, {"arxiv_id": "2206.07137", "title": "Prioritized Training on Points that are Learnable, Worth Learning, and Not Yet Learnt", "year": 2022}], "gt_label": [1, 1], "date": "2022-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_529", "valid": true}
{"query": "What works covered methods that factor in agreement between positive pixel pairs for dense predictions?", "cited_paper": [{"arxiv_id": "2011.10043", "title": "Propagate Yourself: Exploring Pixel-Level Consistency for Unsupervised Visual Representation Learning", "year": 2020}, {"arxiv_id": "2207.04398", "title": "Self-supervised Learning with Local Contrastive Loss for Detection and Semantic Segmentation", "year": 2022}], "gt_label": [1, 1], "date": "2022-07", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_530", "valid": true}
{"query": "Could you name some research papers that utilized pre-trained text-to-image diffusion models and SMPL models for avatar generation?", "cited_paper": [{"arxiv_id": "2304.00916", "title": "DreamAvatar: Text-and-Shape Guided 3D Human Avatar Generation via Diffusion Models", "year": 2023}, {"arxiv_id": "2303.17606", "title": "AvatarCraft: Transforming Text into Neural Human Avatars with Parameterized Shape and Pose Control", "year": 2023}], "gt_label": [1, 1], "date": "2023-04", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_531", "valid": true}
{"query": "What research proposed multi-plane images for novel-view synthesis?", "cited_paper": [{"arxiv_id": "1805.09817", "title": "Stereo Magnification: Learning View Synthesis using Multiplane Images", "year": 2018}], "gt_label": [1], "date": "2018-05", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_532", "valid": true}
{"query": "What are some studies that have used data statistics, representations, logits, and embedding to avoid exposing privacy in Federated Learning?", "cited_paper": [{"arxiv_id": "2006.05148", "title": "XOR Mixup: Privacy-Preserving Data Augmentation for One-Shot Federated Learning", "year": 2020}, {"arxiv_id": "2104.13417", "title": "Towards Fair Federated Learning with Zero-Shot Data Augmentation", "year": 2021}, {"arxiv_id": "2106.05001", "title": "No Fear of Heterogeneity: Classifier Calibration for Federated Learning with Non-IID Data", "year": 2021}, {"arxiv_id": "2105.00243", "title": "FedProto: Federated Prototype Learning across Heterogeneous Clients", "year": 2021}], "gt_label": [1, 1, 1, 1], "date": "2021-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_533", "valid": true}
{"query": "What are the key works in the field of diffusion models which are a class of generative probabilistic models?", "cited_paper": [{"arxiv_id": "1503.03585", "title": "Deep Unsupervised Learning using Nonequilibrium Thermodynamics", "year": 2015}, {"arxiv_id": "2209.04747", "title": "Diffusion Models in Vision: A Survey", "year": 2022}, {"arxiv_id": "2105.05233", "title": "Diffusion Models Beat GANs on Image Synthesis", "year": 2021}, {"arxiv_id": "2006.11239", "title": "Denoising Diffusion Probabilistic Models", "year": 2020}, {"arxiv_id": "2102.09672", "title": "Improved Denoising Diffusion Probabilistic Models", "year": 2021}], "gt_label": [1, 1, 1, 1, 1], "date": "2022-09", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_534", "valid": true}
{"query": "Are there any works that reported that the volume of input knowledge for each query in ICL is constrained by the maximum input length of PLMs?", "cited_paper": [{"arxiv_id": "2212.06713", "title": "Structured Prompting: Scaling In-Context Learning to 1,000 Examples", "year": 2022}], "gt_label": [1], "date": "2022-12", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_535", "valid": true}
{"query": "Can you name works that have used specific methodology like Androids View Hierarchy, Regions of Interest, or screenshots for representing interfaces?", "cited_paper": [{"arxiv_id": "2209.14927", "title": "Spotlight: Mobile UI Understanding using Vision-Language Models with a Focus", "year": 2022}, {"arxiv_id": "2310.04716", "title": "Reinforced UI Instruction Grounding: Towards a Generic UI Task Automation API", "year": 2023}], "gt_label": [1, 1], "date": "2023-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_536", "valid": true}
{"query": "Which papers demonstrate that language models can map conceptual domains onto grounded world representations?", "cited_paper": [{"arxiv_id": "2005.14165", "title": "Language Models are Few-Shot Learners", "year": 2020}], "gt_label": [1], "date": "2020-05", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_537", "valid": true}
{"query": "What are the task-informed models such as XL-LEXEME based on?", "cited_paper": [{"arxiv_id": "2305.11993", "title": "Interpretable Word Sense Representations via Definition Generation: The Case of Semantic Change Analysis", "year": 2023}], "gt_label": [1], "date": "2023-05", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_538", "valid": true}
{"query": "Any studies that employ the Masked Language Model to obtain corrections in GEC tasks?", "cited_paper": [{"arxiv_id": "2003.10687", "title": "Felix: Flexible Text Editing Through Tagging and Insertion", "year": 2020}], "gt_label": [1], "date": "2023-11", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_539", "valid": true}
{"query": "Could you provide me the works on finding architectures with high accuracy on clean examples not considering their robustness?", "cited_paper": [{"arxiv_id": "2108.01289", "title": "AdvRush: Searching for Adversarially Robust Neural Architectures", "year": 2021}, {"arxiv_id": "2306.06712", "title": "Neural Architecture Design and Robustness: A Dataset", "year": 2023}], "gt_label": [1, 1], "date": "2023-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_540", "valid": true}
{"query": "What does paper `bib.bib31` propose?", "cited_paper": [{"arxiv_id": "2205.06175", "title": "A Generalist Agent", "year": 2022}], "gt_label": [1], "date": "2022-05", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_541", "valid": true}
{"query": "Who looked at prompting closed-source LLMs to leverage their reasoning and planning abilities for web tasks through in-context learning and self-refine?", "cited_paper": [{"arxiv_id": "2306.07863", "title": "Synapse: Trajectory-as-Exemplar Prompting with Memory for Computer Control", "year": 2023}, {"arxiv_id": "2303.17491", "title": "Language Models can Solve Computer Tasks", "year": 2023}], "gt_label": [1, 1], "date": "2023-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_542", "valid": true}
{"query": "Which research papers introduced initial vision-language pre-training models?", "cited_paper": [{"arxiv_id": "1909.11740", "title": "UNITER: UNiversal Image-TExt Representation Learning", "year": 2019}, {"arxiv_id": "1909.11059", "title": "Unified Vision-Language Pre-Training for Image Captioning and VQA", "year": 2019}, {"arxiv_id": "2102.02779", "title": "Unifying Vision-and-Language Tasks via Text Generation", "year": 2021}, {"arxiv_id": "2004.06165", "title": "Oscar: Object-Semantics Aligned Pre-training for Vision-Language Tasks", "year": 2020}, {"arxiv_id": "2102.03334", "title": "ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision", "year": 2021}, {"arxiv_id": "2006.06195", "title": "Large-Scale Adversarial Training for Vision-and-Language Representation Learning", "year": 2020}, {"arxiv_id": "2004.00849", "title": "Pixel-BERT: Aligning Image Pixels with Text by Deep Multi-Modal Transformers", "year": 2020}, {"arxiv_id": "1908.02265", "title": "ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks", "year": 2019}, {"arxiv_id": "1908.08530", "title": "VL-BERT: Pre-training of Generic Visual-Linguistic Representations", "year": 2019}, {"arxiv_id": "1912.02315", "title": "12-in-1: Multi-Task Vision and Language Representation Learning", "year": 2019}, {"arxiv_id": "2104.03135", "title": "Seeing Out of tHe bOx: End-to-End Pre-training for Vision-Language Representation Learning", "year": 2021}, {"arxiv_id": "2108.10904", "title": "SimVLM: Simple Visual Language Model Pretraining with Weak Supervision", "year": 2021}], "gt_label": [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], "date": "2021-08", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_543", "valid": true}
{"query": "What studies have explored incorporating additional conditioning, to generate images with precise control?", "cited_paper": [{"arxiv_id": "2302.05543", "title": "Adding Conditional Control to Text-to-Image Diffusion Models", "year": 2023}, {"arxiv_id": "2302.09778", "title": "Composer: Creative and Controllable Image Synthesis with Composable Conditions", "year": 2023}, {"arxiv_id": "2304.04269", "title": "HumanSD: A Native Skeleton-Guided Diffusion Model for Human Image Generation", "year": 2023}], "gt_label": [1, 1, 1], "date": "2023-04", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_544", "valid": true}
{"query": "Which studies primarily improve ACI by setting the learning rate adaptively?", "cited_paper": [{"arxiv_id": "2208.08401", "title": "Conformal Inference for Online Prediction with Arbitrary Distribution Shifts", "year": 2022}, {"arxiv_id": "2202.07282", "title": "Adaptive Conformal Predictions for Time Series", "year": 2022}, {"arxiv_id": "2302.07869", "title": "Improved Online Conformal Prediction via Strongly Adaptive Online Learning", "year": 2023}], "gt_label": [1, 1, 1], "date": "2023-02", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_545", "valid": true}
{"query": "In what papers do the researchers use the dual potentials to recover the OT map?", "cited_paper": [{"arxiv_id": "1711.02283", "title": "Large-Scale Optimal Transport and Mapping Estimation", "year": 2017}], "gt_label": [1], "date": "2017-11", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_546", "valid": true}
{"query": "What paper proposed an unsupervised global disentanglement score called Distortion?", "cited_paper": [{"arxiv_id": "2205.13182", "title": "Analyzing the Latent Space of GAN through Local Dimension Estimation", "year": 2022}], "gt_label": [1], "date": "2022-05", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_547", "valid": true}
{"query": "Any works achieve higher accuracy on low-textured regions with the help of Transformer in semi-dense matching methods?", "cited_paper": [{"arxiv_id": "2104.00680", "title": "LoFTR: Detector-Free Local Feature Matching with Transformers", "year": 2021}, {"arxiv_id": "2208.14201", "title": "ASpanFormer: Detector-Free Image Matching with Adaptive Span Transformer", "year": 2022}, {"arxiv_id": "2203.09645", "title": "MatchFormer: Interleaving Attention in Transformers for Feature Matching", "year": 2022}], "gt_label": [1, 1, 1], "date": "2022-08", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_548", "valid": true}
{"query": "Which work is related to the theoretical foundations and implementation of Proximal Policy Optimization?", "cited_paper": [], "gt_label": [], "date": "2017-07", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_549", "valid": true}
{"query": "What research papers utilized the embedding-based metrics which make use of PLM embeddings like BERT?", "cited_paper": [{"arxiv_id": "1810.04805", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "year": 2018}, {"arxiv_id": "1904.09675", "title": "BERTScore: Evaluating Text Generation with BERT", "year": 2019}, {"arxiv_id": "1909.02622", "title": "MoverScore: Text Generation Evaluating with Contextualized Embeddings and Earth Mover Distance", "year": 2019}], "gt_label": [1, 1, 1], "date": "2019-09", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_550", "valid": true}
{"query": "What etudies are on Transformer-based models for speech that have been used to test their brain alignment for speech-evoked brain activity?", "cited_paper": [{"arxiv_id": "2005.08392", "title": "Vector-Quantized Autoregressive Predictive Coding", "year": 2020}, {"arxiv_id": "2006.11477", "title": "wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations", "year": 2020}, {"arxiv_id": "2106.07447", "title": "HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units", "year": 2021}, {"arxiv_id": "2206.01685", "title": "Toward a realistic model of speech processing in the brain with self-supervised learning", "year": 2022}, {"arxiv_id": "2205.14252", "title": "Self-supervised models of audio effectively explain human cortical responses to speech", "year": 2022}], "gt_label": [1, 1, 1, 1, 1], "date": "2022-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_551", "valid": true}
{"query": "Could you name some research papers on exploratory attacks to achieve different attack goals such as link re-identification, property inference, membership inference, and model stealing?", "cited_paper": [{"arxiv_id": "2005.02131", "title": "Stealing Links from Graph Neural Networks", "year": 2020}, {"arxiv_id": "2110.02631", "title": "Inference Attacks Against Graph Neural Networks", "year": 2021}, {"arxiv_id": "2010.00906", "title": "Quantifying Privacy Leakage in Graph Embedding", "year": 2020}], "gt_label": [1, 1, 1], "date": "2021-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_552", "valid": true}
{"query": "What studies introduce methods to reduce the learning time by reducing the communication overheads of Federated Learning?", "cited_paper": [{"arxiv_id": "2001.04756", "title": "Adaptive Gradient Sparsification for Efficient Federated Learning: An Online Learning Approach", "year": 2020}], "gt_label": [1], "date": "2020-01", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_553", "valid": true}
{"query": "What works represent the approach of contrastive learning for image-text pre-training?", "cited_paper": [{"arxiv_id": "2103.00020", "title": "Learning Transferable Visual Models From Natural Language Supervision", "year": 2021}, {"arxiv_id": "2102.05918", "title": "Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision", "year": 2021}, {"arxiv_id": "2111.07991", "title": "LiT: Zero-Shot Transfer with Locked-image text Tuning", "year": 2021}, {"arxiv_id": "2111.11432", "title": "Florence: A New Foundation Model for Computer Vision", "year": 2021}], "gt_label": [1, 1, 1, 1], "date": "2021-11", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_554", "valid": true}
{"query": "Are there any methods using hierarchical Reinforcement Learning to decompose complex tasks into sub-tasks?", "cited_paper": [{"arxiv_id": "1604.06057", "title": "Hierarchical Deep Reinforcement Learning: Integrating Temporal Abstraction and Intrinsic Motivation", "year": 2016}, {"arxiv_id": "1609.05140", "title": "The Option-Critic Architecture", "year": 2016}, {"arxiv_id": "1810.01257", "title": "Near-Optimal Representation Learning for Hierarchical Reinforcement Learning", "year": 2018}, {"arxiv_id": "1906.07343", "title": "Language as an Abstraction for Hierarchical Deep Reinforcement Learning", "year": 2019}, {"arxiv_id": "2106.14305", "title": "Unsupervised Skill Discovery with Bottleneck Option Learning", "year": 2021}, {"arxiv_id": "2103.02957", "title": "Toward Robust Long Range Policy Transfer", "year": 2021}], "gt_label": [1, 1, 1, 1, 1, 1], "date": "2021-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_555", "valid": true}
{"query": "Can you give examples of research that aligns the source and target point clouds with an orientation estimation module before using a teacher-student model and a DGCNN backbone to find the correspondence?", "cited_paper": [{"arxiv_id": "2304.05395", "title": "SE-ORNet: Self-Ensembling Orientation-aware Network for Unsupervised Point Cloud Shape Correspondence", "year": 2023}], "gt_label": [1], "date": "2023-04", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_556", "valid": true}
{"query": "What works discuss defense mechanisms that generate adversarially perturbed images to improve neural network's robustness?", "cited_paper": [{"arxiv_id": "1412.6572", "title": "Explaining and Harnessing Adversarial Examples", "year": 2014}, {"arxiv_id": "1706.06083", "title": "Towards Deep Learning Models Resistant to Adversarial Attacks", "year": 2017}, {"arxiv_id": "1511.04599", "title": "DeepFool: a simple and accurate method to fool deep neural networks", "year": 2015}], "gt_label": [1, 1, 1], "date": "2017-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_557", "valid": true}
{"query": "In what studies LMMs directly reason over embedded visual features?", "cited_paper": [{"arxiv_id": "2304.08485", "title": "Visual Instruction Tuning", "year": 2023}, {"arxiv_id": "2310.03744", "title": "Improved Baselines with Visual Instruction Tuning", "year": 2023}, {"arxiv_id": "2204.14198", "title": "Flamingo: a Visual Language Model for Few-Shot Learning", "year": 2022}, {"arxiv_id": "2301.12597", "title": "BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models", "year": 2023}, {"arxiv_id": "2305.06500", "title": "InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning", "year": 2023}, {"arxiv_id": "2304.10592", "title": "MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models", "year": 2023}, {"arxiv_id": "2304.14178", "title": "mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality", "year": 2023}, {"arxiv_id": "2311.04257", "title": "mPLUG-Owl2: Revolutionizing Multi-modal Large Language Model with Modality Collaboration", "year": 2023}, {"arxiv_id": "2305.04790", "title": "MultiModal-GPT: A Vision and Language Model for Dialogue with Humans", "year": 2023}, {"arxiv_id": "2303.03378", "title": "PaLM-E: An Embodied Multimodal Language Model", "year": 2023}], "gt_label": [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], "date": "2023-11", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_558", "valid": true}
{"query": "Could you give me some references about adversarial learning approach in domain adaptation?", "cited_paper": [{"arxiv_id": "1505.07818", "title": "Domain-Adversarial Training of Neural Networks", "year": 2015}, {"arxiv_id": "1711.03213", "title": "CyCADA: Cycle-Consistent Adversarial Domain Adaptation", "year": 2017}, {"arxiv_id": "1910.11319", "title": "Progressive Domain Adaptation for Object Detection", "year": 2019}], "gt_label": [1, 1, 1], "date": "2019-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_559", "valid": true}
{"query": "What works proposed methods to improve computational efficiency by reducing the number of times the underlying field needs to be evaluated in the context of neural scene representations?", "cited_paper": [{"arxiv_id": "2007.11571", "title": "Neural Sparse Voxel Fields", "year": 2020}, {"arxiv_id": "2112.05131", "title": "Plenoxels: Radiance Fields without Neural Networks", "year": 2021}, {"arxiv_id": "2103.14024", "title": "PlenOctrees for Real-time Rendering of Neural Radiance Fields", "year": 2021}, {"arxiv_id": "2201.05989", "title": "Instant Neural Graphics Primitives with a Multiresolution Hash Encoding", "year": 2022}], "gt_label": [1, 1, 1, 1], "date": "2022-01", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_560", "valid": true}
{"query": "Which paper uses counterfactual explanations in explainable recommender systems?", "cited_paper": [{"arxiv_id": "2105.05008", "title": "Counterfactual Explanations for Neural Recommenders", "year": 2021}], "gt_label": [1], "date": "2021-05", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_561", "valid": true}
{"query": "Can you name the methods that tackle the high-dimensional importance weight estimation problem?", "cited_paper": [{"arxiv_id": "2006.12204", "title": "Telescoping Density-Ratio Estimation", "year": 2020}, {"arxiv_id": "2107.02212", "title": "Featurized Density Ratio Estimation", "year": 2021}], "gt_label": [1, 1], "date": "2021-11", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_562", "valid": true}
{"query": "Which works have pioneered in text-editing as a category of generative data augmentation?", "cited_paper": [{"arxiv_id": "1901.11196", "title": "EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification Tasks", "year": 2019}], "gt_label": [1], "date": "2019-01", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_563", "valid": true}
{"query": "Which papers proposed an algorithm to handle non-stationary MDPs with linear mixture function approximation of both transitions and rewards?", "cited_paper": [{"arxiv_id": "2110.08984", "title": "Optimistic Policy Optimization is Provably Efficient in Non-stationary MDPs", "year": 2021}], "gt_label": [1], "date": "2021-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_564", "valid": true}
{"query": "What studies discuss solutions involving interpolation between LR and RP gradients?", "cited_paper": [{"arxiv_id": "1902.01240", "title": "PIPPS: Flexible Model-Based Policy Search Robust to the Curse of Chaos", "year": 2019}, {"arxiv_id": "2202.00817", "title": "Do Differentiable Simulators Give Better Policy Gradients?", "year": 2022}], "gt_label": [1, 1], "date": "2022-02", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_565", "valid": true}
{"query": "Which works have implemented multimodal understanding and generative capacities across modalities?", "cited_paper": [{"arxiv_id": "2305.11846", "title": "Any-to-Any Generation via Composable Diffusion", "year": 2023}, {"arxiv_id": "2305.17216", "title": "Generating Images with Multimodal Language Models", "year": 2023}, {"arxiv_id": "2309.05519", "title": "NExT-GPT: Any-to-Any Multimodal LLM", "year": 2023}, {"arxiv_id": "2307.05222", "title": "Emu: Generative Pretraining in Multimodality", "year": 2023}, {"arxiv_id": "2309.02591", "title": "Scaling Autoregressive Multi-Modal Models: Pretraining and Instruction Tuning", "year": 2023}], "gt_label": [1, 1, 1, 1, 1], "date": "2023-09", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_566", "valid": true}
{"query": "What research conducted studies on zero-shot tasks on images using foundation models?", "cited_paper": [{"arxiv_id": "2103.00020", "title": "Learning Transferable Visual Models From Natural Language Supervision", "year": 2021}, {"arxiv_id": "2205.01917", "title": "CoCa: Contrastive Captioners are Image-Text Foundation Models", "year": 2022}, {"arxiv_id": "2102.05918", "title": "Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision", "year": 2021}, {"arxiv_id": "2111.11432", "title": "Florence: A New Foundation Model for Computer Vision", "year": 2021}], "gt_label": [1, 1, 1, 1], "date": "2022-05", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_567", "valid": true}
{"query": "What paper used shifted window attention to enable information propagation in the area of action recongnition?", "cited_paper": [{"arxiv_id": "2106.13230", "title": "Video Swin Transformer", "year": 2021}], "gt_label": [1], "date": "2021-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_568", "valid": true}
{"query": "What works have explored the field of zero-shot segmentation recently?", "cited_paper": [{"arxiv_id": "2401.14159", "title": "Grounded SAM: Assembling Open-World Models for Diverse Visual Tasks", "year": 2024}, {"arxiv_id": "2206.07045", "title": "ReCo: Retrieve and Co-segment for Zero-shot Transfer", "year": 2022}, {"arxiv_id": "2112.10003", "title": "Image Segmentation Using Text and Image Prompts", "year": 2021}, {"arxiv_id": "1906.00817", "title": "Zero-Shot Semantic Segmentation", "year": 2019}, {"arxiv_id": "2303.11681", "title": "DiffuMask: Synthesizing Images with Pixel-level Annotations for Semantic Segmentation Using Diffusion Models", "year": 2023}, {"arxiv_id": "2112.01071", "title": "Extract Free Dense Labels from CLIP", "year": 2021}], "gt_label": [1, 1, 1, 1, 1, 1], "date": "2024-01", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_569", "valid": true}
{"query": "Could you provide me the research where local visual features aligned with textual concepts in CLIP were revealed?", "cited_paper": [{"arxiv_id": "2112.01071", "title": "Extract Free Dense Labels from CLIP", "year": 2021}], "gt_label": [1], "date": "2021-12", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_570", "valid": true}
{"query": "Could you provide me some works about localization strategies using region-proposal detector or a segmentation network in vision-language models?", "cited_paper": [{"arxiv_id": "2401.14159", "title": "Grounded SAM: Assembling Open-World Models for Diverse Visual Tasks", "year": 2024}], "gt_label": [1], "date": "2024-01", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_571", "valid": true}
{"query": "Any research on probablistic personalized page rank (ProPPR) models?", "cited_paper": [{"arxiv_id": "1305.2254", "title": "Programming with Personalized PageRank: A Locally Groundable First-Order Probabilistic Logic", "year": 2013}], "gt_label": [1], "date": "2013-05", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_572", "valid": true}
{"query": "Which studies offer solutions to deal with accuracy loss when a sub-model is sent to slower devices?", "cited_paper": [{"arxiv_id": "2102.13451", "title": "FjORD: Fair and Accurate Federated Learning under heterogeneous targets with Ordered Dropout", "year": 2021}, {"arxiv_id": "2010.01264", "title": "HeteroFL: Computation and Communication Efficient Federated Learning for Heterogeneous Clients", "year": 2020}], "gt_label": [1, 1], "date": "2021-02", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_573", "valid": true}
{"query": "What paper proposed SuperGLUE after seeing that many models were surpassing non-expert humans on GLUE?", "cited_paper": [{"arxiv_id": "1905.00537", "title": "SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems", "year": 2019}], "gt_label": [1], "date": "2019-05", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_574", "valid": true}
{"query": "Who is responsible for the best-known result for the MMS approximation in additive valuations, 3/4+3/3836?", "cited_paper": [{"arxiv_id": "2307.07304", "title": "Breaking the $3/4$ Barrier for Approximate Maximin Share", "year": 2023}], "gt_label": [1], "date": "2023-07", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_575", "valid": true}
{"query": "Which works pioneered the use of Hessian-based influence functions to understand how training data affects a model's prediction?", "cited_paper": [{"arxiv_id": "1703.04730", "title": "Understanding Black-box Predictions via Influence Functions", "year": 2017}], "gt_label": [1], "date": "2017-03", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_576", "valid": true}
{"query": "What work has used self-supervised pre-training to design a flexible reward function by utilizing a broad dataset of human videos and a small dataset of robot videos?", "cited_paper": [{"arxiv_id": "2103.16817", "title": "Learning Generalizable Robotic Reward Functions from \"In-The-Wild\" Human Videos", "year": 2021}], "gt_label": [1], "date": "2021-03", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_577", "valid": true}
{"query": "Could you provide some works where delayed feedback is studied in stochastic settings for UCB-based methods?", "cited_paper": [{"arxiv_id": "2002.10316", "title": "Bandit Learning with Delayed Impact of Actions", "year": 2020}, {"arxiv_id": "2006.10459", "title": "Stochastic bandits with arm-dependent delays", "year": 2020}], "gt_label": [1, 1], "date": "2020-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_578", "valid": true}
{"query": "Which papers discuss solutions to commonsense reasoning problems?", "cited_paper": [{"arxiv_id": "1811.00937", "title": "CommonsenseQA: A Question Answering Challenge Targeting Commonsense Knowledge", "year": 2018}, {"arxiv_id": "2201.05320", "title": "CommonsenseQA 2.0: Exposing the Limits of AI through Gamification", "year": 2022}, {"arxiv_id": "1909.00277", "title": "Cosmos QA: Machine Reading Comprehension with Contextual Commonsense Reasoning", "year": 2019}, {"arxiv_id": "1908.05739", "title": "Abductive Commonsense Reasoning", "year": 2019}, {"arxiv_id": "1904.09728", "title": "SocialIQA: Commonsense Reasoning about Social Interactions", "year": 2019}], "gt_label": [1, 1, 1, 1, 1], "date": "2022-01", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_579", "valid": true}
{"query": "Could you name some works where diffusion models were applied to represent 3D scenes and motion sequences?", "cited_paper": [{"arxiv_id": "2212.01206", "title": "DiffRF: Rendering-Guided 3D Radiance Field Diffusion", "year": 2022}, {"arxiv_id": "2212.02500", "title": "PhysDiff: Physics-Guided Human Motion Diffusion Model", "year": 2022}, {"arxiv_id": "2209.14916", "title": "Human Motion Diffusion Model", "year": 2022}], "gt_label": [1, 1, 1], "date": "2022-12", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_580", "valid": true}
{"query": "Where is the concept of Generative Adversarial Networks (GANs) introduced?", "cited_paper": [{"arxiv_id": "1406.2661", "title": "Generative Adversarial Networks", "year": 2014}], "gt_label": [1], "date": "2014-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_581", "valid": true}
{"query": "Any works about searching for the optimal scaling factor in the context of quantization-aware training?", "cited_paper": [{"arxiv_id": "1805.06085", "title": "PACT: Parameterized Clipping Activation for Quantized Neural Networks", "year": 2018}, {"arxiv_id": "1606.06160", "title": "DoReFa-Net: Training Low Bitwidth Convolutional Neural Networks with Low Bitwidth Gradients", "year": 2016}, {"arxiv_id": "1902.08153", "title": "Learned Step Size Quantization", "year": 2019}], "gt_label": [1, 1, 1], "date": "2019-02", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_582", "valid": true}
{"query": "Who applied the mutual information in the context of maximising similarity between successive observations for representation learning?", "cited_paper": [{"arxiv_id": "2006.07217", "title": "Deep Reinforcement and InfoMax Learning", "year": 2020}], "gt_label": [1], "date": "2020-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_583", "valid": true}
{"query": "Are there any studies that have explored teacher assistant-based and student-friendly distillation to alleviate the problem of performance degradation in larger LMs?", "cited_paper": [{"arxiv_id": "1902.03393", "title": "Improved Knowledge Distillation via Teacher Assistant", "year": 2019}, {"arxiv_id": "1910.01348", "title": "On the Efficacy of Knowledge Distillation", "year": 2019}, {"arxiv_id": "2203.08679", "title": "Decoupled Knowledge Distillation", "year": 2022}, {"arxiv_id": "2305.12129", "title": "Lifting the Curse of Capacity Gap in Distilling Language Models", "year": 2023}], "gt_label": [1, 1, 1, 1], "date": "2023-05", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_584", "valid": true}
{"query": "What works used data-driven approaches in AutoRL methods to learn various algorithmic components?", "cited_paper": [{"arxiv_id": "1906.05374", "title": "Meta-Learning via Learned Loss", "year": 2019}, {"arxiv_id": "2007.08433", "title": "Meta-Gradient Reinforcement Learning with an Objective Discovered Online", "year": 2020}, {"arxiv_id": "2211.09760", "title": "VeLO: Training Versatile Learned Optimizers by Scaling Up", "year": 2022}], "gt_label": [1, 1, 1], "date": "2022-11", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_585", "valid": true}
{"query": "What works have modified compressed sensing approaches to include a cross-validation step?", "cited_paper": [], "gt_label": [], "date": "2008-03", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_586", "valid": true}
{"query": "Are there works that investigate the implementation of gradient descent and stochastic gradient descent based on the Polyak-Lojasiewicz (PL) assumption?", "cited_paper": [{"arxiv_id": "1608.04636", "title": "Linear Convergence of Gradient and Proximal-Gradient Methods Under the Polyak-ojasiewicz Condition", "year": 2016}], "gt_label": [1], "date": "2020-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_587", "valid": true}
{"query": "Which work used counterfactual interventions to determine the unfaithfulness of explanations of a LLM's predictions?", "cited_paper": [{"arxiv_id": "2305.18029", "title": "Faithfulness Tests for Natural Language Explanations", "year": 2023}], "gt_label": [1], "date": "2023-05", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_588", "valid": true}
{"query": "Which work was the first to propose Generation-Augmented Retrieval in question answering?", "cited_paper": [{"arxiv_id": "2009.08553", "title": "Generation-Augmented Retrieval for Open-domain Question Answering", "year": 2020}], "gt_label": [1], "date": "2020-09", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_589", "valid": true}
{"query": "What studies discuss the introduction of large-scale continuous sign language datasets?", "cited_paper": [{"arxiv_id": "2007.12131", "title": "BSL-1K: Scaling up co-articulated sign language recognition using mouthing cues", "year": 2020}, {"arxiv_id": "2008.08143", "title": "How2Sign: A Large-scale Multimodal Dataset for Continuous American Sign Language", "year": 2020}], "gt_label": [1, 1], "date": "2020-08", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_590", "valid": true}
{"query": "What papers proposed architectural modifications to recurrence equations for irregular time series data?", "cited_paper": [{"arxiv_id": "1710.04110", "title": "Discrete Event, Continuous Time RNNs", "year": 2017}, {"arxiv_id": "1606.01865", "title": "Recurrent Neural Networks for Multivariate Time Series with Missing Values", "year": 2016}, {"arxiv_id": "1511.06464", "title": "Unitary Evolution Recurrent Neural Networks", "year": 2015}], "gt_label": [1, 1, 1], "date": "2017-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_591", "valid": true}
{"query": "Could you provide me with the papers that proposed using graph homomorphism counts for feature embedding in learning tasks?", "cited_paper": [{"arxiv_id": "2005.01214", "title": "Graph Homomorphism Convolution", "year": 2020}], "gt_label": [1], "date": "2020-05", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_592", "valid": true}
{"query": "Which research applied inversion attacks to models across different domains like computational genetics, computer vision, and NLP?", "cited_paper": [{"arxiv_id": "2004.00053", "title": "Information Leakage in Embedding Models", "year": 2020}], "gt_label": [1], "date": "2020-04", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_593", "valid": true}
{"query": "Which works expanded on the contrastive methods of representation learning by integrating additional views into the mutual information maximization objective?", "cited_paper": [{"arxiv_id": "2006.05582", "title": "Contrastive Multi-View Representation Learning on Graphs", "year": 2020}, {"arxiv_id": "2006.04131", "title": "Deep Graph Contrastive Representation Learning", "year": 2020}], "gt_label": [1, 1], "date": "2020-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_594", "valid": true}
{"query": "Can you provide me studies about decomposing MOCO problems into a series of single-objective combinatorial optimization problems?", "cited_paper": [{"arxiv_id": "2210.08495", "title": "Pareto Set Learning for Expensive Multi-Objective Optimization", "year": 2022}, {"arxiv_id": "2010.04104", "title": "Learning the Pareto Front with Hypernetworks", "year": 2020}, {"arxiv_id": "1912.12854", "title": "Pareto Multi-Task Learning", "year": 2019}], "gt_label": [1, 1, 1], "date": "2022-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_595", "valid": true}
{"query": "Which papers focused on the calibration of classifiers in foundation models for natural language processing?", "cited_paper": [{"arxiv_id": "2012.00955", "title": "How Can We Know When Language Models Know? On the Calibration of Language Models for Question Answering", "year": 2020}, {"arxiv_id": "2003.07892", "title": "Calibration of Pre-trained Transformers", "year": 2020}], "gt_label": [1, 1], "date": "2020-12", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_596", "valid": true}
{"query": "Which works have been established for regret minimization in two types of MDPs under linear function approximation?", "cited_paper": [{"arxiv_id": "1905.10389", "title": "Reinforcement Learning in Feature Space: Matrix Bandit, Kernels, and Regret Bound", "year": 2019}, {"arxiv_id": "2006.01107", "title": "Model-Based Reinforcement Learning with Value-Targeted Regression", "year": 2020}, {"arxiv_id": "1902.04779", "title": "Sample-Optimal Parametric Q-Learning Using Linearly Additive Features", "year": 2019}, {"arxiv_id": "1907.05388", "title": "Provably Efficient Reinforcement Learning with Linear Function Approximation", "year": 2019}], "gt_label": [1, 1, 1, 1], "date": "2020-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_597", "valid": true}
{"query": "What works have focused on integrating constraints into sequential decision problems?", "cited_paper": [{"arxiv_id": "1705.10528", "title": "Constrained Policy Optimization", "year": 2017}, {"arxiv_id": "2003.02189", "title": "Exploration-Exploitation in Constrained MDPs", "year": 2020}, {"arxiv_id": "1805.07708", "title": "A Lyapunov-based Approach to Safe Reinforcement Learning", "year": 2018}, {"arxiv_id": "1910.13393", "title": "Constrained Reinforcement Learning Has Zero Duality Gap", "year": 2019}], "gt_label": [1, 1, 1, 1], "date": "2020-03", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_598", "valid": true}
{"query": "What work analysed VAE-based disentanglement techniques on correlated data?", "cited_paper": [{"arxiv_id": "2006.07886", "title": "On Disentangled Representations Learned From Correlated Data", "year": 2020}], "gt_label": [1], "date": "2020-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_599", "valid": true}
{"query": "Could you provide me some studies that investigate where factual information is stored in transformers?", "cited_paper": [{"arxiv_id": "2104.08696", "title": "Knowledge Neurons in Pretrained Transformers", "year": 2021}, {"arxiv_id": "2202.05262", "title": "Locating and Editing Factual Associations in GPT", "year": 2022}, {"arxiv_id": "2210.07229", "title": "Mass-Editing Memory in a Transformer", "year": 2022}], "gt_label": [1, 1, 1], "date": "2022-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_600", "valid": true}
{"query": "What publications discuss the usage of Bayesian neural network in enhancing the efficiency of FL?", "cited_paper": [{"arxiv_id": "1905.12022", "title": "Bayesian Nonparametric Federated Learning of Neural Networks", "year": 2019}, {"arxiv_id": "1911.00218", "title": "Statistical Model Aggregation via Parameter Matching", "year": 2019}], "gt_label": [1, 1], "date": "2019-11", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_601", "valid": true}
{"query": "What papers delved into the conformal prediction methods for drug property prediction?", "cited_paper": [], "gt_label": [], "date": "2019-08", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_602", "valid": true}
{"query": "What papers evaluated Generation-Augmented Retrieval in passage retrieval and fine-tuning?", "cited_paper": [{"arxiv_id": "2212.10496", "title": "Precise Zero-Shot Dense Retrieval without Relevance Labels", "year": 2022}, {"arxiv_id": "2303.07678", "title": "Query2doc: Query Expansion with Large Language Models", "year": 2023}], "gt_label": [1, 1], "date": "2023-03", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_603", "valid": true}
{"query": "Can you provide any works that adopted transition sampling for long-term generation of motion?", "cited_paper": [{"arxiv_id": "2308.01850", "title": "Synthesizing Long-Term Human Motions with Diffusion Models via Coherent Sampling", "year": 2023}], "gt_label": [1], "date": "2023-08", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_604", "valid": true}
{"query": "Which work is the only one investigating training a hypernetwork end-to-end to arbitrarily modify the weights of a policy in meta-RL?", "cited_paper": [{"arxiv_id": "2210.11348", "title": "Hypernetworks in Meta-Reinforcement Learning", "year": 2022}], "gt_label": [1], "date": "2022-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_605", "valid": true}
{"query": "Can you mention some studies that combined 3D Gaussian Splatting with diffusion models for efficient text-to-3D generation?", "cited_paper": [{"arxiv_id": "2309.16653", "title": "DreamGaussian: Generative Gaussian Splatting for Efficient 3D Content Creation", "year": 2023}, {"arxiv_id": "2309.16585", "title": "Text-to-3D using Gaussian Splatting", "year": 2023}], "gt_label": [1, 1], "date": "2023-09", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_606", "valid": true}
{"query": "Does there exist a work which demonstrates that the PAC-Bayes framework can't be used to derive distribution-free PAC learning bounds for classes that have infinite Littlestone dimension?", "cited_paper": [{"arxiv_id": "2006.13508", "title": "A Limitation of the PAC-Bayes Framework", "year": 2020}], "gt_label": [1], "date": "2020-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_607", "valid": true}
{"query": "What works tackle the problem of designing data augmentations for graph classification?", "cited_paper": [{"arxiv_id": "2202.08871", "title": "Graph Data Augmentation for Graph Machine Learning: A Survey", "year": 2022}, {"arxiv_id": "2202.08235", "title": "Data Augmentation for Deep Graph Learning: A Survey", "year": 2022}], "gt_label": [1, 1], "date": "2022-02", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_608", "valid": true}
{"query": "In what papers are the singular vectors of the first weight matrix proposed as global disentangled perturbations?", "cited_paper": [], "gt_label": [], "date": "2020-07", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_609", "valid": true}
{"query": "Which works propose alternate perspectives of robustness in explanations?", "cited_paper": [], "gt_label": [], "date": "2019-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_610", "valid": true}
{"query": "Which references explore reproducibility in optimization?", "cited_paper": [{"arxiv_id": "2202.04598", "title": "Reproducibility in Optimization: Theoretical Framework and Limits", "year": 2022}], "gt_label": [1], "date": "2022-02", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_611", "valid": true}
{"query": "What research have used normalizing flows in molecular structure design?", "cited_paper": [], "gt_label": [], "date": "2021-05", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_612", "valid": true}
{"query": "Could you provide me some studies of zero-order methods applying strategies that the more expensive oracles are used infrequently?", "cited_paper": [], "gt_label": [], "date": "2021-07", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_613", "valid": true}
{"query": "What work adds a buffer of demonstrations to the RL framework?", "cited_paper": [{"arxiv_id": "1709.10089", "title": "Overcoming Exploration in Reinforcement Learning with Demonstrations", "year": 2017}], "gt_label": [1], "date": "2017-09", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_614", "valid": true}
{"query": "What works are amongst the most influential in relation to U-Net?", "cited_paper": [{"arxiv_id": "1807.10165", "title": "UNet++: A Nested U-Net Architecture for Medical Image Segmentation", "year": 2018}, {"arxiv_id": "1804.03999", "title": "Attention U-Net: Learning Where to Look for the Pancreas", "year": 2018}, {"arxiv_id": "1606.06650", "title": "3D U-Net: Learning Dense Volumetric Segmentation from Sparse Annotation", "year": 2016}, {"arxiv_id": "2006.11239", "title": "Denoising Diffusion Probabilistic Models", "year": 2020}, {"arxiv_id": "1809.10486", "title": "nnU-Net: Self-adapting Framework for U-Net-Based Medical Image Segmentation", "year": 2018}, {"arxiv_id": "1806.05034", "title": "A Probabilistic U-Net for Segmentation of Ambiguous Images", "year": 2018}, {"arxiv_id": "1804.04694", "title": "A Variational U-Net for Conditional Appearance and Shape Generation", "year": 2018}, {"arxiv_id": "1711.10684", "title": "Road Extraction by Deep Residual U-Net", "year": 2017}], "gt_label": [1, 1, 1, 1, 1, 1, 1, 1], "date": "2020-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_615", "valid": true}
{"query": "What studies discuss the issue of undefined importance weights in disjoint source and target?", "cited_paper": [], "gt_label": [], "date": "2022-04", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_616", "valid": true}
{"query": "What works emphasize that an LLM's acquired knowledge should mirror established facts?", "cited_paper": [{"arxiv_id": "2312.07000", "title": "Alignment for Honesty", "year": 2023}], "gt_label": [1], "date": "2023-12", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_617", "valid": true}
{"query": "What work propose the task of emotional support conversation and release the corresponding dataset?", "cited_paper": [{"arxiv_id": "2106.01144", "title": "Towards Emotional Support Dialog Systems", "year": 2021}], "gt_label": [1], "date": "2021-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_618", "valid": true}
{"query": "Which papers discussed the task of temporal or timeline summarization?", "cited_paper": [{"arxiv_id": "1810.07949", "title": "A Temporally Sensitive Submodularity Framework for Timeline Summarization", "year": 2018}], "gt_label": [1], "date": "2018-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_619", "valid": true}
{"query": "Can you name the studies that provided allocation algorithms that guarantee ex-ante and EF1 ex-post for additive valuations?", "cited_paper": [{"arxiv_id": "2005.14122", "title": "Best of Both Worlds: Ex-Ante and Ex-Post Fairness in Resource Allocation", "year": 2020}], "gt_label": [1], "date": "2020-05", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_620", "valid": true}
{"query": "What works used text-based language models to predict text-evoked and speech-evoked brain activity?", "cited_paper": [{"arxiv_id": "1905.11833", "title": "Interpreting and improving natural-language processing (in machines) with natural language-processing (in the brain)", "year": 2019}, {"arxiv_id": "1911.03268", "title": "Inducing brain-relevant bias in natural language processing models", "year": 2019}, {"arxiv_id": "1906.11861", "title": "Relating Simple Sentence Representations in Deep Neural Networks and the Brain", "year": 2019}, {"arxiv_id": "2106.05426", "title": "Low-Dimensional Structure in the Space of Language Representations is Reflected in Brain Responses", "year": 2021}, {"arxiv_id": "2205.01404", "title": "Neural Language Taskonomy: Which NLP Tasks are the most Predictive of fMRI Brain Activity?", "year": 2022}, {"arxiv_id": "2212.00596", "title": "Language models and brains align due to more than next-word prediction and word-level information", "year": 2022}, {"arxiv_id": "2212.08094", "title": "Joint processing of linguistic properties in brains and language models", "year": 2022}], "gt_label": [1, 1, 1, 1, 1, 1, 1], "date": "2022-12", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_621", "valid": true}
{"query": "Could you list the studies that discuss the strong correlation between the supervised disentanglement score and the global-basis-compatibility?", "cited_paper": [{"arxiv_id": "2205.13182", "title": "Analyzing the Latent Space of GAN through Local Dimension Estimation", "year": 2022}], "gt_label": [1], "date": "2022-05", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_622", "valid": true}
{"query": "What works initially used a one-time retrieval method but faced knowledge omissions issues in handling knowledge-sensitive tasks?", "cited_paper": [{"arxiv_id": "2112.04426", "title": "Improving language models by retrieving from trillions of tokens", "year": 2021}, {"arxiv_id": "2203.05115", "title": "Internet-augmented language models through few-shot prompting for open-domain question answering", "year": 2022}, {"arxiv_id": "2208.03299", "title": "Atlas: Few-shot Learning with Retrieval Augmented Language Models", "year": 2022}], "gt_label": [1, 1, 1], "date": "2022-08", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_623", "valid": true}
{"query": "Can you provide scholarly works that discuss domain-specific knowledge in Knowledge Graphs?", "cited_paper": [{"arxiv_id": "2009.13081", "title": "What Disease does this Patient Have? A Large-scale Open Domain Question Answering Dataset from Medical Exams", "year": 2020}], "gt_label": [1], "date": "2020-09", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_624", "valid": true}
{"query": "Are there any research papers studying VR HMD settings using public datasets such as AMASS?", "cited_paper": [{"arxiv_id": "1904.03278", "title": "AMASS: Archive of Motion Capture as Surface Shapes", "year": 2019}], "gt_label": [1], "date": "2019-04", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_625", "valid": true}
{"query": "What research paper is related to and compared with the one in this section, where it considers CT model with latent variables, subsections, and an additional loss term for the integration error?", "cited_paper": [{"arxiv_id": "2006.02915", "title": "Continuous-time system identification with neural networks: Model structures and fitting criteria", "year": 2020}], "gt_label": [1], "date": "2020-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_626", "valid": true}
{"query": "Can you provide studies that used transformer-based architectures to tackle German sign language production?", "cited_paper": [{"arxiv_id": "2004.14874", "title": "Progressive Transformers for End-to-End Sign Language Production", "year": 2020}, {"arxiv_id": "2003.13830", "title": "Sign Language Transformers: Joint End-to-end Sign Language Recognition and Translation", "year": 2020}], "gt_label": [1, 1], "date": "2020-04", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_627", "valid": true}
{"query": "Are there any references showing that the chain-of-thoughts generated by LLMs are often arbitrary or contradictory?", "cited_paper": [{"arxiv_id": "2307.08678", "title": "Do Models Explain Themselves? Counterfactual Simulatability of Natural Language Explanations", "year": 2023}, {"arxiv_id": "2309.11495", "title": "Chain-of-Verification Reduces Hallucination in Large Language Models", "year": 2023}], "gt_label": [1, 1], "date": "2023-09", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_628", "valid": true}
{"query": "Which paper applied the Gumbel-Softmax relaxation within gradient-based BOED for contextual optimization?", "cited_paper": [{"arxiv_id": "2207.05250", "title": "Efficient Real-world Testing of Causal Decision Making via Bayesian Experimental Design for Contextual Optimisation", "year": 2022}], "gt_label": [1], "date": "2022-07", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_629", "valid": true}
{"query": "Which papers used the BookCorpus, also known as the Toronto Books Corpus, for pretraining?", "cited_paper": [{"arxiv_id": "1810.04805", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "year": 2018}, {"arxiv_id": "1907.11692", "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach", "year": 2019}, {"arxiv_id": "1910.10683", "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer", "year": 2019}], "gt_label": [1, 1, 1], "date": "2019-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_630", "valid": true}
{"query": "Which papers discussed strategies to adjust the spatiotemporal resolution as a way to fine-tune TAD models?", "cited_paper": [{"arxiv_id": "2204.01680", "title": "TALLFormer: Temporal Action Localization with a Long-memory Transformer", "year": 2022}, {"arxiv_id": "2211.14053", "title": "Re^2TAL: Rewiring Pretrained Video Backbones for Reversible Temporal Action Localization", "year": 2022}, {"arxiv_id": "2207.10448", "title": "An Efficient Spatio-Temporal Pyramid Transformer for Action Detection", "year": 2022}], "gt_label": [1, 1, 1], "date": "2022-11", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_631", "valid": true}
{"query": "What previous works have analyzed errors in models reasoning in black-box LLMs?", "cited_paper": [{"arxiv_id": "2307.13702", "title": "Measuring Faithfulness in Chain-of-Thought Reasoning", "year": 2023}, {"arxiv_id": "2311.09603", "title": "Self-Contradictory Reasoning Evaluation and Detection", "year": 2023}, {"arxiv_id": "2212.10001", "title": "Towards Understanding Chain-of-Thought Prompting: An Empirical Study of What Matters", "year": 2022}], "gt_label": [1, 1, 1], "date": "2023-11", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_632", "valid": true}
{"query": "Could you point me to some papers that discuss the HiPPO framework and its applications in time-series modelling?", "cited_paper": [{"arxiv_id": "2008.07669", "title": "HiPPO: Recurrent Memory with Optimal Polynomial Projections", "year": 2020}, {"arxiv_id": "2111.00396", "title": "Efficiently Modeling Long Sequences with Structured State Spaces", "year": 2021}], "gt_label": [1, 1], "date": "2021-11", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_633", "valid": true}
{"query": "What research has been done in using Deep Neural Networks and graph representations with reinforcement learning in resolving propositional logic problems?", "cited_paper": [{"arxiv_id": "1805.11799", "title": "Automated proof synthesis for propositional logic with deep neural networks", "year": 2018}, {"arxiv_id": "1811.00796", "title": "Automated Theorem Proving in Intuitionistic Propositional Logic by Deep Reinforcement Learning", "year": 2018}], "gt_label": [1, 1], "date": "2018-11", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_634", "valid": true}
{"query": "Which paper first tackled the pose estimation of novel objects without CAD models using RGB images?", "cited_paper": [{"arxiv_id": "2204.10776", "title": "Gen6D: Generalizable Model-Free 6-DoF Object Pose Estimation from RGB Images", "year": 2022}], "gt_label": [1], "date": "2022-04", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_635", "valid": true}
{"query": "Can you cite works that have automated QA in online courses?", "cited_paper": [], "gt_label": [], "date": "2021-12", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_636", "valid": true}
{"query": "Which studies looked at robustness of MRL to distributional shifts?", "cited_paper": [{"arxiv_id": "2006.07178", "title": "Meta-Reinforcement Learning Robust to Distributional Shift via Model Identification and Experience Relabeling", "year": 2020}, {"arxiv_id": "2210.03104", "title": "Distributionally Adaptive Meta Reinforcement Learning", "year": 2022}], "gt_label": [1, 1], "date": "2022-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_637", "valid": true}
{"query": "Which studies have transferred the knowledge from pre-trained vision-language models to object detectors?", "cited_paper": [{"arxiv_id": "2104.13921", "title": "Open-vocabulary Object Detection via Vision and Language Knowledge Distillation", "year": 2021}], "gt_label": [1], "date": "2021-04", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_638", "valid": true}
{"query": "Could you provide me some work where the concept of LoRA is proposed?", "cited_paper": [{"arxiv_id": "2106.09685", "title": "LoRA: Low-Rank Adaptation of Large Language Models", "year": 2021}], "gt_label": [1], "date": "2021-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_639", "valid": true}
{"query": "Any links to studies considering a single update at each client and focusing on random walks?", "cited_paper": [{"arxiv_id": "2009.01790", "title": "Private Weighted Random Walk Stochastic Gradient Descent", "year": 2020}, {"arxiv_id": "1804.06568", "title": "Walkman: A Communication-Efficient Random-Walk Algorithm for Decentralized Optimization", "year": 2018}], "gt_label": [1, 1], "date": "2020-12", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_640", "valid": true}
{"query": "Which study introduced the Neural Tangent Kernel (NTK)?", "cited_paper": [{"arxiv_id": "1806.07572", "title": "Neural Tangent Kernel: Convergence and Generalization in Neural Networks", "year": 2018}], "gt_label": [1], "date": "2018-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_641", "valid": true}
{"query": "Has anyone tried to generalize the feature embedding approach to adaptive features?", "cited_paper": [{"arxiv_id": "2010.07154", "title": "Learning Deep Features in Instrumental Variable Regression", "year": 2020}, {"arxiv_id": "2106.03907", "title": "Deep Proxy Causal Learning and its Application to Confounded Bandit Policy Evaluation", "year": 2021}], "gt_label": [1, 1], "date": "2021-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_642", "valid": true}
{"query": "Who proposed property-aware relation networks (PAR) in the context of few-shot learning?", "cited_paper": [{"arxiv_id": "2107.07994", "title": "Property-Aware Relation Networks for Few-Shot Molecular Property Prediction", "year": 2021}], "gt_label": [1], "date": "2021-07", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_643", "valid": true}
{"query": "What study models reasoning procedures as BFS or DFS search on reasoning trees?", "cited_paper": [{"arxiv_id": "2305.10601", "title": "Tree of Thoughts: Deliberate Problem Solving with Large Language Models", "year": 2023}], "gt_label": [1], "date": "2023-05", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_644", "valid": true}
{"query": "Which studies focus on evaluation of a targeted update in knowledge updating?", "cited_paper": [{"arxiv_id": "2202.05262", "title": "Locating and Editing Factual Associations in GPT", "year": 2022}, {"arxiv_id": "2110.11309", "title": "Fast Model Editing at Scale", "year": 2021}], "gt_label": [1, 1], "date": "2022-02", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_645", "valid": true}
{"query": "What works describe the role of the Neural Tangent Kernel in the density of neural networks?", "cited_paper": [{"arxiv_id": "1806.07572", "title": "Neural Tangent Kernel: Convergence and Generalization in Neural Networks", "year": 2018}], "gt_label": [1], "date": "2018-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_646", "valid": true}
{"query": "What studies analyze the effect of multiple regularizations employed in deep learning, like weight decay, early stopping, or drop-outs, on the generalization abilities?", "cited_paper": [{"arxiv_id": "1503.00036", "title": "Norm-Based Capacity Control in Neural Networks", "year": 2015}, {"arxiv_id": "1402.3811", "title": "Dropout Rademacher Complexity of Deep Neural Networks", "year": 2014}], "gt_label": [1, 1], "date": "2015-03", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_647", "valid": true}
{"query": "Could you provide me some studies that relate to early works on aligning text and image embeddings?", "cited_paper": [{"arxiv_id": "1412.2306", "title": "Deep Visual-Semantic Alignments for Generating Image Descriptions", "year": 2014}], "gt_label": [1], "date": "2014-12", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_648", "valid": true}
{"query": "Which paper introduced joint differential privacy (JDP)?", "cited_paper": [], "gt_label": [], "date": "2015-12", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_649", "valid": true}
{"query": "Which research papers explored the implementation of state space models-based architectures?", "cited_paper": [{"arxiv_id": "2111.00396", "title": "Efficiently Modeling Long Sequences with Structured State Spaces", "year": 2021}, {"arxiv_id": "2203.14343", "title": "Diagonal State Spaces are as Effective as Structured State Spaces", "year": 2022}, {"arxiv_id": "2206.11893", "title": "On the Parameterization and Initialization of Diagonal State Space Models", "year": 2022}], "gt_label": [1, 1, 1], "date": "2022-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_650", "valid": true}
{"query": "Which papers propose goal-driven forecasting to predict goal locations for future human walking trajectories?", "cited_paper": [{"arxiv_id": "2007.03672", "title": "Long-term Human Motion Prediction with Scene Context", "year": 2020}], "gt_label": [1], "date": "2020-07", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_651", "valid": true}
{"query": "Could you list the works exploring the methodologies to effectively utilize the inherent prior knowledge within CAD?", "cited_paper": [{"arxiv_id": "2010.04762", "title": "Counterfactually-Augmented SNLI Training Data Does Not Yield Better Generalization Than Unaugmented Data", "year": 2020}, {"arxiv_id": "2107.00753", "title": "An Investigation of the (In)effectiveness of Counterfactually Augmented Data", "year": 2021}, {"arxiv_id": "2310.06666", "title": "Unlock the Potential of Counterfactually-Augmented Data in Out-Of-Distribution Generalization", "year": 2023}], "gt_label": [1, 1, 1], "date": "2023-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_652", "valid": true}
{"query": "Which papers discuss the concept of semantic meaning and algebraic structure of popular embeddings?", "cited_paper": [{"arxiv_id": "1810.04882", "title": "Towards Understanding Linear Word Analogies", "year": 2018}, {"arxiv_id": "2302.14383", "title": "Linear Spaces of Meanings: Compositional Structures in Vision-Language Models", "year": 2023}, {"arxiv_id": "2306.00310", "title": "Prompt Algebra for Task Composition", "year": 2023}], "gt_label": [1, 1, 1], "date": "2023-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_653", "valid": true}
{"query": "Which works illustrate that transformer-based LLMs have been trained on diverse, large-scale datasets to simultaneously learn several language understanding tasks?", "cited_paper": [{"arxiv_id": "1910.10683", "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer", "year": 2019}, {"arxiv_id": "2210.11416", "title": "Scaling Instruction-Finetuned Language Models", "year": 2022}], "gt_label": [1, 1], "date": "2022-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_654", "valid": true}
{"query": "What studies discuss techniques for data efficiency in deep learning?", "cited_paper": [{"arxiv_id": "1801.05401", "title": "Low-Shot Learning from Imaginary Data", "year": 2018}, {"arxiv_id": "2307.12168", "title": "Hallucination Improves the Performance of Unsupervised Visual Representation Learning", "year": 2023}, {"arxiv_id": "2307.14612", "title": "GenCo: An Auxiliary Generator from Contrastive Learning for Enhanced Few-Shot Learning in Remote Sensing", "year": 2023}], "gt_label": [1, 1, 1], "date": "2023-07", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_655", "valid": true}
{"query": "Could you provide me some works that focus on improving the quality of input reconstruction?", "cited_paper": [{"arxiv_id": "2003.14053", "title": "Inverting Gradients -- How easy is it to break privacy in federated learning?", "year": 2020}, {"arxiv_id": "2104.07586", "title": "See through Gradients: Image Batch Recovery via GradInversion", "year": 2021}, {"arxiv_id": "2110.14962", "title": "Gradient Inversion with Generative Image Prior", "year": 2021}], "gt_label": [1, 1, 1], "date": "2021-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_656", "valid": true}
{"query": "Could you provide me some works about model averaging methods that are further related to federated learning and ensemble methods?", "cited_paper": [{"arxiv_id": "2002.06440", "title": "Federated Learning with Matched Averaging", "year": 2020}, {"arxiv_id": "1910.05653", "title": "Model Fusion via Optimal Transport", "year": 2019}, {"arxiv_id": "2203.05482", "title": "Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time", "year": 2022}], "gt_label": [1, 1, 1], "date": "2022-03", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_657", "valid": true}
{"query": "Are there studies focused on mitigation of hallucinations during the pre-training stage by the process of dataset curating and cleaning?", "cited_paper": [{"arxiv_id": "2301.04449", "title": "Diving Deep into Modes of Fact Hallucinations in Dialogue Systems", "year": 2023}, {"arxiv_id": "2307.16883", "title": "HAGRID: A Human-LLM Collaborative Dataset for Generative Information-Seeking with Attribution", "year": 2023}, {"arxiv_id": "2307.15343", "title": "Med-HALT: Medical Domain Hallucination Test for Large Language Models", "year": 2023}], "gt_label": [1, 1, 1], "date": "2023-07", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_658", "valid": true}
{"query": "Could you provide examples of image-text datasets that have their own preprocessing techniques?", "cited_paper": [{"arxiv_id": "2103.00020", "title": "Learning Transferable Visual Models From Natural Language Supervision", "year": 2021}, {"arxiv_id": "2102.05918", "title": "Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision", "year": 2021}, {"arxiv_id": "2111.10050", "title": "Combined Scaling for Zero-shot Transfer Learning", "year": 2021}, {"arxiv_id": "2102.08981", "title": "Conceptual 12M: Pushing Web-Scale Image-Text Pre-Training To Recognize Long-Tail Visual Concepts", "year": 2021}, {"arxiv_id": "2111.11431", "title": "RedCaps: web-curated image-text data created by the people, for the people", "year": 2021}, {"arxiv_id": "2210.08402", "title": "LAION-5B: An open large-scale dataset for training next generation image-text models", "year": 2022}], "gt_label": [1, 1, 1, 1, 1, 1], "date": "2022-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_659", "valid": true}
{"query": "Which works consider the instruction-following abilities of LLMs?", "cited_paper": [{"arxiv_id": "2302.13971", "title": "LLaMA: Open and Efficient Foundation Language Models", "year": 2023}, {"arxiv_id": "2307.09288", "title": "Llama 2: Open Foundation and Fine-Tuned Chat Models", "year": 2023}], "gt_label": [1, 1], "date": "2023-07", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_660", "valid": true}
{"query": "Could you point out the studies which discusses the high variance problem of likelihood-ratio gradient?", "cited_paper": [{"arxiv_id": "1506.02438", "title": "High-Dimensional Continuous Control Using Generalized Advantage Estimation", "year": 2015}], "gt_label": [1], "date": "2015-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_661", "valid": true}
{"query": "What papers have introduced single-camera methods for egocentric pose estimation?", "cited_paper": [{"arxiv_id": "1803.05959", "title": "Mo2Cap2: Real-time Mobile 3D Motion Capture with a Cap-mounted Fisheye Camera", "year": 2018}, {"arxiv_id": "1907.10045", "title": "xR-EgoPose: Egocentric 3D Human Pose from an HMD Camera", "year": 2019}], "gt_label": [1, 1], "date": "2019-07", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_662", "valid": true}
{"query": "Which works propose a novel framework or expand the idea to obtain complex and difficult instructions gradually?", "cited_paper": [{"arxiv_id": "2304.12244", "title": "WizardLM: Empowering large pre-trained language models to follow complex instructions", "year": 2023}, {"arxiv_id": "2306.08568", "title": "WizardCoder: Empowering Code Large Language Models with Evol-Instruct", "year": 2023}, {"arxiv_id": "2306.11644", "title": "Textbooks Are All You Need", "year": 2023}], "gt_label": [1, 1, 1], "date": "2023-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_663", "valid": true}
{"query": "Could you provide me some studies that have applied the concept of teacher-student network?", "cited_paper": [{"arxiv_id": "1503.02531", "title": "Distilling the Knowledge in a Neural Network", "year": 2015}, {"arxiv_id": "2006.05525", "title": "Knowledge Distillation: A Survey", "year": 2020}, {"arxiv_id": "1906.01916", "title": "Semi-supervised semantic segmentation needs strong, varied perturbations", "year": 2019}, {"arxiv_id": "2110.05474", "title": "Semi-Supervised Semantic Segmentation via Adaptive Equalization Learning", "year": 2021}, {"arxiv_id": "2010.09713", "title": "PseudoSeg: Designing Pseudo Labels for Semantic Segmentation", "year": 2020}, {"arxiv_id": "2208.09910", "title": "Revisiting Weak-to-Strong Consistency in Semi-Supervised Semantic Segmentation", "year": 2022}, {"arxiv_id": "2102.09480", "title": "Unbiased Teacher for Semi-Supervised Object Detection", "year": 2021}, {"arxiv_id": "2106.10456", "title": "Humble Teachers Teach Better Students for Semi-Supervised Object Detection", "year": 2021}, {"arxiv_id": "2212.09335", "title": "Distilling Vision-Language Pre-training to Collaborate with Weakly-Supervised Temporal Action Localization", "year": 2022}, {"arxiv_id": "2106.09018", "title": "End-to-End Semi-Supervised Object Detection with Soft Teacher", "year": 2021}], "gt_label": [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], "date": "2022-12", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_664", "valid": true}
{"query": "What papers extended the methods to address pose estimation for novel objects or low-textured objects?", "cited_paper": [{"arxiv_id": "2205.12257", "title": "OnePose: One-Shot Object Pose Estimation without CAD Models", "year": 2022}, {"arxiv_id": "2301.07673", "title": "OnePose++: Keypoint-Free One-Shot Object Pose Estimation without CAD Models", "year": 2023}], "gt_label": [1, 1], "date": "2023-01", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_665", "valid": true}
{"query": "What studies propose generative adversarial networks for 3D shape generation?", "cited_paper": [{"arxiv_id": "1707.02392", "title": "Learning Representations and Generative Models for 3D Point Clouds", "year": 2017}, {"arxiv_id": "1905.06292", "title": "3D Point Cloud Generative Adversarial Network Based on Tree Structured Graph Convolutions", "year": 2019}], "gt_label": [1, 1], "date": "2019-05", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_666", "valid": true}
{"query": "What paper demonstrated that a model trained on synthetic captions can perform better than one trained on human-provided captions?", "cited_paper": [{"arxiv_id": "2207.07635", "title": "Is a Caption Worth a Thousand Images? A Controlled Study for Representation Learning", "year": 2022}], "gt_label": [1], "date": "2022-07", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_667", "valid": true}
{"query": "Are there any studies that added a few trainable parameters representing new knowledge to LLMs, while keeping the original parameters frozen?", "cited_paper": [{"arxiv_id": "2210.03329", "title": "Calibrating Factual Knowledge in Pretrained Language Models", "year": 2022}, {"arxiv_id": "2301.09785", "title": "Transformer-Patcher: One Mistake worth One Neuron", "year": 2023}, {"arxiv_id": "2211.13317", "title": "Rank-One Editing of Encoder-Decoder Models", "year": 2022}, {"arxiv_id": "2208.00399", "title": "Neural Knowledge Bank for Pretrained Transformers", "year": 2022}], "gt_label": [1, 1, 1, 1], "date": "2023-01", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_668", "valid": true}
{"query": "Can you provide references where global non-convex optimization is applied in machine learning?", "cited_paper": [{"arxiv_id": "1001.4475", "title": "X-Armed Bandits", "year": 2010}], "gt_label": [1], "date": "2017-03", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_669", "valid": true}
{"query": "Which works are known for tackling various tasks simultaneously in the field of vision-language models?", "cited_paper": [{"arxiv_id": "2103.00020", "title": "Learning Transferable Visual Models From Natural Language Supervision", "year": 2021}, {"arxiv_id": "2204.14198", "title": "Flamingo: a Visual Language Model for Few-Shot Learning", "year": 2022}, {"arxiv_id": "2104.00743", "title": "Towards General Purpose Vision Systems", "year": 2021}, {"arxiv_id": "2111.11430", "title": "Class-agnostic Object Detection with Multi-modal Transformer", "year": 2021}], "gt_label": [1, 1, 1, 1], "date": "2022-04", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_670", "valid": true}
{"query": "What datasets are used for knowledge editing?", "cited_paper": [{"arxiv_id": "1706.04115", "title": "Zero-Shot Relation Extraction via Reading Comprehension", "year": 2017}, {"arxiv_id": "2202.05262", "title": "Locating and Editing Factual Associations in GPT", "year": 2022}, {"arxiv_id": "2305.14795", "title": "MQuAKE: Assessing Knowledge Editing in Language Models via Multi-Hop Questions", "year": 2023}, {"arxiv_id": "2308.09954", "title": "DocTER: Evaluating Document-based Knowledge Editing", "year": 2023}], "gt_label": [1, 1, 1, 1], "date": "2023-08", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_671", "valid": true}
{"query": "What works proposed a method to learn a linear subspace for the disentanglement of written text from visual components?", "cited_paper": [{"arxiv_id": "2206.07835", "title": "Disentangling visual and written concepts in CLIP", "year": 2022}], "gt_label": [1], "date": "2022-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_672", "valid": true}
{"query": "Could you provide me some studies about the use of case-based reasoning?", "cited_paper": [{"arxiv_id": "2104.08762", "title": "Case-based Reasoning for Natural Language Queries over Knowledge Bases", "year": 2021}, {"arxiv_id": "2204.08554", "title": "CBR-iKB: A Case-Based Reasoning Approach for Question Answering over Incomplete Knowledge Bases", "year": 2022}, {"arxiv_id": "2202.10610", "title": "Knowledge Base Question Answering by Case-based Reasoning over Subgraphs", "year": 2022}], "gt_label": [1, 1, 1], "date": "2022-04", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_673", "valid": true}
{"query": "Any existing research on generating the 3D human avatars with predefined parametric human templates?", "cited_paper": [{"arxiv_id": "2211.14589", "title": "AvatarGen: A 3D Generative Model for Animatable Human Avatars", "year": 2022}, {"arxiv_id": "2210.04888", "title": "EVA3D: Compositional 3D Human Generation from 2D Image Collections", "year": 2022}, {"arxiv_id": "2204.08839", "title": "Unsupervised Learning of Efficient Geometry-Aware Neural Articulated Representations", "year": 2022}, {"arxiv_id": "2206.14314", "title": "Generative Neural Articulated Radiance Fields", "year": 2022}, {"arxiv_id": "2112.01422", "title": "3D-Aware Semantic-Guided Generative Model for Human Synthesis", "year": 2021}], "gt_label": [1, 1, 1, 1, 1], "date": "2022-11", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_674", "valid": true}
{"query": "What papers provided a theoratical analysis of the mechanism with respect to mean squared error of the noise?", "cited_paper": [{"arxiv_id": "2211.05006", "title": "Almost Tight Error Bounds on Differentially Private Continual Counting", "year": 2022}], "gt_label": [1], "date": "2022-11", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_675", "valid": true}
{"query": "Which papers propose solutions for panel detection in comic understanding?", "cited_paper": [{"arxiv_id": "1803.08670", "title": "Object Detection for Comics using Manga109 Annotations", "year": 2018}], "gt_label": [1], "date": "2018-03", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_676", "valid": true}
{"query": "What studies reported varying performance of LLMs when generating different types of reasoning processes?", "cited_paper": [{"arxiv_id": "2211.12588", "title": "Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks", "year": 2022}, {"arxiv_id": "2211.10435", "title": "PAL: Program-aided Language Models", "year": 2022}, {"arxiv_id": "2305.17812", "title": "Tab-CoT: Zero-shot Tabular Chain of Thought", "year": 2023}], "gt_label": [1, 1, 1], "date": "2023-05", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_677", "valid": true}
{"query": "Which papers propose heuristic-based uncertainty metrics for generative Large Language Models (LLMs) considering machine translation?", "cited_paper": [{"arxiv_id": "2006.08344", "title": "Wat zei je? Detecting Out-of-Distribution Translations with Variational Transformers", "year": 2020}, {"arxiv_id": "2005.10608", "title": "Unsupervised Quality Estimation for Neural Machine Translation", "year": 2020}], "gt_label": [1, 1], "date": "2020-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_678", "valid": true}
{"query": "Which studies focus on 3D pose estimation task through a single RGB image input?", "cited_paper": [{"arxiv_id": "1712.06584", "title": "End-to-end Recovery of Human Shape and Pose", "year": 2017}, {"arxiv_id": "1909.12828", "title": "Learning to Reconstruct 3D Human Pose and Shape via Model-fitting in the Loop", "year": 2019}], "gt_label": [1, 1], "date": "2019-09", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_679", "valid": true}
{"query": "Which papers propose methods for audio-visual segmentation task?", "cited_paper": [{"arxiv_id": "2112.11749", "title": "Class-aware Sounding Objects Localization via Audiovisual Correspondence", "year": 2021}, {"arxiv_id": "2010.05466", "title": "Discriminative Sounding Objects Localization via Self-supervised Audiovisual Matching", "year": 2020}, {"arxiv_id": "1807.03094", "title": "Deep Multimodal Clustering for Unsupervised Audiovisual Learning", "year": 2018}, {"arxiv_id": "2104.00315", "title": "Unsupervised Sound Localization via Iterative Contrastive Learning", "year": 2021}, {"arxiv_id": "2104.02691", "title": "Localizing Visual Sounds the Hard Way", "year": 2021}, {"arxiv_id": "2206.12772", "title": "Exploiting Transformation Invariance and Equivariance for Self-supervised Sound Localisation", "year": 2022}, {"arxiv_id": "1803.03849", "title": "Learning to Localize Sound Source in Visual Scenes", "year": 2018}, {"arxiv_id": "2007.06355", "title": "Multiple Sound Sources Localization from Coarse to Fine", "year": 2020}, {"arxiv_id": "2305.11019", "title": "Annotation-free Audio-Visual Segmentation", "year": 2023}], "gt_label": [1, 1, 1, 1, 1, 1, 1, 1, 1], "date": "2023-05", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_680", "valid": true}
{"query": "Which works are variants of the FedAvg algorithm aimed to resolve the minimization problem in federated learning?", "cited_paper": [{"arxiv_id": "1805.09767", "title": "Local SGD Converges Fast and Communicates Little", "year": 2018}, {"arxiv_id": "1807.06629", "title": "Parallel Restarted SGD with Faster Convergence and Less Communication: Demystifying Why Model Averaging Works for Deep Learning", "year": 2018}, {"arxiv_id": "1905.03817", "title": "On the Linear Speedup Analysis of Communication Efficient Momentum SGD for Distributed Non-Convex Optimization", "year": 2019}], "gt_label": [1, 1, 1], "date": "2019-05", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_681", "valid": true}
{"query": "What was the research that put forward a data augmentation-dependent method for contrastive learning for object-centric representations?", "cited_paper": [{"arxiv_id": "2203.05997", "title": "Towards Self-Supervised Learning of Global and Object-Centric Representations", "year": 2022}], "gt_label": [1], "date": "2022-03", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_682", "valid": true}
{"query": "Which studies focus on table-based EHR question answering?", "cited_paper": [{"arxiv_id": "2111.14703", "title": "Question Answering for Complex Electronic Health Records Database using Unified Encoder-Decoder Architecture", "year": 2021}, {"arxiv_id": "2301.07695", "title": "EHRSQL: A Practical Text-to-SQL Benchmark for Electronic Health Records", "year": 2023}, {"arxiv_id": "2303.12898", "title": "Towards Understanding the Generalization of Medical Text-to-SQL Models and Datasets", "year": 2023}, {"arxiv_id": "1908.01839", "title": "Text-to-SQL Generation for Question Answering on Electronic Medical Records", "year": 2019}], "gt_label": [1, 1, 1, 1], "date": "2023-04", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_683", "valid": true}
{"query": "Can you provide examples of studies about the methodology of studying games that gradually change over time?", "cited_paper": [{"arxiv_id": "1505.00720", "title": "Econometrics for Learning Agents", "year": 2015}, {"arxiv_id": "1505.00391", "title": "Learning and Efficiency in Games with Dynamic Population", "year": 2015}, {"arxiv_id": "1605.03838", "title": "An Experimental Evaluation of Regret-Based Econometrics", "year": 2016}], "gt_label": [1, 1, 1], "date": "2020-09", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_684", "valid": true}
{"query": "Are there any studies where the shuffling-based method was applied to FL?", "cited_paper": [{"arxiv_id": "2110.10342", "title": "Minibatch vs Local SGD with Shuffling: Tight Convergence Bounds and Beyond", "year": 2021}], "gt_label": [1], "date": "2023-02", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_685", "valid": true}
{"query": "Can you list the research works where the number of timesteps used to train SNNs have been reduced?", "cited_paper": [{"arxiv_id": "2005.01807", "title": "Enabling Deep Spiking Neural Networks with Hybrid Conversion and Spike Timing Dependent Backpropagation", "year": 2020}, {"arxiv_id": "2306.03693", "title": "ESL-SNNs: An Evolutionary Structure Learning Strategy for Spiking Neural Networks", "year": 2023}, {"arxiv_id": "2011.05280", "title": "Going Deeper With Directly-Trained Larger Spiking Neural Networks", "year": 2020}], "gt_label": [1, 1, 1], "date": "2023-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_686", "valid": true}
{"query": "Could you mention the studies that focused on lifting 2D pre-trained models to create 3D models from textual prompts?", "cited_paper": [{"arxiv_id": "2211.10440", "title": "Magic3D: High-Resolution Text-to-3D Content Creation", "year": 2022}, {"arxiv_id": "2303.13873", "title": "Fantasia3D: Disentangling Geometry and Appearance for High-quality Text-to-3D Content Creation", "year": 2023}, {"arxiv_id": "2305.16213", "title": "ProlificDreamer: High-Fidelity and Diverse Text-to-3D Generation with Variational Score Distillation", "year": 2023}, {"arxiv_id": "2306.07349", "title": "ATT3D: Amortized Text-to-3D Object Synthesis", "year": 2023}, {"arxiv_id": "2305.16411", "title": "ZeroAvatar: Zero-shot 3D Avatar Generation from a Single Image", "year": 2023}, {"arxiv_id": "2307.01097", "title": "MVDiffusion: Enabling Holistic Multi-view Image Generation with Correspondence-Aware Diffusion", "year": 2023}, {"arxiv_id": "2308.14078", "title": "Sparse3D: Distilling Multiview-Consistent Diffusion for Object Reconstruction from Sparse Views", "year": 2023}, {"arxiv_id": "2308.16512", "title": "MVDream: Multi-view Diffusion for 3D Generation", "year": 2023}], "gt_label": [1, 1, 1, 1, 1, 1, 1, 1], "date": "2023-08", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_687", "valid": true}
{"query": "Which studies have discussed a two-step approach for Temporal Action Detection (TAD)?", "cited_paper": [{"arxiv_id": "1907.09702", "title": "BMN: Boundary-Matching Network for Temporal Action Proposal Generation", "year": 2019}, {"arxiv_id": "1806.02964", "title": "BSN: Boundary Sensitive Network for Temporal Action Proposal Generation", "year": 2018}, {"arxiv_id": "1911.11462", "title": "G-TAD: Sub-Graph Localization for Temporal Action Detection", "year": 2019}, {"arxiv_id": "2011.14598", "title": "Video Self-Stitching Graph Network for Temporal Action Localization", "year": 2020}], "gt_label": [1, 1, 1, 1], "date": "2020-11", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_688", "valid": true}
{"query": "Could you provide me the study that constructed the emrKBQA dataset for patient-specific QA on MIMIC-III?", "cited_paper": [], "gt_label": [], "date": "2018-09", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_689", "valid": true}
{"query": "What research papers are provided for additive valuations with binary marginals?", "cited_paper": [{"arxiv_id": "2002.10171", "title": "A Probabilistic Approach to Voting, Allocation, Matching, and Coalition Formation", "year": 2020}, {"arxiv_id": "2007.06073", "title": "Fair Division with Binary Valuations: One Rule to Rule Them All", "year": 2020}], "gt_label": [1, 1], "date": "2020-07", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_690", "valid": true}
{"query": "What research studies propose changing the weight in convolutional layers according to input features using the attention mechanism?", "cited_paper": [{"arxiv_id": "1904.04971", "title": "CondConv: Conditionally Parameterized Convolutions for Efficient Inference", "year": 2019}, {"arxiv_id": "2102.04906", "title": "Dynamic Neural Networks: A Survey", "year": 2021}], "gt_label": [1, 1], "date": "2021-02", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_691", "valid": true}
{"query": "Can you inform me about papers which suggested that pretraining on domain-specific text can enhance language model performance on related tasks?", "cited_paper": [{"arxiv_id": "1903.10676", "title": "SciBERT: A Pretrained Language Model for Scientific Text", "year": 2019}, {"arxiv_id": "2007.15779", "title": "Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing", "year": 2020}, {"arxiv_id": "2010.02559", "title": "LEGAL-BERT: The Muppets straight out of Law School", "year": 2020}, {"arxiv_id": "2211.17135", "title": "BudgetLongformer: Can we Cheaply Pretrain a SotA Legal Language Model From Scratch?", "year": 2022}], "gt_label": [1, 1, 1, 1], "date": "2022-11", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_692", "valid": true}
{"query": "Which papers have discussed Bound Propagation methods and analyzed the output bounds based on input bounds?", "cited_paper": [{"arxiv_id": "1811.01057", "title": "Semidefinite relaxations for certifying robustness to adversarial examples", "year": 2018}, {"arxiv_id": "1711.00851", "title": "Provable defenses against adversarial examples via the convex outer adversarial polytope", "year": 2017}, {"arxiv_id": "1811.00866", "title": "Efficient Neural Network Robustness Certification with General Activation Functions", "year": 2018}, {"arxiv_id": "1906.12269", "title": "Certifiable Robustness and Robust Training for Graph Convolutional Networks", "year": 2019}, {"arxiv_id": "2302.02829", "title": "Collective Robustness Certificates: Exploiting Interdependence in Graph Neural Networks", "year": 2023}], "gt_label": [1, 1, 1, 1, 1], "date": "2023-02", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_693", "valid": true}
{"query": "What studies present the application of the measurement theory concepts to NLP where desirable model capabilities are unobservable constructs?", "cited_paper": [{"arxiv_id": "2305.14889", "title": "Evaluating Evaluation Metrics: A Framework for Analyzing NLG Evaluation Metrics using Measurement Theory", "year": 2023}], "gt_label": [1], "date": "2023-05", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_694", "valid": true}
{"query": "Any works about continual pretraining for encoder-decoder LMs?", "cited_paper": [{"arxiv_id": "2302.09170", "title": "KILM: Knowledge Injection into Encoder-Decoder Language Models", "year": 2023}, {"arxiv_id": "2110.08534", "title": "Lifelong Pretraining: Continually Adapting Language Models to Emerging Corpora", "year": 2021}], "gt_label": [1, 1], "date": "2023-02", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_695", "valid": true}
{"query": "Which study proposed to minimize a robust loss and find worst-case perturbation during training with projected gradient descent?", "cited_paper": [{"arxiv_id": "1706.06083", "title": "Towards Deep Learning Models Resistant to Adversarial Attacks", "year": 2017}], "gt_label": [1], "date": "2017-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_696", "valid": true}
{"query": "Which works have investigated the advantages of decoupling policy and value functions for generalization in RL?", "cited_paper": [{"arxiv_id": "2102.10330", "title": "Decoupling Value and Policy for Generalization in Reinforcement Learning", "year": 2021}, {"arxiv_id": "2009.04416", "title": "Phasic Policy Gradient", "year": 2020}], "gt_label": [1, 1], "date": "2021-02", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_697", "valid": true}
{"query": "Which work considered the notion of envy-freeness up to any item (EFX) in fractionally subadditive valuations?", "cited_paper": [{"arxiv_id": "1707.04769", "title": "Almost Envy-Freeness with General Valuations", "year": 2017}], "gt_label": [1], "date": "2017-07", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_698", "valid": true}
{"query": "Which papers demonstrate story generation strategies for better coherency in story-telling using language models?", "cited_paper": [{"arxiv_id": "1811.05701", "title": "Plan-And-Write: Towards Better Automatic Storytelling", "year": 2018}], "gt_label": [1], "date": "2018-11", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_699", "valid": true}
{"query": "Which studies utilized the concept of query for different applications like detection of different objects, video instance segmentation, multiple object tracking, and video object detection?", "cited_paper": [{"arxiv_id": "2005.12872", "title": "End-to-End Object Detection with Transformers", "year": 2020}, {"arxiv_id": "2105.03247", "title": "MOTR: End-to-End Multiple-Object Tracking with Transformer", "year": 2021}, {"arxiv_id": "2101.02702", "title": "TrackFormer: Multi-Object Tracking with Transformers", "year": 2021}, {"arxiv_id": "2011.14503", "title": "End-to-End Video Instance Segmentation with Transformers", "year": 2020}], "gt_label": [1, 1, 1, 1], "date": "2021-05", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_700", "valid": true}
{"query": "Can you list some research papers that adopted a similar formulation for advertising?", "cited_paper": [{"arxiv_id": "1202.1483", "title": "Send Mixed Signals -- Earn More, Work Less", "year": 2012}, {"arxiv_id": "1202.1590", "title": "Signaling Schemes for Revenue Maximization", "year": 2012}], "gt_label": [1, 1], "date": "2022-12", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_701", "valid": true}
{"query": "What papers use LLM to retrieve reasoning paths from KGs based on relation 'plans' grounded by KGs?", "cited_paper": [{"arxiv_id": "2310.01061", "title": "Reasoning on Graphs: Faithful and Interpretable Large Language Model Reasoning", "year": 2023}], "gt_label": [1], "date": "2023-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_702", "valid": true}
{"query": "What study focused on examining node classification performance of one-layer GATs on a random graph model?", "cited_paper": [{"arxiv_id": "2202.13060", "title": "Graph Attention Retrospective", "year": 2022}], "gt_label": [1], "date": "2022-02", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_703", "valid": true}
{"query": "What works proposed variants on normalisation, which is complementary to unit scaling?", "cited_paper": [{"arxiv_id": "1502.03167", "title": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift", "year": 2015}, {"arxiv_id": "1607.06450", "title": "Layer Normalization", "year": 2016}, {"arxiv_id": "2106.03743", "title": "Proxy-Normalizing Activations to Match Batch Normalization while Removing Batch Dependence", "year": 2021}, {"arxiv_id": "1602.07868", "title": "Weight Normalization: A Simple Reparameterization to Accelerate Training of Deep Neural Networks", "year": 2016}], "gt_label": [1, 1, 1, 1], "date": "2021-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_704", "valid": true}
{"query": "Which papers introduced algorithms regarding distributed optimization in a full participation setting using deterministic methods?", "cited_paper": [{"arxiv_id": "1312.7853", "title": "Communication Efficient Distributed Optimization using an Approximate Newton-type Method", "year": 2013}, {"arxiv_id": "1908.02246", "title": "On Convergence of Distributed Approximate Newton Methods: Globalization, Sharper Bounds and Beyond", "year": 2019}, {"arxiv_id": "2002.10726", "title": "Statistically Preconditioned Accelerated Gradient Method for Distributed Optimization", "year": 2020}, {"arxiv_id": "2102.06780", "title": "Newton Method over Networks is Fast up to the Statistical Precision", "year": 2021}], "gt_label": [1, 1, 1, 1], "date": "2021-03", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_705", "valid": true}
{"query": "Which works used alignment approaches to capture the feature of domain invariant characteristics?", "cited_paper": [{"arxiv_id": "1711.03213", "title": "CyCADA: Cycle-Consistent Adversarial Domain Adaptation", "year": 2017}, {"arxiv_id": "1811.08585", "title": "Progressive Feature Alignment for Unsupervised Domain Adaptation", "year": 2018}, {"arxiv_id": "1612.02649", "title": "FCNs in the Wild: Pixel-level Adversarial and Constraint-based Adaptation", "year": 2016}, {"arxiv_id": "2303.14360", "title": "Both Style and Distortion Matter: Dual-Path Unsupervised Domain Adaptation for Panoramic Semantic Segmentation", "year": 2023}], "gt_label": [1, 1, 1, 1], "date": "2023-03", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_706", "valid": true}
{"query": "Which works extended the binary meta-learners in the CATE's estimation without a thorough theoretical analysis of their behaviour?", "cited_paper": [{"arxiv_id": "1908.05372", "title": "Uplift Modeling for Multiple Treatments with Cost Optimization", "year": 2019}], "gt_label": [1], "date": "2019-08", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_707", "valid": true}
{"query": "Can you specify the papers that introduced novel pre-training tasks for transformer-based pre-trained models?", "cited_paper": [{"arxiv_id": "2002.08155", "title": "CodeBERT: A Pre-Trained Model for Programming and Natural Languages", "year": 2020}, {"arxiv_id": "2109.00859", "title": "CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation", "year": 2021}, {"arxiv_id": "2203.03850", "title": "UniXcoder: Unified Cross-Modal Pre-training for Code Representation", "year": 2022}], "gt_label": [1, 1, 1], "date": "2022-03", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_708", "valid": true}
{"query": "Can you provide me some studies that use clustering or topic modeling to identify aspects in documents?", "cited_paper": [{"arxiv_id": "0801.1063", "title": "Modeling Online Reviews with Multi-grain Topic Models", "year": 2008}], "gt_label": [1], "date": "2008-01", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_709", "valid": true}
{"query": "What studies utilized VGG, ResNet, LSTM, and customized loss functions to enhance vision tasks in deep learning?", "cited_paper": [{"arxiv_id": "1409.1556", "title": "Very Deep Convolutional Networks for Large-Scale Image Recognition", "year": 2014}, {"arxiv_id": "1512.03385", "title": "Deep Residual Learning for Image Recognition", "year": 2015}, {"arxiv_id": "1611.07890", "title": "Image-based localization using LSTMs for structured feature correlation", "year": 2016}], "gt_label": [1, 1, 1], "date": "2016-11", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_710", "valid": true}
{"query": "What works are about data-driven LiDAR simulators?", "cited_paper": [{"arxiv_id": "2006.09348", "title": "LiDARsim: Realistic LiDAR Simulation by Leveraging the Real World", "year": 2020}, {"arxiv_id": "2112.00050", "title": "Pattern-Aware Data Augmentation for LiDAR 3D Object Detection", "year": 2021}, {"arxiv_id": "2111.12083", "title": "VISTA 2.0: An Open, Data-driven Simulator for Multimodal Sensing and Policy Learning for Autonomous Vehicles", "year": 2021}, {"arxiv_id": "2111.12137", "title": "Learning Interactive Driving Policies via Data-driven Simulation", "year": 2021}], "gt_label": [1, 1, 1, 1], "date": "2021-12", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_711", "valid": true}
{"query": "Can you specify some multimodal pretraining methods that require paired or interleaved data?", "cited_paper": [{"arxiv_id": "2111.02358", "title": "VLMo: Unified Vision-Language Pre-Training with Mixture-of-Modality-Experts", "year": 2021}, {"arxiv_id": "2108.10904", "title": "SimVLM: Simple Visual Language Model Pretraining with Weak Supervision", "year": 2021}, {"arxiv_id": "2208.10442", "title": "Image as a Foreign Language: BEiT Pretraining for All Vision and Vision-Language Tasks", "year": 2022}, {"arxiv_id": "2204.14198", "title": "Flamingo: a Visual Language Model for Few-Shot Learning", "year": 2022}], "gt_label": [1, 1, 1, 1], "date": "2022-08", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_712", "valid": true}
{"query": "What references mention the use of generative adversarial networks (GANs) in recent advancements in reconstruction from fMRI techniques?", "cited_paper": [{"arxiv_id": "2210.01769", "title": "Mind Reader: Reconstructing complex images from brain activities", "year": 2022}, {"arxiv_id": "2001.11761", "title": "Reconstructing Natural Scenes from fMRI Patterns using BigBiGAN", "year": 2020}, {"arxiv_id": "2202.12692", "title": "Reconstruction of Perceived Images from fMRI Patterns and Semantic Brain Exploration using Instance-Conditioned GANs", "year": 2022}], "gt_label": [1, 1, 1], "date": "2022-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_713", "valid": true}
{"query": "Which papers discuss the two mainstream approaches for multi-hop KGQA: Semantic Parsing and Information Retrieval?", "cited_paper": [{"arxiv_id": "2105.11644", "title": "A Survey on Complex Knowledge Base Question Answering: Methods, Challenges and Solutions", "year": 2021}], "gt_label": [1], "date": "2021-05", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_714", "valid": true}
{"query": "Which publications introduced the maximum entropy exploration (maxEnt) framework?", "cited_paper": [{"arxiv_id": "1812.02690", "title": "Provably Efficient Maximum Entropy Exploration", "year": 2018}, {"arxiv_id": "2007.04640", "title": "Task-Agnostic Exploration via Policy Gradient of a Non-Parametric State Entropy Estimate", "year": 2020}], "gt_label": [1, 1], "date": "2020-07", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_715", "valid": true}
{"query": "What papers recently gave attention to maximum entropy policies in the context of reinforcement learning (RL)?", "cited_paper": [{"arxiv_id": "2103.04551", "title": "Behavior From the Void: Unsupervised Active Pre-Training", "year": 2021}, {"arxiv_id": "2108.13956", "title": "APS: Active Pretraining with Successor Features", "year": 2021}, {"arxiv_id": "2102.09430", "title": "State Entropy Maximization with Random Encoders for Efficient Exploration", "year": 2021}, {"arxiv_id": "1812.02690", "title": "Provably Efficient Maximum Entropy Exploration", "year": 2018}, {"arxiv_id": "2007.04640", "title": "Task-Agnostic Exploration via Policy Gradient of a Non-Parametric State Entropy Estimate", "year": 2020}], "gt_label": [1, 1, 1, 1, 1], "date": "2021-08", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_716", "valid": true}
{"query": "Which research papers have introduced more sophisticated network structures for the mapping between hazy and clear images in the context of image dehazing?", "cited_paper": [{"arxiv_id": "2111.14813", "title": "TransWeather: Transformer-based Restoration of Images Degraded by Adverse Weather Conditions", "year": 2021}, {"arxiv_id": "2308.14036", "title": "MB-TaylorFormer: Multi-branch Efficient Transformer Expanded by Taylor Formula for Image Dehazing", "year": 2023}], "gt_label": [1, 1], "date": "2023-08", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_717", "valid": true}
{"query": "Which papers discuss that Large Language Models (LLMs) memorize data both from their original large training corpora and smaller private datasets used for downstream tasks?", "cited_paper": [{"arxiv_id": "2202.07646", "title": "Quantifying Memorization Across Neural Language Models", "year": 2022}, {"arxiv_id": "2110.02782", "title": "How BPE Affects Memorization in Transformers", "year": 2021}, {"arxiv_id": "2205.10770", "title": "Memorization Without Overfitting: Analyzing the Training Dynamics of Large Language Models", "year": 2022}, {"arxiv_id": "2112.12938", "title": "Counterfactual Memorization in Neural Language Models", "year": 2021}, {"arxiv_id": "2205.12506", "title": "Memorization in NLP Fine-tuning Methods", "year": 2022}], "gt_label": [1, 1, 1, 1, 1], "date": "2022-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_718", "valid": true}
{"query": "Which papers exploited feature-space/n-gram discrepancy measures for selecting data in domain adaptation setting?", "cited_paper": [{"arxiv_id": "1707.05246", "title": "Learning to select data for transfer learning with Bayesian Optimization", "year": 2017}], "gt_label": [1], "date": "2017-07", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_719", "valid": true}
{"query": "Any works that employed a consistency-based method for confidence estimation?", "cited_paper": [{"arxiv_id": "2311.08401", "title": "Fine-tuning Language Models for Factuality", "year": 2023}], "gt_label": [1], "date": "2023-11", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_720", "valid": true}
{"query": "Could you mention a study that proposed to decouple the bi-level optimization of dataset condensation?", "cited_paper": [{"arxiv_id": "2306.13092", "title": "Squeeze, Recover and Relabel: Dataset Condensation at ImageNet Scale From A New Perspective", "year": 2023}], "gt_label": [1], "date": "2023-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_721", "valid": true}
{"query": "What research papers have been involved in the use of kernel fusion for efficient inference techniques in large language models (LLMs)?", "cited_paper": [{"arxiv_id": "2205.14135", "title": "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness", "year": 2022}], "gt_label": [1], "date": "2022-05", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_722", "valid": true}
{"query": "What works used Moore-Lewis selection in selection of examples?", "cited_paper": [{"arxiv_id": "1709.02279", "title": "Cynical Selection of Language Model Training Data", "year": 2017}, {"arxiv_id": "2210.10951", "title": "Automatic Document Selection for Efficient Encoder Pretraining", "year": 2022}], "gt_label": [1, 1], "date": "2022-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_723", "valid": true}
{"query": "Which work first introduced the wait-k policy for simultaneous text translation?", "cited_paper": [], "gt_label": [], "date": "2018-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_724", "valid": true}
{"query": "In which study did the researchers design the Poolingformer technique?", "cited_paper": [{"arxiv_id": "2105.04371", "title": "Poolingformer: Long Document Modeling with Pooling Attention", "year": 2021}], "gt_label": [1], "date": "2021-05", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_725", "valid": true}
{"query": "Which works studied the offline total variation denoising problem?", "cited_paper": [{"arxiv_id": "1410.7690", "title": "Trend Filtering on Graphs", "year": 2014}], "gt_label": [1], "date": "2017-02", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_726", "valid": true}
{"query": "Who initially introduced the Diffusion Generative Model?", "cited_paper": [{"arxiv_id": "1503.03585", "title": "Deep Unsupervised Learning using Nonequilibrium Thermodynamics", "year": 2015}], "gt_label": [1], "date": "2015-03", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_727", "valid": true}
{"query": "What studies have applied machine learning models for connectivity analysis on brain networks?", "cited_paper": [{"arxiv_id": "2004.13321", "title": "Machine Learning Methods for Brain Network Classification: Application to Autism Diagnosis using Cortical Morphological Networks", "year": 2020}, {"arxiv_id": "2006.05176", "title": "Explainable Classification of Brain Networks via Contrast Subgraphs", "year": 2020}], "gt_label": [1, 1], "date": "2020-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_728", "valid": true}
{"query": "Could you provide me some works that involve the application of contrastive learning in diverse data modalities?", "cited_paper": [{"arxiv_id": "2002.05709", "title": "A Simple Framework for Contrastive Learning of Visual Representations", "year": 2020}, {"arxiv_id": "2103.00020", "title": "Learning Transferable Visual Models From Natural Language Supervision", "year": 2021}], "gt_label": [1, 1], "date": "2021-03", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_729", "valid": true}
{"query": "What work introduced a K-Planes decomposition technique designed to reconstruct static 3D scenes and dynamic 4D videos?", "cited_paper": [{"arxiv_id": "2301.10241", "title": "K-Planes: Explicit Radiance Fields in Space, Time, and Appearance", "year": 2023}], "gt_label": [1], "date": "2023-01", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_730", "valid": true}
{"query": "In which papers did the researchers discuss the KRR in misspecified case?", "cited_paper": [{"arxiv_id": "1805.10074", "title": "Statistical Optimality of Stochastic Gradient Descent on Hard Learning Problems through Multiple Passes", "year": 2018}], "gt_label": [1], "date": "2022-08", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_731", "valid": true}
{"query": "Which papers implemented neural networks like CNNs and RNNs to enhance co-embedding methods?", "cited_paper": [{"arxiv_id": "2201.03545", "title": "A ConvNet for the 2020s", "year": 2022}, {"arxiv_id": "1512.03385", "title": "Deep Residual Learning for Image Recognition", "year": 2015}, {"arxiv_id": "1409.4842", "title": "Going Deeper with Convolutions", "year": 2014}, {"arxiv_id": "1409.1556", "title": "Very Deep Convolutional Networks for Large-Scale Image Recognition", "year": 2014}, {"arxiv_id": "1808.03314", "title": "Fundamentals of Recurrent Neural Network (RNN) and Long Short-Term Memory (LSTM) Network", "year": 2018}], "gt_label": [1, 1, 1, 1, 1], "date": "2022-01", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_732", "valid": true}
{"query": "Which papers approached the problem of Source-free Unsupervised Domain Adaptation (SFUDA) in the absence of source domain data?", "cited_paper": [{"arxiv_id": "2108.11249", "title": "Generalize then Adapt: Source-Free Domain Adaptive Semantic Segmentation", "year": 2021}, {"arxiv_id": "2110.03374", "title": "Model Adaptation: Historical Contrastive Learning for Unsupervised Domain Adaptation without Source Data", "year": 2021}], "gt_label": [1, 1], "date": "2021-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_733", "valid": true}
{"query": "Could you provide me some works about training convolutional networks for 6D Pose Estimation?", "cited_paper": [{"arxiv_id": "1703.06870", "title": "Mask R-CNN", "year": 2017}], "gt_label": [1], "date": "2017-03", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_734", "valid": true}
{"query": "Could you provide studies about ensemble methods for yielding pixel-wise uncertainty estimates?", "cited_paper": [{"arxiv_id": "1612.01474", "title": "Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles", "year": 2016}], "gt_label": [1], "date": "2016-12", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_735", "valid": true}
{"query": "What papers propose certified defenses which conduct a layer-by-layer analysis to derive the certified robustness guarantee of an unimodal model?", "cited_paper": [{"arxiv_id": "1711.00851", "title": "Provable defenses against adversarial examples via the convex outer adversarial polytope", "year": 2017}, {"arxiv_id": "1810.12715", "title": "On the Effectiveness of Interval Bound Propagation for Training Verifiably Robust Models", "year": 2018}, {"arxiv_id": "1702.01135", "title": "Reluplex: An Efficient SMT Solver for Verifying Deep Neural Networks", "year": 2017}], "gt_label": [1, 1, 1], "date": "2018-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_736", "valid": true}
{"query": "Could you provide me some studies that have addressed self-consistency in Large Language Models?", "cited_paper": [{"arxiv_id": "2203.11171", "title": "Self-Consistency Improves Chain of Thought Reasoning in Language Models", "year": 2022}], "gt_label": [1], "date": "2022-03", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_737", "valid": true}
{"query": "What datasets contain scenes where multiple people are performing various actions concurrently?", "cited_paper": [{"arxiv_id": "1705.08421", "title": "AVA: A Video Dataset of Spatio-temporally Localized Atomic Visual Actions", "year": 2017}, {"arxiv_id": "2005.00214", "title": "The AVA-Kinetics Localized Human Actions Video Dataset", "year": 2020}, {"arxiv_id": "2105.07404", "title": "MultiSports: A Multi-Person Video Dataset of Spatio-Temporally Localized Sports Actions", "year": 2021}], "gt_label": [1, 1, 1], "date": "2021-05", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_738", "valid": true}
{"query": "What papers are about analyzing and understanding conventions in overcoming coordination problems?", "cited_paper": [{"arxiv_id": "1811.01267", "title": "Legible Normativity for AI Alignment: The Value of Silly Rules", "year": 2018}], "gt_label": [1], "date": "2018-11", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_739", "valid": true}
{"query": "Could you give an example of research papers on techniques for distinguishing uncertain areas as the dimension of the goal space increases, in the field of curriculum goal generation?", "cited_paper": [{"arxiv_id": "1908.09540", "title": "Uncertainty-Aware Anticipation of Activities", "year": 2019}, {"arxiv_id": "2211.14304", "title": "BeLFusion: Latent Diffusion for Behavior-Driven Human Motion Prediction", "year": 2022}], "gt_label": [1, 1], "date": "2022-11", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_740", "valid": true}
{"query": "What are some studies about Operator Learning that leverage the Fourier transform?", "cited_paper": [{"arxiv_id": "2010.08895", "title": "Fourier Neural Operator for Parametric Partial Differential Equations", "year": 2020}, {"arxiv_id": "2111.13802", "title": "Factorized Fourier Neural Operators", "year": 2021}], "gt_label": [1, 1], "date": "2021-11", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_741", "valid": true}
{"query": "What work proposed the eNCE method and identified its limitations?", "cited_paper": [{"arxiv_id": "2110.11271", "title": "Analyzing and Improving the Optimization Landscape of Noise-Contrastive Estimation", "year": 2021}], "gt_label": [1], "date": "2021-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_742", "valid": true}
{"query": "Which publications use autoregressive models for molecular structure design?", "cited_paper": [], "gt_label": [], "date": "2019-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_743", "valid": true}
{"query": "Could you provide me some studies exploring to make Gaussian diffusion faster and more data efficient in training?", "cited_paper": [{"arxiv_id": "2304.12526", "title": "Patch Diffusion: Faster and More Data-Efficient Training of Diffusion Models", "year": 2023}, {"arxiv_id": "2303.09556", "title": "Efficient Diffusion Training via Min-SNR Weighting Strategy", "year": 2023}, {"arxiv_id": "2211.13449", "title": "Fast Sampling of Diffusion Models via Operator Learning", "year": 2022}, {"arxiv_id": "2306.06991", "title": "Fast Diffusion Model", "year": 2023}], "gt_label": [1, 1, 1, 1], "date": "2023-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_744", "valid": true}
{"query": "What research discusses the advantages of the PRM over the ORM in providing detailed feedback to enhance generators?", "cited_paper": [{"arxiv_id": "2306.01693", "title": "Fine-Grained Human Feedback Gives Better Rewards for Language Model Training", "year": 2023}], "gt_label": [1], "date": "2023-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_745", "valid": true}
{"query": "What research studies constructed datasets targeting multiple image issues and proposed context schemes to better understand interleaved inputs?", "cited_paper": [{"arxiv_id": "2309.07915", "title": "MMICL: Empowering Vision-language Model with Multi-Modal In-Context Learning", "year": 2023}, {"arxiv_id": "2305.03726", "title": "Otter: A Multi-Modal Model with In-Context Instruction Tuning", "year": 2023}], "gt_label": [1, 1], "date": "2023-09", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_746", "valid": true}
{"query": "Can you name some studies that tackled the objective mismatch issue in RLHF learning schemes?", "cited_paper": [{"arxiv_id": "2311.00168", "title": "The Alignment Ceiling: Objective Mismatch in Reinforcement Learning from Human Feedback", "year": 2023}], "gt_label": [1], "date": "2023-11", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_747", "valid": true}
{"query": "Can you provide papers that discuss the important role of the noise distribution choice in success of Noise-contrastive estimation?", "cited_paper": [{"arxiv_id": "1406.2661", "title": "Generative Adversarial Networks", "year": 2014}, {"arxiv_id": "1912.00589", "title": "Flow Contrastive Estimation of Energy-Based Models", "year": 2019}, {"arxiv_id": "2110.11271", "title": "Analyzing and Improving the Optimization Landscape of Noise-Contrastive Estimation", "year": 2021}], "gt_label": [1, 1, 1], "date": "2021-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_748", "valid": true}
{"query": "Which paper introduced the slot attention for object-centric learning?", "cited_paper": [{"arxiv_id": "2006.15055", "title": "Object-Centric Learning with Slot Attention", "year": 2020}], "gt_label": [1], "date": "2020-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_749", "valid": true}
{"query": "Can you show me some papers that focus on improving sample efficiency with sub-optimal external policies?", "cited_paper": [{"arxiv_id": "2002.07418", "title": "KoGuN: Accelerating Deep Reinforcement Learning via Integrating Human Suboptimal Knowledge", "year": 2020}], "gt_label": [1], "date": "2020-02", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_750", "valid": true}
{"query": "Which works studied the concept of co-training involving separate models to generate improved pseudolabels?", "cited_paper": [{"arxiv_id": "1804.06872", "title": "Co-teaching: Robust Training of Deep Neural Networks with Extremely Noisy Labels", "year": 2018}, {"arxiv_id": "2202.00828", "title": "Co-training Improves Prompt-based Learning for Large Language Models", "year": 2022}], "gt_label": [1, 1], "date": "2022-02", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_751", "valid": true}
{"query": "Can you provide references regarding data-driven approaches for stereo-matching?", "cited_paper": [{"arxiv_id": "1512.02134", "title": "A Large Dataset to Train Convolutional Networks for Disparity, Optical Flow, and Scene Flow Estimation", "year": 2015}, {"arxiv_id": "2211.05783", "title": "Unifying Flow, Stereo and Depth Estimation", "year": 2022}, {"arxiv_id": "1803.08669", "title": "Pyramid Stereo Matching Network", "year": 2018}, {"arxiv_id": "1904.06587", "title": "GA-Net: Guided Aggregation Net for End-to-end Stereo Matching", "year": 2019}, {"arxiv_id": "1512.02134", "title": "A Large Dataset to Train Convolutional Networks for Disparity, Optical Flow, and Scene Flow Estimation", "year": 2015}], "gt_label": [1, 1, 1, 1, 1], "date": "2022-11", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_752", "valid": true}
{"query": "What papers focus on developing kernel methods for learning molecular potentials?", "cited_paper": [], "gt_label": [], "date": "2019-09", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_753", "valid": true}
{"query": "What early works utilize CNN-based image translation in portrait and face relighting?", "cited_paper": [{"arxiv_id": "1603.06078", "title": "Deep Shading: Convolutional Neural Networks for Screen-Space Shading", "year": 2016}, {"arxiv_id": "1904.12356", "title": "Deferred Neural Rendering: Image Synthesis using Neural Textures", "year": 2019}], "gt_label": [1, 1], "date": "2019-04", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_754", "valid": true}
{"query": "Which papers discuss the application of specific criteria to remove weights in post-hoc pruning?", "cited_paper": [{"arxiv_id": "1608.04493", "title": "Dynamic Network Surgery for Efficient DNNs", "year": 2016}, {"arxiv_id": "1705.07565", "title": "Learning to Prune Deep Neural Networks via Layer-wise Optimal Brain Surgeon", "year": 2017}, {"arxiv_id": "1802.10399", "title": "Compressing Neural Networks using the Variational Information Bottleneck", "year": 2018}, {"arxiv_id": "1711.05908", "title": "NISP: Pruning Networks using Neuron Importance Score Propagation", "year": 2017}, {"arxiv_id": "1906.10771", "title": "Importance Estimation for Neural Network Pruning", "year": 2019}], "gt_label": [1, 1, 1, 1, 1], "date": "2019-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_755", "valid": true}
{"query": "Can you identify any works that aimed to improve computationally efficient FL with personalized local models using quantization and model parameter decoupling?", "cited_paper": [{"arxiv_id": "2107.13892", "title": "QuPeD: Quantized Personalization via Distillation with Applications to Federated Learning", "year": 2021}, {"arxiv_id": "2203.09747", "title": "Efficient Split-Mix Federated Learning for On-Demand and In-Situ Customization", "year": 2022}, {"arxiv_id": "2010.01264", "title": "HeteroFL: Computation and Communication Efficient Federated Learning for Heterogeneous Clients", "year": 2020}, {"arxiv_id": "2102.07078", "title": "Exploiting Shared Representations for Personalized Federated Learning", "year": 2021}, {"arxiv_id": "2201.11380", "title": "Achieving Personalized Federated Learning with Sparse Local Models", "year": 2022}], "gt_label": [1, 1, 1, 1, 1], "date": "2022-03", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_756", "valid": true}
{"query": "Which papers explore the application of parameter-efficient fine-tuning methods, such as Prefix-Tuning and LoRA?", "cited_paper": [{"arxiv_id": "2101.00190", "title": "Prefix-Tuning: Optimizing Continuous Prompts for Generation", "year": 2021}, {"arxiv_id": "2106.09685", "title": "LoRA: Low-Rank Adaptation of Large Language Models", "year": 2021}], "gt_label": [1, 1], "date": "2021-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_757", "valid": true}
{"query": "Could you tell me about the studies that have proposed goal-driven clustering for personalising the grouping of text corpora?", "cited_paper": [{"arxiv_id": "2305.13749", "title": "Goal-Driven Explainable Clustering via Language Descriptions", "year": 2023}], "gt_label": [1], "date": "2023-05", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_758", "valid": true}
{"query": "Are there any studies that used a few-shot manner to train the CDR model?", "cited_paper": [{"arxiv_id": "2105.04166", "title": "Few-Shot Conversational Dense Retrieval", "year": 2021}, {"arxiv_id": "2401.16659", "title": "History-Aware Conversational Dense Retrieval", "year": 2024}], "gt_label": [1, 1], "date": "2024-01", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_759", "valid": true}
{"query": "Can you list any studies that utilize differentiable logical rule learning", "cited_paper": [{"arxiv_id": "1412.6575", "title": "Embedding Entities and Relations for Learning and Inference in Knowledge Bases", "year": 2014}, {"arxiv_id": "1702.08367", "title": "Differentiable Learning of Logical Rules for Knowledge Base Reasoning", "year": 2017}, {"arxiv_id": "1911.00055", "title": "DRUM: End-To-End Differentiable Rule Mining On Knowledge Graphs", "year": 2019}, {"arxiv_id": "1707.06690", "title": "DeepPath: A Reinforcement Learning Method for Knowledge Graph Reasoning", "year": 2017}, {"arxiv_id": "1803.06581", "title": "Variational Knowledge Graph Reasoning", "year": 2018}, {"arxiv_id": "1711.05851", "title": "Go for a Walk and Arrive at the Answer: Reasoning Over Paths in Knowledge Bases using Reinforcement Learning", "year": 2017}, {"arxiv_id": "1808.10568", "title": "Multi-Hop Knowledge Graph Reasoning with Reward Shaping", "year": 2018}, {"arxiv_id": "1802.04394", "title": "M-Walk: Learning to Walk over Graphs using Monte Carlo Tree Search", "year": 2018}], "gt_label": [1, 1, 1, 1, 1, 1, 1, 1], "date": "2019-11", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_760", "valid": true}
{"query": "What studies provide insight into provably efficient exploration techniques in RL?", "cited_paper": [{"arxiv_id": "1406.1853", "title": "Model-based Reinforcement Learning and the Eluder Dimension", "year": 2014}, {"arxiv_id": "2004.10019", "title": "Almost Optimal Model-Free Reinforcement Learning via Reference-Advantage Decomposition", "year": 2020}, {"arxiv_id": "2106.04895", "title": "Policy Finetuning: Bridging Sample-Efficient Offline and Online Reinforcement Learning", "year": 2021}, {"arxiv_id": "2003.00153", "title": "Learning Near Optimal Policies with Low Inherent Bellman Error", "year": 2020}, {"arxiv_id": "1907.05388", "title": "Provably Efficient Reinforcement Learning with Linear Function Approximation", "year": 2019}], "gt_label": [1, 1, 1, 1, 1], "date": "2021-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_761", "valid": true}
{"query": "What work proposed a term-graph rewriting system for marginalizing a log joint density with conjugacy?", "cited_paper": [{"arxiv_id": "1811.11926", "title": "Autoconj: Recognizing and Exploiting Conjugacy Without a Domain-Specific Language", "year": 2018}], "gt_label": [1], "date": "2018-11", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_762", "valid": true}
{"query": "What studies discuss about adversarial attacks as potential safety risks in machine learning models?", "cited_paper": [{"arxiv_id": "1706.06083", "title": "Towards Deep Learning Models Resistant to Adversarial Attacks", "year": 2017}, {"arxiv_id": "1901.08573", "title": "Theoretically Principled Trade-off between Robustness and Accuracy", "year": 2019}], "gt_label": [1, 1], "date": "2019-01", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_763", "valid": true}
{"query": "Which works used a simple peak detection algorithm to extract atomic coordinates from the generated voxel grids similar to the method used in the current study?", "cited_paper": [{"arxiv_id": "2010.08687", "title": "Learning a Continuous Representation of 3D Molecular Structures with Deep Generative Models", "year": 2020}], "gt_label": [1], "date": "2020-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_764", "valid": true}
{"query": "Which work focuses on 'Cause on Tape' (CoT) explanations where the explanation precedes the answer?", "cited_paper": [{"arxiv_id": "2305.04388", "title": "Language Models Don't Always Say What They Think: Unfaithful Explanations in Chain-of-Thought Prompting", "year": 2023}], "gt_label": [1], "date": "2023-05", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_765", "valid": true}
{"query": "Which works have introduced domain adversarial methods to learn domain-invariant embeddings across the source domain and the target domain in unsupervised graph domain adaption problem?", "cited_paper": [{"arxiv_id": "1906.00684", "title": "DANE: Domain Adaptive Network Embedding", "year": 2019}], "gt_label": [1], "date": "2019-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_766", "valid": true}
{"query": "Which work first incorporated Differentiable Inductive Logic Programming to RL domain?", "cited_paper": [{"arxiv_id": "1904.10729", "title": "Neural Logic Reinforcement Learning", "year": 2019}], "gt_label": [1], "date": "2019-04", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_767", "valid": true}
{"query": "What papers propose 3D pretraining methods utilizing local augmentations?", "cited_paper": [{"arxiv_id": "2101.02691", "title": "Self-Supervised Pretraining of 3D Features on any Point-Cloud", "year": 2021}], "gt_label": [1], "date": "2021-01", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_768", "valid": true}
{"query": "Can you name the studies that looked at CVaR optimization using PG?", "cited_paper": [{"arxiv_id": "1610.01283", "title": "EPOpt: Learning Robust Neural Network Policies Using Model Ensembles", "year": 2016}, {"arxiv_id": "1905.09191", "title": "Learning Robust Options by Conditional Value at Risk Optimization", "year": 2019}], "gt_label": [1, 1], "date": "2019-05", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_769", "valid": true}
{"query": "Are there any studies about the policy update of AMPO to mirror descent algorithm based on value iteration and Bellman operators?", "cited_paper": [{"arxiv_id": "1901.11275", "title": "A Theory of Regularized Markov Decision Processes", "year": 2019}], "gt_label": [1], "date": "2019-01", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_770", "valid": true}
{"query": "What papers have examined the effect of smaller design decisions like the loss function or policy regularization?", "cited_paper": [{"arxiv_id": "2009.10897", "title": "Revisiting Design Choices in Proximal Policy Optimization", "year": 2020}], "gt_label": [1], "date": "2020-09", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_771", "valid": true}
{"query": "Any works that developed representations of statistical and causal dependencies between latent factors and auxiliary variables?", "cited_paper": [{"arxiv_id": "1605.06336", "title": "Unsupervised Feature Extraction by Time-Contrastive Learning and Nonlinear ICA", "year": 2016}, {"arxiv_id": "2002.02886", "title": "Weakly-Supervised Disentanglement Without Compromises", "year": 2020}, {"arxiv_id": "1905.06642", "title": "The Incomplete Rosetta Stone Problem: Identifiability Results for Multi-View Nonlinear ICA", "year": 2019}, {"arxiv_id": "1805.08651", "title": "Nonlinear ICA Using Auxiliary Variables and Generalized Contrastive Learning", "year": 2018}, {"arxiv_id": "1907.04809", "title": "Variational Autoencoders and Nonlinear ICA: A Unifying Framework", "year": 2019}, {"arxiv_id": "2002.11537", "title": "ICE-BeeM: Identifiable Conditional Energy-Based Deep Models Based on Nonlinear ICA", "year": 2020}, {"arxiv_id": "2007.10930", "title": "Towards Nonlinear Disentanglement in Natural Data with Temporal Sparse Coding", "year": 2020}], "gt_label": [1, 1, 1, 1, 1, 1, 1], "date": "2021-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_772", "valid": true}
{"query": "What studies have demonstrated the effectiveness of contrastive methods in learning useful representations for downstream tasks?", "cited_paper": [{"arxiv_id": "1807.03748", "title": "Representation Learning with Contrastive Predictive Coding", "year": 2018}, {"arxiv_id": "2002.05709", "title": "A Simple Framework for Contrastive Learning of Visual Representations", "year": 2020}, {"arxiv_id": "1911.05722", "title": "Momentum Contrast for Unsupervised Visual Representation Learning", "year": 2019}, {"arxiv_id": "2106.04156", "title": "Provable Guarantees for Self-Supervised Deep Learning with Spectral Contrastive Loss", "year": 2021}, {"arxiv_id": "1807.03748", "title": "Representation Learning with Contrastive Predictive Coding", "year": 2018}, {"arxiv_id": "1808.06670", "title": "Learning deep representations by mutual information estimation and maximization", "year": 2018}, {"arxiv_id": "1906.00910", "title": "Learning Representations by Maximizing Mutual Information Across Views", "year": 2019}, {"arxiv_id": "1906.05849", "title": "Contrastive Multiview Coding", "year": 2019}, {"arxiv_id": "1907.13625", "title": "On Mutual Information Maximization for Representation Learning", "year": 2019}, {"arxiv_id": "2005.10243", "title": "What Makes for Good Views for Contrastive Learning?", "year": 2020}, {"arxiv_id": "2005.10242", "title": "Understanding Contrastive Representation Learning through Alignment and Uniformity on the Hypersphere", "year": 2020}, {"arxiv_id": "1807.03748", "title": "Representation Learning with Contrastive Predictive Coding", "year": 2018}, {"arxiv_id": "1807.03748", "title": "Representation Learning with Contrastive Predictive Coding", "year": 2018}, {"arxiv_id": "2106.04156", "title": "Provable Guarantees for Self-Supervised Deep Learning with Spectral Contrastive Loss", "year": 2021}, {"arxiv_id": "0809.0853", "title": "Estimating divergence functionals and the likelihood ratio by convex risk minimization", "year": 2008}], "gt_label": [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], "date": "2021-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_773", "valid": true}
{"query": "What research introduces competition-level problems integrating mathematical logic and background knowledge?", "cited_paper": [{"arxiv_id": "2103.03874", "title": "Measuring Mathematical Problem Solving With the MATH Dataset", "year": 2021}, {"arxiv_id": "2301.13867", "title": "Mathematical Capabilities of ChatGPT", "year": 2023}, {"arxiv_id": "2305.15074", "title": "Have LLMs Advanced Enough? A Challenging Problem Solving Benchmark For Large Language Models", "year": 2023}], "gt_label": [1, 1, 1], "date": "2023-05", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_774", "valid": true}
{"query": "Could you provide examples of studies that use dense voxel grids in conjunction with shallow multilayer perceptrons to expedite 3D reconstruction?", "cited_paper": [{"arxiv_id": "2111.11215", "title": "Direct Voxel Grid Optimization: Super-fast Convergence for Radiance Fields Reconstruction", "year": 2021}, {"arxiv_id": "2206.05085", "title": "Improved Direct Voxel Grid Optimization for Radiance Fields Reconstruction", "year": 2022}, {"arxiv_id": "2301.09632", "title": "HexPlane: A Fast Representation for Dynamic Scenes", "year": 2023}], "gt_label": [1, 1, 1], "date": "2023-01", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_775", "valid": true}
{"query": "Could you provide me some studies about meta-learning in DG strategies?", "cited_paper": [{"arxiv_id": "1902.00113", "title": "Episodic Training for Domain Generalization", "year": 2019}, {"arxiv_id": "2007.07645", "title": "Learning to Learn with Variational Information Bottleneck for Domain Generalization", "year": 2020}, {"arxiv_id": "1710.03463", "title": "Learning to Generalize: Meta-Learning for Domain Generalization", "year": 2017}, {"arxiv_id": "2012.00417", "title": "Learning to Generalize Unseen Domains via Memory-based Multi-Source Meta-Learning for Person Re-Identification", "year": 2020}], "gt_label": [1, 1, 1, 1], "date": "2020-12", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_776", "valid": true}
{"query": "What research study discusses protein docking methods such as EquiDock without prior knowledge of the epitope and the paratope?", "cited_paper": [{"arxiv_id": "2111.07786", "title": "Independent SE(3)-Equivariant Models for End-to-End Rigid Protein Docking", "year": 2021}], "gt_label": [1], "date": "2021-11", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_777", "valid": true}
{"query": "Could you provide me some papers that investigated adversarial perturbations in MRI and CT image reconstruction?", "cited_paper": [{"arxiv_id": "2011.04268", "title": "Solving Inverse Problems With Deep Neural Networks -- Robustness Included?", "year": 2020}], "gt_label": [1], "date": "2020-11", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_778", "valid": true}
{"query": "Can you provide some studies dealing with the strong quadratic cost in Optimal Transport?", "cited_paper": [{"arxiv_id": "1908.10962", "title": "Optimal transport mapping via input convex neural networks", "year": 2019}], "gt_label": [1], "date": "2019-09", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_779", "valid": true}
{"query": "Which paper proposed a fully differentiable equivariant model that can predict coordinates of docked poses?", "cited_paper": [{"arxiv_id": "2202.05146", "title": "EquiBind: Geometric Deep Learning for Drug Binding Structure Prediction", "year": 2022}], "gt_label": [1], "date": "2022-02", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_780", "valid": true}
{"query": "Which studies have explored improved choices for the matrices in the continual counting context?", "cited_paper": [{"arxiv_id": "2211.05006", "title": "Almost Tight Error Bounds on Differentially Private Continual Counting", "year": 2022}], "gt_label": [1], "date": "2022-11", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_781", "valid": true}
{"query": "Which works are about using dynamic computation graph by skipping different blocks based on samples?", "cited_paper": [{"arxiv_id": "1711.09485", "title": "SkipNet: Learning Dynamic Routing in Convolutional Networks", "year": 2017}, {"arxiv_id": "1711.08393", "title": "BlockDrop: Dynamic Inference Paths in Residual Networks", "year": 2017}], "gt_label": [1, 1], "date": "2017-11", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_782", "valid": true}
{"query": "Could you provide me some works that discuss improving model's robustness through data augmentation?", "cited_paper": [{"arxiv_id": "2006.16241", "title": "The Many Faces of Robustness: A Critical Analysis of Out-of-Distribution Generalization", "year": 2020}, {"arxiv_id": "1912.02781", "title": "AugMix: A Simple Data Processing Method to Improve Robustness and Uncertainty", "year": 2019}, {"arxiv_id": "2001.06057", "title": "A simple way to make neural networks robust against diverse image corruptions", "year": 2020}, {"arxiv_id": "2110.13771", "title": "AugMax: Adversarial Composition of Random Augmentations for Robust Training", "year": 2021}], "gt_label": [1, 1, 1, 1], "date": "2021-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_783", "valid": true}
{"query": "What research has been done on the interaction between the Bregman projected policy class and the expected Lipschitz and smooth policies?", "cited_paper": [{"arxiv_id": "2211.07937", "title": "An Improved Analysis of (Variance-Reduced) Policy Gradient and Natural Policy Gradient Methods", "year": 2022}], "gt_label": [1], "date": "2022-11", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_784", "valid": true}
{"query": "Can you give me examples of LLM papers that use the MCP approach for evaluation?", "cited_paper": [{"arxiv_id": "2112.11446", "title": "Scaling Language Models: Methods, Analysis & Insights from Training Gopher", "year": 2021}, {"arxiv_id": "2203.15556", "title": "Training Compute-Optimal Large Language Models", "year": 2022}], "gt_label": [1, 1], "date": "2022-03", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_785", "valid": true}
{"query": "In what papers were methods described that locate and edit the parameters and neurons in the LLMs in light of specific knowledge?", "cited_paper": [{"arxiv_id": "2202.05262", "title": "Locating and Editing Factual Associations in GPT", "year": 2022}, {"arxiv_id": "2104.08696", "title": "Knowledge Neurons in Pretrained Transformers", "year": 2021}, {"arxiv_id": "2210.07229", "title": "Mass-Editing Memory in a Transformer", "year": 2022}, {"arxiv_id": "2112.01008", "title": "Editing a classifier by rewriting its prediction rules", "year": 2021}, {"arxiv_id": "2203.14680", "title": "Transformer Feed-Forward Layers Build Predictions by Promoting Concepts in the Vocabulary Space", "year": 2022}], "gt_label": [1, 1, 1, 1, 1], "date": "2022-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_786", "valid": true}
{"query": "What study proposed the method PatchFlow to solve the technical challenge of per-view detection of feature points in SfM?", "cited_paper": [{"arxiv_id": "2003.08348", "title": "Multi-View Optimization of Local Feature Geometry", "year": 2020}], "gt_label": [1], "date": "2020-03", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_787", "valid": true}
{"query": "Which papers discuss the use of Visual Instruction Fine-tuning in Large Multimodal Models?", "cited_paper": [{"arxiv_id": "2304.08485", "title": "Visual Instruction Tuning", "year": 2023}, {"arxiv_id": "2307.04087", "title": "SVIT: Scaling up Visual Instruction Tuning", "year": 2023}], "gt_label": [1, 1], "date": "2023-07", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_788", "valid": true}
{"query": "What papers have employed use of persona prompts in LLMs?", "cited_paper": [{"arxiv_id": "2209.06899", "title": "Out of One, Many: Using Language Models to Simulate Human Samples", "year": 2022}, {"arxiv_id": "2310.05984", "title": "Simulating Social Media Using Large Language Models to Evaluate Alternative News Feed Algorithms", "year": 2023}], "gt_label": [1, 1], "date": "2023-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_789", "valid": true}
{"query": "Which work introduced the notion of (,)-competitiveness?", "cited_paper": [{"arxiv_id": "2110.13116", "title": "Learning-Augmented Dynamic Power Management with Multiple States via New Ski Rental Bounds", "year": 2021}], "gt_label": [1], "date": "2021-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_790", "valid": true}
{"query": "What research investigates higher-order grammatical feature representation across languages using probing classifiers trained on mBERT embeddings?", "cited_paper": [{"arxiv_id": "2101.11043", "title": "Deep Subjecthood: Higher-Order Grammatical Features in Multilingual BERT", "year": 2021}], "gt_label": [1], "date": "2021-01", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_791", "valid": true}
{"query": "Which studies explored techniques in learning process design to address biases and instability in in-context learning?", "cited_paper": [{"arxiv_id": "2108.04106", "title": "Noisy Channel Language Model Prompting for Few-Shot Text Classification", "year": 2021}, {"arxiv_id": "2110.15943", "title": "MetaICL: Learning to Learn In Context", "year": 2021}], "gt_label": [1, 1], "date": "2021-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_792", "valid": true}
{"query": "Which works have studied the underlying representations in diffusion models and proposed using them for various downstream tasks?", "cited_paper": [{"arxiv_id": "2211.12572", "title": "Plug-and-Play Diffusion Features for Text-Driven Image-to-Image Translation", "year": 2022}, {"arxiv_id": "2112.03126", "title": "Label-Efficient Semantic Segmentation with Diffusion Models", "year": 2021}], "gt_label": [1, 1], "date": "2022-11", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_793", "valid": true}
{"query": "What studies proposed gradient-guided autoregressive models by using the decoders activations as a gradient-friendly latent space?", "cited_paper": [{"arxiv_id": "1912.02164", "title": "Plug and Play Language Models: A Simple Approach to Controlled Text Generation", "year": 2019}, {"arxiv_id": "2104.05218", "title": "FUDGE: Controlled Text Generation With Future Discriminators", "year": 2021}], "gt_label": [1, 1], "date": "2021-04", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_794", "valid": true}
{"query": "Which research papers report on response-based distillation methods for object detection?", "cited_paper": [{"arxiv_id": "1503.02531", "title": "Distilling the Knowledge in a Neural Network", "year": 2015}], "gt_label": [1], "date": "2015-03", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_795", "valid": true}
{"query": "Are there any studies based on using pre-trained language model agents for role-play in text-based games?", "cited_paper": [{"arxiv_id": "1903.03094", "title": "Learning to Speak and Act in a Fantasy Text Adventure Game", "year": 2019}, {"arxiv_id": "2107.08408", "title": "Pre-trained Language Models as Prior Knowledge for Playing Text-based Games", "year": 2021}, {"arxiv_id": "2309.04658", "title": "Exploring Large Language Models for Communication Games: An Empirical Study on Werewolf", "year": 2023}], "gt_label": [1, 1, 1], "date": "2023-09", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_796", "valid": true}
{"query": "Which works combined models used for ARA with tradition linguistic features?", "cited_paper": [{"arxiv_id": "2006.00377", "title": "Linguistic Features for Readability Assessment", "year": 2020}, {"arxiv_id": "2106.07935", "title": "BERT Embeddings for Automatic Readability Assessment", "year": 2021}], "gt_label": [1, 1], "date": "2021-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_797", "valid": true}
{"query": "Which paper distills pre-trained Stable Diffusion using Score Distillation Sampling (SDS) to extract a Neural Radiance Field (NeRF) from a given text prompt?", "cited_paper": [{"arxiv_id": "2209.14988", "title": "DreamFusion: Text-to-3D using 2D Diffusion", "year": 2022}], "gt_label": [1], "date": "2022-09", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_798", "valid": true}
{"query": "What papers focus on extending the signal-prediction framework to forecast aggregation settings?", "cited_paper": [{"arxiv_id": "2102.02666", "title": "The Wisdom of the Crowd and Higher-Order Beliefs", "year": 2021}], "gt_label": [1], "date": "2021-02", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_799", "valid": true}
{"query": "Which works were pertinent in the development of the Large Multimodal Models?", "cited_paper": [{"arxiv_id": "1910.10683", "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer", "year": 2019}, {"arxiv_id": "2204.02311", "title": "PaLM: Scaling Language Modeling with Pathways", "year": 2022}, {"arxiv_id": "2205.05131", "title": "UL2: Unifying Language Learning Paradigms", "year": 2022}, {"arxiv_id": "2103.00020", "title": "Learning Transferable Visual Models From Natural Language Supervision", "year": 2021}, {"arxiv_id": "2201.12086", "title": "BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation", "year": 2022}, {"arxiv_id": "2205.14459", "title": "CyCLIP: Cyclic Contrastive Language-Image Pretraining", "year": 2022}], "gt_label": [1, 1, 1, 1, 1, 1], "date": "2022-05", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_800", "valid": true}
{"query": "Are there any articles proposing methods to evaluate the complexity of the hierarchical structure of a graph?", "cited_paper": [{"arxiv_id": "2206.13510", "title": "Structural Entropy Guided Graph Hierarchical Pooling", "year": 2022}, {"arxiv_id": "2206.02404", "title": "A Simple yet Effective Method for Graph Classification", "year": 2022}], "gt_label": [1, 1], "date": "2022-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_801", "valid": true}
{"query": "Which papers discuss the role of Transformers in the field of NLP?", "cited_paper": [{"arxiv_id": "1706.03762", "title": "Attention Is All You Need", "year": 2017}], "gt_label": [1], "date": "2017-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_802", "valid": true}
{"query": "Which papers describe the basic annotation approach in creating NLI data?", "cited_paper": [{"arxiv_id": "1508.05326", "title": "A large annotated corpus for learning natural language inference", "year": 2015}, {"arxiv_id": "1704.05426", "title": "A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference", "year": 2017}], "gt_label": [1, 1], "date": "2017-04", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_803", "valid": true}
{"query": "Could you provide references about the PixelHelp dataset which includes task goals and step-by-step instructions for Android?", "cited_paper": [{"arxiv_id": "2005.03776", "title": "Mapping Natural Language Instructions to Mobile UI Action Sequences", "year": 2020}], "gt_label": [1], "date": "2020-05", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_804", "valid": true}
{"query": "What studies utilize first-order logic to inspect and improve model's logical consistency in synthetic compositional reasoning tasks?", "cited_paper": [{"arxiv_id": "2305.12295", "title": "Logic-LM: Empowering Large Language Models with Symbolic Solvers for Faithful Logical Reasoning", "year": 2023}, {"arxiv_id": "2310.15164", "title": "LINC: A Neurosymbolic Approach for Logical Reasoning by Combining Language Models with First-Order Logic Provers", "year": 2023}, {"arxiv_id": "2203.10261", "title": "FaiRR: Faithful and Robust Deductive Reasoning over Natural Language", "year": 2022}, {"arxiv_id": "2111.12038", "title": "Learning Symbolic Rules for Reasoning in Quasi-Natural Language", "year": 2021}], "gt_label": [1, 1, 1, 1], "date": "2023-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_805", "valid": true}
{"query": "Which studies propose interpolation-based mixup methods for graph augmentations?", "cited_paper": [{"arxiv_id": "2202.07179", "title": "G-Mixup: Graph Data Augmentation for Graph Classification", "year": 2022}, {"arxiv_id": "2111.05639", "title": "Graph Transplant: Node Saliency-Guided Graph Mixup with Local Structure Preservation", "year": 2021}], "gt_label": [1, 1], "date": "2022-02", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_806", "valid": true}
{"query": "What research provided theoretical studies on the convergence of the LocalSGDM algorithm?", "cited_paper": [{"arxiv_id": "1905.03817", "title": "On the Linear Speedup Analysis of Communication Efficient Momentum SGD for Distributed Non-Convex Optimization", "year": 2019}], "gt_label": [1], "date": "2019-05", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_807", "valid": true}
{"query": "What works present GALOIS framework and its sketch setting?", "cited_paper": [{"arxiv_id": "2205.13728", "title": "GALOIS: Boosting Deep Reinforcement Learning via Generalizable Logic Synthesis", "year": 2022}], "gt_label": [1], "date": "2022-05", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_808", "valid": true}
{"query": "Can you provide me some researches that support sensor simulation in the context of autonomous driving perception?", "cited_paper": [{"arxiv_id": "1711.03938", "title": "CARLA: An Open Urban Driving Simulator", "year": 2017}, {"arxiv_id": "2202.05263", "title": "Block-NeRF: Scalable Large Scene Neural View Synthesis", "year": 2022}, {"arxiv_id": "1911.04074", "title": "SUMMIT: A Simulator for Urban Driving in Massive Mixed Traffic", "year": 2019}, {"arxiv_id": "2111.12083", "title": "VISTA 2.0: An Open, Data-driven Simulator for Multimodal Sensing and Policy Learning for Autonomous Vehicles", "year": 2021}], "gt_label": [1, 1, 1, 1], "date": "2022-02", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_809", "valid": true}
{"query": "What are some of the recent research papers that achieved success in solving downstream language tasks using finetuning methods?", "cited_paper": [{"arxiv_id": "1810.04805", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "year": 2018}, {"arxiv_id": "1909.11942", "title": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations", "year": 2019}, {"arxiv_id": "1907.11692", "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach", "year": 2019}, {"arxiv_id": "1906.08237", "title": "XLNet: Generalized Autoregressive Pretraining for Language Understanding", "year": 2019}], "gt_label": [1, 1, 1, 1], "date": "2019-09", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_810", "valid": true}
{"query": "Who proposed a GNN framework arising from a mixture of parabolic and hyperbolic PDEs on graphs with convolutional coupling operators?", "cited_paper": [{"arxiv_id": "2108.01938", "title": "PDE-GCN: Novel Architectures for Graph Neural Networks Motivated by Partial Differential Equations", "year": 2021}], "gt_label": [1], "date": "2021-08", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_811", "valid": true}
{"query": "Which works have utilized gating in classical recurrent neural network (RNN) architectures such as LSTM and GRU?", "cited_paper": [{"arxiv_id": "1406.1078", "title": "Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation", "year": 2014}], "gt_label": [1], "date": "2014-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_812", "valid": true}
{"query": "Can you provide papers that explored novel ways of training CNNs for FGVC?", "cited_paper": [{"arxiv_id": "2106.03432", "title": "Channel DropBlock: An Improved Regularization Method for Fine-Grained Visual Classification", "year": 2021}], "gt_label": [1], "date": "2021-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_813", "valid": true}
{"query": "What studies proposed deep learning models for protein sequence design using structure-based generative models?", "cited_paper": [{"arxiv_id": "2205.15019", "title": "Protein Structure and Sequence Generation with Equivariant Denoising Diffusion Probabilistic Models", "year": 2022}, {"arxiv_id": "2009.01411", "title": "Learning from Protein Structure with Geometric Vector Perceptrons", "year": 2020}, {"arxiv_id": "2306.04899", "title": "Multi-level Protein Representation Learning for Blind Mutational Effect Prediction", "year": 2023}], "gt_label": [1, 1, 1], "date": "2023-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_814", "valid": true}
{"query": "Which work proposed the model-free algorithm MOFFLE for low-nonnegative-rank MDPs in the field of reward-free reinforcement learning?", "cited_paper": [{"arxiv_id": "2102.07035", "title": "Model-free Representation Learning and Exploration in Low-rank MDPs", "year": 2021}], "gt_label": [1], "date": "2021-02", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_815", "valid": true}
{"query": "What papers utilize the Monte Carlo Dropout method in variational Bayesian methods for approximating the intractable integrals arising in Bayesian inference?", "cited_paper": [{"arxiv_id": "1506.02142", "title": "Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning", "year": 2015}, {"arxiv_id": "1506.02158", "title": "Bayesian Convolutional Neural Networks with Bernoulli Approximate Variational Inference", "year": 2015}], "gt_label": [1, 1], "date": "2015-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_816", "valid": true}
{"query": "Which studies extended the work of cage-based deformation in NeRF editing?", "cited_paper": [{"arxiv_id": "2303.11537", "title": "Interactive Geometry Editing of Neural Radiance Fields", "year": 2023}], "gt_label": [1], "date": "2023-03", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_817", "valid": true}
{"query": "What studies discuss the training of Pre-trained Language Models(PLMs) for predicting masked words?", "cited_paper": [{"arxiv_id": "2211.04898", "title": "Mask More and Mask Later: Efficient Pre-training of Masked Language Models by Disentangling the [MASK] Token", "year": 2022}, {"arxiv_id": "1810.04805", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "year": 2018}, {"arxiv_id": "1907.11692", "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach", "year": 2019}, {"arxiv_id": "1910.13461", "title": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension", "year": 2019}, {"arxiv_id": "1905.02450", "title": "MASS: Masked Sequence to Sequence Pre-training for Language Generation", "year": 2019}, {"arxiv_id": "1911.02116", "title": "Unsupervised Cross-lingual Representation Learning at Scale", "year": 2019}, {"arxiv_id": "1901.07291", "title": "Cross-lingual Language Model Pretraining", "year": 2019}], "gt_label": [1, 1, 1, 1, 1, 1, 1], "date": "2022-11", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_818", "valid": true}
{"query": "What research focused on constructing a purpose-driven affordance dataset?", "cited_paper": [{"arxiv_id": "2106.14747", "title": "One-Shot Affordance Detection", "year": 2021}], "gt_label": [1], "date": "2021-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_819", "valid": true}
{"query": "What works have been done in the field of image editing in computer vision?", "cited_paper": [{"arxiv_id": "2211.12572", "title": "Plug-and-Play Diffusion Features for Text-Driven Image-to-Image Translation", "year": 2022}, {"arxiv_id": "2304.06720", "title": "Expressive Text-to-Image Generation with Rich Text", "year": 2023}], "gt_label": [1, 1], "date": "2023-04", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_820", "valid": true}
{"query": "Could you provide me some works that focus on source domain data estimation in the context of SFUDA?", "cited_paper": [{"arxiv_id": "2103.16372", "title": "Source-Free Domain Adaptation for Semantic Segmentation", "year": 2021}], "gt_label": [1], "date": "2021-03", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_821", "valid": true}
{"query": "Which work combines the semantic and instance segmentation tasks effectively?", "cited_paper": [{"arxiv_id": "2107.06278", "title": "Per-Pixel Classification is Not All You Need for Semantic Segmentation", "year": 2021}, {"arxiv_id": "2112.01527", "title": "Masked-attention Mask Transformer for Universal Image Segmentation", "year": 2021}], "gt_label": [1, 1], "date": "2021-12", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_822", "valid": true}
{"query": "Which studies presented automatic machine learning (AutoML) approaches?", "cited_paper": [{"arxiv_id": "2006.13799", "title": "Auto-PyTorch Tabular: Multi-Fidelity MetaLearning for Efficient and Robust AutoDL", "year": 2020}], "gt_label": [1], "date": "2020-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_823", "valid": true}
{"query": "Which papers discussed federated learning in the context of autonomous driving?", "cited_paper": [{"arxiv_id": "2202.13670", "title": "FedDrive: Generalizing Federated Learning to Semantic Segmentation in Autonomous Driving", "year": 2022}, {"arxiv_id": "2110.05754", "title": "Deep Federated Learning for Autonomous Driving", "year": 2021}], "gt_label": [1, 1], "date": "2022-02", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_824", "valid": true}
{"query": "Could you provide me studies that propose approaches to unite segmentation datasets from multiple domains?", "cited_paper": [{"arxiv_id": "2112.13762", "title": "MSeg: A Composite Dataset for Multi-domain Semantic Segmentation", "year": 2021}, {"arxiv_id": "2106.04121", "title": "Multi-dataset Pretraining: A Unified Model for Semantic Segmentation", "year": 2021}], "gt_label": [1, 1], "date": "2021-12", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_825", "valid": true}
{"query": "Which papers have investigated the simplicity bias in Deep Neural Networks (DNNs)?", "cited_paper": [{"arxiv_id": "1710.10174", "title": "SGD Learns Over-parameterized Networks that Provably Generalize on Linearly Separable Data", "year": 2017}, {"arxiv_id": "1710.10345", "title": "The Implicit Bias of Gradient Descent on Separable Data", "year": 2017}, {"arxiv_id": "1806.00468", "title": "Implicit Bias of Gradient Descent on Linear Convolutional Networks", "year": 2018}, {"arxiv_id": "1811.12231", "title": "ImageNet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness", "year": 2018}, {"arxiv_id": "1911.09071", "title": "The Origins and Prevalence of Texture Bias in Convolutional Neural Networks", "year": 2019}], "gt_label": [1, 1, 1, 1, 1], "date": "2019-11", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_826", "valid": true}
{"query": "What works propose to utilize the MVS approach using cost volume for better occlusion handling in generalizable view synthesis?", "cited_paper": [{"arxiv_id": "2103.15595", "title": "MVSNeRF: Fast Generalizable Radiance Field Reconstruction from Multi-View Stereo", "year": 2021}, {"arxiv_id": "2111.13539", "title": "GeoNeRF: Generalizing NeRF with Geometry Priors", "year": 2021}, {"arxiv_id": "2107.13421", "title": "Neural Rays for Occlusion-aware Image-based Rendering", "year": 2021}], "gt_label": [1, 1, 1], "date": "2021-11", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_827", "valid": true}
{"query": "Which papers discuss earlier datasets focused on sign language recognition using isolated signs?", "cited_paper": [{"arxiv_id": "1812.01053", "title": "MS-ASL: A Large-Scale Data Set and Benchmark for Understanding American Sign Language", "year": 2018}, {"arxiv_id": "1910.11006", "title": "Word-level Deep Sign Language Recognition from Video: A New Large-scale Dataset and Methods Comparison", "year": 2019}], "gt_label": [1, 1], "date": "2019-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_828", "valid": true}
{"query": "Any works that propose a view synthesis framework using two images with small overlapped regions?", "cited_paper": [{"arxiv_id": "2304.08463", "title": "Learning to Render Novel Views from Wide-Baseline Stereo Pairs", "year": 2023}], "gt_label": [1], "date": "2023-04", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_829", "valid": true}
{"query": "Which papers proposed a multi-stage language model for TTS with phonemes as input and acoustic tokens as output?", "cited_paper": [{"arxiv_id": "2301.02111", "title": "Neural Codec Language Models are Zero-Shot Text to Speech Synthesizers", "year": 2023}], "gt_label": [1], "date": "2023-01", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_830", "valid": true}
{"query": "Any studies about enhancing PLMs through pretraining on domain-relevant documents?", "cited_paper": [{"arxiv_id": "2106.11520", "title": "BARTScore: Evaluating Generated Text as Text Generation", "year": 2021}], "gt_label": [1], "date": "2021-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_831", "valid": true}
{"query": "What research works give a gradient-based approach to interpretability methods?", "cited_paper": [{"arxiv_id": "1604.00825", "title": "Layer-wise Relevance Propagation for Neural Networks with Local Renormalization Layers", "year": 2016}, {"arxiv_id": "1703.01365", "title": "Axiomatic Attribution for Deep Networks", "year": 2017}], "gt_label": [1, 1], "date": "2017-03", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_832", "valid": true}
{"query": "Which works have utilized the relationship between multiple images and between network layers in FGVC?", "cited_paper": [{"arxiv_id": "1909.04412", "title": "Cross-X Learning for Fine-Grained Visual Categorization", "year": 2019}], "gt_label": [1], "date": "2019-09", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_833", "valid": true}
{"query": "Which research proposed the Conditional Mutual Information (CMI) approach for learning disentangled representations?", "cited_paper": [{"arxiv_id": "2112.14754", "title": "Disentanglement and Generalization Under Correlation Shifts", "year": 2021}], "gt_label": [1], "date": "2021-12", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_834", "valid": true}
{"query": "Could you provide me some studies about delayed sampling which uses automatic marginalization to improve inference?", "cited_paper": [{"arxiv_id": "1810.01539", "title": "Automated learning with a probabilistic programming language: Birch", "year": 2018}, {"arxiv_id": "1810.09538", "title": "Pyro: Deep Universal Probabilistic Programming", "year": 2018}, {"arxiv_id": "1902.03210", "title": "Tensor Variable Elimination for Plated Factor Graphs", "year": 2019}], "gt_label": [1, 1, 1], "date": "2022-09", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_835", "valid": true}
{"query": "Can you name some studies that use Recurrent Neural Network or Graph Neural Network for improving the geometry shapes of building extraction results?", "cited_paper": [{"arxiv_id": "1812.01497", "title": "Topological Map Extraction from Overhead Images", "year": 2018}, {"arxiv_id": "2111.15491", "title": "PolyWorld: Polygonal Building Extraction with Graph Neural Networks in Satellite Images", "year": 2021}], "gt_label": [1, 1], "date": "2021-11", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_836", "valid": true}
{"query": "What studies examined the performance issues in machine translation models?", "cited_paper": [{"arxiv_id": "1803.00047", "title": "Analyzing Uncertainty in Neural Machine Translation", "year": 2018}], "gt_label": [1], "date": "2018-03", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_837", "valid": true}
{"query": "Which studies are about leveraging demonstrations into the policy-update steps of Reinforcement Learning?", "cited_paper": [{"arxiv_id": "1709.10087", "title": "Learning Complex Dexterous Manipulation with Deep Reinforcement Learning and Demonstrations", "year": 2017}, {"arxiv_id": "1707.08817", "title": "Leveraging Demonstrations for Deep Reinforcement Learning on Robotics Problems with Sparse Rewards", "year": 2017}, {"arxiv_id": "1709.10089", "title": "Overcoming Exploration in Reinforcement Learning with Demonstrations", "year": 2017}, {"arxiv_id": "1805.07095", "title": "Reinforced Imitation: Sample Efficient Deep Reinforcement Learning for Map-less Navigation by Leveraging Prior Demonstrations", "year": 2018}, {"arxiv_id": "1910.04281", "title": "Integrating Behavior Cloning and Reinforcement Learning for Improved Performance in Dense and Sparse Reward Environments", "year": 2019}], "gt_label": [1, 1, 1, 1, 1], "date": "2019-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_838", "valid": true}
{"query": "Can you cite the research papers that investigated the use of six IMUs for full-body motion estimation?", "cited_paper": [{"arxiv_id": "1703.08014", "title": "Sparse Inertial Poser: Automatic 3D Human Pose Estimation from Sparse IMUs", "year": 2017}, {"arxiv_id": "1810.04703", "title": "Deep Inertial Poser: Learning to Reconstruct Human Pose from Sparse Inertial Measurements in Real Time", "year": 2018}, {"arxiv_id": "2105.04605", "title": "TransPose: Real-time 3D Human Translation and Pose Estimation with Six Inertial Sensors", "year": 2021}, {"arxiv_id": "2203.08528", "title": "Physical Inertial Poser (PIP): Physics-aware Real-time Human Motion Tracking from Sparse Inertial Sensors", "year": 2022}], "gt_label": [1, 1, 1, 1], "date": "2022-03", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_839", "valid": true}
{"query": "Which work marked a significant development in Vision-Language Pre-training (VLP) and has trained encoders on a large amount of data?", "cited_paper": [{"arxiv_id": "2103.00020", "title": "Learning Transferable Visual Models From Natural Language Supervision", "year": 2021}], "gt_label": [1], "date": "2021-03", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_840", "valid": true}
{"query": "What papers are about contextualized models for encoding word meaning in the LSC task?", "cited_paper": [{"arxiv_id": "2103.07259", "title": "Explaining and Improving BERT Performance on Lexical Semantic Change Detection", "year": 2021}], "gt_label": [1], "date": "2021-03", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_841", "valid": true}
{"query": "Could you mention any works that proposed the concept of learning EBM by using a ConvNet as the energy function?", "cited_paper": [], "gt_label": [], "date": "2016-02", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_842", "valid": true}
{"query": "Which investigation proposes a method that mitigates the issue of model bias and generalization in zero-/few-shot anomaly detection?", "cited_paper": [{"arxiv_id": "2203.14506", "title": "Catching Both Gray and Black Swans: Open-set Supervised Anomaly Detection", "year": 2022}, {"arxiv_id": "2207.01463", "title": "Explicit Boundary Guided Semi-Push-Pull Contrastive Learning for Supervised Anomaly Detection", "year": 2022}], "gt_label": [1, 1], "date": "2022-07", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_843", "valid": true}
{"query": "Who introduced the approach of dataset condensation that leverages gradient-based hyper-parameter optimization?", "cited_paper": [{"arxiv_id": "1811.10959", "title": "Dataset Distillation", "year": 2018}], "gt_label": [1], "date": "2018-11", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_844", "valid": true}
{"query": "What works have used frame averaging to produce equivariant output from non-equivariant architecture backbones?", "cited_paper": [{"arxiv_id": "2110.03336", "title": "Frame Averaging for Invariant and Equivariant Network Design", "year": 2021}, {"arxiv_id": "2112.01741", "title": "Frame Averaging for Equivariant Shape Space Learning", "year": 2021}, {"arxiv_id": "2305.05577", "title": "FAENet: Frame Averaging Equivariant GNN for Materials Modeling", "year": 2023}], "gt_label": [1, 1, 1], "date": "2023-05", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_845", "valid": true}
{"query": "What papers focus on HMT techniques using depth sensors?", "cited_paper": [{"arxiv_id": "2103.03663", "title": "Real-time RGBD-based Extended Body Pose Estimation", "year": 2021}, {"arxiv_id": "2104.14837", "title": "RobustFusion: Robust Volumetric Performance Reconstruction under Human-object Interactions from Monocular RGBD Stream", "year": 2021}, {"arxiv_id": "1804.06023", "title": "DoubleFusion: Real-time Capture of Human Performances with Inner Body Shapes from a Single Depth Sensor", "year": 2018}], "gt_label": [1, 1, 1], "date": "2021-04", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_846", "valid": true}
{"query": "What study modifies questions and answers by counterfactual presupposition in VQAv2 for a new challenging scenario for complementary MLLMs?", "cited_paper": [{"arxiv_id": "1612.00837", "title": "Making the V in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering", "year": 2016}], "gt_label": [1], "date": "2016-12", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_847", "valid": true}
{"query": "Can you mention any work about Out-of-distribution (OOD) generalization on graphs?", "cited_paper": [{"arxiv_id": "2202.07987", "title": "Out-Of-Distribution Generalization on Graphs: A Survey", "year": 2022}, {"arxiv_id": "2108.01099", "title": "Shift-Robust GNNs: Overcoming the Limitations of Localized Graph Training Data", "year": 2021}], "gt_label": [1, 1], "date": "2022-02", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_848", "valid": true}
{"query": "Can you tell me about the studies that propose tuning methods for CLIP?", "cited_paper": [{"arxiv_id": "2103.00020", "title": "Learning Transferable Visual Models From Natural Language Supervision", "year": 2021}], "gt_label": [1], "date": "2021-03", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_849", "valid": true}
{"query": "Which research papers focus on addressing label-shift scenarios with open-set domain adaptation (OSDA), partial-set domain adaptation (PDA), and open-partial-set domain adaptation (OPDA)?", "cited_paper": [{"arxiv_id": "1804.10427", "title": "Open Set Domain Adaptation by Backpropagation", "year": 2018}, {"arxiv_id": "1707.07901", "title": "Partial Transfer Learning with Selective Adversarial Networks", "year": 2017}, {"arxiv_id": "1903.12230", "title": "Learning to Transfer Examples for Partial Domain Adaptation", "year": 2019}, {"arxiv_id": "2104.03344", "title": "OVANet: One-vs-All Network for Universal Domain Adaptation", "year": 2021}], "gt_label": [1, 1, 1, 1], "date": "2021-04", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_850", "valid": true}
{"query": "What research studies use hard pseudolabels from teachers to train student models in the outcontext of low-resource semi-supervised sequence generation?", "cited_paper": [{"arxiv_id": "1606.07947", "title": "Sequence-Level Knowledge Distillation", "year": 2016}, {"arxiv_id": "2212.10450", "title": "Is GPT-3 a Good Data Annotator?", "year": 2022}, {"arxiv_id": "2104.08826", "title": "GPT3Mix: Leveraging Large-scale Language Models for Text Augmentation", "year": 2021}, {"arxiv_id": "2108.13487", "title": "Want To Reduce Labeling Cost? GPT-3 Can Help", "year": 2021}, {"arxiv_id": "2303.15056", "title": "ChatGPT Outperforms Crowd-Workers for Text-Annotation Tasks", "year": 2023}, {"arxiv_id": "2212.10071", "title": "Large Language Models Are Reasoning Teachers", "year": 2022}], "gt_label": [1, 1, 1, 1, 1, 1], "date": "2023-03", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_851", "valid": true}
{"query": "What papers dealt with the classical transduction setting, a special case of semi-supervised learning?", "cited_paper": [], "gt_label": [], "date": "2013-01", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_852", "valid": true}
{"query": "Which papers introduce an asymmetric mechanism within the Multi-Agent Debate framework?", "cited_paper": [{"arxiv_id": "2305.19118", "title": "Encouraging Divergent Thinking in Large Language Models through Multi-Agent Debate", "year": 2023}, {"arxiv_id": "2308.07201", "title": "ChatEval: Towards Better LLM-based Evaluators through Multi-Agent Debate", "year": 2023}], "gt_label": [1, 1], "date": "2023-08", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_853", "valid": true}
{"query": "Which works applied data augmentation strategy or generative adversarial networks (GANs) in Domain Generalization (DG) to handle the domain shift?", "cited_paper": [{"arxiv_id": "2007.12256", "title": "Towards Recognizing Unseen Categories in Unseen Domains", "year": 2020}, {"arxiv_id": "2007.03304", "title": "Learning to Generate Novel Domains for Domain Generalization", "year": 2020}], "gt_label": [1, 1], "date": "2020-07", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_854", "valid": true}
{"query": "Are there any references that proposed the first general gradient inversion method?", "cited_paper": [{"arxiv_id": "1906.08935", "title": "Deep Leakage from Gradients", "year": 2019}], "gt_label": [1], "date": "2019-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_855", "valid": true}
{"query": "Can you give examples of studies that have provided physics questions in multiple-choice format and require multistep reasoning?", "cited_paper": [{"arxiv_id": "2009.03300", "title": "Measuring Massive Multitask Language Understanding", "year": 2020}, {"arxiv_id": "2305.08322", "title": "C-Eval: A Multi-Level Multi-Discipline Chinese Evaluation Suite for Foundation Models", "year": 2023}, {"arxiv_id": "2305.15074", "title": "Have LLMs Advanced Enough? A Challenging Problem Solving Benchmark For Large Language Models", "year": 2023}], "gt_label": [1, 1, 1], "date": "2023-05", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_856", "valid": true}
{"query": "Are there any studies on vision-language models that train to generate text autoregressively?", "cited_paper": [{"arxiv_id": "1411.4389", "title": "Long-term Recurrent Convolutional Networks for Visual Recognition and Description", "year": 2014}, {"arxiv_id": "1411.4555", "title": "Show and Tell: A Neural Image Caption Generator", "year": 2014}, {"arxiv_id": "2102.02779", "title": "Unifying Vision-and-Language Tasks via Text Generation", "year": 2021}, {"arxiv_id": "2205.00949", "title": "Answer-Me: Multi-Task Open-Vocabulary Visual Question Answering", "year": 2022}], "gt_label": [1, 1, 1, 1], "date": "2022-05", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_857", "valid": true}
{"query": "Which work leveraged orthogonal transformations to avoid the direct computation of Jacobian determinants?", "cited_paper": [{"arxiv_id": "1611.09630", "title": "Improving Variational Auto-Encoders using Householder Flow", "year": 2016}], "gt_label": [1], "date": "2016-11", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_858", "valid": true}
{"query": "What studies contribute to formality in cross-style learning?", "cited_paper": [{"arxiv_id": "1803.06535", "title": "Dear Sir or Madam, May I introduce the GYAFC Dataset: Corpus, Benchmarks and Metrics for Formality Style Transfer", "year": 2018}], "gt_label": [1], "date": "2018-03", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_859", "valid": true}
{"query": "Can you provide some references that introduce code filtering strategies where functional correctness prediction is achieved without code execution?", "cited_paper": [{"arxiv_id": "2206.03865", "title": "Fault-Aware Neural Code Rankers", "year": 2022}, {"arxiv_id": "2302.08468", "title": "LEVER: Learning to Verify Language-to-Code Generation with Execution", "year": 2023}, {"arxiv_id": "2211.16490", "title": "Coder Reviewer Reranking for Code Generation", "year": 2022}], "gt_label": [1, 1, 1], "date": "2023-02", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_860", "valid": true}
{"query": "Could you provide examples of research employ additional convolutional modules to learn a hierarchical feature space?", "cited_paper": [{"arxiv_id": "1709.09890", "title": "B-CNN: Branch Convolutional Neural Network for Hierarchical Classification", "year": 2017}, {"arxiv_id": "1906.01536", "title": "Visual Tree Convolutional Neural Network in Image Classification", "year": 2019}], "gt_label": [1, 1], "date": "2019-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_861", "valid": true}
{"query": "Could you provide me some studies that focus on modelling the user's state alongside the strategies in ESC systems?", "cited_paper": [{"arxiv_id": "2210.04242", "title": "Improving Multi-turn Emotional Support Dialogue Generation with Lookahead Strategy Planning", "year": 2022}, {"arxiv_id": "2310.07700", "title": "Knowledge-enhanced Memory Model for Emotional Support Conversation", "year": 2023}], "gt_label": [1, 1], "date": "2023-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_862", "valid": true}
{"query": "Are there any studies on decoupling the modeling of the environment using a semantic map from the end-to-end network?", "cited_paper": [{"arxiv_id": "2004.05155", "title": "Learning to Explore using Active Neural SLAM", "year": 2020}], "gt_label": [1], "date": "2020-04", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_863", "valid": true}
{"query": "Could you provide the study that reevaluated the methods of [bib.bibx36] and reported small robustness gains compared to adversarial training?", "cited_paper": [{"arxiv_id": "1711.08478", "title": "MagNet and \"Efficient Defenses Against Adversarial Attacks\" are Not Robust to Adversarial Examples", "year": 2017}], "gt_label": [1], "date": "2017-11", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_864", "valid": true}
{"query": "Which papers discuss multi-agent autonomous driving simulators which use real driving data to initialize scenarios and logged behavior?", "cited_paper": [{"arxiv_id": "2206.09889", "title": "Nocturne: a scalable driving benchmark for bringing multi-agent learning one step closer to the real world", "year": 2022}, {"arxiv_id": "2109.12674", "title": "MetaDrive: Composing Diverse Driving Scenarios for Generalizable Reinforcement Learning", "year": 2021}, {"arxiv_id": "2106.11810", "title": "NuPlan: A closed-loop ML-based planning benchmark for autonomous vehicles", "year": 2021}], "gt_label": [1, 1, 1], "date": "2022-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_865", "valid": true}
{"query": "What studies used cross-entropy method for both optimization and sampling in standard RL?", "cited_paper": [{"arxiv_id": "2205.05138", "title": "Efficient Risk-Averse Reinforcement Learning", "year": 2022}], "gt_label": [1], "date": "2022-05", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_866", "valid": true}
{"query": "What work can recover Lipschitz contextual bandits, despite having suboptimal dynamic regret bounds?", "cited_paper": [], "gt_label": [], "date": "2020-07", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_867", "valid": true}
{"query": "What works are about text/speech balloon detection in manga?", "cited_paper": [{"arxiv_id": "1803.08670", "title": "Object Detection for Comics using Manga109 Annotations", "year": 2018}, {"arxiv_id": "2207.04675", "title": "COO: Comic Onomatopoeia Dataset for Recognizing Arbitrary or Truncated Texts", "year": 2022}], "gt_label": [1, 1], "date": "2022-07", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_868", "valid": true}
{"query": "What works have explored prompting and style conversion for generative data augmentation in low-resource NLP?", "cited_paper": [{"arxiv_id": "2202.07922", "title": "ZeroGen: Efficient Zero-shot Learning via Dataset Generation", "year": 2022}, {"arxiv_id": "2310.14192", "title": "PromptMix: A Class Boundary Augmentation Method for Large Language Model Distillation", "year": 2023}, {"arxiv_id": "2210.07916", "title": "Style Transfer as Data Augmentation: A Case Study on Named Entity Recognition", "year": 2022}], "gt_label": [1, 1, 1], "date": "2023-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_869", "valid": true}
{"query": "Can you cite studies showcasing the effectiveness of LLM-powered data augmentation in cross-lingual commonsense reasoning?", "cited_paper": [{"arxiv_id": "2305.14288", "title": "LLM-powered Data Augmentation for Enhanced Cross-lingual Performance", "year": 2023}], "gt_label": [1], "date": "2023-05", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_870", "valid": true}
{"query": "Could you provide me the work that conducted analysis on the embedding layer of mT5 and XLM-R?", "cited_paper": [{"arxiv_id": "2311.18034", "title": "Hyperpolyglot LLMs: Cross-Lingual Interpretability in Token Embeddings", "year": 2023}], "gt_label": [1], "date": "2023-11", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_871", "valid": true}
{"query": "Which works use Graph Neural Networks and Recurrent Neural Networks to update encodings in temporal graph learning?", "cited_paper": [{"arxiv_id": "1612.07659", "title": "Structured Sequence Modeling with Graph Convolutional Recurrent Networks", "year": 2016}, {"arxiv_id": "1811.05320", "title": "T-GCN: A Temporal Graph ConvolutionalNetwork for Traffic Prediction", "year": 2018}, {"arxiv_id": "2208.07239", "title": "ROLAND: Graph Learning Framework for Dynamic Graphs", "year": 2022}, {"arxiv_id": "2303.08964", "title": "CS-TGN: Community Search via Temporal Graph Neural Networks", "year": 2023}, {"arxiv_id": "1908.01207", "title": "Predicting Dynamic Embedding Trajectory in Temporal Interaction Networks", "year": 2019}, {"arxiv_id": "2211.08378", "title": "Anomaly Detection in Multiplex Dynamic Networks: from Blockchain Security to Brain Disease Prediction", "year": 2022}], "gt_label": [1, 1, 1, 1, 1, 1], "date": "2023-03", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_872", "valid": true}
{"query": "Which papers solved classification and detection problems in LiDAR perception using deep learning?", "cited_paper": [{"arxiv_id": "2106.05304", "title": "Revisiting Point Cloud Shape Classification with a Simple and Effective Baseline", "year": 2021}, {"arxiv_id": "2202.03377", "title": "Benchmarking and Analyzing Point Cloud Classification under Corruptions", "year": 2022}, {"arxiv_id": "2112.02413", "title": "PointCLIP: Point Cloud Understanding by CLIP", "year": 2021}, {"arxiv_id": "1812.05784", "title": "PointPillars: Fast Encoders for Object Detection from Point Clouds", "year": 2018}, {"arxiv_id": "2102.00463", "title": "PV-RCNN++: Point-Voxel Feature Set Abstraction With Local Vector Representation for 3D Object Detection", "year": 2021}, {"arxiv_id": "1906.06310", "title": "Pseudo-LiDAR++: Accurate Depth for 3D Object Detection in Autonomous Driving", "year": 2019}], "gt_label": [1, 1, 1, 1, 1, 1], "date": "2022-02", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_873", "valid": true}
{"query": "Could you provide the work that developed an off-policy algorithm capable of computing the inner integral analytically?", "cited_paper": [], "gt_label": [], "date": "2018-01", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_874", "valid": true}
{"query": "What works propose to make use of loss functions to handle known symmetries in object pose prediction?", "cited_paper": [{"arxiv_id": "1711.00199", "title": "PoseCNN: A Convolutional Neural Network for 6D Object Pose Estimation in Cluttered Scenes", "year": 2017}, {"arxiv_id": "1901.02970", "title": "Normalized Object Coordinate Space for Category-Level 6D Object Pose and Size Estimation", "year": 2019}], "gt_label": [1, 1], "date": "2019-01", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_875", "valid": true}
{"query": "Which studies evaluated the generalization of an RL agent by training and testing them in totally different environments?", "cited_paper": [{"arxiv_id": "1812.02341", "title": "Quantifying Generalization in Reinforcement Learning", "year": 2018}, {"arxiv_id": "2012.02096", "title": "Emergent Complexity and Zero-shot Transfer via Unsupervised Environment Design", "year": 2020}], "gt_label": [1, 1], "date": "2020-12", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_876", "valid": true}
{"query": "Which work showed that the ranking of attention scores computed by a GAT layer is unconditioned on the query node?", "cited_paper": [{"arxiv_id": "2105.14491", "title": "How Attentive are Graph Attention Networks?", "year": 2021}], "gt_label": [1], "date": "2021-05", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_877", "valid": true}
{"query": "What studies introduced a strategy for normalizing multi-modal attributes to ensure consistency across modalities?", "cited_paper": [{"arxiv_id": "2304.01563", "title": "Attribute-Consistent Knowledge Graph Representation Learning for Multi-Modal Entity Alignment", "year": 2023}], "gt_label": [1], "date": "2023-04", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_878", "valid": true}
{"query": "Which papers have created datasets to support research in human behavior understanding?", "cited_paper": [{"arxiv_id": "1703.09788", "title": "Towards Automatic Learning of Procedures from Web Instructional Videos", "year": 2017}, {"arxiv_id": "1212.0402", "title": "UCF101: A Dataset of 101 Human Actions Classes From Videos in The Wild", "year": 2012}], "gt_label": [1, 1], "date": "2017-03", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_879", "valid": true}
{"query": "What work demonstrated the faster learning and better generalization when NTK-target alignment is high?", "cited_paper": [{"arxiv_id": "2106.06770", "title": "What can linearized neural networks actually say about generalization?", "year": 2021}], "gt_label": [1], "date": "2021-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_880", "valid": true}
{"query": "What research has been conducted on Memory-efficient training methods for reducing the memory footprint during the training process?", "cited_paper": [{"arxiv_id": "2302.04869", "title": "Reversible Vision Transformers", "year": 2023}, {"arxiv_id": "2001.04451", "title": "Reformer: The Efficient Transformer", "year": 2020}, {"arxiv_id": "1604.06174", "title": "Training Deep Nets with Sublinear Memory Cost", "year": 2016}], "gt_label": [1, 1, 1], "date": "2023-02", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_881", "valid": true}
{"query": "What research focuses on distilling reasoning processes?", "cited_paper": [{"arxiv_id": "2209.15189", "title": "Learning by Distilling Context", "year": 2022}], "gt_label": [1], "date": "2022-09", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_882", "valid": true}
{"query": "What research focuses on manipulating visual features to target the issue of image-text isolation?", "cited_paper": [{"arxiv_id": "2305.06500", "title": "InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning", "year": 2023}, {"arxiv_id": "2305.15023", "title": "Cheap and Quick: Efficient Vision-Language Instruction Tuning for Large Language Models", "year": 2023}], "gt_label": [1, 1], "date": "2023-05", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_883", "valid": true}
{"query": "Which studies focused on bottom-up methods in instance segmentation in 3D perception?", "cited_paper": [{"arxiv_id": "2108.02350", "title": "Hierarchical Aggregation for 3D Instance Segmentation", "year": 2021}, {"arxiv_id": "1812.07003", "title": "3D-SIS: 3D Semantic Instance Segmentation of RGB-D Scans", "year": 2018}, {"arxiv_id": "2204.07761", "title": "Language-Grounded Indoor 3D Semantic Segmentation in the Wild", "year": 2022}, {"arxiv_id": "2108.07478", "title": "Instance Segmentation in 3D Scenes using Semantic Superpoint Tree Networks", "year": 2021}], "gt_label": [1, 1, 1, 1], "date": "2022-04", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_884", "valid": true}
{"query": "Any work discusses unigram similarity metrics related to the downstream performance?", "cited_paper": [{"arxiv_id": "2004.10964", "title": "Don't Stop Pretraining: Adapt Language Models to Domains and Tasks", "year": 2020}], "gt_label": [1], "date": "2020-04", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_885", "valid": true}
{"query": "What papers incorporate rationalization methodologies into supervised NLI models to enhance their resilience against adversarial datasets?", "cited_paper": [{"arxiv_id": "2204.11790", "title": "Can Rationalization Improve Robustness?", "year": 2022}, {"arxiv_id": "2104.08142", "title": "Supervising Model Attention with Human Explanations for Robust Natural Language Inference", "year": 2021}], "gt_label": [1, 1], "date": "2022-04", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_886", "valid": true}
{"query": "What studies provide solutions for feature matching in low-textured regions using dense or semi-dense matching methods?", "cited_paper": [{"arxiv_id": "2101.01710", "title": "Learning Accurate Dense Correspondences and When to Trust Them", "year": 2021}, {"arxiv_id": "1810.10510", "title": "Neighbourhood Consensus Networks", "year": 2018}, {"arxiv_id": "2006.08844", "title": "Dual-Resolution Correspondence Networks", "year": 2020}, {"arxiv_id": "2104.00680", "title": "LoFTR: Detector-Free Local Feature Matching with Transformers", "year": 2021}, {"arxiv_id": "2201.02767", "title": "QuadTree Attention for Vision Transformers", "year": 2022}, {"arxiv_id": "2208.14201", "title": "ASpanFormer: Detector-Free Image Matching with Adaptive Span Transformer", "year": 2022}, {"arxiv_id": "2203.09645", "title": "MatchFormer: Interleaving Attention in Transformers for Feature Matching", "year": 2022}], "gt_label": [1, 1, 1, 1, 1, 1, 1], "date": "2022-08", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_887", "valid": true}
{"query": "Who has defined simplicity bias based on the number of linear components to define a decision boundary and studied its effects?", "cited_paper": [{"arxiv_id": "2006.07710", "title": "The Pitfalls of Simplicity Bias in Neural Networks", "year": 2020}], "gt_label": [1], "date": "2020-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_888", "valid": true}
{"query": "Which works proposed the extension of 3D Gaussian Splatting technique to dynamic scenes?", "cited_paper": [{"arxiv_id": "2310.08528", "title": "4D Gaussian Splatting for Real-Time Dynamic Scene Rendering", "year": 2023}, {"arxiv_id": "2308.09713", "title": "Dynamic 3D Gaussians: Tracking by Persistent Dynamic View Synthesis", "year": 2023}, {"arxiv_id": "2310.10642", "title": "Real-time Photorealistic Dynamic Scene Representation and Rendering with 4D Gaussian Splatting", "year": 2023}], "gt_label": [1, 1, 1], "date": "2023-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_889", "valid": true}
{"query": "Any works discussed about the introduction of additional layers into the model architecture instead of updating a large number of model parameters?", "cited_paper": [{"arxiv_id": "2110.06500", "title": "Differentially Private Fine-tuning of Language Models", "year": 2021}], "gt_label": [1], "date": "2021-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_890", "valid": true}
{"query": "Which publications discuss HMT methods utilizing ego-centric views?", "cited_paper": [{"arxiv_id": "2208.01633", "title": "UnrealEgo: A New Dataset for Robust Egocentric 3D Human Motion Capture", "year": 2022}, {"arxiv_id": "2212.11684", "title": "Scene-aware Egocentric 3D Human Pose Estimation", "year": 2022}, {"arxiv_id": "2104.13454", "title": "Estimating Egocentric 3D Human Pose in Global Space", "year": 2021}, {"arxiv_id": "2212.04636", "title": "Ego-Body Pose Estimation via Ego-Head Pose Estimation", "year": 2022}], "gt_label": [1, 1, 1, 1], "date": "2022-12", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_891", "valid": true}
{"query": "Which studies focus on the integration of contrastive signals in dataset condensation?", "cited_paper": [{"arxiv_id": "2202.02916", "title": "Dataset Condensation with Contrastive Signals", "year": 2022}], "gt_label": [1], "date": "2022-02", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_892", "valid": true}
{"query": "Which studies focused on using synthetic data to create new datasets or augment existing ones?", "cited_paper": [{"arxiv_id": "1504.06852", "title": "FlowNet: Learning Optical Flow with Convolutional Networks", "year": 2015}, {"arxiv_id": "1710.06924", "title": "VisDA: The Visual Domain Adaptation Challenge", "year": 2017}, {"arxiv_id": "1612.06890", "title": "CLEVR: A Diagnostic Dataset for Compositional Language and Elementary Visual Reasoning", "year": 2016}, {"arxiv_id": "1908.00222", "title": "Structured3D: A Large Photo-realistic Dataset for Structured 3D Modeling", "year": 2019}, {"arxiv_id": "2007.04954", "title": "ThreeDWorld: A Platform for Interactive Multi-Modal Physical Simulation", "year": 2020}], "gt_label": [1, 1, 1, 1, 1], "date": "2020-07", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_893", "valid": true}
{"query": "Which works employed contrastive learning for graph representation learning?", "cited_paper": [{"arxiv_id": "1809.10341", "title": "Deep Graph Infomax", "year": 2018}, {"arxiv_id": "1908.01000", "title": "InfoGraph: Unsupervised and Semi-supervised Graph-Level Representation Learning via Mutual Information Maximization", "year": 2019}, {"arxiv_id": "2010.13902", "title": "Graph Contrastive Learning with Augmentations", "year": 2020}, {"arxiv_id": "2006.04131", "title": "Deep Graph Contrastive Representation Learning", "year": 2020}, {"arxiv_id": "2006.09963", "title": "GCC: Graph Contrastive Coding for Graph Neural Network Pre-Training", "year": 2020}], "gt_label": [1, 1, 1, 1, 1], "date": "2020-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_894", "valid": true}
{"query": "What papers leverage pre-trained object detection models to extract image regional features offline for training multi-modal transformers?", "cited_paper": [{"arxiv_id": "2006.06195", "title": "Large-Scale Adversarial Training for Vision-and-Language Representation Learning", "year": 2020}], "gt_label": [1], "date": "2020-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_895", "valid": true}
{"query": "Which study formally introduced reward-free exploration for tabular MDP?", "cited_paper": [{"arxiv_id": "2002.02794", "title": "Reward-Free Exploration for Reinforcement Learning", "year": 2020}], "gt_label": [1], "date": "2020-02", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_896", "valid": true}
{"query": "What research proposed the gradient-sliding method for addressing separated structure in optimization problems?", "cited_paper": [], "gt_label": [], "date": "2014-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_897", "valid": true}
{"query": "Any studies that work on alignment of LLMs evaluators to human evaluation standards?", "cited_paper": [{"arxiv_id": "2309.13308", "title": "Calibrating LLM-Based Evaluator", "year": 2023}], "gt_label": [1], "date": "2023-09", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_898", "valid": true}
{"query": "Which works offer end-to-end methods for multimodal Language Models?", "cited_paper": [{"arxiv_id": "2204.14198", "title": "Flamingo: a Visual Language Model for Few-Shot Learning", "year": 2022}, {"arxiv_id": "2301.12597", "title": "BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models", "year": 2023}, {"arxiv_id": "2305.06500", "title": "InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning", "year": 2023}, {"arxiv_id": "2201.12086", "title": "BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation", "year": 2022}, {"arxiv_id": "2106.08254", "title": "BEiT: BERT Pre-Training of Image Transformers", "year": 2021}, {"arxiv_id": "2208.10442", "title": "Image as a Foreign Language: BEiT Pretraining for All Vision and Vision-Language Tasks", "year": 2022}, {"arxiv_id": "2304.08485", "title": "Visual Instruction Tuning", "year": 2023}, {"arxiv_id": "2304.14178", "title": "mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality", "year": 2023}, {"arxiv_id": "2304.10592", "title": "MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models", "year": 2023}, {"arxiv_id": "2303.16199", "title": "LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention", "year": 2023}, {"arxiv_id": "2305.03726", "title": "Otter: A Multi-Modal Model with In-Context Instruction Tuning", "year": 2023}, {"arxiv_id": "2202.03052", "title": "OFA: Unifying Architectures, Tasks, and Modalities Through a Simple Sequence-to-Sequence Learning Framework", "year": 2022}, {"arxiv_id": "2209.06794", "title": "PaLI: A Jointly-Scaled Multilingual Language-Image Model", "year": 2022}], "gt_label": [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], "date": "2023-05", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_899", "valid": true}
{"query": "Could you tell me the works related to representer point selection for estimating training data influence?", "cited_paper": [{"arxiv_id": "1811.09720", "title": "Representer Point Selection for Explaining Deep Neural Networks", "year": 2018}, {"arxiv_id": "2205.00359", "title": "Adapting and Evaluating Influence-Estimation Methods for Gradient-Boosted Decision Trees", "year": 2022}], "gt_label": [1, 1], "date": "2022-05", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_900", "valid": true}
{"query": "Which works are responsible for the introduction of unsupervised models for perceptual grouping?", "cited_paper": [{"arxiv_id": "2012.05208", "title": "On the Binding Problem in Artificial Neural Networks", "year": 2020}], "gt_label": [1], "date": "2020-12", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_901", "valid": true}
{"query": "Can you name some examples of projects that integrated machine learning, particularly LLMs, into automated theorem proving?", "cited_paper": [{"arxiv_id": "1905.10501", "title": "Learning to Reason in Large Theories without Imitation", "year": 2019}, {"arxiv_id": "2104.14516", "title": "Constructions in combinatorics via neural networks", "year": 2021}, {"arxiv_id": "2306.15626", "title": "LeanDojo: Theorem Proving with Retrieval-Augmented Language Models", "year": 2023}, {"arxiv_id": "2009.03393", "title": "Generative Language Modeling for Automated Theorem Proving", "year": 2020}, {"arxiv_id": "2102.06203", "title": "Proof Artifact Co-training for Theorem Proving with Language Models", "year": 2021}, {"arxiv_id": "2104.01112", "title": "NaturalProofs: Mathematical Theorem Proving in Natural Language", "year": 2021}, {"arxiv_id": "2210.12283", "title": "Draft, Sketch, and Prove: Guiding Formal Theorem Provers with Informal Proofs", "year": 2022}], "gt_label": [1, 1, 1, 1, 1, 1, 1], "date": "2023-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_902", "valid": true}
{"query": "Could you provide me some works centered on multilingual machine translation?", "cited_paper": [{"arxiv_id": "2010.11125", "title": "Beyond English-Centric Multilingual Machine Translation", "year": 2020}], "gt_label": [1], "date": "2020-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_903", "valid": true}
{"query": "What studies have used techniques like residual structure, skip connection, and dropout in basic CNN frameworks for image restoration?", "cited_paper": [{"arxiv_id": "1511.04587", "title": "Accurate Image Super-Resolution Using Very Deep Convolutional Networks", "year": 2015}, {"arxiv_id": "2008.13751", "title": "Plug-and-Play Image Restoration with Deep Denoiser Prior", "year": 2020}, {"arxiv_id": "1807.02758", "title": "Image Super-Resolution Using Very Deep Residual Channel Attention Networks", "year": 2018}, {"arxiv_id": "1812.10477", "title": "Residual Dense Network for Image Restoration", "year": 2018}, {"arxiv_id": "2112.12089", "title": "Reflash Dropout in Image Super-Resolution", "year": 2021}], "gt_label": [1, 1, 1, 1, 1], "date": "2021-12", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_904", "valid": true}
{"query": "Are there studies which focus on refining generated code through iterative reviews and improvements based on execution results?", "cited_paper": [{"arxiv_id": "2303.11366", "title": "Reflexion: Language Agents with Verbal Reinforcement Learning", "year": 2023}], "gt_label": [1], "date": "2023-03", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_905", "valid": true}
{"query": "Which works have been done on representation learning on hypergraphs?", "cited_paper": [{"arxiv_id": "1809.09401", "title": "Hypergraph Neural Networks", "year": 2018}], "gt_label": [1], "date": "2022-07", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_906", "valid": true}
{"query": "Which studies showed that the fine-tuning of large vision-language foundational models with a few examples from the target dataset can enhance performance?", "cited_paper": [{"arxiv_id": "2203.05557", "title": "Conditional Prompt Learning for Vision-Language Models", "year": 2022}, {"arxiv_id": "2109.01134", "title": "Learning to Prompt for Vision-Language Models", "year": 2021}, {"arxiv_id": "2203.12119", "title": "Visual Prompt Tuning", "year": 2022}, {"arxiv_id": "2210.03117", "title": "MaPLe: Multi-modal Prompt Learning", "year": 2022}], "gt_label": [1, 1, 1, 1], "date": "2022-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_907", "valid": true}
{"query": "Could you provide me some examples of works that introduced a parallel iterative routing?", "cited_paper": [{"arxiv_id": "2002.04764", "title": "Capsules with Inverted Dot-Product Attention Routing", "year": 2020}], "gt_label": [1], "date": "2020-02", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_908", "valid": true}
{"query": "Which papers extended the ideas from non-stationary multi-armed bandits to various contextual bandit settings?", "cited_paper": [{"arxiv_id": "1805.09365", "title": "Learning Contextual Bandits in a Non-stationary Environment", "year": 2018}, {"arxiv_id": "2102.05406", "title": "Non-stationary Reinforcement Learning without Prior Knowledge: An Optimal Black-box Approach", "year": 2021}], "gt_label": [1, 1], "date": "2021-02", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_909", "valid": true}
{"query": "What papers discussed techniques on disentanglement based on Variation Autoencoder (VAE)?", "cited_paper": [{"arxiv_id": "1312.6114", "title": "Auto-Encoding Variational Bayes", "year": 2013}], "gt_label": [1], "date": "2013-12", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_910", "valid": true}
{"query": "Which papers cover the convergence of PFL on homogeneous data?", "cited_paper": [], "gt_label": [], "date": "2019-09", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_911", "valid": true}
{"query": "What papers made advancements with hybrid pose-based methods in deep learning?", "cited_paper": [{"arxiv_id": "1707.09733", "title": "Camera Relocalization by Computing Pairwise Relative Poses Using Convolutional Neural Network", "year": 2017}], "gt_label": [1], "date": "2017-07", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_912", "valid": true}
{"query": "What prior works discussed random walk-based unsupervised representation learning methods?", "cited_paper": [{"arxiv_id": "1607.00653", "title": "node2vec: Scalable Feature Learning for Networks", "year": 2016}], "gt_label": [1], "date": "2016-07", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_913", "valid": true}
{"query": "Which research provides learning methods for object-level anomaly detection during meta-training?", "cited_paper": [{"arxiv_id": "2007.04146", "title": "Few-Shot One-Class Classification via Meta-Learning", "year": 2020}], "gt_label": [1], "date": "2020-07", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_914", "valid": true}
{"query": "Which papers proposed the Preference Ranking Optimization as an alternative to PPO?", "cited_paper": [{"arxiv_id": "2306.17492", "title": "Preference Ranking Optimization for Human Alignment", "year": 2023}], "gt_label": [1], "date": "2023-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_915", "valid": true}
{"query": "Any works about combining NeRFs by inserting objects into pre-existing NeRF scenes?", "cited_paper": [{"arxiv_id": "2204.10850", "title": "Control-NeRF: Editable Feature Volumes for Scene Rendering and Manipulation", "year": 2022}], "gt_label": [1], "date": "2022-04", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_916", "valid": true}
{"query": "What studies require an additional boundedness assumption when f* falls into a less-smooth interpolation space?", "cited_paper": [{"arxiv_id": "1801.07226", "title": "Optimal Convergence for Distributed Learning with Stochastic Gradient Methods and Spectral Algorithms", "year": 2018}], "gt_label": [1], "date": "2022-08", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_917", "valid": true}
{"query": "Which research papers adopted a 3D-Unet architecture to produce video volumes directly from an input image?", "cited_paper": [{"arxiv_id": "1804.01523", "title": "Stochastic Adversarial Video Prediction", "year": 2018}, {"arxiv_id": "2307.06940", "title": "Animate-A-Story: Storytelling with Retrieval-Augmented Video Generation", "year": 2023}, {"arxiv_id": "2307.04725", "title": "AnimateDiff: Animate Your Personalized Text-to-Image Diffusion Models without Specific Tuning", "year": 2023}, {"arxiv_id": "2105.04551", "title": "Stochastic Image-to-Video Synthesis using cINNs", "year": 2021}, {"arxiv_id": "2205.09853", "title": "MCVD: Masked Conditional Video Diffusion for Prediction, Generation, and Interpolation", "year": 2022}, {"arxiv_id": "2206.07696", "title": "Diffusion Models for Video Prediction and Infilling", "year": 2022}], "gt_label": [1, 1, 1, 1, 1, 1], "date": "2023-07", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_918", "valid": true}
{"query": "Who first explored the food bank problem in the context of additive valuations?", "cited_paper": [{"arxiv_id": "1502.07571", "title": "Online Fair Division: analysing a Food Bank problem", "year": 2015}], "gt_label": [1], "date": "2015-02", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_919", "valid": true}
{"query": "Which paper proposed an enhanced Key-Value Memory neural network for Information Retrieval-based methods?", "cited_paper": [{"arxiv_id": "1606.03126", "title": "Key-Value Memory Networks for Directly Reading Documents", "year": 2016}], "gt_label": [1], "date": "2016-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_920", "valid": true}
{"query": "What works studied enabling more robust operation for certain tasks through recovering explicit localization information in model representations?", "cited_paper": [{"arxiv_id": "2210.09996", "title": "Perceptual Grouping in Contrastive Vision-Language Models", "year": 2022}], "gt_label": [1], "date": "2022-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_921", "valid": true}
{"query": "Are there any existing datasets that concentrate on the novel use of word meanings and omit conventional examples?", "cited_paper": [{"arxiv_id": "2212.08395", "title": "Metaphorical Polysemy Detection: Conventional Metaphor meets Word Sense Disambiguation", "year": 2022}], "gt_label": [1], "date": "2022-12", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_922", "valid": true}
{"query": "Which papers present the use of ordinary and partial differential equations in designing, interpreting, and analyzing graph machine learning architectures?", "cited_paper": [{"arxiv_id": "1911.09554", "title": "Discrete and Continuous Deep Residual Learning Over Graphs", "year": 2019}, {"arxiv_id": "1911.07532", "title": "Graph Neural Ordinary Differential Equations", "year": 2019}, {"arxiv_id": "1912.00967", "title": "Continuous Graph Neural Networks", "year": 2019}], "gt_label": [1, 1, 1], "date": "2019-12", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_923", "valid": true}
{"query": "Which papers focus on distributed optimization using stochastic methods through client sampling?", "cited_paper": [{"arxiv_id": "2209.02257", "title": "Faster federated optimization under second-order similarity", "year": 2022}], "gt_label": [1], "date": "2022-09", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_924", "valid": true}
{"query": "What work explores learning an anomaly detector in the feature space with a category-agnostic model?", "cited_paper": [{"arxiv_id": "2207.07361", "title": "Registration based Few-Shot Anomaly Detection", "year": 2022}], "gt_label": [1], "date": "2022-07", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_925", "valid": true}
{"query": "Which papers propose first-order methods for efficiently solving min-max optimization problems in Weak Minty Variational Inequalities?", "cited_paper": [{"arxiv_id": "2009.09623", "title": "The Complexity of Constrained Min-Max Optimization", "year": 2020}, {"arxiv_id": "2011.00364", "title": "Efficient Methods for Structured Nonconvex-Nonconcave Min-Max Optimization", "year": 2020}, {"arxiv_id": "2302.09831", "title": "Escaping limit cycles: Global convergence for constrained nonconvex-nonconcave minimax problems", "year": 2023}, {"arxiv_id": "2302.09029", "title": "Solving stochastic weak Minty variational inequalities without increasing batch size", "year": 2023}], "gt_label": [1, 1, 1, 1], "date": "2023-02", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_926", "valid": true}
{"query": "Could you tell me which papers introduced RL from AI Feedback training approach in the context of RLHF?", "cited_paper": [{"arxiv_id": "2212.08073", "title": "Constitutional AI: Harmlessness from AI Feedback", "year": 2022}], "gt_label": [1], "date": "2022-12", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_927", "valid": true}
{"query": "Which works propose methods for character re-identification in manga?", "cited_paper": [{"arxiv_id": "2204.04621", "title": "Unsupervised Manga Character Re-identification via Face-body and Spatial-temporal Associated Clustering", "year": 2022}, {"arxiv_id": "2308.09096", "title": "Identity-Aware Semi-Supervised Learning for Comic Character Re-Identification", "year": 2023}], "gt_label": [1, 1], "date": "2023-08", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_928", "valid": true}
{"query": "Which papers discuss the RP gradient based on the reparameterization trick?", "cited_paper": [{"arxiv_id": "1312.6114", "title": "Auto-Encoding Variational Bayes", "year": 2013}], "gt_label": [1], "date": "2013-12", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_929", "valid": true}
{"query": "Which work showed that simple classifiers can detect images created by a single category of networks?", "cited_paper": [{"arxiv_id": "1901.08971", "title": "FaceForensics++: Learning to Detect Manipulated Facial Images", "year": 2019}], "gt_label": [1], "date": "2019-01", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_930", "valid": true}
{"query": "Which research introduced acoustic tokens into semantic token modeling and proposed a multi-stage generative framework in speech language models?", "cited_paper": [{"arxiv_id": "2209.03143", "title": "AudioLM: a Language Modeling Approach to Audio Generation", "year": 2022}], "gt_label": [1], "date": "2022-09", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_931", "valid": true}
{"query": "What studies proposed Stochastic Gradient Descent Ascent (SGDA) algorithms to address stochastic minimax optimization problems?", "cited_paper": [{"arxiv_id": "2010.15768", "title": "A Single-Loop Smoothed Gradient Descent-Ascent Algorithm for Nonconvex-Concave Min-Max Problems", "year": 2020}, {"arxiv_id": "1906.00331", "title": "On Gradient Descent Ascent for Nonconvex-Concave Minimax Problems", "year": 2019}, {"arxiv_id": "2008.10103", "title": "Single-Timescale Stochastic Nonconvex-Concave Optimization for Smooth Nonlinear TD Learning", "year": 2020}, {"arxiv_id": "2002.05309", "title": "Optimal Epoch Stochastic Gradient Descent Ascent Methods for Min-Max Optimization", "year": 2020}], "gt_label": [1, 1, 1, 1], "date": "2020-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_932", "valid": true}
{"query": "Could you provide me the studies of part correspondence?", "cited_paper": [{"arxiv_id": "1706.04496", "title": "Learning Local Shape Descriptors from Part Correspondences With Multi-view Convolutional Networks", "year": 2017}], "gt_label": [1], "date": "2017-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_933", "valid": true}
{"query": "Which studies fall under the category of sparse-point annotation methods in weakly supervised 3D instance segmentation?", "cited_paper": [{"arxiv_id": "2012.09165", "title": "Exploring Data-Efficient 3D Scene Understanding with Contrastive Scene Contexts", "year": 2020}, {"arxiv_id": "2007.10985", "title": "PointContrast: Unsupervised Pre-training for 3D Point Cloud Understanding", "year": 2020}], "gt_label": [1, 1], "date": "2020-12", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_934", "valid": true}
{"query": "Any studies about anomaly score improvement in reconstruction-based techniques by combining forecasting error and reconstruction probability?", "cited_paper": [{"arxiv_id": "2009.02040", "title": "Multivariate Time-series Anomaly Detection via Graph Attention Network", "year": 2020}], "gt_label": [1], "date": "2020-09", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_935", "valid": true}
{"query": "Can you provide works that used feature-based distillation methods in object detection?", "cited_paper": [{"arxiv_id": "1412.6550", "title": "FitNets: Hints for Thin Deep Nets", "year": 2014}, {"arxiv_id": "1906.03609", "title": "Distilling Object Detectors with Fine-grained Feature Imitation", "year": 2019}, {"arxiv_id": "2103.14475", "title": "Distilling Object Detectors via Decoupled Features", "year": 2021}], "gt_label": [1, 1, 1], "date": "2021-03", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_936", "valid": true}
{"query": "Which papers provide a comprehensive overview of transfer learning?", "cited_paper": [{"arxiv_id": "1911.02685", "title": "A Comprehensive Survey on Transfer Learning", "year": 2019}], "gt_label": [1], "date": "2019-11", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_937", "valid": true}
{"query": "Could you tell me some studies that implemented honesty-based fine-tuning and trained LLMs to admit limitations?", "cited_paper": [{"arxiv_id": "2312.07000", "title": "Alignment for Honesty", "year": 2023}], "gt_label": [1], "date": "2023-12", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_938", "valid": true}
{"query": "Could you tell me which paper proposed the fixed pre-decision module to bridge the gap between SimulMT and SimulST?", "cited_paper": [{"arxiv_id": "2011.02048", "title": "SimulMT to SimulST: Adapting Simultaneous Text Translation to End-to-End Simultaneous Speech Translation", "year": 2020}], "gt_label": [1], "date": "2020-11", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_939", "valid": true}
{"query": "Which papers discuss the use of barriers or immediate switching between the objective and constraint in CMDP framework?", "cited_paper": [{"arxiv_id": "1910.09615", "title": "IPO: Interior-point Policy Optimization under Constraints", "year": 2019}, {"arxiv_id": "2011.05869", "title": "CRPO: A New Approach for Safe Reinforcement Learning with Convergence Guarantee", "year": 2020}], "gt_label": [1, 1], "date": "2020-11", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_940", "valid": true}
{"query": "Which work modified the algorithm to get the Probabilistic Serial fractional outcome while showing a weak notion of efficiency with a simpler proof?", "cited_paper": [{"arxiv_id": "2002.10171", "title": "A Probabilistic Approach to Voting, Allocation, Matching, and Coalition Formation", "year": 2020}], "gt_label": [1], "date": "2020-02", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_941", "valid": true}
{"query": "Which work introduced the use of orthogonality in feature space to encourage inter-class separation and intra-class clustering?", "cited_paper": [{"arxiv_id": "2103.14021", "title": "Orthogonal Projection Loss", "year": 2021}, {"arxiv_id": "1702.00071", "title": "On orthogonality and learning recurrent networks with long term dependencies", "year": 2017}], "gt_label": [1, 1], "date": "2021-03", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_942", "valid": true}
{"query": "What paper employs self and cross-reconstruction modules to learn discriminative and smooth representations and uses DGCNN for learning the per-point feature embeddings?", "cited_paper": [{"arxiv_id": "2110.08636", "title": "DPC: Unsupervised Deep Point Correspondence via Cross and Self Construction", "year": 2021}], "gt_label": [1], "date": "2021-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_943", "valid": true}
{"query": "Could you mention some studies that contributed to remarkable text-to-3D generation results?", "cited_paper": [{"arxiv_id": "2209.14988", "title": "DreamFusion: Text-to-3D using 2D Diffusion", "year": 2022}, {"arxiv_id": "2211.10440", "title": "Magic3D: High-Resolution Text-to-3D Content Creation", "year": 2022}, {"arxiv_id": "2303.13873", "title": "Fantasia3D: Disentangling Geometry and Appearance for High-quality Text-to-3D Content Creation", "year": 2023}, {"arxiv_id": "2306.12422", "title": "DreamTime: An Improved Optimization Strategy for Diffusion-Guided 3D Generation", "year": 2023}], "gt_label": [1, 1, 1, 1], "date": "2023-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_944", "valid": true}
{"query": "What studies advanced diffusion probabilistic models to generate high-resolution and diverse images?", "cited_paper": [{"arxiv_id": "1503.03585", "title": "Deep Unsupervised Learning using Nonequilibrium Thermodynamics", "year": 2015}, {"arxiv_id": "2006.11239", "title": "Denoising Diffusion Probabilistic Models", "year": 2020}, {"arxiv_id": "2112.10741", "title": "GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models", "year": 2021}, {"arxiv_id": "2112.10752", "title": "High-Resolution Image Synthesis with Latent Diffusion Models", "year": 2021}, {"arxiv_id": "2205.11487", "title": "Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding", "year": 2022}], "gt_label": [1, 1, 1, 1, 1], "date": "2022-05", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_945", "valid": true}
{"query": "Which works have sought to detect important skill neurons in performing a specific task?", "cited_paper": [{"arxiv_id": "2302.06600", "title": "Task-Specific Skill Localization in Fine-tuned Language Models", "year": 2023}, {"arxiv_id": "2211.07349", "title": "Finding Skill Neurons in Pre-trained Transformer-based Language Models", "year": 2022}, {"arxiv_id": "2205.04157", "title": "Task-specific Compression for Multi-task Language Models using Attribution-based Pruning", "year": 2022}], "gt_label": [1, 1, 1], "date": "2023-02", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_946", "valid": true}
{"query": "Which studies have proposed methods to detect skill neurons within already trained models using an inference-based method?", "cited_paper": [{"arxiv_id": "2211.07349", "title": "Finding Skill Neurons in Pre-trained Transformer-based Language Models", "year": 2022}, {"arxiv_id": "2205.04157", "title": "Task-specific Compression for Multi-task Language Models using Attribution-based Pruning", "year": 2022}], "gt_label": [1, 1], "date": "2022-11", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_947", "valid": true}
{"query": "Which papers investigated CLIP-based classifiers and their performance in open-granularity classification?", "cited_paper": [{"arxiv_id": "2103.00020", "title": "Learning Transferable Visual Models From Natural Language Supervision", "year": 2021}, {"arxiv_id": "2203.05557", "title": "Conditional Prompt Learning for Vision-Language Models", "year": 2022}, {"arxiv_id": "2109.01134", "title": "Learning to Prompt for Vision-Language Models", "year": 2021}], "gt_label": [1, 1, 1], "date": "2022-03", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_948", "valid": true}
{"query": "What studies have produced abundant equivariant neural networks?", "cited_paper": [{"arxiv_id": "1802.08219", "title": "Tensor field networks: Rotationand translation-equivariant neural networks for 3D point clouds", "year": 2018}, {"arxiv_id": "2003.03123", "title": "Directional Message Passing for Molecular Graphs", "year": 2020}, {"arxiv_id": "2006.10503", "title": "SE(3)-Transformers: 3D Roto-Translation Equivariant Attention Networks", "year": 2020}, {"arxiv_id": "2102.09844", "title": "E(n) Equivariant Graph Neural Networks", "year": 2021}], "gt_label": [1, 1, 1, 1], "date": "2021-02", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_949", "valid": true}
{"query": "Which studies used transformers and diffusion models for creating high-fidelity images from text?", "cited_paper": [{"arxiv_id": "2204.08583", "title": "VQGAN-CLIP: Open Domain Image Generation and Editing with Natural Language Guidance", "year": 2022}, {"arxiv_id": "2204.14217", "title": "CogView2: Faster and Better Text-to-Image Generation via Hierarchical Transformers", "year": 2022}, {"arxiv_id": "2006.11239", "title": "Denoising Diffusion Probabilistic Models", "year": 2020}, {"arxiv_id": "2111.14818", "title": "Blended Diffusion for Text-driven Editing of Natural Images", "year": 2021}, {"arxiv_id": "2204.02491", "title": "Text2LIVE: Text-Driven Layered Image and Video Editing", "year": 2022}, {"arxiv_id": "2208.01626", "title": "Prompt-to-Prompt Image Editing with Cross Attention Control", "year": 2022}, {"arxiv_id": "2110.02711", "title": "DiffusionCLIP: Text-Guided Diffusion Models for Robust Image Manipulation", "year": 2021}, {"arxiv_id": "2112.05744", "title": "More Control for Free! Image Synthesis with Semantic Diffusion Guidance", "year": 2021}, {"arxiv_id": "2112.10741", "title": "GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models", "year": 2021}], "gt_label": [1, 1, 1, 1, 1, 1, 1, 1, 1], "date": "2022-08", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_950", "valid": true}
{"query": "What research works have explored how to address the susceptibility of current large language models to certain errors?", "cited_paper": [{"arxiv_id": "2107.07566", "title": "Internet-Augmented Dialogue Generation", "year": 2021}, {"arxiv_id": "2201.08239", "title": "LaMDA: Language Models for Dialog Applications", "year": 2022}, {"arxiv_id": "2211.10435", "title": "PAL: Program-aided Language Models", "year": 2022}, {"arxiv_id": "2302.04761", "title": "Toolformer: Language Models Can Teach Themselves to Use Tools", "year": 2023}], "gt_label": [1, 1, 1, 1], "date": "2023-02", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_951", "valid": true}
{"query": "Are there any works that have further strengthened text embedders?", "cited_paper": [{"arxiv_id": "2212.03533", "title": "Text Embeddings by Weakly-Supervised Contrastive Pre-training", "year": 2022}, {"arxiv_id": "2309.07597", "title": "C-Pack: Packed Resources For General Chinese Embeddings", "year": 2023}, {"arxiv_id": "2112.07899", "title": "Large Dual Encoders Are Generalizable Retrievers", "year": 2021}], "gt_label": [1, 1, 1], "date": "2023-09", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_952", "valid": true}
{"query": "Could you provide some works that discussed the problem of selling information in economics and computer science?", "cited_paper": [{"arxiv_id": "1204.5519", "title": "Optimal Mechanisms for Selling Information", "year": 2012}, {"arxiv_id": "2011.14570", "title": "How to Sell Information Optimally: an Algorithmic Study", "year": 2020}, {"arxiv_id": "2102.13289", "title": "Optimal Pricing of Information", "year": 2021}, {"arxiv_id": "2202.09013", "title": "Is Selling Complete Information (Approximately) Optimal?", "year": 2022}], "gt_label": [1, 1, 1, 1], "date": "2022-02", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_953", "valid": true}
{"query": "Which studies approached the combination of Imitation learning and Reinforcement learning by treating RL as a sequence modelling problem and train an autoregressive model using offline data?", "cited_paper": [{"arxiv_id": "2202.05607", "title": "Online Decision Transformer", "year": 2022}, {"arxiv_id": "2106.02039", "title": "Offline Reinforcement Learning as One Big Sequence Modeling Problem", "year": 2021}, {"arxiv_id": "2106.01345", "title": "Decision Transformer: Reinforcement Learning via Sequence Modeling", "year": 2021}], "gt_label": [1, 1, 1], "date": "2022-02", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_954", "valid": true}
{"query": "Could you provide me studies that improved the performance of small-scale models by fine-tuning them using reasoning processes generated by LLMs?", "cited_paper": [{"arxiv_id": "2110.14168", "title": "Training Verifiers to Solve Math Word Problems", "year": 2021}, {"arxiv_id": "2212.10071", "title": "Large Language Models Are Reasoning Teachers", "year": 2022}, {"arxiv_id": "2305.01879", "title": "SCOTT: Self-Consistent Chain-of-Thought Distillation", "year": 2023}, {"arxiv_id": "2212.08410", "title": "Teaching Small Language Models to Reason", "year": 2022}], "gt_label": [1, 1, 1, 1], "date": "2023-05", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_955", "valid": true}
{"query": "Could you provide me some works that show asymmetric quantization methods outperform their symmetric counterparts?", "cited_paper": [{"arxiv_id": "2311.01792", "title": "AFPQ: Asymmetric Floating Point Quantization for LLMs", "year": 2023}], "gt_label": [1], "date": "2023-11", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_956", "valid": true}
{"query": "Could you provide me some works focusing on video visual relation detection using datasets like AG, VidVRD and VidOR?", "cited_paper": [{"arxiv_id": "1604.01753", "title": "Hollywood in Homes: Crowdsourcing Data Collection for Activity Understanding", "year": 2016}], "gt_label": [1], "date": "2016-04", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_957", "valid": true}
{"query": "Could you mention a study that proposed plain diffusion Transformer architecture to learn the denoising diffusion process on latent patches?", "cited_paper": [{"arxiv_id": "2212.09748", "title": "Scalable Diffusion Models with Transformers", "year": 2022}], "gt_label": [1], "date": "2022-12", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_958", "valid": true}
{"query": "What works discusses the combination of the KD and model quantization method to achieve high compression ratios?", "cited_paper": [{"arxiv_id": "2009.12812", "title": "TernaryBERT: Distillation-aware Ultra-low Bit BERT", "year": 2020}, {"arxiv_id": "2211.11014", "title": "Understanding and Improving Knowledge Distillation for Quantization-Aware Training of Large Transformer Encoders", "year": 2022}], "gt_label": [1, 1], "date": "2022-11", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_959", "valid": true}
{"query": "Which research first studied a local minimax risk for instance optimality in differential privacy?", "cited_paper": [{"arxiv_id": "2005.10630", "title": "Near Instance-Optimality in Differential Privacy", "year": 2020}], "gt_label": [1], "date": "2020-05", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_960", "valid": true}
{"query": "Are there any works related to the framework of detector-free Structure-from-Motion?", "cited_paper": [{"arxiv_id": "1805.03879", "title": "Structure-from-Motion using Dense CNN Features with Keypoint Relocalization", "year": 2018}, {"arxiv_id": "2301.07673", "title": "OnePose++: Keypoint-Free One-Shot Object Pose Estimation without CAD Models", "year": 2023}], "gt_label": [1, 1], "date": "2023-01", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_961", "valid": true}
{"query": "Which work discusses the generation of CDRs on a single chain in regards to pipeline-based antibody design?", "cited_paper": [{"arxiv_id": "2110.04624", "title": "Iterative Refinement Graph Neural Network for Antibody Sequence-Structure Co-design", "year": 2021}], "gt_label": [1], "date": "2021-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_962", "valid": true}
{"query": "Which research works presented improvements on the top of EDM?", "cited_paper": [{"arxiv_id": "2209.05710", "title": "MDM: Molecular Diffusion Model for 3D Molecule Generation", "year": 2022}, {"arxiv_id": "2209.00865", "title": "Diffusion-based Molecule Generation with Informative Prior Bridges", "year": 2022}, {"arxiv_id": "2302.09048", "title": "MiDi: Mixed Graph and 3D Denoising Diffusion for Molecule Generation", "year": 2023}, {"arxiv_id": "2305.01140", "title": "Geometric Latent Diffusion Models for 3D Molecule Generation", "year": 2023}], "gt_label": [1, 1, 1, 1], "date": "2023-05", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_963", "valid": true}
{"query": "Could you provide me some works that have developed RL, behavioral cloning, and LLM-based models on the web front?", "cited_paper": [{"arxiv_id": "1812.09195", "title": "Learning to Navigate the Web", "year": 2018}, {"arxiv_id": "1802.08802", "title": "Reinforcement Learning on Web Interfaces Using Workflow-Guided Exploration", "year": 2018}, {"arxiv_id": "2202.08137", "title": "A data-driven approach for learning to control computers", "year": 2022}], "gt_label": [1, 1, 1], "date": "2022-02", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_964", "valid": true}
{"query": "Which papers introduced factorization along spatial and temporal dimensions on the granularity of the encoder?", "cited_paper": [{"arxiv_id": "2103.15691", "title": "ViViT: A Video Vision Transformer", "year": 2021}, {"arxiv_id": "2102.05095", "title": "Is Space-Time Attention All You Need for Video Understanding?", "year": 2021}], "gt_label": [1, 1], "date": "2021-03", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_965", "valid": true}
{"query": "Can you mention some studies that address the issue of diversity in single-imgae 3D generation, especially in face generation or starting from text for 3D generation?", "cited_paper": [{"arxiv_id": "2112.00879", "title": "Generating Diverse 3D Reconstructions from a Single Occluded Face Image", "year": 2021}, {"arxiv_id": "2305.16213", "title": "ProlificDreamer: High-Fidelity and Diverse Text-to-3D Generation with Variational Score Distillation", "year": 2023}], "gt_label": [1, 1], "date": "2023-05", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_966", "valid": true}
{"query": "Can you provide a study that effectively tackles domain adaptation and active domain adaptation tasks for 3D semantic segmentation?", "cited_paper": [{"arxiv_id": "2212.10390", "title": "UniDA3D: Unified Domain Adaptive 3D Semantic Segmentation Pipeline", "year": 2022}], "gt_label": [1], "date": "2022-12", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_967", "valid": true}
{"query": "Which study proposed to use rooted homomorphism counts as node features in a graph neural network (GNN)?", "cited_paper": [{"arxiv_id": "2106.06707", "title": "Graph Neural Networks with Local Graph Parameters", "year": 2021}], "gt_label": [1], "date": "2021-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_968", "valid": true}
{"query": "Which studies used feature decomposition to enhance feature alignment in domain adaptation and domain generalization?", "cited_paper": [{"arxiv_id": "2201.01929", "title": "Decompose to Adapt: Cross-domain Object Detection via Feature Disentanglement", "year": 2022}, {"arxiv_id": "2008.12839", "title": "Learning to Balance Specificity and Invariance for In and Out of Domain Generalization", "year": 2020}, {"arxiv_id": "2003.12815", "title": "Efficient Domain Generalization via Common-Specific Low-Rank Decomposition", "year": 2020}, {"arxiv_id": "2303.07123", "title": "Modality-Agnostic Debiasing for Single Domain Generalization", "year": 2023}], "gt_label": [1, 1, 1, 1], "date": "2023-03", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_969", "valid": true}
{"query": "Could you provide me some studies that focus on the impact of noisy information on retrieval-augmented generation?", "cited_paper": [{"arxiv_id": "2311.09210", "title": "Chain-of-Note: Enhancing Robustness in Retrieval-Augmented Language Models", "year": 2023}, {"arxiv_id": "2310.01558", "title": "Making Retrieval-Augmented Language Models Robust to Irrelevant Context", "year": 2023}, {"arxiv_id": "2309.01431", "title": "Benchmarking Large Language Models in Retrieval-Augmented Generation", "year": 2023}], "gt_label": [1, 1, 1], "date": "2023-11", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_970", "valid": true}
{"query": "What studies tackled the challenge of missing data within MMKGs with adversarial methods?", "cited_paper": [{"arxiv_id": "1809.01341", "title": "Embedding Multimodal Relational Data for Knowledge Base Completion", "year": 2018}], "gt_label": [1], "date": "2018-09", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_971", "valid": true}
{"query": "What papers utilized the strategy of transforming categorical data into a continuous space and then applying Gaussian diffusion?", "cited_paper": [{"arxiv_id": "2208.04202", "title": "Analog Bits: Generating Discrete Data using Diffusion Models with Self-Conditioning", "year": 2022}, {"arxiv_id": "2203.17003", "title": "Equivariant Diffusion for Molecule Generation in 3D", "year": 2022}], "gt_label": [1, 1], "date": "2022-08", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_972", "valid": true}
{"query": "Name a research work that involves the use of neural networks as alternative function classes in BO methods?", "cited_paper": [], "gt_label": [], "date": "2015-02", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_973", "valid": true}
{"query": "Which studies explored the effect of the choice of the in-context samples on the performance of in-context learning?", "cited_paper": [{"arxiv_id": "2101.06804", "title": "What Makes Good In-Context Examples for GPT-$3$?", "year": 2021}], "gt_label": [1], "date": "2021-01", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_974", "valid": true}
{"query": "Could you mention any research that was focused on applying CLIP to downstream tasks?", "cited_paper": [{"arxiv_id": "2109.01134", "title": "Learning to Prompt for Vision-Language Models", "year": 2021}, {"arxiv_id": "2203.05557", "title": "Conditional Prompt Learning for Vision-Language Models", "year": 2022}, {"arxiv_id": "2110.04544", "title": "CLIP-Adapter: Better Vision-Language Models with Feature Adapters", "year": 2021}, {"arxiv_id": "2111.03930", "title": "Tip-Adapter: Training-free CLIP-Adapter for Better Vision-Language Modeling", "year": 2021}], "gt_label": [1, 1, 1, 1], "date": "2022-03", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_975", "valid": true}
{"query": "What are the studies where the RL agents are evaluated by seeing if they can quickly adapt from one task to another?", "cited_paper": [{"arxiv_id": "1711.03938", "title": "CARLA: An Open Urban Driving Simulator", "year": 2017}, {"arxiv_id": "1810.08272", "title": "BabyAI: A Platform to Study the Sample Efficiency of Grounded Language Learning", "year": 2018}, {"arxiv_id": "2005.07648", "title": "Language Conditioned Imitation Learning over Unstructured Data", "year": 2020}], "gt_label": [1, 1, 1], "date": "2020-05", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_976", "valid": true}
{"query": "Which works focused on the evaluation of RL agents by changing the surfaces of the objects in the environment?", "cited_paper": [{"arxiv_id": "1703.06907", "title": "Domain Randomization for Transferring Deep Neural Networks from Simulation to the Real World", "year": 2017}, {"arxiv_id": "1811.06032", "title": "Natural Environment Benchmarks for Reinforcement Learning", "year": 2018}, {"arxiv_id": "1902.07015", "title": "Investigating Generalisation in Continuous Deep Reinforcement Learning", "year": 2019}, {"arxiv_id": "1812.07252", "title": "Sim-to-Real via Sim-to-Sim: Data-efficient Robotic Grasping via Randomized-to-Canonical Adaptation Networks", "year": 2018}], "gt_label": [1, 1, 1, 1], "date": "2019-02", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_977", "valid": true}
{"query": "Could you cite the works where multilingual LLMs were evaluated on individual tasks such as Translation, Question-Answering, Summarization, and Reasoning?", "cited_paper": [{"arxiv_id": "1910.11856", "title": "On the Cross-lingual Transferability of Monolingual Representations", "year": 2019}, {"arxiv_id": "2010.11856", "title": "XOR QA: Cross-lingual Open-Retrieval Question Answering", "year": 2020}, {"arxiv_id": "2106.13822", "title": "XL-Sum: Large-Scale Multilingual Abstractive Summarization for 44 Languages", "year": 2021}, {"arxiv_id": "2112.08804", "title": "CrossSum: Beyond English-Centric Cross-Lingual Summarization for 1,500+ Language Pairs", "year": 2021}, {"arxiv_id": "2210.03057", "title": "Language Models are Multilingual Chain-of-Thought Reasoners", "year": 2022}, {"arxiv_id": "2005.00333", "title": "XCOPA: A Multilingual Dataset for Causal Commonsense Reasoning", "year": 2020}], "gt_label": [1, 1, 1, 1, 1, 1], "date": "2022-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_978", "valid": true}
{"query": "Can you provide the references that propose techniques for enhancing hierarchical classification consistency?", "cited_paper": [{"arxiv_id": "2210.07225", "title": "Unified Vision and Language Prompt Learning", "year": 2022}, {"arxiv_id": "2210.03117", "title": "MaPLe: Multi-modal Prompt Learning", "year": 2022}, {"arxiv_id": "2210.01115", "title": "LASP: Text-to-Text Optimization for Language-Aware Soft Prompting of Vision & Language Models", "year": 2022}], "gt_label": [1, 1, 1], "date": "2022-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_979", "valid": true}
{"query": "Could you provide some references focusing on achieving -stationary points under non-convex settings?", "cited_paper": [], "gt_label": [], "date": "2019-04", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_980", "valid": true}
{"query": "What studies have been conducted to predict future actions from a sequence of observed actions?", "cited_paper": [{"arxiv_id": "1908.09540", "title": "Uncertainty-Aware Anticipation of Activities", "year": 2019}, {"arxiv_id": "1804.00892", "title": "When will you do what? - Anticipating Temporal Occurrences of Activities", "year": 2018}, {"arxiv_id": "2105.12414", "title": "Anticipating human actions by correlating past with the future with Jaccard similarity measures", "year": 2021}], "gt_label": [1, 1, 1], "date": "2021-05", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_981", "valid": true}
{"query": "Are there works that demonstrate attempts to make images unlearnable or uneditable to handle unauthorized data usage issues in diffusion models?", "cited_paper": [{"arxiv_id": "2302.04222", "title": "Glaze: Protecting Artists from Style Mimicry by Text-to-Image Models", "year": 2023}, {"arxiv_id": "2302.06588", "title": "Raising the Cost of Malicious AI-Powered Image Editing", "year": 2023}], "gt_label": [1, 1], "date": "2023-02", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_982", "valid": true}
{"query": "Can you list the research that introduced the Mirror Descent Policy Optimization?", "cited_paper": [{"arxiv_id": "2005.09814", "title": "Mirror Descent Policy Optimization", "year": 2020}], "gt_label": [1], "date": "2020-05", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_983", "valid": true}
{"query": "Which research projects looked into misclassification issues of black-box classifiers where human text was mistaken as LLM-generated?", "cited_paper": [{"arxiv_id": "2303.11156", "title": "Can AI-Generated Text be Reliably Detected?", "year": 2023}], "gt_label": [1], "date": "2023-03", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_984", "valid": true}
{"query": "Could you provide examples of the works on building structured and interpretable models such as Hamiltonian Neural Networks (HNN), Lagrangian Neural Networks (LNN), and Sparse Identification of Nonlinear Dynamics (SINDy)?", "cited_paper": [{"arxiv_id": "1906.01563", "title": "Hamiltonian Neural Networks", "year": 2019}, {"arxiv_id": "2003.04630", "title": "Lagrangian Neural Networks", "year": 2020}], "gt_label": [1, 1], "date": "2020-03", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_985", "valid": true}
{"query": "Did any researchers create a synchrony-based model with no reliance on explicit supervision for grouping?", "cited_paper": [{"arxiv_id": "1312.6115", "title": "Neuronal Synchrony in Complex-Valued Deep Networks", "year": 2013}, {"arxiv_id": "2204.02075", "title": "Complex-Valued Autoencoders for Object Discovery", "year": 2022}], "gt_label": [1, 1], "date": "2022-04", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_986", "valid": true}
{"query": "Who proposed analytic methods for gradient inversion?", "cited_paper": [{"arxiv_id": "2010.07733", "title": "R-GAP: Recursive Gradient Attack on Privacy", "year": 2020}, {"arxiv_id": "2006.11601", "title": "Rethinking Privacy Preserving Deep Learning: How to Evaluate and Thwart Privacy Attacks", "year": 2020}], "gt_label": [1, 1], "date": "2020-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_987", "valid": true}
{"query": "Are there any research related to meta-learning?", "cited_paper": [{"arxiv_id": "1703.03400", "title": "Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks", "year": 2017}], "gt_label": [1], "date": "2017-03", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_988", "valid": true}
{"query": "Could you provide me some studies that detected hallucinations by analyzing the relationship between input prompts and the LLM's output responses?", "cited_paper": [{"arxiv_id": "2309.11064", "title": "Exploring the Relationship between LLM Hallucinations and Prompt Linguistic Nuances: Readability, Formality, and Concreteness", "year": 2023}, {"arxiv_id": "2303.08896", "title": "SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models", "year": 2023}, {"arxiv_id": "2310.01469", "title": "LLM Lies: Hallucinations are not Bugs, but Features as Adversarial Examples", "year": 2023}], "gt_label": [1, 1, 1], "date": "2023-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_989", "valid": true}
{"query": "Which papers mentioned the application of transformer-based pre-trained models in code search?", "cited_paper": [{"arxiv_id": "2009.08366", "title": "GraphCodeBERT: Pre-training Code Representations with Data Flow", "year": 2020}, {"arxiv_id": "2210.09597", "title": "Soft-Labeled Contrastive Pre-training for Function-level Code Representation", "year": 2022}], "gt_label": [1, 1], "date": "2022-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_990", "valid": true}
{"query": "Could you provide references that discuss alternative approaches to the matrix mechanism that reduce the variance by adding bias?", "cited_paper": [{"arxiv_id": "1012.4763", "title": "A simple and practical algorithm for differentially private data release", "year": 2010}, {"arxiv_id": "2103.06641", "title": "Differentially Private Query Release Through Adaptive Projection", "year": 2021}, {"arxiv_id": "1402.1526", "title": "Dual Query: Practical Private Query Release for High Dimensional Data", "year": 2014}, {"arxiv_id": "2201.12677", "title": "AIM: An Adaptive and Iterative Mechanism for Differentially Private Synthetic Data", "year": 2022}, {"arxiv_id": "2106.07153", "title": "Iterative Methods for Private Synthetic Data: Unifying Framework and New Methods", "year": 2021}, {"arxiv_id": "2007.05453", "title": "New Oracle-Efficient Algorithms for Private Synthetic Data Release", "year": 2020}], "gt_label": [1, 1, 1, 1, 1, 1], "date": "2022-01", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_991", "valid": true}
{"query": "What papers are about using synthetic data to improve the domain-generalization of NLI models?", "cited_paper": [{"arxiv_id": "2002.09599", "title": "Training Question Answering Models From Synthetic Data", "year": 2020}, {"arxiv_id": "2106.06168", "title": "Generate, Annotate, and Learn: NLP with Synthetic Text", "year": 2021}, {"arxiv_id": "2211.08264", "title": "QAmeleon: Multilingual QA with Only 5 Examples", "year": 2022}, {"arxiv_id": "2310.07849", "title": "Synthetic Data Generation with Large Language Models for Text Classification: Potential and Limitations", "year": 2023}], "gt_label": [1, 1, 1, 1], "date": "2023-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_992", "valid": true}
{"query": "Which works describe the advancements in 3D reconstruction and novel view synthesis with NeRF?", "cited_paper": [{"arxiv_id": "2003.08934", "title": "NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis", "year": 2020}, {"arxiv_id": "2008.02268", "title": "NeRF in the Wild: Neural Radiance Fields for Unconstrained Photo Collections", "year": 2020}, {"arxiv_id": "2103.13415", "title": "Mip-NeRF: A Multiscale Representation for Anti-Aliasing Neural Radiance Fields", "year": 2021}, {"arxiv_id": "2103.15595", "title": "MVSNeRF: Fast Generalizable Radiance Field Reconstruction from Multi-View Stereo", "year": 2021}, {"arxiv_id": "2307.11335", "title": "Tri-MipRF: Tri-Mip Representation for Efficient Anti-Aliasing Neural Radiance Fields", "year": 2023}], "gt_label": [1, 1, 1, 1, 1], "date": "2023-07", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_993", "valid": true}
{"query": "What works evaluated the applicability of MAML and its variants to graph neural networks (GNNs)?", "cited_paper": [], "gt_label": [], "date": "2020-03", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_994", "valid": true}
{"query": "What papers discuss client-side local distillation to transfer global knowledge to local models in generic FL?", "cited_paper": [{"arxiv_id": "2105.10056", "title": "Data-Free Knowledge Distillation for Heterogeneous Federated Learning", "year": 2021}], "gt_label": [1], "date": "2021-05", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_995", "valid": true}
{"query": "Which papers describe the analysis of the semantic property of intermediate latent space by its local geometry?", "cited_paper": [{"arxiv_id": "2106.06959", "title": "Do Not Escape From the Manifold: Discovering the Local Coordinates on the Latent Space of GANs", "year": 2021}], "gt_label": [1], "date": "2021-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_996", "valid": true}
{"query": "What are the examples of research done on SLAM methods in the context of object navigation tasks?", "cited_paper": [{"arxiv_id": "2007.00643", "title": "Object Goal Navigation using Goal-Oriented Semantic Exploration", "year": 2020}, {"arxiv_id": "2201.10029", "title": "PONI: Potential Functions for ObjectGoal Navigation with Interaction-free Learning", "year": 2022}], "gt_label": [1, 1], "date": "2022-01", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_997", "valid": true}
{"query": "What works discuss the performance degradation in NLI models when presented with adversarial datasets?", "cited_paper": [{"arxiv_id": "1806.00692", "title": "Stress Test Evaluation for Natural Language Inference", "year": 2018}, {"arxiv_id": "1902.01007", "title": "Right for the Wrong Reasons: Diagnosing Syntactic Heuristics in Natural Language Inference", "year": 2019}, {"arxiv_id": "1910.14599", "title": "Adversarial NLI: A New Benchmark for Natural Language Understanding", "year": 2019}, {"arxiv_id": "2010.03777", "title": "An Empirical Study on Model-agnostic Debiasing Strategies for Robust Natural Language Inference", "year": 2020}], "gt_label": [1, 1, 1, 1], "date": "2020-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_998", "valid": true}
{"query": "What studies have focused on training language like models with privacy guarantees?", "cited_paper": [{"arxiv_id": "2108.01624", "title": "Large-Scale Differentially Private BERT", "year": 2021}], "gt_label": [1], "date": "2021-08", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_test_999", "valid": true}
{"query": "Could you list the works exploit knowledge from pre-trained vision-language models for text-guided queries in 3D scenes?", "cited_paper": [{"arxiv_id": "2210.05663", "title": "CLIP-Fields: Weakly Supervised Semantic Fields for Robotic Memory", "year": 2022}, {"arxiv_id": "2302.07241", "title": "ConceptFusion: Open-set Multimodal 3D Mapping", "year": 2023}, {"arxiv_id": "2211.15654", "title": "OpenScene: 3D Scene Understanding with Open Vocabularies", "year": 2022}], "gt_label": [1, 1, 1], "date": "2023-02", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_0", "valid": true}
{"query": "Could you provide me some works related to representer theorems in machine learning?", "cited_paper": [{"arxiv_id": "1709.10441", "title": "A representer theorem for deep kernel learning", "year": 2017}, {"arxiv_id": "1802.09210", "title": "A representer theorem for deep neural networks", "year": 2018}], "gt_label": [1, 1], "date": "2018-02", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_1", "valid": true}
{"query": "Which studies discuss the effects and mechanisms of weight decay regularization in machine learning?", "cited_paper": [{"arxiv_id": "1810.12281", "title": "Three Mechanisms of Weight Decay Regularization", "year": 2018}], "gt_label": [1], "date": "2018-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_2", "valid": true}
{"query": "Could you mention some papers that propose neural algorithms for f-divergence regularized costs?", "cited_paper": [{"arxiv_id": "1605.08527", "title": "Stochastic Optimization for Large-scale Optimal Transport", "year": 2016}, {"arxiv_id": "1711.02283", "title": "Large-Scale Optimal Transport and Mapping Estimation", "year": 2017}, {"arxiv_id": "2110.03237", "title": "Score-based Generative Neural Networks for Large-Scale Optimal Transport", "year": 2021}], "gt_label": [1, 1, 1], "date": "2021-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_3", "valid": true}
{"query": "What works implemented temporal pixel-wise audio-visual interaction module in FCN-based methods for AVS?", "cited_paper": [{"arxiv_id": "2207.05042", "title": "Audio-Visual Segmentation", "year": 2022}], "gt_label": [1], "date": "2022-07", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_4", "valid": true}
{"query": "Which papers describe work aimed at increasing the faithfulness of Large Language Models (LLMs) by changing the prediction generation method?", "cited_paper": [{"arxiv_id": "2306.17806", "title": "Stay on topic with Classifier-Free Guidance", "year": 2023}, {"arxiv_id": "2205.09712", "title": "Selection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning", "year": 2022}, {"arxiv_id": "2307.11768", "title": "Question Decomposition Improves the Faithfulness of Model-Generated Reasoning", "year": 2023}, {"arxiv_id": "2301.13379", "title": "Faithful Chain-of-Thought Reasoning", "year": 2023}, {"arxiv_id": "2310.00603", "title": "Faithful Explanations of Black-box NLP Models Using LLM-generated Counterfactuals", "year": 2023}], "gt_label": [1, 1, 1, 1, 1], "date": "2023-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_5", "valid": true}
{"query": "Which works focus on face verification models and discuss their performance in unconstrained environments?", "cited_paper": [{"arxiv_id": "1801.07698", "title": "ArcFace: Additive Angular Margin Loss for Deep Face Recognition", "year": 2018}, {"arxiv_id": "2004.00288", "title": "CurricularFace: Adaptive Curriculum Learning Loss for Deep Face Recognition", "year": 2020}, {"arxiv_id": "2204.00964", "title": "AdaFace: Quality Adaptive Margin for Face Recognition", "year": 2022}], "gt_label": [1, 1, 1], "date": "2022-04", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_6", "valid": true}
{"query": "Any works about hallucination assessment in GPT-4V?", "cited_paper": [{"arxiv_id": "2311.03287", "title": "Holistic Analysis of Hallucination in GPT-4V(ision): Bias and Interference Challenges", "year": 2023}], "gt_label": [1], "date": "2023-11", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_7", "valid": true}
{"query": "Which papers discussed alternative resolution schemes related to unit scaling?", "cited_paper": [{"arxiv_id": "2102.06171", "title": "High-Performance Large-Scale Image Recognition Without Normalization", "year": 2021}], "gt_label": [1], "date": "2021-02", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_8", "valid": true}
{"query": "What studies have shown the global convergence of Gradient Descent (GD) for simple linear networks and two-layer networks?", "cited_paper": [{"arxiv_id": "1710.10174", "title": "SGD Learns Over-parameterized Networks that Provably Generalize on Linearly Separable Data", "year": 2017}, {"arxiv_id": "1710.10345", "title": "The Implicit Bias of Gradient Descent on Separable Data", "year": 2017}, {"arxiv_id": "1901.08584", "title": "Fine-Grained Analysis of Optimization and Generalization for Overparameterized Two-Layer Neural Networks", "year": 2019}], "gt_label": [1, 1, 1], "date": "2019-01", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_9", "valid": true}
{"query": "What papers discuss the use of knowledge distillation for model compression?", "cited_paper": [{"arxiv_id": "1903.12136", "title": "Distilling Task-Specific Knowledge from BERT into Simple Neural Networks", "year": 2019}, {"arxiv_id": "1909.10351", "title": "TinyBERT: Distilling BERT for Natural Language Understanding", "year": 2019}, {"arxiv_id": "2004.02984", "title": "MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices", "year": 2020}], "gt_label": [1, 1, 1], "date": "2020-04", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_10", "valid": true}
{"query": "Could you tell me about the research that revealed different variants of pretrained transformer models?", "cited_paper": [{"arxiv_id": "1706.03762", "title": "Attention Is All You Need", "year": 2017}], "gt_label": [1], "date": "2017-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_11", "valid": true}
{"query": "What research studies have been cited for applying the CLIP model for various downstream applications like image-based object detection, segmentation, and video applications?", "cited_paper": [{"arxiv_id": "2207.03482", "title": "Bridging the Gap between Object and Image-level Representations for Open-Vocabulary Detection", "year": 2022}, {"arxiv_id": "2210.04150", "title": "Open-Vocabulary Semantic Segmentation with Mask-adapted CLIP", "year": 2022}, {"arxiv_id": "2204.07761", "title": "Language-Grounded Indoor 3D Semantic Segmentation in the Wild", "year": 2022}, {"arxiv_id": "2208.02816", "title": "Expanding Language-Image Pretrained Models for General Video Recognition", "year": 2022}, {"arxiv_id": "2109.08472", "title": "ActionCLIP: A New Paradigm for Video Action Recognition", "year": 2021}], "gt_label": [1, 1, 1, 1, 1], "date": "2022-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_12", "valid": true}
{"query": "Which studies indicate a natural conflict between adversarial robustness and standard accuracy?", "cited_paper": [{"arxiv_id": "1805.12152", "title": "Robustness May Be at Odds with Accuracy", "year": 2018}, {"arxiv_id": "1901.08573", "title": "Theoretically Principled Trade-off between Robustness and Accuracy", "year": 2019}], "gt_label": [1, 1], "date": "2019-01", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_13", "valid": true}
{"query": "Which paper introduced the concept of dataset distillation?", "cited_paper": [{"arxiv_id": "1811.10959", "title": "Dataset Distillation", "year": 2018}], "gt_label": [1], "date": "2018-11", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_14", "valid": true}
{"query": "Which papers tried to accelerate Neural Differential Equations models using higher-order regularization terms?", "cited_paper": [{"arxiv_id": "2007.04504", "title": "Learning Differential Equations that are Easy to Solve", "year": 2020}], "gt_label": [1], "date": "2020-07", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_15", "valid": true}
{"query": "In what works are SMLD diffusion models considered?", "cited_paper": [{"arxiv_id": "1907.05600", "title": "Generative Modeling by Estimating Gradients of the Data Distribution", "year": 2019}], "gt_label": [1], "date": "2019-07", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_16", "valid": true}
{"query": "Which works use standard fine-tuning to attempt to localize edits in parameter updating methods?", "cited_paper": [{"arxiv_id": "2104.08164", "title": "Editing Factual Knowledge in Language Models", "year": 2021}, {"arxiv_id": "2110.11309", "title": "Fast Model Editing at Scale", "year": 2021}], "gt_label": [1, 1], "date": "2021-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_17", "valid": true}
{"query": "Could you provide me some studies performed on adversarial training?", "cited_paper": [{"arxiv_id": "1711.03213", "title": "CyCADA: Cycle-Consistent Adversarial Domain Adaptation", "year": 2017}, {"arxiv_id": "1909.00589", "title": "Self-Ensembling with GAN-based Data Augmentation for Domain Adaptation in Semantic Segmentation", "year": 2019}, {"arxiv_id": "1711.06969", "title": "Learning from Synthetic Data: Addressing Domain Shift for Semantic Segmentation", "year": 2017}, {"arxiv_id": "1802.10349", "title": "Learning to Adapt Structured Output Space for Semantic Segmentation", "year": 2018}, {"arxiv_id": "2303.14360", "title": "Both Style and Distortion Matter: Dual-Path Unsupervised Domain Adaptation for Panoramic Semantic Segmentation", "year": 2023}], "gt_label": [1, 1, 1, 1, 1], "date": "2023-03", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_18", "valid": true}
{"query": "Which papers worked on lower bound theory for ReLU Networks?", "cited_paper": [{"arxiv_id": "1709.02540", "title": "The Expressive Power of Neural Networks: A View from the Width", "year": 2017}, {"arxiv_id": "1710.11278", "title": "Approximating Continuous Functions by ReLU Nets of Minimal Width", "year": 2017}], "gt_label": [1, 1], "date": "2017-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_19", "valid": true}
{"query": "Are there any studies which used conditional Gaussian distributions over feature spaces in the context of adversarial robustness?", "cited_paper": [{"arxiv_id": "1802.09308", "title": "Max-Mahalanobis Linear Discriminant Analysis Networks", "year": 2018}, {"arxiv_id": "2011.09066", "title": "Shaping Deep Feature Space towards Gaussian Mixture for Visual Classification", "year": 2020}], "gt_label": [1, 1], "date": "2020-11", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_20", "valid": true}
{"query": "What research focused on the development of sequential action understanding datasets?", "cited_paper": [{"arxiv_id": "1804.04527", "title": "SoccerNet: A Scalable Dataset for Action Spotting in Soccer Videos", "year": 2018}], "gt_label": [1], "date": "2018-04", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_21", "valid": true}
{"query": "Which papers cover information-sharing strategies to mitigate heterogeneity in Federated Learning?", "cited_paper": [{"arxiv_id": "1806.00582", "title": "Federated Learning with Non-IID Data", "year": 2018}, {"arxiv_id": "1811.11479", "title": "Communication-Efficient On-Device Machine Learning: Federated Distillation and Augmentation under Non-IID Private Data", "year": 2018}], "gt_label": [1, 1], "date": "2019-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_22", "valid": true}
{"query": "What were the methods proposed for learning the prompt from downstream data in continual input embedding space?", "cited_paper": [{"arxiv_id": "2101.00190", "title": "Prefix-Tuning: Optimizing Continuous Prompts for Generation", "year": 2021}], "gt_label": [1], "date": "2021-01", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_23", "valid": true}
{"query": "Could you provide me some works focused on 3D diffusion models based on implicit fields?", "cited_paper": [{"arxiv_id": "2211.16677", "title": "3D Neural Field Generation using Triplane Diffusion", "year": 2022}, {"arxiv_id": "2210.06978", "title": "LION: Latent Point Diffusion Models for 3D Shape Generation", "year": 2022}, {"arxiv_id": "2212.03293", "title": "Diffusion-SDF: Text-to-Shape via Voxelized Diffusion", "year": 2022}, {"arxiv_id": "2212.04493", "title": "SDFusion: Multimodal 3D Shape Completion, Reconstruction, and Generation", "year": 2022}, {"arxiv_id": "2302.00190", "title": "Neural Wavelet-domain Diffusion for 3D Shape Generation, Inversion, and Manipulation", "year": 2023}, {"arxiv_id": "2303.10406", "title": "3DQD: Generalized Deep 3D Shape Prior via Part-Discretized Diffusion Process", "year": 2023}, {"arxiv_id": "2303.17015", "title": "HyperDiffusion: Generating Implicit Neural Fields with Weight-Space Diffusion", "year": 2023}], "gt_label": [1, 1, 1, 1, 1, 1, 1], "date": "2023-03", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_24", "valid": true}
{"query": "Any research on building a prompting pipeline where the LLM reasons over the extracted KG subgraphs?", "cited_paper": [{"arxiv_id": "2308.09729", "title": "MindMap: Knowledge Graph Prompting Sparks Graph of Thoughts in Large Language Models", "year": 2023}], "gt_label": [1], "date": "2023-08", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_25", "valid": true}
{"query": "Who proposed improvements to the accuracy of optimization-based attacks using different image priors?", "cited_paper": [{"arxiv_id": "2001.02610", "title": "iDLG: Improved Deep Leakage from Gradients", "year": 2020}, {"arxiv_id": "2003.14053", "title": "Inverting Gradients -- How easy is it to break privacy in federated learning?", "year": 2020}, {"arxiv_id": "2104.07586", "title": "See through Gradients: Image Batch Recovery via GradInversion", "year": 2021}], "gt_label": [1, 1, 1], "date": "2021-04", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_26", "valid": true}
{"query": "Which studies are related to the empowerment of modern natural language processing systems by text embedders?", "cited_paper": [{"arxiv_id": "2004.04906", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "year": 2020}], "gt_label": [1], "date": "2020-04", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_27", "valid": true}
{"query": "Could you provide me some works that lean on uncertainty or diversity criteria for their selection strategies?", "cited_paper": [{"arxiv_id": "2211.05997", "title": "LiDAL: Inter-frame Uncertainty Based Active Learning for 3D LiDAR Semantic Segmentation", "year": 2022}, {"arxiv_id": "2302.13824", "title": "Dirichlet-based Uncertainty Calibration for Active Domain Adaptation", "year": 2023}, {"arxiv_id": "2202.12588", "title": "Active Learning for Point Cloud Semantic Segmentation via Spatial-Structural Diversity Reasoning", "year": 2022}], "gt_label": [1, 1, 1], "date": "2023-02", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_28", "valid": true}
{"query": "What work proposed a model that estimates pixel-wise weights for pre-specified WB presets?", "cited_paper": [{"arxiv_id": "2109.08750", "title": "Auto White-Balance Correction for Mixed-Illuminant Scenes", "year": 2021}], "gt_label": [1], "date": "2021-09", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_29", "valid": true}
{"query": "Which papers introduced the use of VAEs and 3D convolutional networks to generate voxelized molecules?", "cited_paper": [{"arxiv_id": "2010.08687", "title": "Learning a Continuous Representation of 3D Molecular Structures with Deep Generative Models", "year": 2020}], "gt_label": [1], "date": "2020-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_30", "valid": true}
{"query": "Which research papers propose motion trajectory conditioned on scene image?", "cited_paper": [{"arxiv_id": "2007.03672", "title": "Long-term Human Motion Prediction with Scene Context", "year": 2020}], "gt_label": [1], "date": "2020-07", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_31", "valid": true}
{"query": "What is a noteworthy study that discusses Federated Learning with cyclic client participation?", "cited_paper": [], "gt_label": [], "date": "2023-02", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_32", "valid": true}
{"query": "Can you provide references where rewards are shaped by training a reinforcement learning agent to learn and complete intermediate tasks guided by language?", "cited_paper": [{"arxiv_id": "1903.02020", "title": "Using Natural Language for Reward Shaping in Reinforcement Learning", "year": 2019}, {"arxiv_id": "2206.09674", "title": "EAGER: Asking and Answering Questions for Automatic Reward Shaping in Language-guided RL", "year": 2022}, {"arxiv_id": "2103.05825", "title": "ELLA: Exploration through Learned Language Abstraction", "year": 2021}], "gt_label": [1, 1, 1], "date": "2022-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_33", "valid": true}
{"query": "Which work first introduced the idea of converting visual features into readable embeddings for LLMs?", "cited_paper": [{"arxiv_id": "2106.13884", "title": "Multimodal Few-Shot Learning with Frozen Language Models", "year": 2021}], "gt_label": [1], "date": "2021-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_34", "valid": true}
{"query": "What researches used averaging and max pooling in feature aggregation on a multi-view rendering-based method?", "cited_paper": [{"arxiv_id": "1706.04496", "title": "Learning Local Shape Descriptors from Part Correspondences With Multi-view Convolutional Networks", "year": 2017}, {"arxiv_id": "2210.15904", "title": "Self-Supervised Learning with Multi-View Rendering for 3D Point Cloud Analysis", "year": 2022}], "gt_label": [1, 1], "date": "2022-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_35", "valid": true}
{"query": "Which studies proposed to increase the representation ability of quantization by replacing uniform quantization with non-uniform quantization?", "cited_paper": [{"arxiv_id": "2111.14826", "title": "Nonuniform-to-Uniform Quantization: Towards Accurate Quantization via Generalized Straight-Through Estimation", "year": 2021}, {"arxiv_id": "1909.13144", "title": "Additive Powers-of-Two Quantization: An Efficient Non-uniform Discretization for Neural Networks", "year": 2019}], "gt_label": [1, 1], "date": "2021-11", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_36", "valid": true}
{"query": "What research suggests that duplicate examples can hurt the performance in document retrieval?", "cited_paper": [{"arxiv_id": "2107.06499", "title": "Deduplicating Training Data Makes Language Models Better", "year": 2021}, {"arxiv_id": "2205.10487", "title": "Scaling Laws and Interpretability of Learning from Repeated Data", "year": 2022}], "gt_label": [1, 1], "date": "2022-05", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_37", "valid": true}
{"query": "Could you list some works that tried to re-introduce hierarchical designs into transformer?", "cited_paper": [{"arxiv_id": "2104.11227", "title": "Multiscale Vision Transformers", "year": 2021}, {"arxiv_id": "2106.13230", "title": "Video Swin Transformer", "year": 2021}], "gt_label": [1, 1], "date": "2021-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_38", "valid": true}
{"query": "Can you name the studies that used Counterfactual examples as data augmentation in Natural Language Processing (NLP)?", "cited_paper": [{"arxiv_id": "1909.12434", "title": "Learning the Difference that Makes a Difference with Counterfactually-Augmented Data", "year": 2019}, {"arxiv_id": "2101.00288", "title": "Polyjuice: Generating Counterfactuals for Explaining, Evaluating, and Improving Models", "year": 2021}, {"arxiv_id": "2004.02709", "title": "Evaluating Models' Local Decision Boundaries via Contrast Sets", "year": 2020}], "gt_label": [1, 1, 1], "date": "2021-01", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_39", "valid": true}
{"query": "What are some works that have improved the theoretical convergence rate of local stochastic gradient descent ascent algorithms in federated learning?", "cited_paper": [], "gt_label": [], "date": "2022-05", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_40", "valid": true}
{"query": "What studies structured the learned RL algorithm as a black box using a neural network as a general purpose sequence model in meta-RL methods?", "cited_paper": [{"arxiv_id": "1611.02779", "title": "RL$^2$: Fast Reinforcement Learning via Slow Reinforcement Learning", "year": 2016}, {"arxiv_id": "1611.05763", "title": "Learning to reinforcement learn", "year": 2016}, {"arxiv_id": "1707.03141", "title": "A Simple Neural Attentive Meta-Learner", "year": 2017}, {"arxiv_id": "1910.13406", "title": "Generalization of Reinforcement Learners with Working and Episodic Memory", "year": 2019}, {"arxiv_id": "2006.03662", "title": "Rapid Task-Solving in Novel Environments", "year": 2020}, {"arxiv_id": "2110.05038", "title": "Recurrent Model-Free RL Can Be a Strong Baseline for Many POMDPs", "year": 2021}], "gt_label": [1, 1, 1, 1, 1, 1], "date": "2021-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_41", "valid": true}
{"query": "What papers proposed text data augmentation techniques like synonym replacement, positional swaps and back translation?", "cited_paper": [{"arxiv_id": "1511.06709", "title": "Improving Neural Machine Translation Models with Monolingual Data", "year": 2015}], "gt_label": [1], "date": "2015-11", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_42", "valid": true}
{"query": "What studies propose the usage of diffusion models for processing graph data?", "cited_paper": [{"arxiv_id": "2210.01549", "title": "Diffusion Models for Graphs Benefit From Discrete State Spaces", "year": 2022}, {"arxiv_id": "2202.02514", "title": "Score-based Generative Modeling of Graphs via the System of Stochastic Differential Equations", "year": 2022}], "gt_label": [1, 1], "date": "2022-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_43", "valid": true}
{"query": "Can you show some applications of diffusion models in zero-shot classification and supervised segmentation?", "cited_paper": [{"arxiv_id": "2303.16203", "title": "Your Diffusion Model is Secretly a Zero-Shot Classifier", "year": 2023}, {"arxiv_id": "2112.00390", "title": "SegDiff: Image Segmentation with Diffusion Probabilistic Models", "year": 2021}], "gt_label": [1, 1], "date": "2023-03", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_44", "valid": true}
{"query": "Any works on breaking the representational symmetry based on spatial coordinates or specific object types?", "cited_paper": [{"arxiv_id": "2001.02407", "title": "SPACE: Unsupervised Object-Oriented Scene Representation via Spatial Attention and Decomposition", "year": 2020}], "gt_label": [1], "date": "2020-01", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_45", "valid": true}
{"query": "What works talk about the LLM-Blender, which uses a pair-ranker model for optimal LLM output selection?", "cited_paper": [{"arxiv_id": "2306.02561", "title": "LLM-Blender: Ensembling Large Language Models with Pairwise Ranking and Generative Fusion", "year": 2023}], "gt_label": [1], "date": "2023-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_46", "valid": true}
{"query": "What studies demonstrated the capability of LLMs to provide chain-of-thought explanations that elucidate their reasoning processes?", "cited_paper": [{"arxiv_id": "2201.11903", "title": "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models", "year": 2022}, {"arxiv_id": "2205.11916", "title": "Large Language Models are Zero-Shot Reasoners", "year": 2022}], "gt_label": [1, 1], "date": "2022-05", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_47", "valid": true}
{"query": "Which works focused on designing interventions for causal discovery?", "cited_paper": [{"arxiv_id": "2203.02016", "title": "Interventions, Where and How? Experimental Design for Causal Models at Scale", "year": 2022}], "gt_label": [1], "date": "2022-03", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_48", "valid": true}
{"query": "What research focused on deploying the PW-learner and the RA-learner in the estimation of the CATE?", "cited_paper": [], "gt_label": [], "date": "2021-01", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_49", "valid": true}
{"query": "Which study approaches the problem of convergence rates of classic TD from the perspective of Ordinary Differential Equations (ODE) analysis?", "cited_paper": [], "gt_label": [], "date": "2019-02", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_50", "valid": true}
{"query": "Which works focus on the application of Bayesian Optimization in the area of global non-convex optimization?", "cited_paper": [{"arxiv_id": "2008.08757", "title": "On Lower Bounds for Standard and Robust Gaussian Process Bandit Optimization", "year": 2020}], "gt_label": [1], "date": "2020-08", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_51", "valid": true}
{"query": "Which research contains over thousands of single-choice questions covering numerous different ability dimensions?", "cited_paper": [{"arxiv_id": "2307.06281", "title": "MMBench: Is Your Multi-modal Model an All-around Player?", "year": 2023}], "gt_label": [1], "date": "2023-07", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_52", "valid": true}
{"query": "Which study used a weak supervision type, foreground mask, as a substitute for costly 3D CAD annotations?", "cited_paper": [{"arxiv_id": "1705.10904", "title": "Weakly supervised 3D Reconstruction with Adversarial Constraint", "year": 2017}], "gt_label": [1], "date": "2017-05", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_53", "valid": true}
{"query": "What are the studies manipulate the generation process of a pre-trained model to implicitly control the generated content?", "cited_paper": [{"arxiv_id": "2209.15264", "title": "Diffusion-based Image Translation using Disentangled Style and Content Representation", "year": 2022}, {"arxiv_id": "2211.12572", "title": "Plug-and-Play Diffusion Features for Text-Driven Image-to-Image Translation", "year": 2022}, {"arxiv_id": "2208.01626", "title": "Prompt-to-Prompt Image Editing with Cross Attention Control", "year": 2022}, {"arxiv_id": "2111.14818", "title": "Blended Diffusion for Text-driven Editing of Natural Images", "year": 2021}, {"arxiv_id": "2108.02938", "title": "ILVR: Conditioning Method for Denoising Diffusion Probabilistic Models", "year": 2021}, {"arxiv_id": "2211.09794", "title": "Null-text Inversion for Editing Real Images using Guided Diffusion Models", "year": 2022}, {"arxiv_id": "2210.11427", "title": "DiffEdit: Diffusion-based semantic image editing with mask guidance", "year": 2022}, {"arxiv_id": "2210.05872", "title": "Leveraging Off-the-shelf Diffusion Model for Multi-attribute Fashion Image Manipulation", "year": 2022}], "gt_label": [1, 1, 1, 1, 1, 1, 1, 1], "date": "2022-11", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_54", "valid": true}
{"query": "What research works explore human bodies rendering through multi-view videos?", "cited_paper": [{"arxiv_id": "2001.04947", "title": "Neural Human Video Rendering by Learning Dynamic Textures and Rendering-to-Video Translation", "year": 2020}, {"arxiv_id": "2104.03110", "title": "Neural Articulated Radiance Field", "year": 2021}, {"arxiv_id": "2012.12884", "title": "Vid2Actor: Free-viewpoint Animatable Person Synthesis from Video in the Wild", "year": 2020}], "gt_label": [1, 1, 1], "date": "2021-04", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_55", "valid": true}
{"query": "What is the research that estimates a transformation between two point clouds by deforming a template shape?", "cited_paper": [{"arxiv_id": "1806.05228", "title": "3D-CODED : 3D Correspondences by Deep Deformation", "year": 2018}], "gt_label": [1], "date": "2018-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_56", "valid": true}
{"query": "Which papers studied learning invariant representations for domain adaptation?", "cited_paper": [{"arxiv_id": "1505.07818", "title": "Domain-Adversarial Training of Neural Networks", "year": 2015}], "gt_label": [1], "date": "2015-05", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_57", "valid": true}
{"query": "Can you give me examples of papers that used optimization and translation models to recover atomic coordinates from generated voxel grids?", "cited_paper": [{"arxiv_id": "2010.08687", "title": "Learning a Continuous Representation of 3D Molecular Structures with Deep Generative Models", "year": 2020}], "gt_label": [1], "date": "2020-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_58", "valid": true}
{"query": "Could you provide some works that tackle regression as an ordinal classification problem?", "cited_paper": [{"arxiv_id": "1806.02446", "title": "Deep Ordinal Regression Network for Monocular Depth Estimation", "year": 2018}, {"arxiv_id": "1901.07884", "title": "Rank consistent ordinal regression for neural networks with application to age estimation", "year": 2019}, {"arxiv_id": "2111.08851", "title": "Deep Neural Networks for Rank-Consistent Ordinal Regression Based On Conditional Probabilities", "year": 2021}], "gt_label": [1, 1, 1], "date": "2021-11", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_59", "valid": true}
{"query": "What research has been done on learning the relative position of texts in the latent embedding space?", "cited_paper": [{"arxiv_id": "1908.10084", "title": "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks", "year": 2019}, {"arxiv_id": "2104.08821", "title": "SimCSE: Simple Contrastive Learning of Sentence Embeddings", "year": 2021}], "gt_label": [1, 1], "date": "2021-04", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_60", "valid": true}
{"query": "What applications for federated learning are discussed in the field of healthcare?", "cited_paper": [{"arxiv_id": "2005.05752", "title": "A Secure Federated Learning Framework for 5G Networks", "year": 2020}], "gt_label": [1], "date": "2020-05", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_61", "valid": true}
{"query": "Which papers used guidance to influence the sampling procedure within their diffusion models?", "cited_paper": [{"arxiv_id": "2105.05233", "title": "Diffusion Models Beat GANs on Image Synthesis", "year": 2021}, {"arxiv_id": "2207.12598", "title": "Classifier-Free Diffusion Guidance", "year": 2022}], "gt_label": [1, 1], "date": "2022-07", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_62", "valid": true}
{"query": "What studies are about the emergence of sequence-structure co-design methods and their superiority over previous methods?", "cited_paper": [{"arxiv_id": "2110.04624", "title": "Iterative Refinement Graph Neural Network for Antibody Sequence-Structure Co-design", "year": 2021}, {"arxiv_id": "2208.06073", "title": "Conditional Antibody Design as 3D Equivariant Graph Translation", "year": 2022}], "gt_label": [1, 1], "date": "2022-08", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_63", "valid": true}
{"query": "Which studies illustrate usage of semantic tokens for ASR or speech resynthesis?", "cited_paper": [{"arxiv_id": "2112.08352", "title": "Textless Speech-to-Speech Translation on Real Data", "year": 2021}, {"arxiv_id": "2210.04062", "title": "CoBERT: Self-Supervised Speech Representation Learning Through Code Representation Learning", "year": 2022}], "gt_label": [1, 1], "date": "2022-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_64", "valid": true}
{"query": "What research papers joined the recent considerations of the regret minimization when the sender repeatedly interacts with receivers?", "cited_paper": [{"arxiv_id": "2106.06480", "title": "Multi-Receiver Online Bayesian Persuasion", "year": 2021}, {"arxiv_id": "2102.10156", "title": "Learning to Persuade on the Fly: Robustness Against Ignorance", "year": 2021}, {"arxiv_id": "2202.06135", "title": "Online Bayesian Recommendation with No Regret", "year": 2022}], "gt_label": [1, 1, 1], "date": "2022-02", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_65", "valid": true}
{"query": "Is there any recent work that used an implicit model to produce a non-parametric distribution over SO(3) that can model objects with large symmetry groups?", "cited_paper": [{"arxiv_id": "2106.05965", "title": "Implicit-PDF: Non-Parametric Representation of Probability Distributions on the Rotation Manifold", "year": 2021}], "gt_label": [1], "date": "2021-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_66", "valid": true}
{"query": "What are the studies that talk about using prompts and example-based definitions with regards to in-context learning (ICL)?", "cited_paper": [{"arxiv_id": "2104.08786", "title": "Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot Prompt Order Sensitivity", "year": 2021}, {"arxiv_id": "2201.11903", "title": "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models", "year": 2022}], "gt_label": [1, 1], "date": "2022-01", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_67", "valid": true}
{"query": "Which studies have updated challenge test sets in a dynamic way similar to RealTime QA?", "cited_paper": [{"arxiv_id": "2104.14337", "title": "Dynabench: Rethinking Benchmarking in NLP", "year": 2021}, {"arxiv_id": "2012.15349", "title": "DynaSent: A Dynamic Benchmark for Sentiment Analysis", "year": 2020}, {"arxiv_id": "2106.06052", "title": "Dynaboard: An Evaluation-As-A-Service Platform for Holistic Next-Generation Benchmarking", "year": 2021}], "gt_label": [1, 1, 1], "date": "2021-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_68", "valid": true}
{"query": "Can you provide works that extended the zero-shot learning capability of CLIP to monocular depth estimation?", "cited_paper": [{"arxiv_id": "2207.01077", "title": "Can Language Understand Depth?", "year": 2022}, {"arxiv_id": "2311.01034", "title": "Learning to Adapt CLIP for Few-Shot Monocular Depth Estimation", "year": 2023}], "gt_label": [1, 1], "date": "2023-11", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_69", "valid": true}
{"query": "Which paper discusses indirect measurements of hypothesized theoretical entities known as constructs in social sciences?", "cited_paper": [{"arxiv_id": "1912.05511", "title": "Measurement and Fairness", "year": 2019}], "gt_label": [1], "date": "2019-12", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_70", "valid": true}
{"query": "What works have previously used continuation techniques when the objective function is differentiable?", "cited_paper": [{"arxiv_id": "1205.0079", "title": "Complexity Analysis of the Lasso Regularization Path", "year": 2012}], "gt_label": [1], "date": "2012-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_71", "valid": true}
{"query": "Which work was first to use open-source models for QA systems with citation capability?", "cited_paper": [{"arxiv_id": "2306.07906", "title": "WebGLM: Towards An Efficient Web-Enhanced Question Answering System with Human Preferences", "year": 2023}], "gt_label": [1], "date": "2023-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_72", "valid": true}
{"query": "Could you provide me the work that scales the training dataset to billions in the field of Vision-Language Pre-training?", "cited_paper": [{"arxiv_id": "2102.05918", "title": "Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision", "year": 2021}], "gt_label": [1], "date": "2021-02", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_73", "valid": true}
{"query": "Which studies have focused on grounding elements in mobile UI based on instructions?", "cited_paper": [{"arxiv_id": "2005.03776", "title": "Mapping Natural Language Instructions to Mobile UI Action Sequences", "year": 2020}, {"arxiv_id": "2202.02312", "title": "A Dataset for Interactive Vision-Language Navigation with Unknown Command Feasibility", "year": 2022}, {"arxiv_id": "2112.05692", "title": "VUT: Versatile UI Transformer for Multi-Modal Multi-Task User Interface Modeling", "year": 2021}, {"arxiv_id": "2209.14927", "title": "Spotlight: Mobile UI Understanding using Vision-Language Models with a Focus", "year": 2022}], "gt_label": [1, 1, 1, 1], "date": "2022-09", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_74", "valid": true}
{"query": "Any studies that introduced a special [LENGTH] token to the encoder for response length prediction?", "cited_paper": [{"arxiv_id": "1904.09324", "title": "Mask-Predict: Parallel Decoding of Conditional Masked Language Models", "year": 2019}, {"arxiv_id": "1810.04805", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "year": 2018}], "gt_label": [1, 1], "date": "2019-04", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_75", "valid": true}
{"query": "Who used the spawning method to explore the connection between LMC and the Neural Tangent Kernel dynamics?", "cited_paper": [{"arxiv_id": "2010.15110", "title": "Deep learning versus kernel learning: an empirical study of loss landscape geometry and the time evolution of the Neural Tangent Kernel", "year": 2020}], "gt_label": [1], "date": "2020-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_76", "valid": true}
{"query": "Which papers researched on few-shot learning in drug discovery?", "cited_paper": [{"arxiv_id": "2102.07916", "title": "Few-Shot Graph Learning for Molecular Property Prediction", "year": 2021}, {"arxiv_id": "2107.07994", "title": "Property-Aware Relation Networks for Few-Shot Molecular Property Prediction", "year": 2021}], "gt_label": [1, 1], "date": "2021-07", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_77", "valid": true}
{"query": "Which papers investigated the use of diffusion models in specific domains such as images?", "cited_paper": [{"arxiv_id": "2105.05233", "title": "Diffusion Models Beat GANs on Image Synthesis", "year": 2021}, {"arxiv_id": "2102.09672", "title": "Improved Denoising Diffusion Probabilistic Models", "year": 2021}, {"arxiv_id": "2205.11487", "title": "Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding", "year": 2022}, {"arxiv_id": "2204.06125", "title": "Hierarchical Text-Conditional Image Generation with CLIP Latents", "year": 2022}, {"arxiv_id": "2112.10752", "title": "High-Resolution Image Synthesis with Latent Diffusion Models", "year": 2021}], "gt_label": [1, 1, 1, 1, 1], "date": "2022-05", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_78", "valid": true}
{"query": "What are the studies that focus on quadratic reward functions?", "cited_paper": [{"arxiv_id": "1902.03035", "title": "Bandit Principal Component Analysis", "year": 2019}, {"arxiv_id": "2106.01660", "title": "Bandit Phase Retrieval", "year": 2021}, {"arxiv_id": "2006.02948", "title": "Low-Rank Generalized Linear Bandit Problems", "year": 2020}, {"arxiv_id": "1606.05693", "title": "Structured Stochastic Linear Bandits", "year": 2016}, {"arxiv_id": "1609.01508", "title": "Low-rank Bandits with Latent Mixtures", "year": 2016}, {"arxiv_id": "1901.09490", "title": "Stochastic Linear Bandits with Hidden Low Rank Structure", "year": 2019}], "gt_label": [1, 1, 1, 1, 1, 1], "date": "2021-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_79", "valid": true}
{"query": "What works developed contrastive learning for OOD detection?", "cited_paper": [{"arxiv_id": "2007.09070", "title": "Hybrid Discriminative-Generative Training via Contrastive Learning", "year": 2020}, {"arxiv_id": "2007.05566", "title": "Contrastive Training for Improved Out-of-Distribution Detection", "year": 2020}], "gt_label": [1, 1], "date": "2022-04", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_80", "valid": true}
{"query": "What work tried to estimate the global translation by marrying a supporting-foot-based method with an RNN-based root translation regression model?", "cited_paper": [{"arxiv_id": "2105.04605", "title": "TransPose: Real-time 3D Human Translation and Pose Estimation with Six Inertial Sensors", "year": 2021}], "gt_label": [1], "date": "2021-05", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_81", "valid": true}
{"query": "Could you provide me some works related to policy-gradient methods?", "cited_paper": [{"arxiv_id": "1806.03836", "title": "Bayesian Model-Agnostic Meta-Learning", "year": 2018}, {"arxiv_id": "1703.03400", "title": "Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks", "year": 2017}, {"arxiv_id": "1910.13616", "title": "Multimodal Model-Agnostic Meta-Learning via Task-Aware Modulation", "year": 2019}, {"arxiv_id": "1810.03642", "title": "Fast Context Adaptation via Meta-Learning", "year": 2018}], "gt_label": [1, 1, 1, 1], "date": "2019-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_82", "valid": true}
{"query": "Which paper converts behavioral cloning into a conditional energy-based modeling problem?", "cited_paper": [{"arxiv_id": "2109.00137", "title": "Implicit Behavioral Cloning", "year": 2021}], "gt_label": [1], "date": "2021-09", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_83", "valid": true}
{"query": "Which works are related to Networks with discrete Key-Value bottlenecks?", "cited_paper": [{"arxiv_id": "2207.11240", "title": "Discrete Key-Value Bottleneck", "year": 2022}], "gt_label": [1], "date": "2022-07", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_84", "valid": true}
{"query": "What research has been done on using variational inference or Latent Variable Modeling for radiance field uncertainty in NeRF?", "cited_paper": [{"arxiv_id": "2109.02123", "title": "Stochastic Neural Radiance Fields: Quantifying Uncertainty in Implicit 3D Representations", "year": 2021}, {"arxiv_id": "2203.10192", "title": "Conditional-Flow NeRF: Accurate 3D Modelling with Reliable Uncertainty Quantification", "year": 2022}], "gt_label": [1, 1], "date": "2022-03", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_85", "valid": true}
{"query": "In what works were approaches proposed to mitigate the computational burden of NeRF training?", "cited_paper": [{"arxiv_id": "2111.11215", "title": "Direct Voxel Grid Optimization: Super-fast Convergence for Radiance Fields Reconstruction", "year": 2021}, {"arxiv_id": "2206.05085", "title": "Improved Direct Voxel Grid Optimization for Radiance Fields Reconstruction", "year": 2022}, {"arxiv_id": "2112.05131", "title": "Plenoxels: Radiance Fields without Neural Networks", "year": 2021}, {"arxiv_id": "2201.05989", "title": "Instant Neural Graphics Primitives with a Multiresolution Hash Encoding", "year": 2022}], "gt_label": [1, 1, 1, 1], "date": "2022-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_86", "valid": true}
{"query": "Can you list the references where they have classified the method of knowledge updating and model editing for language-literature models?", "cited_paper": [{"arxiv_id": "2305.13172", "title": "Editing Large Language Models: Problems, Methods, and Opportunities", "year": 2023}, {"arxiv_id": "2310.16218", "title": "Knowledge Editing for Large Language Models: A Survey", "year": 2023}], "gt_label": [1, 1], "date": "2023-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_87", "valid": true}
{"query": "Can you provide me some works where LLMs are utilized to perform summarization with the fixed aspects provided by humans?", "cited_paper": [{"arxiv_id": "2209.12356", "title": "News Summarization and Evaluation in the Era of GPT-3", "year": 2022}, {"arxiv_id": "2302.08081", "title": "Exploring the Limits of ChatGPT for Query or Aspect-based Text Summarization", "year": 2023}], "gt_label": [1, 1], "date": "2023-02", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_88", "valid": true}
{"query": "Can you name the papers that have discussed strategies to counteract the high variance issue of LR gradient?", "cited_paper": [{"arxiv_id": "1506.02438", "title": "High-Dimensional Continuous Control Using Generalized Advantage Estimation", "year": 2015}], "gt_label": [1], "date": "2015-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_89", "valid": true}
{"query": "Are there any studies employing GPT-3 Codex in training-free neural-symbolic framework?", "cited_paper": [{"arxiv_id": "2210.02875", "title": "Binding Language Models in Symbolic Languages", "year": 2022}, {"arxiv_id": "2107.03374", "title": "Evaluating Large Language Models Trained on Code", "year": 2021}], "gt_label": [1, 1], "date": "2022-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_90", "valid": true}
{"query": "What are some studies that have adapted the representation of Gaussian splatting for text-to-3D generation?", "cited_paper": [{"arxiv_id": "2309.16653", "title": "DreamGaussian: Generative Gaussian Splatting for Efficient 3D Content Creation", "year": 2023}, {"arxiv_id": "2309.16585", "title": "Text-to-3D using Gaussian Splatting", "year": 2023}], "gt_label": [1, 1], "date": "2023-09", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_91", "valid": true}
{"query": "What studies made advances in the photorealistic synthesis of images conditioned on text prompts?", "cited_paper": [{"arxiv_id": "2105.05233", "title": "Diffusion Models Beat GANs on Image Synthesis", "year": 2021}, {"arxiv_id": "2205.11487", "title": "Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding", "year": 2022}, {"arxiv_id": "2204.06125", "title": "Hierarchical Text-Conditional Image Generation with CLIP Latents", "year": 2022}, {"arxiv_id": "2112.10752", "title": "High-Resolution Image Synthesis with Latent Diffusion Models", "year": 2021}, {"arxiv_id": "2202.04200", "title": "MaskGIT: Masked Generative Image Transformer", "year": 2022}, {"arxiv_id": "2301.00704", "title": "Muse: Text-To-Image Generation via Masked Generative Transformers", "year": 2023}], "gt_label": [1, 1, 1, 1, 1, 1], "date": "2023-01", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_92", "valid": true}
{"query": "What papers involve prefix techniques used in LLMs?", "cited_paper": [{"arxiv_id": "2101.00190", "title": "Prefix-Tuning: Optimizing Continuous Prompts for Generation", "year": 2021}, {"arxiv_id": "2110.07602", "title": "P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks", "year": 2021}], "gt_label": [1, 1], "date": "2021-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_93", "valid": true}
{"query": "What papers analyse stochastic optimization methods under the arbitrary sampling paradigm?", "cited_paper": [{"arxiv_id": "1310.3438", "title": "On Optimal Probabilities in Stochastic Coordinate Descent Methods", "year": 2013}, {"arxiv_id": "1412.8060", "title": "Coordinate Descent with Arbitrary Sampling I: Algorithms and Complexity", "year": 2014}, {"arxiv_id": "1706.01108", "title": "Stochastic Reformulations of Linear Systems: Algorithms and Convergence Theory", "year": 2017}, {"arxiv_id": "1712.09677", "title": "Momentum and Stochastic Momentum for Stochastic Gradient, Newton, Proximal Point and Subspace Descent Methods", "year": 2017}, {"arxiv_id": "1903.07971", "title": "Convergence Analysis of Inexact Randomized Iterative Methods", "year": 2019}, {"arxiv_id": "1610.04714", "title": "A New Perspective on Randomized Gossip Algorithms", "year": 2016}, {"arxiv_id": "1905.08645", "title": "Revisiting Randomized Gossip Algorithms: General Framework, Convergence Rates and Novel Block and Accelerated Protocols", "year": 2019}, {"arxiv_id": "1901.09401", "title": "SGD: General Analysis and Improved Rates", "year": 2019}, {"arxiv_id": "1901.08669", "title": "SAGA with Arbitrary Sampling", "year": 2019}, {"arxiv_id": "2007.04202", "title": "Stochastic Hamiltonian Gradient Methods for Smooth Games", "year": 2020}, {"arxiv_id": "2107.00052", "title": "Stochastic Gradient Descent-Ascent and Consensus Optimization for Smooth Games: Convergence Analysis under Expected Co-coercivity", "year": 2021}], "gt_label": [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], "date": "2021-11", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_94", "valid": true}
{"query": "What papers used Hermite expansion to study the Neural Tangent Kernel?", "cited_paper": [{"arxiv_id": "1602.05897", "title": "Toward Deeper Understanding of Neural Networks: The Power of Initialization and a Dual View on Expressivity", "year": 2016}, {"arxiv_id": "2106.03186", "title": "Reverse Engineering the Neural Tangent Kernel", "year": 2021}, {"arxiv_id": "2002.07867", "title": "Global Convergence of Deep Networks with One Wide Layer Followed by Pyramidal Topology", "year": 2020}, {"arxiv_id": "1908.05660", "title": "Effect of Activation Functions on the Training of Overparametrized Neural Nets", "year": 2019}, {"arxiv_id": "2209.04121", "title": "Fast Neural Kernel Embeddings for General Activations", "year": 2022}], "gt_label": [1, 1, 1, 1, 1], "date": "2022-09", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_95", "valid": true}
{"query": "What works defined the settings of the DDPM model based on the continuous limit of t?", "cited_paper": [{"arxiv_id": "2006.11239", "title": "Denoising Diffusion Probabilistic Models", "year": 2020}], "gt_label": [1], "date": "2020-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_96", "valid": true}
{"query": "Which papers established improvements over the no-regret framework when specific learning dynamics are in place?", "cited_paper": [{"arxiv_id": "1507.00407", "title": "Fast Convergence of Regularized Learning in Games", "year": 2015}, {"arxiv_id": "2108.06924", "title": "Near-Optimal No-Regret Learning in General Games", "year": 2021}], "gt_label": [1, 1], "date": "2021-08", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_97", "valid": true}
{"query": "What works researched the effect of adversarial perturbations on image-to-image tasks?", "cited_paper": [{"arxiv_id": "2104.15022", "title": "Deep Image Destruction: Vulnerability of Deep Image-to-Image Models against Adversarial Attacks", "year": 2021}, {"arxiv_id": "2201.04397", "title": "Towards Adversarially Robust Deep Image Denoising", "year": 2022}, {"arxiv_id": "1904.06097", "title": "Evaluating Robustness of Deep Image Super-Resolution against Adversarial Attacks", "year": 2019}], "gt_label": [1, 1, 1], "date": "2022-01", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_98", "valid": true}
{"query": "What works explore the vulnerability of models to inversion attacks causing the leakage of private information?", "cited_paper": [{"arxiv_id": "1610.05820", "title": "Membership Inference Attacks against Machine Learning Models", "year": 2016}, {"arxiv_id": "1812.00910", "title": "Comprehensive Privacy Analysis of Deep Learning: Passive and Active White-box Inference Attacks against Centralized and Federated Learning", "year": 2018}], "gt_label": [1, 1], "date": "2018-12", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_99", "valid": true}
{"query": "What papers provided strategies to enable image generation conditioned on text and other modalities in guided diffusion models?", "cited_paper": [{"arxiv_id": "2207.12598", "title": "Classifier-Free Diffusion Guidance", "year": 2022}, {"arxiv_id": "2110.02711", "title": "DiffusionCLIP: Text-Guided Diffusion Models for Robust Image Manipulation", "year": 2021}, {"arxiv_id": "2302.07121", "title": "Universal Guidance for Diffusion Models", "year": 2023}, {"arxiv_id": "2305.13050", "title": "AudioToken: Adaptation of Text-Conditioned Diffusion Models for Audio-to-Image Generation", "year": 2023}, {"arxiv_id": "2309.04509", "title": "The Power of Sound (TPoS): Audio Reactive Video Generation with Stable Diffusion", "year": 2023}, {"arxiv_id": "2102.09672", "title": "Improved Denoising Diffusion Probabilistic Models", "year": 2021}, {"arxiv_id": "2302.07121", "title": "Universal Guidance for Diffusion Models", "year": 2023}, {"arxiv_id": "2112.10752", "title": "High-Resolution Image Synthesis with Latent Diffusion Models", "year": 2021}], "gt_label": [1, 1, 1, 1, 1, 1, 1, 1], "date": "2023-09", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_100", "valid": true}
{"query": "What are the studies that designed sample-efficient RL algorithms with general function approximations in static RL setting?", "cited_paper": [{"arxiv_id": "1406.1853", "title": "Model-based Reinforcement Learning and the Eluder Dimension", "year": 2014}, {"arxiv_id": "2005.10804", "title": "Reinforcement Learning with General Value Function Approximation: Provably Efficient Approach via Bounded Eluder Dimension", "year": 2020}], "gt_label": [1, 1], "date": "2020-05", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_101", "valid": true}
{"query": "What work uses keyword-based retrieval (BM25) for semi-supervised learning?", "cited_paper": [{"arxiv_id": "2111.04130", "title": "NLP From Scratch Without Large-Scale Pretraining: A Simple and Efficient Framework", "year": 2021}], "gt_label": [1], "date": "2021-11", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_102", "valid": true}
{"query": "In what work do the researchers use a Python interpreter to make the prediction of an LLM more likely to be faithful?", "cited_paper": [{"arxiv_id": "2301.13379", "title": "Faithful Chain-of-Thought Reasoning", "year": 2023}], "gt_label": [1], "date": "2023-01", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_103", "valid": true}
{"query": "What research papers have presented algorithms that can control the dynamic regret in non-stationary online learning?", "cited_paper": [{"arxiv_id": "1501.06225", "title": "Online Optimization : Competing with Dynamic Comparators", "year": 2015}, {"arxiv_id": "1307.5449", "title": "Non-stationary Stochastic Optimization", "year": 2013}, {"arxiv_id": "1605.04638", "title": "Tracking Slowly Moving Clairvoyant: Optimal Dynamic Regret of Online Learning with True and Noisy Gradient", "year": 2016}, {"arxiv_id": "1810.10815", "title": "Adaptive Online Learning in Dynamic Environments", "year": 2018}, {"arxiv_id": "2007.03479", "title": "Dynamic Regret of Convex and Smooth Functions", "year": 2020}, {"arxiv_id": "2006.05876", "title": "Improved Analysis for Dynamic Regret of Strongly Convex and Smooth Functions", "year": 2020}, {"arxiv_id": "2102.03758", "title": "Non-stationary Online Learning with Memory and Non-stochastic Control", "year": 2021}, {"arxiv_id": "2203.00444", "title": "Parameter-free Mirror Descent", "year": 2022}], "gt_label": [1, 1, 1, 1, 1, 1, 1, 1], "date": "2022-05", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_104", "valid": true}
{"query": "What papers improved convergence rate for soft policies by analyzing NAC under Markovian sampling?", "cited_paper": [{"arxiv_id": "2004.12956", "title": "Improving Sample Complexity Bounds for (Natural) Actor-Critic Algorithms", "year": 2020}], "gt_label": [1], "date": "2020-04", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_105", "valid": true}
{"query": "What research fine-tunes subset layers of cross-attention in the UNet for personalized visual content generation?", "cited_paper": [{"arxiv_id": "2212.04488", "title": "Multi-Concept Customization of Text-to-Image Diffusion", "year": 2022}], "gt_label": [1], "date": "2022-12", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_106", "valid": true}
{"query": "What studies have achieved strong results in text conditioned image generation through Diffusion generative models?", "cited_paper": [{"arxiv_id": "2112.10752", "title": "High-Resolution Image Synthesis with Latent Diffusion Models", "year": 2021}, {"arxiv_id": "2102.12092", "title": "Zero-Shot Text-to-Image Generation", "year": 2021}, {"arxiv_id": "2112.10741", "title": "GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models", "year": 2021}, {"arxiv_id": "2208.12242", "title": "DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation", "year": 2022}], "gt_label": [1, 1, 1, 1], "date": "2022-08", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_107", "valid": true}
{"query": "Which references assert that even for graphs such as trees or paths, computing subgraph counts is NP-hard?", "cited_paper": [{"arxiv_id": "1307.2187", "title": "Everything you always wanted to know about the parameterized complexity of Subgraph Isomorphism (but were afraid to ask)", "year": 2013}], "gt_label": [1], "date": "2013-07", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_108", "valid": true}
{"query": "Which paper introduced a hierarchical transformer structure to progressively shrink the spatiotemporal resolution of feature maps and increase channels in the case of action recognition?", "cited_paper": [{"arxiv_id": "2104.11227", "title": "Multiscale Vision Transformers", "year": 2021}], "gt_label": [1], "date": "2021-04", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_109", "valid": true}
{"query": "Which papers introduced LLMs such as OPT, LLaMA, BLOOM and PaLM?", "cited_paper": [{"arxiv_id": "2205.01068", "title": "OPT: Open Pre-trained Transformer Language Models", "year": 2022}, {"arxiv_id": "2302.13971", "title": "LLaMA: Open and Efficient Foundation Language Models", "year": 2023}, {"arxiv_id": "2307.09288", "title": "Llama 2: Open Foundation and Fine-Tuned Chat Models", "year": 2023}, {"arxiv_id": "2211.05100", "title": "BLOOM: A 176B-Parameter Open-Access Multilingual Language Model", "year": 2022}, {"arxiv_id": "2204.02311", "title": "PaLM: Scaling Language Modeling with Pathways", "year": 2022}, {"arxiv_id": "2305.10403", "title": "PaLM 2 Technical Report", "year": 2023}], "gt_label": [1, 1, 1, 1, 1, 1], "date": "2023-07", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_110", "valid": true}
{"query": "Which works used approaches like particle-based graph neural network dynamics predictors?", "cited_paper": [{"arxiv_id": "1612.00222", "title": "Interaction Networks for Learning about Objects, Relations and Physics", "year": 2016}, {"arxiv_id": "1806.01261", "title": "Relational inductive biases, deep learning, and graph networks", "year": 2018}, {"arxiv_id": "1810.01566", "title": "Learning Particle Dynamics for Manipulating Rigid Bodies, Deformable Objects, and Fluids", "year": 2018}], "gt_label": [1, 1, 1], "date": "2020-02", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_111", "valid": true}
{"query": "Could you list some works that improved performance using Auxiliary Learning methods?", "cited_paper": [{"arxiv_id": "1803.03642", "title": "Deep Auxiliary Learning for Visual Localization and Odometry", "year": 2018}, {"arxiv_id": "1804.08366", "title": "VLocNet++: Deep Multitask Learning for Semantic Visual Localization and Odometry", "year": 2018}, {"arxiv_id": "1812.07869", "title": "Deep Global-Relative Networks for End-to-End 6-DoF Visual Localization and Odometry", "year": 2018}], "gt_label": [1, 1, 1], "date": "2018-12", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_112", "valid": true}
{"query": "Which works improved the efficiency of TRPO using an ensemble of environment models?", "cited_paper": [{"arxiv_id": "1802.10592", "title": "Model-Ensemble Trust-Region Policy Optimization", "year": 2018}], "gt_label": [1], "date": "2018-02", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_113", "valid": true}
{"query": "What works developed a unified framework for estimating expected information gain and optimizing designs with gradient-based methods in the field of Differentiable Bayesian Optimal Experimental Design?", "cited_paper": [{"arxiv_id": "2002.08129", "title": "Bayesian Experimental Design for Implicit Models by Mutual Information Neural Estimation", "year": 2020}, {"arxiv_id": "2105.04379", "title": "Gradient-based Bayesian Experimental Design for Implicit Models using Mutual Information Lower Bounds", "year": 2021}], "gt_label": [1, 1], "date": "2021-05", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_114", "valid": true}
{"query": "Which studies have utilized dual pathways in Birds Eye View(BEV) detection?", "cited_paper": [{"arxiv_id": "2109.01134", "title": "Learning to Prompt for Vision-Language Models", "year": 2021}, {"arxiv_id": "2111.03930", "title": "Tip-Adapter: Training-free CLIP-Adapter for Better Vision-Language Modeling", "year": 2021}, {"arxiv_id": "2203.05557", "title": "Conditional Prompt Learning for Vision-Language Models", "year": 2022}, {"arxiv_id": "2109.08472", "title": "ActionCLIP: A New Paradigm for Video Action Recognition", "year": 2021}, {"arxiv_id": "2109.01903", "title": "Robust fine-tuning of zero-shot models", "year": 2021}], "gt_label": [1, 1, 1, 1, 1], "date": "2022-03", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_115", "valid": true}
{"query": "Which studies showed that original optimization based attacks still work to some extent if very small learning rates are used?", "cited_paper": [{"arxiv_id": "2003.14053", "title": "Inverting Gradients -- How easy is it to break privacy in federated learning?", "year": 2020}, {"arxiv_id": "2204.13784", "title": "AGIC: Approximate Gradient Inversion Attack on Federated Learning", "year": 2022}], "gt_label": [1, 1], "date": "2022-04", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_116", "valid": true}
{"query": "What research has been conducted on the convergence of PFL with respect to system heterogeneity?", "cited_paper": [{"arxiv_id": "2007.07481", "title": "Tackling the Objective Inconsistency Problem in Heterogeneous Federated Optimization", "year": 2020}], "gt_label": [1], "date": "2020-07", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_117", "valid": true}
{"query": "What research has been done on the usage of Transformer for low-level vision tasks?", "cited_paper": [{"arxiv_id": "2006.04139", "title": "Learning Texture Transformer Network for Image Super-Resolution", "year": 2020}, {"arxiv_id": "2012.00364", "title": "Pre-Trained Image Processing Transformer", "year": 2020}, {"arxiv_id": "2108.10257", "title": "SwinIR: Image Restoration Using Swin Transformer", "year": 2021}, {"arxiv_id": "2111.09881", "title": "Restormer: Efficient Transformer for High-Resolution Image Restoration", "year": 2021}, {"arxiv_id": "2106.03106", "title": "Uformer: A General U-Shaped Transformer for Image Restoration", "year": 2021}], "gt_label": [1, 1, 1, 1, 1], "date": "2021-11", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_118", "valid": true}
{"query": "What research has been done to establish lower bounds for networks with monotone activation functions or its variants?", "cited_paper": [{"arxiv_id": "1810.00393", "title": "Deep, Skinny Neural Networks are not Universal Approximators", "year": 2018}], "gt_label": [1], "date": "2018-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_119", "valid": true}
{"query": "What studies started the research line in designing appropriate prompts for large language models?", "cited_paper": [{"arxiv_id": "2101.06804", "title": "What Makes Good In-Context Examples for GPT-$3$?", "year": 2021}, {"arxiv_id": "2110.08207", "title": "Multitask Prompted Training Enables Zero-Shot Task Generalization", "year": 2021}], "gt_label": [1, 1], "date": "2021-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_120", "valid": true}
{"query": "What are the research papers that contribute to Diffusion Models?", "cited_paper": [{"arxiv_id": "2006.11239", "title": "Denoising Diffusion Probabilistic Models", "year": 2020}, {"arxiv_id": "2105.05233", "title": "Diffusion Models Beat GANs on Image Synthesis", "year": 2021}, {"arxiv_id": "2102.09672", "title": "Improved Denoising Diffusion Probabilistic Models", "year": 2021}, {"arxiv_id": "2211.13287", "title": "HouseDiffusion: Vector Floorplan Generation via a Diffusion Model with Discrete and Continuous Denoising", "year": 2022}, {"arxiv_id": "2112.10741", "title": "GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models", "year": 2021}, {"arxiv_id": "2207.06635", "title": "EGSDE: Unpaired Image-to-Image Translation via Energy-Guided Stochastic Differential Equations", "year": 2022}, {"arxiv_id": "2204.06125", "title": "Hierarchical Text-Conditional Image Generation with CLIP Latents", "year": 2022}, {"arxiv_id": "2112.10752", "title": "High-Resolution Image Synthesis with Latent Diffusion Models", "year": 2021}, {"arxiv_id": "2104.07636", "title": "Image Super-Resolution via Iterative Refinement", "year": 2021}, {"arxiv_id": "2104.14951", "title": "SRDiff: Single Image Super-Resolution with Diffusion Probabilistic Models", "year": 2021}, {"arxiv_id": "2206.02779", "title": "Blended Latent Diffusion", "year": 2022}, {"arxiv_id": "2112.03126", "title": "Label-Efficient Semantic Segmentation with Diffusion Models", "year": 2021}, {"arxiv_id": "2303.11579", "title": "Diffusion-Based 3D Human Pose Estimation with Multi-Hypothesis Aggregation", "year": 2023}, {"arxiv_id": "2301.09629", "title": "LEGO-Net: Learning Regular Rearrangements of Objects in Rooms", "year": 2023}], "gt_label": [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], "date": "2023-03", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_121", "valid": true}
{"query": "What papers have implemented Generative Adversarial Imitation Learning (GAIL) for imitation learning from observations?", "cited_paper": [{"arxiv_id": "1606.03476", "title": "Generative Adversarial Imitation Learning", "year": 2016}, {"arxiv_id": "1807.06158", "title": "Generative Adversarial Imitation from Observation", "year": 2018}, {"arxiv_id": "2206.11693", "title": "Learning Agile Skills via Adversarial Imitation of Rough Partial Demonstrations", "year": 2022}], "gt_label": [1, 1, 1], "date": "2022-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_122", "valid": true}
{"query": "What are some papers that used OpenWebText for pretraining of models?", "cited_paper": [{"arxiv_id": "1907.11692", "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach", "year": 2019}, {"arxiv_id": "1909.08053", "title": "Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism", "year": 2019}], "gt_label": [1, 1], "date": "2019-09", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_123", "valid": true}
{"query": "Which studies released LexFiles, an English legal corpus, and trained two new legal English PLMs using this corpus?", "cited_paper": [{"arxiv_id": "2305.07507", "title": "LeXFiles and LegalLAMA: Facilitating English Multinational Legal Language Model Development", "year": 2023}], "gt_label": [1], "date": "2023-05", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_124", "valid": true}
{"query": "Which works suggested synthesizing video sequences by extending the generated image tensors along a time dimension?", "cited_paper": [{"arxiv_id": "2210.02303", "title": "Imagen Video: High Definition Video Generation with Diffusion Models", "year": 2022}, {"arxiv_id": "2112.14683", "title": "StyleGAN-V: A Continuous Video Generator with the Price, Image Quality and Perks of StyleGAN2", "year": 2021}, {"arxiv_id": "2304.08818", "title": "Align your Latents: High-Resolution Video Synthesis with Latent Diffusion Models", "year": 2023}, {"arxiv_id": "2302.07685", "title": "Video Probabilistic Diffusion Models in Projected Latent Space", "year": 2023}, {"arxiv_id": "2211.11018", "title": "MagicVideo: Efficient Video Generation With Latent Diffusion Models", "year": 2022}, {"arxiv_id": "2303.08320", "title": "VideoFusion: Decomposed Diffusion Models for High-Quality Video Generation", "year": 2023}, {"arxiv_id": "2206.03429", "title": "Generating Long Videos of Dynamic Scenes", "year": 2022}], "gt_label": [1, 1, 1, 1, 1, 1, 1], "date": "2023-04", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_125", "valid": true}
{"query": "What references proposed the concept of a hypernetwork?", "cited_paper": [], "gt_label": [], "date": "2016-09", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_126", "valid": true}
{"query": "What are the notable contributions in the field of text-infilling for generative data augmentation?", "cited_paper": [{"arxiv_id": "2108.13655", "title": "MELM: Data Augmentation with Masked Entity Language Modeling for Low-Resource NER", "year": 2021}, {"arxiv_id": "2211.10330", "title": "GENIUS: Sketch-based Language Model Pre-training via Extreme and Selective Masking for Text Generation and Augmentation", "year": 2022}, {"arxiv_id": "2306.00928", "title": "ACLM: A Selective-Denoising based Generative Data Augmentation Approach for Low-Resource Complex NER", "year": 2023}, {"arxiv_id": "2310.15799", "title": "DALE: Generative Data Augmentation for Low-Resource Legal NLP", "year": 2023}, {"arxiv_id": "2305.10647", "title": "BioAug: Conditional Generation based Data Augmentation for Low-Resource Biomedical NER", "year": 2023}], "gt_label": [1, 1, 1, 1, 1], "date": "2023-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_127", "valid": true}
{"query": "Which research papers observed linear mode connectivity (LMC) in models trained on MNIST starting from the same random initialization?", "cited_paper": [{"arxiv_id": "1902.04742", "title": "Uniform convergence may be unable to explain generalization in deep learning", "year": 2019}], "gt_label": [1], "date": "2019-02", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_128", "valid": true}
{"query": "Which papers discuss the exact methods for multi-objective combinatorial optimization (MOCO)?", "cited_paper": [{"arxiv_id": "1802.08637", "title": "Network Models for Multiobjective Discrete Optimization", "year": 2018}], "gt_label": [1], "date": "2018-02", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_129", "valid": true}
{"query": "Which works propose the use of learnable prompts at the CLIP text input for fine-tuning on few-shot examples?", "cited_paper": [{"arxiv_id": "2109.01134", "title": "Learning to Prompt for Vision-Language Models", "year": 2021}], "gt_label": [1], "date": "2021-09", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_130", "valid": true}
{"query": "What research employ multi-modal models in the field of object detection?", "cited_paper": [{"arxiv_id": "2104.09224", "title": "Multi-Modal Fusion Transformer for End-to-End Autonomous Driving", "year": 2021}], "gt_label": [1], "date": "2021-04", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_131", "valid": true}
{"query": "What research used a strategy that incorporates a prior probability derived from local color variance and further tracks photometric error throughout training with an adaptive quadtree structure?", "cited_paper": [{"arxiv_id": "2208.06821", "title": "Fast Learning Radiance Fields by Shooting Much Fewer Rays", "year": 2022}], "gt_label": [1], "date": "2022-08", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_132", "valid": true}
{"query": "Which paper proposed the idea of sharing all layers within a transformer model?", "cited_paper": [{"arxiv_id": "1807.03819", "title": "Universal Transformers", "year": 2018}], "gt_label": [1], "date": "2018-07", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_133", "valid": true}
{"query": "What works have emerged in the field of compression with INRs, and have been effective in compressing various data like images, climate data, videos and 3D scenes?", "cited_paper": [{"arxiv_id": "2103.03123", "title": "COIN: COmpression with Implicit Neural representations", "year": 2021}, {"arxiv_id": "2201.12904", "title": "COIN++: Neural Compression Across Modalities", "year": 2022}, {"arxiv_id": "2110.13903", "title": "NeRV: Neural Representations for Videos", "year": 2021}, {"arxiv_id": "2104.12456", "title": "3D Scene Compression through Entropy Penalized Neural Representation Functions", "year": 2021}], "gt_label": [1, 1, 1, 1], "date": "2022-01", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_134", "valid": true}
{"query": "What research prioritizes a sample if it significantly improves the probability of correctly predicting the true label?", "cited_paper": [{"arxiv_id": "2008.03703", "title": "What Neural Networks Memorize and Why: Discovering the Long Tail via Influence Estimation", "year": 2020}], "gt_label": [1], "date": "2020-08", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_135", "valid": true}
{"query": "What works discuss the challenge of hallucinations in Large Language Models?", "cited_paper": [{"arxiv_id": "2309.01219", "title": "Siren's Song in the AI Ocean: A Survey on Hallucination in Large Language Models", "year": 2023}, {"arxiv_id": "2305.11747", "title": "HaluEval: A Large-Scale Hallucination Evaluation Benchmark for Large Language Models", "year": 2023}, {"arxiv_id": "2311.08648", "title": "Explore Spurious Correlations at the Concept Level in Language Models for Text Classification", "year": 2023}], "gt_label": [1, 1, 1], "date": "2024-07", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_136", "valid": true}
{"query": "What works have studied and utilized the high-dimensional visual features in a diffusion model?", "cited_paper": [{"arxiv_id": "2303.16203", "title": "Your Diffusion Model is Secretly a Zero-Shot Classifier", "year": 2023}, {"arxiv_id": "2112.00390", "title": "SegDiff: Image Segmentation with Diffusion Probabilistic Models", "year": 2021}, {"arxiv_id": "2112.03126", "title": "Label-Efficient Semantic Segmentation with Diffusion Models", "year": 2021}, {"arxiv_id": "2305.15581", "title": "Unsupervised Semantic Correspondence Using Stable Diffusion", "year": 2023}, {"arxiv_id": "2305.15347", "title": "A Tale of Two Features: Stable Diffusion Complements DINO for Zero-Shot Semantic Correspondence", "year": 2023}, {"arxiv_id": "2303.04803", "title": "Open-Vocabulary Panoptic Segmentation with Text-to-Image Diffusion Models", "year": 2023}], "gt_label": [1, 1, 1, 1, 1, 1], "date": "2023-05", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_137", "valid": true}
{"query": "Could you provide some works discussing the inference cost as a drawback of Seq2Seq models in GEC?", "cited_paper": [{"arxiv_id": "2106.04970", "title": "Instantaneous Grammatical Error Correction with Shallow Aggressive Decoding", "year": 2021}], "gt_label": [1], "date": "2021-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_138", "valid": true}
{"query": "What are some examples of research that explore improvement of communication efficiency in Federated Learning through methods based on gradient compression?", "cited_paper": [{"arxiv_id": "1610.02132", "title": "QSGD: Communication-Efficient SGD via Gradient Quantization and Encoding", "year": 2016}, {"arxiv_id": "2109.12519", "title": "AsySQN: Faster Vertical Federated Learning Algorithms with Better Computation Resource Utilization", "year": 2021}], "gt_label": [1, 1], "date": "2021-09", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_139", "valid": true}
{"query": "Are there any studies that work on shuffling-based methods in Federated Learning?", "cited_paper": [{"arxiv_id": "2110.10342", "title": "Minibatch vs Local SGD with Shuffling: Tight Convergence Bounds and Beyond", "year": 2021}], "gt_label": [1], "date": "2023-02", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_140", "valid": true}
{"query": "What research has been done on the effect of persona variables on hate speech detection?", "cited_paper": [{"arxiv_id": "2106.04511", "title": "Designing Toxic Content Classification for a Diversity of Perspectives", "year": 2021}, {"arxiv_id": "2111.07997", "title": "Annotators with Attitudes: How Annotator Beliefs And Identities Bias Toxic Language Detection", "year": 2021}, {"arxiv_id": "2306.06826", "title": "When Do Annotator Demographics Matter? Measuring the Influence of Annotator Demographics with the POPQUORN Dataset", "year": 2023}, {"arxiv_id": "2306.01943", "title": "NLPositionality: Characterizing Design Biases of Datasets and Models", "year": 2023}, {"arxiv_id": "2309.01288", "title": "How Crowd Worker Factors Influence Subjective Annotations: A Study of Tagging Misogynistic Hate Speech in Tweets", "year": 2023}], "gt_label": [1, 1, 1, 1, 1], "date": "2023-09", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_141", "valid": true}
{"query": "Which studies used perturbation techniques similar to ours for measuring contamination in the test questions?", "cited_paper": [{"arxiv_id": "2012.07805", "title": "Extracting Training Data from Large Language Models", "year": 2020}, {"arxiv_id": "1711.09050", "title": "Ethical Challenges in Data-Driven Dialogue Systems", "year": 2017}, {"arxiv_id": "2401.06059", "title": "Investigating Data Contamination for Pre-training Language Models", "year": 2024}, {"arxiv_id": "2006.07490", "title": "Understanding Unintended Memorization in Federated Learning", "year": 2020}], "gt_label": [1, 1, 1, 1], "date": "2024-01", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_142", "valid": true}
{"query": "Which paper ensured diversity sampling by selecting core sets in LiDAR point clouds?", "cited_paper": [{"arxiv_id": "1708.00489", "title": "Active Learning for Convolutional Neural Networks: A Core-Set Approach", "year": 2017}], "gt_label": [1], "date": "2017-08", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_143", "valid": true}
{"query": "What are some works that use sparse voxels for 3D scene understanding?", "cited_paper": [{"arxiv_id": "1711.10275", "title": "3D Semantic Segmentation with Submanifold Sparse Convolutional Networks", "year": 2017}, {"arxiv_id": "1904.08755", "title": "4D Spatio-Temporal ConvNets: Minkowski Convolutional Neural Networks", "year": 2019}], "gt_label": [1, 1], "date": "2019-04", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_144", "valid": true}
{"query": "Could you provide me some examples of research dealing with unauthorized data usage in diffusion models?", "cited_paper": [{"arxiv_id": "1411.1784", "title": "Conditional Generative Adversarial Nets", "year": 2014}, {"arxiv_id": "2204.06125", "title": "Hierarchical Text-Conditional Image Generation with CLIP Latents", "year": 2022}], "gt_label": [1, 1], "date": "2022-04", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_145", "valid": true}
{"query": "Which works have explored visual representations supervised by language in the context of multi-modal studies?", "cited_paper": [{"arxiv_id": "2103.00020", "title": "Learning Transferable Visual Models From Natural Language Supervision", "year": 2021}, {"arxiv_id": "2102.05918", "title": "Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision", "year": 2021}, {"arxiv_id": "2104.12836", "title": "Multimodal Contrastive Training for Visual Representation Learning", "year": 2021}, {"arxiv_id": "2205.01917", "title": "CoCa: Contrastive Captioners are Image-Text Foundation Models", "year": 2022}, {"arxiv_id": "2006.06666", "title": "VirTex: Learning Visual Representations from Textual Annotations", "year": 2020}, {"arxiv_id": "2108.10904", "title": "SimVLM: Simple Visual Language Model Pretraining with Weak Supervision", "year": 2021}, {"arxiv_id": "2204.14198", "title": "Flamingo: a Visual Language Model for Few-Shot Learning", "year": 2022}], "gt_label": [1, 1, 1, 1, 1, 1, 1], "date": "2022-05", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_146", "valid": true}
{"query": "Which paper evaluated only a single specialized meta-RL method?", "cited_paper": [{"arxiv_id": "1910.08348", "title": "VariBAD: A Very Good Method for Bayes-Adaptive Deep RL via Meta-Learning", "year": 2019}], "gt_label": [1], "date": "2019-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_147", "valid": true}
{"query": "Any works about semantic segmentation applying multi-modal models?", "cited_paper": [{"arxiv_id": "2008.11351", "title": "SNE-RoadSeg: Incorporating Surface Normal Information into Semantic Segmentation for Accurate Freespace Detection", "year": 2020}], "gt_label": [1], "date": "2020-08", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_148", "valid": true}
{"query": "Which papers demonstrated the performance of LLMs leveraging RLHF for alignment and generation?", "cited_paper": [{"arxiv_id": "2203.02155", "title": "Training language models to follow instructions with human feedback", "year": 2022}, {"arxiv_id": "2303.08774", "title": "GPT-4 Technical Report", "year": 2023}, {"arxiv_id": "2204.05862", "title": "Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback", "year": 2022}, {"arxiv_id": "1909.08593", "title": "Fine-Tuning Language Models from Human Preferences", "year": 2019}], "gt_label": [1, 1, 1, 1], "date": "2023-03", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_149", "valid": true}
{"query": "Could you provide me some works that enhance ray adjacency consistency for scene reconstruction?", "cited_paper": [{"arxiv_id": "2112.15399", "title": "InfoNeRF: Ray Entropy Minimization for Few-Shot Neural Volume Rendering", "year": 2021}], "gt_label": [1], "date": "2021-12", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_150", "valid": true}
{"query": "Are there any works on the acceleration of the generation process of AudioLM?", "cited_paper": [{"arxiv_id": "2305.09636", "title": "SoundStorm: Efficient Parallel Audio Generation", "year": 2023}], "gt_label": [1], "date": "2023-05", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_151", "valid": true}
{"query": "Are there any research papers that utilize LLMs to aid the training stage of CQR in conversational search?", "cited_paper": [{"arxiv_id": "2310.09716", "title": "Enhancing Conversational Search: Large Language Model-Aided Informative Query Rewriting", "year": 2023}], "gt_label": [1], "date": "2023-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_152", "valid": true}
{"query": "Have any research examined biasedness as a writing style?", "cited_paper": [{"arxiv_id": "1911.09709", "title": "Automatically Neutralizing Subjective Bias in Text", "year": 2019}], "gt_label": [1], "date": "2019-11", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_153", "valid": true}
{"query": "Which work does the paper refer to when discussing a special case of AMPO?", "cited_paper": [{"arxiv_id": "2201.07443", "title": "On the Convergence Rates of Policy Gradient Methods", "year": 2022}], "gt_label": [1], "date": "2022-01", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_154", "valid": true}
{"query": "What works demonstrate linear convergence of PG for the softmax tabular policy without regularization?", "cited_paper": [{"arxiv_id": "2105.06072", "title": "Leveraging Non-uniformity in First-order Non-convex Optimization", "year": 2021}], "gt_label": [1], "date": "2021-05", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_155", "valid": true}
{"query": "Which study proposed the Iterative Refinement Long Short-Term Memory approach in few-shot learning?", "cited_paper": [], "gt_label": [], "date": "2016-11", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_156", "valid": true}
{"query": "What works have attempted to accelerate the computation of Jacobian determinants in the ML objective by exploiting linear transformations with special structures?", "cited_paper": [{"arxiv_id": "1907.07945", "title": "MintNet: Building Invertible Neural Networks with Masked Convolutions", "year": 2019}, {"arxiv_id": "1901.11137", "title": "Emerging Convolutions for Generative Normalizing Flows", "year": 2019}, {"arxiv_id": "1902.04208", "title": "MaCow: Masked Convolutional Generative Flow", "year": 2019}, {"arxiv_id": "2002.12229", "title": "Woodbury Transformations for Deep Generative Flows", "year": 2020}, {"arxiv_id": "2209.13774", "title": "ButterflyFlow: Building Invertible Layers with Butterfly Matrices", "year": 2022}, {"arxiv_id": "1611.09630", "title": "Improving Variational Auto-Encoders using Householder Flow", "year": 2016}], "gt_label": [1, 1, 1, 1, 1, 1], "date": "2022-09", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_157", "valid": true}
{"query": "What studies exploit artificial techniques to increase the number of training tasks for generalization in RL, such as procedural generation, augmentations, or task interpolation?", "cited_paper": [{"arxiv_id": "1812.02341", "title": "Quantifying Generalization in Reinforcement Learning", "year": 2018}, {"arxiv_id": "1912.01588", "title": "Leveraging Procedural Generation to Benchmark Reinforcement Learning", "year": 2019}, {"arxiv_id": "2004.13649", "title": "Image Augmentation Is All You Need: Regularizing Deep Reinforcement Learning from Pixels", "year": 2020}, {"arxiv_id": "2001.09908", "title": "Rotation, Translation, and Cropping for Zero-Shot Generalization", "year": 2020}, {"arxiv_id": "1910.05396", "title": "Network Randomization: A Simple Technique for Generalization in Deep Reinforcement Learning", "year": 2019}, {"arxiv_id": "2006.12862", "title": "Automatic Data Augmentation for Generalization in Deep Reinforcement Learning", "year": 2020}, {"arxiv_id": "2106.02695", "title": "Meta-Learning with Fewer Tasks through Task Interpolation", "year": 2021}], "gt_label": [1, 1, 1, 1, 1, 1, 1], "date": "2021-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_158", "valid": true}
{"query": "Could you provide me with the research that proposed an alternative dual form of UOT, which resembles the dual form of OT with a Lagrangian regularizer?", "cited_paper": [{"arxiv_id": "2010.05862", "title": "Robust Optimal Transport with Applications in Generative Modeling and Domain Adaptation", "year": 2020}], "gt_label": [1], "date": "2020-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_159", "valid": true}
{"query": "Which research studies require training a reference model for data selection?", "cited_paper": [{"arxiv_id": "2206.07137", "title": "Prioritized Training on Points that are Learnable, Worth Learning, and Not Yet Learnt", "year": 2022}], "gt_label": [1], "date": "2022-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_160", "valid": true}
{"query": "Could you tell me some research exploring the use of LLMs in recommendation tasks?", "cited_paper": [{"arxiv_id": "2307.14225", "title": "Large Language Models are Competitive Near Cold-start Recommenders for Languageand Item-based Preferences", "year": 2023}, {"arxiv_id": "2202.07371", "title": "Personalized Prompt Learning for Explainable Recommendation", "year": 2022}], "gt_label": [1, 1], "date": "2023-07", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_161", "valid": true}
{"query": "What works are concerned with CXR-to-report generation in medical VLMs for chest radiographs?", "cited_paper": [{"arxiv_id": "2010.16056", "title": "Generating Radiology Reports via Memory-driven Transformer", "year": 2020}, {"arxiv_id": "2204.13258", "title": "Cross-modal Memory Networks for Radiology Report Generation", "year": 2022}, {"arxiv_id": "2207.04818", "title": "Cross-modal Prototype Driven Network for Radiology Report Generation", "year": 2022}, {"arxiv_id": "2305.07176", "title": "Automatic Radiology Report Generation by Learning with Increasingly Hard Negatives", "year": 2023}, {"arxiv_id": "2112.15011", "title": "Radiology Report Generation with a Learned Knowledge Base and Multi-modal Alignment", "year": 2021}], "gt_label": [1, 1, 1, 1, 1], "date": "2023-05", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_162", "valid": true}
{"query": "Which papers consider influence functions to estimate training data's influence on a test point?", "cited_paper": [{"arxiv_id": "1703.04730", "title": "Understanding Black-box Predictions via Influence Functions", "year": 2017}, {"arxiv_id": "1611.05923", "title": "\"Influence Sketching\": Finding Influential Samples In Large-Scale Regressions", "year": 2016}, {"arxiv_id": "1810.10118", "title": "Interpreting Black Box Predictions using Fisher Kernels", "year": 2018}, {"arxiv_id": "2209.05364", "title": "If Influence Functions are the Answer, Then What is the Question?", "year": 2022}], "gt_label": [1, 1, 1, 1], "date": "2022-09", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_163", "valid": true}
{"query": "Could you provide me works that discussed the upper bounds of ReLU networks?", "cited_paper": [{"arxiv_id": "1709.02540", "title": "The Expressive Power of Neural Networks: A View from the Width", "year": 2017}, {"arxiv_id": "1710.11278", "title": "Approximating Continuous Functions by ReLU Nets of Minimal Width", "year": 2017}, {"arxiv_id": "1708.02691", "title": "Universal Function Approximation by Deep Neural Nets with Bounded Width and ReLU Activations", "year": 2017}, {"arxiv_id": "2006.08859", "title": "Minimum Width for Universal Approximation", "year": 2020}], "gt_label": [1, 1, 1, 1], "date": "2020-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_164", "valid": true}
{"query": "Can you provide references discussing similar sampling-based approaches for the graphlet kernel and frequent-subtree kernels?", "cited_paper": [{"arxiv_id": "2010.08270", "title": "Fast Graph Kernel with Optical Random Features", "year": 2020}], "gt_label": [1], "date": "2020-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_165", "valid": true}
{"query": "What studies use Graph Convolutional Networks for activity recognition?", "cited_paper": [{"arxiv_id": "1609.02907", "title": "Semi-Supervised Classification with Graph Convolutional Networks", "year": 2016}], "gt_label": [1], "date": "2016-09", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_166", "valid": true}
{"query": "Can you provide me with the studies that focused on functional requirements in code generation?", "cited_paper": [{"arxiv_id": "2105.09938", "title": "Measuring Coding Challenge Competence With APPS", "year": 2021}, {"arxiv_id": "2108.07732", "title": "Program Synthesis with Large Language Models", "year": 2021}, {"arxiv_id": "2107.03374", "title": "Evaluating Large Language Models Trained on Code", "year": 2021}], "gt_label": [1, 1, 1], "date": "2022-03", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_167", "valid": true}
{"query": "What studies further refined the without-replacement Policy Gradient (PG) estimator by using without-replacement samples as a free baseline?", "cited_paper": [{"arxiv_id": "2002.06043", "title": "Estimating Gradients for Discrete Random Variables by Sampling without Replacement", "year": 2020}], "gt_label": [1], "date": "2020-02", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_168", "valid": true}
{"query": "What works propose the hybridization of classical numerical methods with contemporary data-driven deep learning techniques?", "cited_paper": [{"arxiv_id": "1807.09519", "title": "A machine learning framework for data driven acceleration of computations of differential equations", "year": 2018}], "gt_label": [1], "date": "2022-04", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_169", "valid": true}
{"query": "Which studies describe autoregressive modeling of semantic tokens for generating speech continuations?", "cited_paper": [{"arxiv_id": "2102.01192", "title": "Generative Spoken Language Modeling from Raw Audio", "year": 2021}], "gt_label": [1], "date": "2021-02", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_170", "valid": true}
{"query": "Which paper proposes the method of generating noise samples with the help of observed data in Conditional NCE?", "cited_paper": [], "gt_label": [], "date": "2018-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_171", "valid": true}
{"query": "What benchmark does the variant version of HumanEval-NFR in the study use as a reference?", "cited_paper": [{"arxiv_id": "2107.03374", "title": "Evaluating Large Language Models Trained on Code", "year": 2021}], "gt_label": [1], "date": "2021-07", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_172", "valid": true}
{"query": "What research proposes distilling features on various sophisticatedly-selected sub-regions of the feature map to solve imbalance issue?", "cited_paper": [{"arxiv_id": "1906.03609", "title": "Distilling Object Detectors with Fine-grained Feature Imitation", "year": 2019}, {"arxiv_id": "2006.13108", "title": "Distilling Object Detectors with Task Adaptive Regularization", "year": 2020}, {"arxiv_id": "2103.02340", "title": "General Instance Distillation for Object Detection", "year": 2021}, {"arxiv_id": "2103.14475", "title": "Distilling Object Detectors via Decoupled Features", "year": 2021}, {"arxiv_id": "2111.11837", "title": "Focal and Global Knowledge Distillation for Detectors", "year": 2021}], "gt_label": [1, 1, 1, 1, 1], "date": "2021-11", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_173", "valid": true}
{"query": "Are there any works that develop accurate probes for factuality detection in LLM without relying on annotated training data?", "cited_paper": [{"arxiv_id": "2304.13734", "title": "The Internal State of an LLM Knows When It's Lying", "year": 2023}, {"arxiv_id": "2310.01405", "title": "Representation Engineering: A Top-Down Approach to AI Transparency", "year": 2023}], "gt_label": [1, 1], "date": "2024-07", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_174", "valid": true}
{"query": "Which studies have addressed the issue of capturing electron-electron interactions beyond a mean-field approximation?", "cited_paper": [], "gt_label": [], "date": "2007-04", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_175", "valid": true}
{"query": "What work proposes to decompose the voxel tensor into feature planes and vectors?", "cited_paper": [{"arxiv_id": "2203.09517", "title": "TensoRF: Tensorial Radiance Fields", "year": 2022}], "gt_label": [1], "date": "2022-03", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_176", "valid": true}
{"query": "What papers propose using model self-consistency for factuality detection in LLM?", "cited_paper": [{"arxiv_id": "2207.05221", "title": "Language Models (Mostly) Know What They Know", "year": 2022}, {"arxiv_id": "2304.13734", "title": "The Internal State of an LLM Knows When It's Lying", "year": 2023}, {"arxiv_id": "2310.01405", "title": "Representation Engineering: A Top-Down Approach to AI Transparency", "year": 2023}], "gt_label": [1, 1, 1], "date": "2023-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_177", "valid": true}
{"query": "Which papers provide interesting examples of recent work in online calibration in the adversarial sequence model?", "cited_paper": [{"arxiv_id": "1607.03594", "title": "Estimating Uncertainty Online Against an Adversary", "year": 2016}], "gt_label": [1], "date": "2016-07", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_178", "valid": true}
{"query": "Could you provide information about works which tried to enhance PLMs through fine-tuning on human or synthetic labels?", "cited_paper": [{"arxiv_id": "2009.09025", "title": "COMET: A Neural Framework for MT Evaluation", "year": 2020}, {"arxiv_id": "2210.07197", "title": "Towards a Unified Multi-Dimensional Evaluator for Text Generation", "year": 2022}], "gt_label": [1, 1], "date": "2022-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_179", "valid": true}
{"query": "What studies focus on interpolation condition in overparameterized models?", "cited_paper": [{"arxiv_id": "1905.13655", "title": "Implicit Regularization in Deep Matrix Factorization", "year": 2019}, {"arxiv_id": "1802.05074", "title": "L4: Practical loss-based stepsize adaptation for deep learning", "year": 2018}, {"arxiv_id": "1905.09997", "title": "Painless Stochastic Gradient: Interpolation, Line-Search, and Convergence Rates", "year": 2019}], "gt_label": [1, 1, 1], "date": "2021-07", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_180", "valid": true}
{"query": "Which work focused on learning of entity embeddings for rule learning but had limitations?", "cited_paper": [{"arxiv_id": "1412.6575", "title": "Embedding Entities and Relations for Learning and Inference in Knowledge Bases", "year": 2014}], "gt_label": [1], "date": "2014-12", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_181", "valid": true}
{"query": "What works proposed the generative alignment of multiple modalities into one joint embedding space?", "cited_paper": [{"arxiv_id": "2205.14100", "title": "GIT: A Generative Image-to-text Transformer for Vision and Language", "year": 2022}, {"arxiv_id": "2301.12597", "title": "BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models", "year": 2023}, {"arxiv_id": "2304.08345", "title": "VALOR: Vision-Audio-Language Omni-Perception Pretraining Model and Dataset", "year": 2023}, {"arxiv_id": "2305.18500", "title": "VAST: A Vision-Audio-Subtitle-Text Omni-Modality Foundation Model and Dataset", "year": 2023}], "gt_label": [1, 1, 1, 1], "date": "2023-05", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_182", "valid": true}
{"query": "Any works demonstrate that acoustic tokens can capture details of audio waveforms, ranging from multi-speaker speech to music and audio effects?", "cited_paper": [{"arxiv_id": "2209.03143", "title": "AudioLM: a Language Modeling Approach to Audio Generation", "year": 2022}, {"arxiv_id": "2301.11325", "title": "MusicLM: Generating Music From Text", "year": 2023}, {"arxiv_id": "2209.15352", "title": "AudioGen: Textually Guided Audio Generation", "year": 2022}], "gt_label": [1, 1, 1], "date": "2023-01", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_183", "valid": true}
{"query": "What models were cited as significantly benefiting from the instructional tuning strategy?", "cited_paper": [{"arxiv_id": "2203.02155", "title": "Training language models to follow instructions with human feedback", "year": 2022}], "gt_label": [1], "date": "2022-03", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_184", "valid": true}
{"query": "Which works called out self-supervised tasks for improving model training in the context of self-supervised learning?", "cited_paper": [{"arxiv_id": "2006.05576", "title": "Self-supervised Learning from a Multi-view Perspective", "year": 2020}, {"arxiv_id": "2008.01064", "title": "Predicting What You Already Know Helps: Provable Self-Supervised Learning", "year": 2020}, {"arxiv_id": "2008.10150", "title": "Contrastive learning, multi-view redundancy, and linear models", "year": 2020}], "gt_label": [1, 1, 1], "date": "2020-08", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_185", "valid": true}
{"query": "Are there any papers that extend neural fields to inverse rendering, where geometry and reflectance are modeled as neural fields?", "cited_paper": [{"arxiv_id": "2303.14190", "title": "WildLight: In-the-wild Inverse Rendering with a Flashlight", "year": 2023}, {"arxiv_id": "2204.02232", "title": "IRON: Inverse Rendering by Optimizing Neural SDFs and Materials from Photometric Images", "year": 2022}, {"arxiv_id": "2211.14086", "title": "ShadowNeuS: Neural SDF Reconstruction by Shadow Ray Supervision", "year": 2022}, {"arxiv_id": "2106.01970", "title": "NeRFactor: Neural Factorization of Shape and Reflectance Under an Unknown Illumination", "year": 2021}, {"arxiv_id": "2104.00674", "title": "PhySG: Inverse Rendering with Spherical Gaussians for Physics-based Material Editing and Relighting", "year": 2021}, {"arxiv_id": "2008.03824", "title": "Neural Reflectance Fields for Appearance Acquisition", "year": 2020}, {"arxiv_id": "2012.03918", "title": "NeRD: Neural Reflectance Decomposition from Image Collections", "year": 2020}, {"arxiv_id": "2111.12503", "title": "Extracting Triangular 3D Models, Materials, and Lighting From Images", "year": 2021}, {"arxiv_id": "2206.03380", "title": "Shape, Light, and Material Decomposition from Images using Monte Carlo Rendering and Denoising", "year": 2022}], "gt_label": [1, 1, 1, 1, 1, 1, 1, 1, 1], "date": "2023-03", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_186", "valid": true}
{"query": "What studies utilized model grafting for detecting skill neurons?", "cited_paper": [{"arxiv_id": "2302.06600", "title": "Task-Specific Skill Localization in Fine-tuned Language Models", "year": 2023}], "gt_label": [1], "date": "2023-02", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_187", "valid": true}
{"query": "Could you provide me some studies that have explored various augmentations on graphs, based on the data augmentation in image analysis?", "cited_paper": [{"arxiv_id": "2010.13902", "title": "Graph Contrastive Learning with Augmentations", "year": 2020}, {"arxiv_id": "2106.07594", "title": "Graph Contrastive Learning Automated", "year": 2021}, {"arxiv_id": "2106.05819", "title": "Adversarial Graph Augmentation to Improve Graph Contrastive Learning", "year": 2021}, {"arxiv_id": "2201.01702", "title": "Bringing Your Own View: Graph Contrastive Learning without Prefabricated Data Augmentations", "year": 2022}, {"arxiv_id": "2206.07869", "title": "Let Invariant Rationale Discovery Inspire Graph Contrastive Learning", "year": 2022}], "gt_label": [1, 1, 1, 1, 1], "date": "2022-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_188", "valid": true}
{"query": "What works fine-tune a pretrained language model for aspect extraction and rely on a manual labeling of comparative data?", "cited_paper": [{"arxiv_id": "2109.00571", "title": "DILBERT: Customized Pre-Training for Domain Adaptation withCategory Shift, with an Application to Aspect Extraction", "year": 2021}], "gt_label": [1], "date": "2021-09", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_189", "valid": true}
{"query": "Can you name some works about semantic occupancy prediction based on RGB data?", "cited_paper": [{"arxiv_id": "2112.00726", "title": "MonoScene: Monocular 3D Semantic Scene Completion", "year": 2021}, {"arxiv_id": "2303.09998", "title": "TBP-Former: Learning Temporal Bird's-Eye-View Pyramid for Joint Perception and Prediction in Vision-Centric Autonomous Driving", "year": 2023}, {"arxiv_id": "2307.10934", "title": "OCTraN: 3D Occupancy Convolutional Transformer Network in Unstructured Traffic Scenarios", "year": 2023}, {"arxiv_id": "2302.07817", "title": "Tri-Perspective View for Vision-Based 3D Semantic Occupancy Prediction", "year": 2023}, {"arxiv_id": "2306.15670", "title": "Symphonize 3D Semantic Scene Completion with Contextual Instance Queries", "year": 2023}, {"arxiv_id": "2302.12251", "title": "VoxFormer: Sparse Voxel Transformer for Camera-based 3D Semantic Scene Completion", "year": 2023}, {"arxiv_id": "2302.13540", "title": "OccDepth: A Depth-Aware Method for 3D Semantic Scene Completion", "year": 2023}, {"arxiv_id": "2306.02851", "title": "Scene as Occupancy", "year": 2023}, {"arxiv_id": "2306.10013", "title": "PanoOcc: Unified Occupancy Representation for Camera-based 3D Panoptic Segmentation", "year": 2023}, {"arxiv_id": "2305.05594", "title": "PET-NeuS: Positional Encoding Tri-Planes for Neural Surfaces", "year": 2023}, {"arxiv_id": "2303.09551", "title": "SurroundOcc: Multi-Camera 3D Occupancy Prediction for Autonomous Driving", "year": 2023}, {"arxiv_id": "2305.16829", "title": "BEV-IO: Enhancing Bird's-Eye-View 3D Detection with Instance Occupancy", "year": 2023}, {"arxiv_id": "2304.05316", "title": "OccFormer: Dual-path Transformer for Vision-based 3D Semantic Occupancy Prediction", "year": 2023}, {"arxiv_id": "2307.01492", "title": "FB-OCC: 3D Occupancy Prediction based on Forward-Backward View Transformation", "year": 2023}, {"arxiv_id": "2310.07522", "title": "S4C: Self-Supervised Semantic Scene Completion with Neural Fields", "year": 2023}], "gt_label": [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], "date": "2023-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_190", "valid": true}
{"query": "Which works used fine-tuning in combination with weight interpolation to improve results on specific tasks?", "cited_paper": [{"arxiv_id": "2208.05592", "title": "Patching open-vocabulary models by interpolating weights", "year": 2022}], "gt_label": [1], "date": "2022-08", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_191", "valid": true}
{"query": "What research work focused on enhancing generation quality in SLP using mixture density networks, Mixture-of-Experts, dictionary representations, and diffusion models?", "cited_paper": [{"arxiv_id": "2103.06982", "title": "Continuous 3D Multi-Channel Sign Language Production via Progressive Transformers and Mixture Density Networks", "year": 2021}, {"arxiv_id": "2107.11317", "title": "Mixed SIGNals: Sign Language Production via a Mixture of Motion Primitives", "year": 2021}, {"arxiv_id": "2203.15354", "title": "Signing at Scale: Learning to Co-Articulate Signs for Large-Scale Photo-Realistic Sign Language Production", "year": 2022}, {"arxiv_id": "2312.02702", "title": "Neural Sign Actors: A diffusion model for 3D sign language production from text", "year": 2023}], "gt_label": [1, 1, 1, 1], "date": "2023-12", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_192", "valid": true}
{"query": "What studies propose the idea of labelled data points for efficient learning progress during online updates in the area of active learning?", "cited_paper": [{"arxiv_id": "1708.00489", "title": "Active Learning for Convolutional Neural Networks: A Core-Set Approach", "year": 2017}, {"arxiv_id": "2111.12880", "title": "Active Learning at the ImageNet Scale", "year": 2021}], "gt_label": [1, 1], "date": "2021-11", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_193", "valid": true}
{"query": "Could you provide me some studies about animating images using motion or 3D geometry priors?", "cited_paper": [{"arxiv_id": "1505.00295", "title": "Dense Optical Flow Prediction from a Static Image", "year": 2015}, {"arxiv_id": "1807.09245", "title": "Visual Dynamics: Stochastic Future Generation via Layered Cross Convolutional Networks", "year": 2018}, {"arxiv_id": "1812.02246", "title": "Photo Wake-Up: 3D Character Animation from a Single Photo", "year": 2018}, {"arxiv_id": "2203.14367", "title": "Thin-Plate Spline Motion Model for Image Animation", "year": 2022}, {"arxiv_id": "2112.01502", "title": "Dimensions of Motion: Monocular Prediction through Flow Subspaces", "year": 2021}, {"arxiv_id": "2210.01794", "title": "Implicit Warping for Animation with Image Sets", "year": 2022}, {"arxiv_id": "2303.13744", "title": "Conditional Image-to-Video Generation with Latent Flow Diffusion Models", "year": 2023}, {"arxiv_id": "2011.15128", "title": "Animating Pictures with Eulerian Motion Fields", "year": 2020}, {"arxiv_id": "2112.03051", "title": "Controllable Animation of Fluid Elements in Still Images", "year": 2021}, {"arxiv_id": "1910.07192", "title": "Animating Landscape: Self-Supervised Learning of Decoupled Motion and Appearance for Single-Image Video Synthesis", "year": 2019}, {"arxiv_id": "2210.04628", "title": "Novel View Synthesis with Diffusion Models", "year": 2022}, {"arxiv_id": "2210.02553", "title": "Water Simulation and Rendering from a Still Photograph", "year": 2022}], "gt_label": [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], "date": "2023-03", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_194", "valid": true}
{"query": "What works are about generating arguments for answering comparative questions?", "cited_paper": [{"arxiv_id": "2109.03171", "title": "Aspect-Controllable Opinion Summarization", "year": 2021}], "gt_label": [1], "date": "2021-09", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_195", "valid": true}
{"query": "Could you provide some research papers that recourse to adversarial imitation learning to handle challenges in behavior cloning?", "cited_paper": [{"arxiv_id": "2205.03195", "title": "Symphony: Learning Realistic and Diverse Agents for Autonomous Driving Simulation", "year": 2022}, {"arxiv_id": "2210.09539", "title": "Hierarchical Model-Based Imitation Learning for Planning in Autonomous Driving", "year": 2022}], "gt_label": [1, 1], "date": "2022-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_196", "valid": true}
{"query": "Which works decomposed the reconstructed volume into geometry, SVBRDF, and illumination?", "cited_paper": [{"arxiv_id": "2012.03927", "title": "NeRV: Neural Reflectance and Visibility Fields for Relighting and View Synthesis", "year": 2020}, {"arxiv_id": "2110.14373", "title": "Neural-PIL: Neural Pre-Integrated Lighting for Reflectance Decomposition", "year": 2021}], "gt_label": [1, 1], "date": "2021-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_197", "valid": true}
{"query": "What research indicates that the trade-off between adversarial robustness and accuracy can be attributed to current adversarial training algorithms?", "cited_paper": [{"arxiv_id": "2003.02460", "title": "A Closer Look at Accuracy vs. Robustness", "year": 2020}, {"arxiv_id": "2202.10103", "title": "Robustness and Accuracy Could Be Reconcilable by (Proper) Definition", "year": 2022}], "gt_label": [1, 1], "date": "2022-02", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_198", "valid": true}
{"query": "What paper estimates the inner integral with a quadrature of N samples from a Q-network?", "cited_paper": [{"arxiv_id": "1910.09093", "title": "All-Action Policy Gradient Methods: A Numerical Integration Approach", "year": 2019}], "gt_label": [1], "date": "2019-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_199", "valid": true}
{"query": "What works fall under the category of post-training quantization (PTQ)?", "cited_paper": [{"arxiv_id": "2106.14156", "title": "Post-Training Quantization for Vision Transformer", "year": 2021}, {"arxiv_id": "2004.10568", "title": "Up or Down? Adaptive Rounding for Post-Training Quantization", "year": 2020}, {"arxiv_id": "1810.05723", "title": "Post-training 4-bit quantization of convolution networks for rapid-deployment", "year": 2018}, {"arxiv_id": "1906.04721", "title": "Data-Free Quantization Through Weight Equalization and Bias Correction", "year": 2019}], "gt_label": [1, 1, 1, 1], "date": "2021-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_200", "valid": true}
{"query": "Could you tell me which works have optimized the mixing weights of the source and target text embeddings for disentangled image editing?", "cited_paper": [{"arxiv_id": "2212.08698", "title": "Uncovering the Disentanglement Capability in Text-to-Image Diffusion Models", "year": 2022}], "gt_label": [1], "date": "2022-12", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_201", "valid": true}
{"query": "Which works tried to reduce the dependency on densely collected data for scene reconstruction by utilizing local semantic relationships across multiple scenes?", "cited_paper": [{"arxiv_id": "2012.02190", "title": "pixelNeRF: Neural Radiance Fields from One or Few Images", "year": 2020}, {"arxiv_id": "2104.06935", "title": "Stereo Radiance Fields (SRF): Learning View Synthesis for Sparse Views of Novel Scenes", "year": 2021}], "gt_label": [1, 1], "date": "2021-04", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_202", "valid": true}
{"query": "What papers have been written on the uses of the Optimal Transport map in generative modeling?", "cited_paper": [{"arxiv_id": "2110.02999", "title": "Generative Modeling with Optimal Transport Maps", "year": 2021}], "gt_label": [1], "date": "2021-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_203", "valid": true}
{"query": "Any works on the development of counter-measures to embedding inversion attacks?", "cited_paper": [{"arxiv_id": "2212.10025", "title": "When Federated Learning Meets Pre-trained Language Models' Parameter-Efficient Tuning Methods", "year": 2022}, {"arxiv_id": "2010.06053", "title": "TextHide: Tackling Data Privacy in Language Understanding Tasks", "year": 2020}, {"arxiv_id": "2010.01285", "title": "Differentially Private Representation for NLP: Formal Guarantee and An Empirical Study on Privacy and Fairness", "year": 2020}], "gt_label": [1, 1, 1], "date": "2022-12", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_204", "valid": true}
{"query": "Which studies used random features for node embeddings and node classification tasks?", "cited_paper": [{"arxiv_id": "2305.00156", "title": "Taming graph kernels with random features", "year": 2023}], "gt_label": [1], "date": "2023-05", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_205", "valid": true}
{"query": "Could you provide me some research that consider global properties of the networks?", "cited_paper": [{"arxiv_id": "1712.06541", "title": "Size-Independent Sample Complexity of Neural Networks", "year": 2017}], "gt_label": [1], "date": "2017-12", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_206", "valid": true}
{"query": "Could you provide some references about global methods for offering sample-independent meaningful perturbations for each latent space?", "cited_paper": [{"arxiv_id": "2103.17249", "title": "StyleCLIP: Text-Driven Manipulation of StyleGAN Imagery", "year": 2021}, {"arxiv_id": "2004.02546", "title": "GANSpace: Discovering Interpretable GAN Controls", "year": 2020}], "gt_label": [1, 1], "date": "2021-03", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_207", "valid": true}
{"query": "Which studies are there on pre-trained models for vision tasks?", "cited_paper": [{"arxiv_id": "2010.11929", "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale", "year": 2020}, {"arxiv_id": "2106.04560", "title": "Scaling Vision Transformers", "year": 2021}], "gt_label": [1, 1], "date": "2021-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_208", "valid": true}
{"query": "Which study introduces CLoM and CCLoM loss terms for notable cross-architecture improvements in distillation methods?", "cited_paper": [{"arxiv_id": "2310.03295", "title": "Can pre-trained models assist in dataset distillation?", "year": 2023}], "gt_label": [1], "date": "2023-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_209", "valid": true}
{"query": "Do any subsequent studies existed that have improved upon the concept of implicit data augmentation for image classification tasks?", "cited_paper": [{"arxiv_id": "2112.07928", "title": "Imagine by Reasoning: A Reasoning-Based Implicit Semantic Data Augmentation for Long-Tailed Classification", "year": 2021}, {"arxiv_id": "2103.12579", "title": "MetaSAug: Meta Semantic Augmentation for Long-Tailed Visual Recognition", "year": 2021}, {"arxiv_id": "2304.13431", "title": "Implicit Counterfactual Data Augmentation for Robust Learning", "year": 2023}], "gt_label": [1, 1, 1], "date": "2023-04", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_210", "valid": true}
{"query": "Can you provide me the papers that applied learning invariant representations for multilingual machine translation?", "cited_paper": [{"arxiv_id": "1611.04558", "title": "Google's Multilingual Neural Machine Translation System: Enabling Zero-Shot Translation", "year": 2016}, {"arxiv_id": "1903.00089", "title": "Massively Multilingual Neural Machine Translation", "year": 2019}, {"arxiv_id": "2008.04510", "title": "On Learning Language-Invariant Representations for Universal Machine Translation", "year": 2020}], "gt_label": [1, 1, 1], "date": "2020-08", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_211", "valid": true}
{"query": "What studies focused on training with the knowledge of some attacks in the context of adversarial detection?", "cited_paper": [{"arxiv_id": "1702.04267", "title": "On Detecting Adversarial Perturbations", "year": 2017}, {"arxiv_id": "1703.00410", "title": "Detecting Adversarial Samples from Artifacts", "year": 2017}, {"arxiv_id": "2004.09179", "title": "GraN: An Efficient Gradient-Norm Based Detector for Adversarial and Misclassified Examples", "year": 2020}, {"arxiv_id": "1803.04765", "title": "Deep k-Nearest Neighbors: Towards Confident, Interpretable and Robust Deep Learning", "year": 2018}, {"arxiv_id": "1801.02613", "title": "Characterizing Adversarial Subspaces Using Local Intrinsic Dimensionality", "year": 2018}], "gt_label": [1, 1, 1, 1, 1], "date": "2020-04", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_212", "valid": true}
{"query": "Which paper initially demonstrated that contrastive learning framework can achieve a performance comparable to fully supervised baselines?", "cited_paper": [{"arxiv_id": "2103.00020", "title": "Learning Transferable Visual Models From Natural Language Supervision", "year": 2021}], "gt_label": [1], "date": "2021-03", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_213", "valid": true}
{"query": "Can you name a few recent models that have contributed significantly in the field of image captioning?", "cited_paper": [{"arxiv_id": "2201.12086", "title": "BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation", "year": 2022}, {"arxiv_id": "2301.12597", "title": "BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models", "year": 2023}, {"arxiv_id": "2204.14198", "title": "Flamingo: a Visual Language Model for Few-Shot Learning", "year": 2022}, {"arxiv_id": "2205.01917", "title": "CoCa: Contrastive Captioners are Image-Text Foundation Models", "year": 2022}], "gt_label": [1, 1, 1, 1], "date": "2023-01", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_214", "valid": true}
{"query": "What works introduced temporal transformers for frame-level relationship encoding in action recognition?", "cited_paper": [{"arxiv_id": "2102.00719", "title": "Video Transformer Network", "year": 2021}, {"arxiv_id": "2103.13915", "title": "An Image is Worth 16x16 Words, What is a Video Worth?", "year": 2021}], "gt_label": [1, 1], "date": "2021-03", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_215", "valid": true}
{"query": "Could you provide a study that discusses how arbitrary input features could influence its reasoning process when generating the explanation, which could result in different reasoning processes for explanation and prediction, and hide the underlying drivers of the prediction?", "cited_paper": [{"arxiv_id": "2305.04388", "title": "Language Models Don't Always Say What They Think: Unfaithful Explanations in Chain-of-Thought Prompting", "year": 2023}], "gt_label": [1], "date": "2023-05", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_216", "valid": true}
{"query": "What work uses the scene graph structure to generate reasoning questions on real-world images to test the compositional reasoning ability?", "cited_paper": [{"arxiv_id": "1902.09506", "title": "GQA: A New Dataset for Real-World Visual Reasoning and Compositional Question Answering", "year": 2019}], "gt_label": [1], "date": "2019-02", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_217", "valid": true}
{"query": "What research introduced memory efficient schemes in the field of optimization-based meta-learning?", "cited_paper": [{"arxiv_id": "1703.03400", "title": "Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks", "year": 2017}, {"arxiv_id": "1803.02999", "title": "On First-Order Meta-Learning Algorithms", "year": 2018}, {"arxiv_id": "1909.04630", "title": "Meta-Learning with Implicit Gradients", "year": 2019}, {"arxiv_id": "2102.07215", "title": "Large-Scale Meta-Learning with Continual Trajectory Shifting", "year": 2021}], "gt_label": [1, 1, 1, 1], "date": "2021-02", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_218", "valid": true}
{"query": "Any works about adapting pretrained vision-language model for fine-grained localization tasks?", "cited_paper": [{"arxiv_id": "2112.01071", "title": "Extract Free Dense Labels from CLIP", "year": 2021}, {"arxiv_id": "2304.05653", "title": "A Closer Look at the Explainability of Contrastive Language-Image Pre-training", "year": 2023}], "gt_label": [1, 1], "date": "2023-04", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_219", "valid": true}
{"query": "Could you provide me studies where it was hypothesized that attention can alleviate oversmoothing in attention-based GNNs?", "cited_paper": [{"arxiv_id": "2003.08414", "title": "Scattering GCN: Overcoming Oversmoothness in Graph Convolutional Networks", "year": 2020}], "gt_label": [1], "date": "2020-03", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_220", "valid": true}
{"query": "Which work first initiated 3D human geometry via a shape VAE network for 3D avatar generation?", "cited_paper": [{"arxiv_id": "2205.08535", "title": "AvatarCLIP: Zero-Shot Text-Driven Generation and Animation of 3D Avatars", "year": 2022}], "gt_label": [1], "date": "2022-05", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_221", "valid": true}
{"query": "Which papers have studied Meta-RL methods based on gradients?", "cited_paper": [{"arxiv_id": "1703.03400", "title": "Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks", "year": 2017}, {"arxiv_id": "1802.07245", "title": "Meta-Reinforcement Learning of Structured Exploration Strategies", "year": 2018}], "gt_label": [1, 1], "date": "2018-02", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_222", "valid": true}
{"query": "What pioneer work uses deep learning models for generating realistic LiDAR point clouds?", "cited_paper": [{"arxiv_id": "2006.09348", "title": "LiDARsim: Realistic LiDAR Simulation by Leveraging the Real World", "year": 2020}], "gt_label": [1], "date": "2020-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_223", "valid": true}
{"query": "Which works used Generative Adversarial Networks for text-to-image generation?", "cited_paper": [{"arxiv_id": "1605.05396", "title": "Generative Adversarial Text to Image Synthesis", "year": 2016}, {"arxiv_id": "1711.10485", "title": "AttnGAN: Fine-Grained Text to Image Generation with Attentional Generative Adversarial Networks", "year": 2017}, {"arxiv_id": "2112.05219", "title": "CLIP2StyleGAN: Unsupervised Extraction of StyleGAN Edit Directions", "year": 2021}, {"arxiv_id": "2108.00946", "title": "StyleGAN-NADA: CLIP-Guided Domain Adaptation of Image Generators", "year": 2021}], "gt_label": [1, 1, 1, 1], "date": "2021-12", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_224", "valid": true}
{"query": "Could you provide me with some works that converted preprocessed neuroimages to brain network datasets?", "cited_paper": [{"arxiv_id": "2006.05176", "title": "Explainable Classification of Brain Networks via Contrast Subgraphs", "year": 2020}, {"arxiv_id": "2007.08663", "title": "TUDataset: A collection of benchmark datasets for learning with graphs", "year": 2020}], "gt_label": [1, 1], "date": "2020-07", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_225", "valid": true}
{"query": "Which works use dataset pruning by keeping hard samples with maximum entropy?", "cited_paper": [{"arxiv_id": "1906.11829", "title": "Selection via Proxy: Efficient Data Selection for Deep Learning", "year": 2019}], "gt_label": [1], "date": "2019-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_226", "valid": true}
{"query": "What paper estimated CMI by reformulating it as a minmax optimization problem?", "cited_paper": [{"arxiv_id": "2005.08226", "title": "C-MI-GAN : Estimation of Conditional Mutual Information using MinMax formulation", "year": 2020}], "gt_label": [1], "date": "2020-05", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_227", "valid": true}
{"query": "Which papers suggest that group-wise quantization approaches can achieve higher accuracy compared to layer-wise or channel-wise methods?", "cited_paper": [{"arxiv_id": "1909.05840", "title": "Q-BERT: Hessian Based Ultra Low Precision Quantization of BERT", "year": 2019}, {"arxiv_id": "2210.17323", "title": "GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers", "year": 2022}], "gt_label": [1, 1], "date": "2022-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_228", "valid": true}
{"query": "Could you provide me examples of literature focusing on improving detection robustness to paraphrasing attacks?", "cited_paper": [{"arxiv_id": "2306.04634", "title": "On the Reliability of Watermarks for Large Language Models", "year": 2023}, {"arxiv_id": "2310.06356", "title": "A Semantic Invariant Robust Watermark for Large Language Models", "year": 2023}], "gt_label": [1, 1], "date": "2023-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_229", "valid": true}
{"query": "Could you provide me some works that applied joint-embedding strategy and generative approaches for self-supervised vision transformer?", "cited_paper": [{"arxiv_id": "2104.14294", "title": "Emerging Properties in Self-Supervised Vision Transformers", "year": 2021}, {"arxiv_id": "2104.02057", "title": "An Empirical Study of Training Self-Supervised Vision Transformers", "year": 2021}, {"arxiv_id": "2111.06377", "title": "Masked Autoencoders Are Scalable Vision Learners", "year": 2021}], "gt_label": [1, 1, 1], "date": "2021-11", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_230", "valid": true}
{"query": "What works demonstrated the usefulness of emergent relationships for learning fine-grained visual features?", "cited_paper": [{"arxiv_id": "2104.14294", "title": "Emerging Properties in Self-Supervised Vision Transformers", "year": 2021}, {"arxiv_id": "2210.02149", "title": "Relational Proxies: Emergent Relationships as Fine-Grained Discriminators", "year": 2022}], "gt_label": [1, 1], "date": "2022-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_231", "valid": true}
{"query": "Which papers proposed concept-based models for few-shot learning settings?", "cited_paper": [{"arxiv_id": "2007.07375", "title": "Concept Learners for Few-Shot Learning", "year": 2020}], "gt_label": [1], "date": "2020-07", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_232", "valid": true}
{"query": "Which papers provided the basis for the researcher's work on Transformer for program synthesis?", "cited_paper": [{"arxiv_id": "2006.03511", "title": "Unsupervised Translation of Programming Languages", "year": 2020}, {"arxiv_id": "2108.07732", "title": "Program Synthesis with Large Language Models", "year": 2021}], "gt_label": [1, 1], "date": "2021-08", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_233", "valid": true}
{"query": "Could you provide me some works about the automatic debiased machine learner (Auto-DML) approach which can handle continuous treatments in the back-door adjustment?", "cited_paper": [], "gt_label": [], "date": "2018-09", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_234", "valid": true}
{"query": "Are there any studies that utilized differential equations to model continuous-time processes?", "cited_paper": [{"arxiv_id": "1806.07366", "title": "Neural Ordinary Differential Equations", "year": 2018}, {"arxiv_id": "2006.04418", "title": "Learning Long-Term Dependencies in Irregularly-Sampled Time Series", "year": 2020}, {"arxiv_id": "2005.08926", "title": "Neural Controlled Differential Equations for Irregular Time Series", "year": 2020}], "gt_label": [1, 1, 1], "date": "2020-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_235", "valid": true}
{"query": "What study proposed a method to specialize the models ability towards a target task with CoT prompting?", "cited_paper": [{"arxiv_id": "2301.12726", "title": "Specializing Smaller Language Models towards Multi-Step Reasoning", "year": 2023}], "gt_label": [1], "date": "2023-01", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_236", "valid": true}
{"query": "Can you name a study that employs selective search to crop out Regions of Interest for dense prediction?", "cited_paper": [{"arxiv_id": "2106.02637", "title": "Aligning Pretraining for Detection via Object-Level Contrastive Learning", "year": 2021}, {"arxiv_id": "2106.11952", "title": "Unsupervised Object-Level Representation Learning from Scene Images", "year": 2021}], "gt_label": [1, 1], "date": "2021-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_237", "valid": true}
{"query": "Which dataset synthetically generates commands for device-control tasks?", "cited_paper": [{"arxiv_id": "2005.03776", "title": "Mapping Natural Language Instructions to Mobile UI Action Sequences", "year": 2020}], "gt_label": [1], "date": "2020-05", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_238", "valid": true}
{"query": "What papers are about the utilization of knowledge for reasoning purposes?", "cited_paper": [{"arxiv_id": "2305.15054", "title": "A Mechanistic Interpretation of Arithmetic Reasoning in Language Models using Causal Mediation Analysis", "year": 2023}, {"arxiv_id": "2310.14491", "title": "Towards a Mechanistic Interpretation of Multi-Step Reasoning Capabilities of Language Models", "year": 2023}], "gt_label": [1, 1], "date": "2023-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_239", "valid": true}
{"query": "Which research work proposes techniques to offload part of the model to the server using split learning?", "cited_paper": [{"arxiv_id": "2107.04271", "title": "FedAdapt: Adaptive Offloading for IoT Devices in Federated Learning", "year": 2021}, {"arxiv_id": "2007.14513", "title": "Group Knowledge Transfer: Federated Learning of Large CNNs at the Edge", "year": 2020}, {"arxiv_id": "2111.01516", "title": "FedFly: Towards Migration in Edge-based Distributed Federated Learning", "year": 2021}], "gt_label": [1, 1, 1], "date": "2021-11", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_240", "valid": true}
{"query": "What research introduced Meta-Prompting, a method that breaks down complex tasks into subtasks?", "cited_paper": [{"arxiv_id": "2401.12954", "title": "Meta-Prompting: Enhancing Language Models with Task-Agnostic Scaffolding", "year": 2024}], "gt_label": [1], "date": "2024-01", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_241", "valid": true}
{"query": "What work aims to approximate the homotopy only enough to generate the conformal prediction set but suffers from accuracy difficulties?", "cited_paper": [{"arxiv_id": "1909.09365", "title": "Computing Full Conformal Prediction Set with Approximate Homotopy", "year": 2019}], "gt_label": [1], "date": "2019-09", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_242", "valid": true}
{"query": "Which studies used techniques like spatial attention maps and an iterative refinement strategy?", "cited_paper": [{"arxiv_id": "1912.11180", "title": "Cascading Convolutional Color Constancy", "year": 2019}], "gt_label": [1], "date": "2019-12", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_243", "valid": true}
{"query": "What paper first connected U-Nets and multi-resolution analysis?", "cited_paper": [{"arxiv_id": "2301.08187", "title": "A Multi-Resolution Framework for U-Nets with Applications to Hierarchical VAEs", "year": 2023}], "gt_label": [1], "date": "2023-01", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_244", "valid": true}
{"query": "What early studies on mechanistic interpretability focused on how the model stores factual knowledge internally?", "cited_paper": [{"arxiv_id": "2202.05262", "title": "Locating and Editing Factual Associations in GPT", "year": 2022}, {"arxiv_id": "2104.08696", "title": "Knowledge Neurons in Pretrained Transformers", "year": 2021}], "gt_label": [1, 1], "date": "2022-02", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_245", "valid": true}
{"query": "What are some recent studies that leverage LLMs as Emotional Support Chat(ECS) systems through in-context learning?", "cited_paper": [{"arxiv_id": "2305.04147", "title": "Controllable Mixed-Initiative Dialogue Generation through Prompting", "year": 2023}, {"arxiv_id": "2308.11584", "title": "Building Emotional Support Chatbots in the Era of LLMs", "year": 2023}], "gt_label": [1, 1], "date": "2023-08", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_246", "valid": true}
{"query": "What works introduce the feasibility of creating adversarial examples that can break LMMs?", "cited_paper": [{"arxiv_id": "2306.13213", "title": "Visual Adversarial Examples Jailbreak Aligned Large Language Models", "year": 2023}, {"arxiv_id": "2306.15447", "title": "Are aligned neural networks adversarially aligned?", "year": 2023}, {"arxiv_id": "2307.15043", "title": "Universal and Transferable Adversarial Attacks on Aligned Language Models", "year": 2023}, {"arxiv_id": "2307.02483", "title": "Jailbroken: How Does LLM Safety Training Fail?", "year": 2023}], "gt_label": [1, 1, 1, 1], "date": "2023-07", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_247", "valid": true}
{"query": "Which work introduced Optima-TT, a promising algorithm for multidimensional optimization?", "cited_paper": [{"arxiv_id": "2209.14808", "title": "Optimization of Functions Given in the Tensor Train Format", "year": 2022}], "gt_label": [1], "date": "2022-09", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_248", "valid": true}
{"query": "Can you provide some papers that discuss how LLMs reproduce human-like text and also replicate biases present in the training data?", "cited_paper": [{"arxiv_id": "2101.05783", "title": "Persistent Anti-Muslim Bias in Large Language Models", "year": 2021}], "gt_label": [1], "date": "2021-01", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_249", "valid": true}
{"query": "Which papers introduced semi-supervised methods that explore the entire eigenspectrum?", "cited_paper": [{"arxiv_id": "2006.07988", "title": "Adaptive Universal Generalized PageRank Graph Neural Network", "year": 2020}, {"arxiv_id": "2106.10994", "title": "BernNet: Learning Arbitrary Graph Spectral Filters via Bernstein Approximation", "year": 2021}, {"arxiv_id": "2112.03499", "title": "A Piece-wise Polynomial Filtering Approach for Graph Neural Networks", "year": 2021}], "gt_label": [1, 1, 1], "date": "2021-12", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_250", "valid": true}
{"query": "What studies use self-supervised and supervised methods to train UI understanding models?", "cited_paper": [{"arxiv_id": "2012.12350", "title": "ActionBert: Leveraging User Actions for Semantic Understanding of User Interfaces", "year": 2020}, {"arxiv_id": "2107.13731", "title": "UIBert: Learning Generic Multimodal Representations for UI Understanding", "year": 2021}, {"arxiv_id": "2301.10165", "title": "Lexi: Self-Supervised Learning of the UI Language", "year": 2023}, {"arxiv_id": "2003.00380", "title": "Unblind Your Apps: Predicting Natural-Language Labels for Mobile GUI Components by Deep Learning", "year": 2020}, {"arxiv_id": "2008.05132", "title": "Object Detection for Graphical User Interface: Old Fashioned or Deep Learning or a Combination?", "year": 2020}, {"arxiv_id": "2101.04893", "title": "Screen Recognition: Creating Accessibility Metadata for Mobile Applications from Pixels", "year": 2021}], "gt_label": [1, 1, 1, 1, 1, 1], "date": "2023-01", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_251", "valid": true}
{"query": "Can you give examples of studies on drug-sized molecules with respect to voxel and point cloud representations?", "cited_paper": [], "gt_label": [], "date": "2020-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_252", "valid": true}
{"query": "Which studies discussed the interaction between privacy and model explainability?", "cited_paper": [{"arxiv_id": "2106.13203", "title": "When Differential Privacy Meets Interpretability: A Case Study", "year": 2021}, {"arxiv_id": "1907.00164", "title": "On the Privacy Risks of Model Explanations", "year": 2019}], "gt_label": [1, 1], "date": "2021-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_253", "valid": true}
{"query": "Which work proposed using the attribution technique for detecting skill neurons?", "cited_paper": [{"arxiv_id": "2205.04157", "title": "Task-specific Compression for Multi-task Language Models using Attribution-based Pruning", "year": 2022}], "gt_label": [1], "date": "2022-05", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_254", "valid": true}
{"query": "What is the first work focusing on preconditioning of on-policy, linear, least-squares forms of TD?", "cited_paper": [], "gt_label": [], "date": "2007-04", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_255", "valid": true}
{"query": "Which works are about using GANs for scene text editing tasks?", "cited_paper": [{"arxiv_id": "1903.01192", "title": "STEFANN: Scene Text Editor using Font Adaptive Neural Network", "year": 2019}, {"arxiv_id": "2207.09649", "title": "GenText: Unsupervised Artistic Text Generation via Decoupled Font and Texture Manipulation", "year": 2022}, {"arxiv_id": "2205.00146", "title": "Look Closer to Supervise Better: One-Shot Font Generation via Component-Based Discriminator", "year": 2022}, {"arxiv_id": "2107.11041", "title": "RewriteNet: Reliable Scene Text Editing with Implicit Decomposition of Text Contents and Styles", "year": 2021}, {"arxiv_id": "2110.01890", "title": "De-rendering Stylized Texts", "year": 2021}, {"arxiv_id": "2003.08152", "title": "SwapText: Image Based Texts Transfer in Scenes", "year": 2020}, {"arxiv_id": "1812.05840", "title": "Spatial Fusion GAN for Image Synthesis", "year": 2018}], "gt_label": [1, 1, 1, 1, 1, 1, 1], "date": "2022-07", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_256", "valid": true}
{"query": "What studies extended their gradient-based recovery strategy from MLPs to Convolutional Neural Networks?", "cited_paper": [{"arxiv_id": "2010.13356", "title": "Exploring the Security Boundary of Data Reconstruction via Neuron Exclusivity Analysis", "year": 2020}, {"arxiv_id": "2010.07733", "title": "R-GAP: Recursive Gradient Attack on Privacy", "year": 2020}, {"arxiv_id": "2112.02918", "title": "When the Curious Abandon Honesty: Federated Learning Is Not Private", "year": 2021}], "gt_label": [1, 1, 1], "date": "2021-12", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_257", "valid": true}
{"query": "Can you provide works that have used Chain of Thought (CoT) in multi-step reasoning tasks?", "cited_paper": [{"arxiv_id": "2201.11903", "title": "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models", "year": 2022}, {"arxiv_id": "2205.11916", "title": "Large Language Models are Zero-Shot Reasoners", "year": 2022}, {"arxiv_id": "2210.00720", "title": "Complexity-Based Prompting for Multi-Step Reasoning", "year": 2022}], "gt_label": [1, 1, 1], "date": "2022-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_258", "valid": true}
{"query": "Can you provide some studies that pointed out inherent ambiguity in questions from human users and proposed a benchmark that provides multiple answers to every question?", "cited_paper": [{"arxiv_id": "2004.10645", "title": "AmbigQA: Answering Ambiguous Open-domain Questions", "year": 2020}], "gt_label": [1], "date": "2020-04", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_259", "valid": true}
{"query": "Which work introduces error span detection and correction to address the GEC problem?", "cited_paper": [{"arxiv_id": "2010.03260", "title": "Improving the Efficiency of Grammatical Error Correction with Erroneous Span Detection and Correction", "year": 2020}], "gt_label": [1], "date": "2020-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_260", "valid": true}
{"query": "Any studies that attempted to construct 3D feature fields?", "cited_paper": [{"arxiv_id": "2205.15585", "title": "Decomposing NeRF for Editing via Feature Field Distillation", "year": 2022}, {"arxiv_id": "2209.03494", "title": "Neural Feature Fusion Fields: 3D Distillation of Self-Supervised 2D Image Representations", "year": 2022}, {"arxiv_id": "2308.07931", "title": "Distilled Feature Fields Enable Few-Shot Language-Guided Manipulation", "year": 2023}, {"arxiv_id": "2103.15875", "title": "In-Place Scene Labelling and Understanding with Implicit Scene Representation", "year": 2021}, {"arxiv_id": "2212.09802", "title": "Panoptic Lifting for 3D Scene Understanding with Neural Fields", "year": 2022}], "gt_label": [1, 1, 1, 1, 1], "date": "2023-08", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_261", "valid": true}
{"query": "Which work introduced the concept of a diachronic word usage graph (DWUG)?", "cited_paper": [{"arxiv_id": "2104.08540", "title": "DWUG: A large Resource of Diachronic Word Usage Graphs in Four Languages", "year": 2021}], "gt_label": [1], "date": "2021-04", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_262", "valid": true}
{"query": "Could you provide me some studies about transformers based architectures for BEV feature generation?", "cited_paper": [{"arxiv_id": "2203.11089", "title": "PersFormer: 3D Lane Detection via Perspective Transformer and the OpenLane Benchmark", "year": 2022}, {"arxiv_id": "2104.10490", "title": "FIERY: Future Instance Prediction in Bird's-Eye View from Surround Monocular Cameras", "year": 2021}, {"arxiv_id": "2205.09743", "title": "BEVerse: Unified Perception and Prediction in Birds-Eye-View for Vision-Centric Autonomous Driving", "year": 2022}, {"arxiv_id": "2203.17270", "title": "BEVFormer: Learning Bird's-Eye-View Representation from Multi-Camera Images via Spatiotemporal Transformers", "year": 2022}], "gt_label": [1, 1, 1, 1], "date": "2022-05", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_263", "valid": true}
{"query": "What works integrate Knowledge Graph (KG) embedding methods using Graph Neural Networks (GNNs) with Language Learning Models (LLMs) during finetuning or pre-training stage?", "cited_paper": [{"arxiv_id": "2104.06378", "title": "QA-GNN: Reasoning with Language Models and Knowledge Graphs for Question Answering", "year": 2021}, {"arxiv_id": "2210.09338", "title": "Deep Bidirectional Language-Knowledge Graph Pretraining", "year": 2022}], "gt_label": [1, 1], "date": "2022-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_264", "valid": true}
{"query": "Could you tell me about the studies that used volumetric entropy for scene geometry in NeRF?", "cited_paper": [{"arxiv_id": "2209.08409", "title": "Uncertainty Guided Policy for Active Robotic 3D Reconstruction using Neural Radiance Fields", "year": 2022}, {"arxiv_id": "2303.16739", "title": "Active Implicit Object Reconstruction using Uncertainty-guided Next-Best-View Optimization", "year": 2023}], "gt_label": [1, 1], "date": "2023-03", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_265", "valid": true}
{"query": "Which papers introduced neural construction methods for combinatorial optimization?", "cited_paper": [{"arxiv_id": "1506.03134", "title": "Pointer Networks", "year": 2015}, {"arxiv_id": "1611.09940", "title": "Neural Combinatorial Optimization with Reinforcement Learning", "year": 2016}, {"arxiv_id": "1802.04240", "title": "Reinforcement Learning for Solving the Vehicle Routing Problem", "year": 2018}], "gt_label": [1, 1, 1], "date": "2018-02", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_266", "valid": true}
{"query": "In what works LVLMs provide bounding boxes for objects while generating responses?", "cited_paper": [{"arxiv_id": "2306.15195", "title": "Shikra: Unleashing Multimodal LLM's Referential Dialogue Magic", "year": 2023}, {"arxiv_id": "2306.14824", "title": "Kosmos-2: Grounding Multimodal Large Language Models to the World", "year": 2023}], "gt_label": [1, 1], "date": "2023-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_267", "valid": true}
{"query": "What papers consider the convergence of PFL with a partial client participation?", "cited_paper": [{"arxiv_id": "1907.02189", "title": "On the Convergence of FedAvg on Non-IID Data", "year": 2019}, {"arxiv_id": "1910.06378", "title": "SCAFFOLD: Stochastic Controlled Averaging for Federated Learning", "year": 2019}, {"arxiv_id": "2101.11203", "title": "Achieving Linear Speedup with Partial Worker Participation in Non-IID Federated Learning", "year": 2021}], "gt_label": [1, 1, 1], "date": "2021-01", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_268", "valid": true}
{"query": "What works achieved state-of-the-art performance on generating unaligned complex objects using big generative models?", "cited_paper": [{"arxiv_id": "2102.12092", "title": "Zero-Shot Text-to-Image Generation", "year": 2021}, {"arxiv_id": "2206.10789", "title": "Scaling Autoregressive Models for Content-Rich Text-to-Image Generation", "year": 2022}, {"arxiv_id": "2204.06125", "title": "Hierarchical Text-Conditional Image Generation with CLIP Latents", "year": 2022}, {"arxiv_id": "2205.11487", "title": "Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding", "year": 2022}, {"arxiv_id": "2112.10752", "title": "High-Resolution Image Synthesis with Latent Diffusion Models", "year": 2021}, {"arxiv_id": "2211.01324", "title": "eDiff-I: Text-to-Image Diffusion Models with an Ensemble of Expert Denoisers", "year": 2022}, {"arxiv_id": "2301.00704", "title": "Muse: Text-To-Image Generation via Masked Generative Transformers", "year": 2023}], "gt_label": [1, 1, 1, 1, 1, 1, 1], "date": "2023-01", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_269", "valid": true}
{"query": "Could you provide me some works that utilized a combination of strong data augmentation functions to improve robustness to common corruptions and random Gaussian noises?", "cited_paper": [{"arxiv_id": "1912.02781", "title": "AugMix: A Simple Data Processing Method to Improve Robustness and Uncertainty", "year": 2019}, {"arxiv_id": "1909.13719", "title": "RandAugment: Practical automated data augmentation with a reduced search space", "year": 2019}], "gt_label": [1, 1], "date": "2019-12", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_270", "valid": true}
{"query": "Could you provide me some studies that have used unlearning to remove sensitive data from a trained model?", "cited_paper": [{"arxiv_id": "1907.05012", "title": "Making AI Forget You: Data Deletion in Machine Learning", "year": 2019}, {"arxiv_id": "1911.04933", "title": "Eternal Sunshine of the Spotless Net: Selective Forgetting in Deep Networks", "year": 2019}, {"arxiv_id": "2103.03279", "title": "Remember What You Want to Forget: Algorithms for Machine Unlearning", "year": 2021}, {"arxiv_id": "2106.04378", "title": "Adaptive Machine Unlearning", "year": 2021}, {"arxiv_id": "2204.07655", "title": "Deep Unlearning via Randomized Conditionally Independent Hessians", "year": 2022}], "gt_label": [1, 1, 1, 1, 1], "date": "2022-04", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_271", "valid": true}
{"query": "Could you provide me some papers that compute closed-form solutions to related or approximate versions of CMDP framework?", "cited_paper": [{"arxiv_id": "1705.10528", "title": "Constrained Policy Optimization", "year": 2017}, {"arxiv_id": "2201.11927", "title": "Constrained Variational Policy Optimization for Safe Reinforcement Learning", "year": 2022}], "gt_label": [1, 1], "date": "2022-01", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_272", "valid": true}
{"query": "Which work introduced the Factor-VAE that encourages disentanglement through a factorial distribution of features?", "cited_paper": [{"arxiv_id": "1802.05983", "title": "Disentangling by Factorising", "year": 2018}], "gt_label": [1], "date": "2018-02", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_273", "valid": true}
{"query": "Could you provide me some works that discuss how LLM can explore and manipulate various attributes of texts?", "cited_paper": [{"arxiv_id": "2307.07099", "title": "Controllable Data Augmentation for Few-Shot Text Mining with Chain-of-Thought Attribute Manipulation", "year": 2023}], "gt_label": [1], "date": "2023-07", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_274", "valid": true}
{"query": "In what papers were the minimax regret rates first established for two-armed bandit and generalized to any number of arms?", "cited_paper": [], "gt_label": [], "date": "2011-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_275", "valid": true}
{"query": "What work uses gradients for data selection?", "cited_paper": [{"arxiv_id": "2103.00123", "title": "GRAD-MATCH: Gradient Matching based Data Subset Selection for Efficient Deep Model Training", "year": 2021}, {"arxiv_id": "2012.10630", "title": "GLISTER: Generalization based Data Subset Selection for Efficient and Robust Learning", "year": 2020}, {"arxiv_id": "1911.10088", "title": "Optimizing Data Usage via Differentiable Rewards", "year": 2019}, {"arxiv_id": "2107.07075", "title": "Deep Learning on a Data Diet: Finding Important Examples Early in Training", "year": 2021}, {"arxiv_id": "1906.01827", "title": "Coresets for Data-efficient Training of Machine Learning Models", "year": 2019}], "gt_label": [1, 1, 1, 1, 1], "date": "2021-07", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_276", "valid": true}
{"query": "Which studies expanded tool use to general API function calling?", "cited_paper": [{"arxiv_id": "2307.16789", "title": "ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs", "year": 2023}, {"arxiv_id": "2306.05301", "title": "ToolAlpaca: Generalized Tool Learning for Language Models with 3000 Simulated Cases", "year": 2023}, {"arxiv_id": "2305.15334", "title": "Gorilla: Large Language Model Connected with Massive APIs", "year": 2023}], "gt_label": [1, 1, 1], "date": "2023-07", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_277", "valid": true}
{"query": "Could you provide me some studies about the difficulties in finding reliable confidence thresholds in self-distillation?", "cited_paper": [{"arxiv_id": "1805.04770", "title": "Born Again Neural Networks", "year": 2018}, {"arxiv_id": "2009.07032", "title": "Noisy Self-Knowledge Distillation for Text Summarization", "year": 2020}], "gt_label": [1, 1], "date": "2020-09", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_278", "valid": true}
{"query": "Which work first popularized the use of neural fields as a method of representing 3D scenes and objects?", "cited_paper": [{"arxiv_id": "1906.01618", "title": "Scene Representation Networks: Continuous 3D-Structure-Aware Neural Scene Representations", "year": 2019}], "gt_label": [1], "date": "2019-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_279", "valid": true}
{"query": "Could you provide me with some studies on soft prompts?", "cited_paper": [{"arxiv_id": "2104.08691", "title": "The Power of Scale for Parameter-Efficient Prompt Tuning", "year": 2021}, {"arxiv_id": "2101.06804", "title": "What Makes Good In-Context Examples for GPT-$3$?", "year": 2021}], "gt_label": [1, 1], "date": "2021-04", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_280", "valid": true}
{"query": "Could you provide me some works on top-down approaches for instance segmentation?", "cited_paper": [{"arxiv_id": "1906.01140", "title": "Learning Object Bounding Boxes for 3D Instance Segmentation on Point Clouds", "year": 2019}, {"arxiv_id": "1812.03320", "title": "GSPN: Generative Shape Proposal Network for 3D Instance Segmentation in Point Cloud", "year": 2018}, {"arxiv_id": "1812.07003", "title": "3D-SIS: 3D Semantic Instance Segmentation of RGB-D Scans", "year": 2018}, {"arxiv_id": "1904.12012", "title": "RevealNet: Seeing Behind Objects in RGB-D Scans", "year": 2019}], "gt_label": [1, 1, 1, 1], "date": "2019-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_281", "valid": true}
{"query": "Which works propose the use of a smaller threshold and a bias shift to better match the activation in ANN-SNN conversion?", "cited_paper": [{"arxiv_id": "2003.01811", "title": "RMP-SNN: Residual Membrane Potential Neuron for Enabling Deeper High-Accuracy and Low-Latency Spiking Neural Network", "year": 2020}, {"arxiv_id": "2103.00476", "title": "Optimal Conversion of Conventional Artificial Neural Networks to Spiking Neural Networks", "year": 2021}], "gt_label": [1, 1], "date": "2021-03", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_282", "valid": true}
{"query": "Are there any papers where the tasks were modeled as sequence generation tasks within a unified paradigm?", "cited_paper": [{"arxiv_id": "2112.01522", "title": "Uni-Perceiver: Pre-training Unified Architecture for Generic Perception for Zero-shot and Few-shot Tasks", "year": 2021}, {"arxiv_id": "2202.03052", "title": "OFA: Unifying Architectures, Tasks, and Modalities Through a Simple Sequence-to-Sequence Learning Framework", "year": 2022}, {"arxiv_id": "2206.08916", "title": "Unified-IO: A Unified Model for Vision, Language, and Multi-Modal Tasks", "year": 2022}, {"arxiv_id": "2206.07669", "title": "A Unified Sequence Interface for Vision Tasks", "year": 2022}, {"arxiv_id": "2111.12085", "title": "UniTAB: Unifying Text and Box Outputs for Grounded Vision-Language Modeling", "year": 2021}], "gt_label": [1, 1, 1, 1, 1], "date": "2022-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_283", "valid": true}
{"query": "Who developed a triangular attention to deduce relations from other relations?", "cited_paper": [{"arxiv_id": "2112.00578", "title": "Systematic Generalization with Edge Transformers", "year": 2021}], "gt_label": [1], "date": "2021-12", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_284", "valid": true}
{"query": "What studies create datasets using templates?", "cited_paper": [{"arxiv_id": "2306.15895", "title": "Large Language Model as Attributed Training Data Generator: A Tale of Diversity and Bias", "year": 2023}], "gt_label": [1], "date": "2023-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_285", "valid": true}
{"query": "Which studies discuss adaptation for better generalization in model's optimization and training?", "cited_paper": [{"arxiv_id": "1301.2115", "title": "Domain Generalization via Invariant Feature Representation", "year": 2013}, {"arxiv_id": "2103.03097", "title": "Generalizing to Unseen Domains: A Survey on Domain Generalization", "year": 2021}], "gt_label": [1, 1], "date": "2021-03", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_286", "valid": true}
{"query": "Could you provide me some works that introduced residual networks?", "cited_paper": [{"arxiv_id": "1512.03385", "title": "Deep Residual Learning for Image Recognition", "year": 2015}], "gt_label": [1], "date": "2015-12", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_287", "valid": true}
{"query": "What research papers propose variants for regression learning that divide the regression range into small bins?", "cited_paper": [{"arxiv_id": "1611.01731", "title": "Deep Label Distribution Learning with Label Ambiguity", "year": 2016}], "gt_label": [1], "date": "2016-11", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_288", "valid": true}
{"query": "Which works used a pre-trained object detector to obtain region-of-interest (ROI) features from images?", "cited_paper": [{"arxiv_id": "1909.11740", "title": "UNITER: UNiversal Image-TExt Representation Learning", "year": 2019}, {"arxiv_id": "2004.06165", "title": "Oscar: Object-Semantics Aligned Pre-training for Vision-Language Tasks", "year": 2020}, {"arxiv_id": "2012.15409", "title": "UNIMO: Towards Unified-Modal Understanding and Generation via Cross-Modal Contrastive Learning", "year": 2020}, {"arxiv_id": "2107.14572", "title": "Product1M: Towards Weakly Supervised Instance-Level Product Retrieval via Cross-modal Pretraining", "year": 2021}], "gt_label": [1, 1, 1, 1], "date": "2021-07", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_289", "valid": true}
{"query": "Can you point out studies that applied hypernetworks to supervised learning?", "cited_paper": [{"arxiv_id": "2312.08399", "title": "Principled Weight Initialization for Hypernetworks", "year": 2023}], "gt_label": [1], "date": "2023-12", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_290", "valid": true}
{"query": "Could you name any works that utilized variance bounds of the local objective gradients to capture data heterogeneity?", "cited_paper": [{"arxiv_id": "1910.06378", "title": "SCAFFOLD: Stochastic Controlled Averaging for Federated Learning", "year": 2019}, {"arxiv_id": "2003.10422", "title": "A Unified Theory of Decentralized SGD with Changing Topology and Local Updates", "year": 2020}], "gt_label": [1, 1], "date": "2020-03", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_291", "valid": true}
{"query": "Which studies focus on monocular building height estimation using deep neural networks?", "cited_paper": [{"arxiv_id": "2112.14985", "title": "THE Benchmark: Transferable Representation Learning for Monocular Height Estimation", "year": 2021}], "gt_label": [1], "date": "2021-12", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_292", "valid": true}
{"query": "Which research focused on understanding the situations where models may obtain correct answers through unfaithful or spurious reasoning shortcuts?", "cited_paper": [], "gt_label": [], "date": "2022-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_293", "valid": true}
{"query": "What work proposed an efficient citation-based QA approach by fine-tuning much smaller LLMs?", "cited_paper": [{"arxiv_id": "2306.07906", "title": "WebGLM: Towards An Efficient Web-Enhanced Question Answering System with Human Preferences", "year": 2023}], "gt_label": [1], "date": "2023-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_294", "valid": true}
{"query": "Could you provide me some works in which fine-tuning was applied to learn new knowledge for model editing?", "cited_paper": [{"arxiv_id": "2012.00363", "title": "Modifying Memories in Transformer Models", "year": 2020}, {"arxiv_id": "2305.13172", "title": "Editing Large Language Models: Problems, Methods, and Opportunities", "year": 2023}], "gt_label": [1, 1], "date": "2023-05", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_295", "valid": true}
{"query": "Which works demonstrate the effectiveness of fine-tuning diffusion models for personalized image generation?", "cited_paper": [{"arxiv_id": "2106.09685", "title": "LoRA: Low-Rank Adaptation of Large Language Models", "year": 2021}, {"arxiv_id": "2208.12242", "title": "DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation", "year": 2022}], "gt_label": [1, 1], "date": "2022-08", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_296", "valid": true}
{"query": "Are there any studies that highlight the need of model simplification when working with causal models?", "cited_paper": [{"arxiv_id": "1707.00819", "title": "Causal Consistency of Structural Equation Models", "year": 2017}, {"arxiv_id": "2301.05893", "title": "Jointly Learning Consistent Causal Abstractions Over Multiple Interventional Distributions", "year": 2023}, {"arxiv_id": "2203.16437", "title": "Weakly supervised causal representation learning", "year": 2022}], "gt_label": [1, 1, 1], "date": "2023-01", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_297", "valid": true}
{"query": "Which works are about the application of orthogonal polynomials in machine learning?", "cited_paper": [{"arxiv_id": "2006.03860", "title": "Do RNN and LSTM have Long Memory?", "year": 2020}, {"arxiv_id": "1412.3555", "title": "Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling", "year": 2014}, {"arxiv_id": "0705.2011", "title": "Multi-Dimensional Recurrent Neural Networks", "year": 2007}], "gt_label": [1, 1, 1], "date": "2020-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_298", "valid": true}
{"query": "Which works carried out the developments in contrastive learning using the InfoNCE loss with augmented data pairs?", "cited_paper": [{"arxiv_id": "1807.03748", "title": "Representation Learning with Contrastive Predictive Coding", "year": 2018}, {"arxiv_id": "1906.05849", "title": "Contrastive Multiview Coding", "year": 2019}, {"arxiv_id": "2002.05709", "title": "A Simple Framework for Contrastive Learning of Visual Representations", "year": 2020}, {"arxiv_id": "1911.05722", "title": "Momentum Contrast for Unsupervised Visual Representation Learning", "year": 2019}, {"arxiv_id": "2110.15348", "title": "Residual Relaxation for Multi-view Representation Learning", "year": 2021}, {"arxiv_id": "2006.10029", "title": "Big Self-Supervised Models are Strong Semi-Supervised Learners", "year": 2020}, {"arxiv_id": "1910.08350", "title": "A Mutual Information Maximization Perspective of Language Representation Learning", "year": 2019}], "gt_label": [1, 1, 1, 1, 1, 1, 1], "date": "2021-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_299", "valid": true}
{"query": "Could you provide me some studies that develop BERT-based Transformers for code syntax?", "cited_paper": [{"arxiv_id": "2001.00059", "title": "Learning and Evaluating Contextual Embedding of Source Code", "year": 2020}, {"arxiv_id": "2002.08155", "title": "CodeBERT: A Pre-Trained Model for Programming and Natural Languages", "year": 2020}, {"arxiv_id": "1810.04805", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "year": 2018}, {"arxiv_id": "2009.08366", "title": "GraphCodeBERT: Pre-training Code Representations with Data Flow", "year": 2020}], "gt_label": [1, 1, 1, 1], "date": "2020-09", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_300", "valid": true}
{"query": "Can you name the papers that considered non-stationary RL under linear MDPs?", "cited_paper": [{"arxiv_id": "2010.12870", "title": "Efficient Learning in Non-Stationary Linear Markov Decision Processes", "year": 2020}], "gt_label": [1], "date": "2020-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_301", "valid": true}
{"query": "What papers suggested practical approaches to mitigate the oversmoothing problem in deep GNNs?", "cited_paper": [{"arxiv_id": "2007.02133", "title": "Simple and Deep Graph Convolutional Networks", "year": 2020}, {"arxiv_id": "1909.12223", "title": "PairNorm: Tackling Oversmoothing in GNNs", "year": 2019}, {"arxiv_id": "1810.05997", "title": "Predict then Propagate: Graph Neural Networks meet Personalized PageRank", "year": 2018}, {"arxiv_id": "1907.10903", "title": "DropEdge: Towards Deep Graph Convolutional Networks on Node Classification", "year": 2019}, {"arxiv_id": "1911.05485", "title": "Diffusion Improves Graph Learning", "year": 2019}], "gt_label": [1, 1, 1, 1, 1], "date": "2020-07", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_302", "valid": true}
{"query": "Does any work define multiple emergent tasks of interest to evaluate the performance of multi-modal LLMs?", "cited_paper": [{"arxiv_id": "2308.02490", "title": "MM-Vet: Evaluating Large Multimodal Models for Integrated Capabilities", "year": 2023}], "gt_label": [1], "date": "2023-08", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_303", "valid": true}
{"query": "In what papers does the researcher address models resilience against various distribution shifts?", "cited_paper": [{"arxiv_id": "2012.07421", "title": "WILDS: A Benchmark of in-the-Wild Distribution Shifts", "year": 2020}], "gt_label": [1], "date": "2020-12", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_304", "valid": true}
{"query": "Which studies are about establishing a functional mapping using eigenfunctions defined on surfaces?", "cited_paper": [{"arxiv_id": "2003.14286", "title": "Deep Geometric Functional Maps: Robust Feature Learning for Shape Correspondence", "year": 2020}], "gt_label": [1], "date": "2020-03", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_305", "valid": true}
{"query": "Which studies introduced the semi-dual approaches that best approximate the OT map?", "cited_paper": [{"arxiv_id": "2110.02999", "title": "Generative Modeling with Optimal Transport Maps", "year": 2021}], "gt_label": [1], "date": "2021-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_306", "valid": true}
{"query": "Which studies focus on augmenting the capability for MLLMs to follow visual instructions?", "cited_paper": [{"arxiv_id": "2304.08485", "title": "Visual Instruction Tuning", "year": 2023}, {"arxiv_id": "2305.06500", "title": "InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning", "year": 2023}, {"arxiv_id": "2305.15023", "title": "Cheap and Quick: Efficient Vision-Language Instruction Tuning for Large Language Models", "year": 2023}, {"arxiv_id": "2311.03079", "title": "CogVLM: Visual Expert for Pretrained Language Models", "year": 2023}, {"arxiv_id": "2304.14178", "title": "mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality", "year": 2023}], "gt_label": [1, 1, 1, 1, 1], "date": "2023-11", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_307", "valid": true}
{"query": "Any works evaluating the potential of using TEES for machine learning in terms of low computing performance in comparison to GPUs?", "cited_paper": [{"arxiv_id": "2110.01229", "title": "3LegRace: Privacy-Preserving DNN Training over TEEs and GPUs", "year": 2021}], "gt_label": [1], "date": "2021-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_308", "valid": true}
{"query": "In which paper the researchers introduced a distilling mechanism step-by-step, extracting LLM rationales as additional supervision for training small models within a multi-task framework?", "cited_paper": [{"arxiv_id": "2305.02301", "title": "Distilling Step-by-Step! Outperforming Larger Language Models with Less Training Data and Smaller Model Sizes", "year": 2023}], "gt_label": [1], "date": "2023-05", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_309", "valid": true}
{"query": "What are the works using supervised learning for AVS on AVSBench dataset?", "cited_paper": [{"arxiv_id": "2207.05042", "title": "Audio-Visual Segmentation", "year": 2022}, {"arxiv_id": "2310.08303", "title": "Multimodal Variational Auto-encoder based Audio-Visual Segmentation", "year": 2023}, {"arxiv_id": "2307.01146", "title": "AVSegFormer: Audio-Visual Segmentation with Transformer", "year": 2023}, {"arxiv_id": "2309.09709", "title": "CATR: Combinatorial-Dependence Audio-Queried Transformer for Audio-Visual Video Segmentation", "year": 2023}, {"arxiv_id": "2307.13236", "title": "Audio-aware Query-enhanced Transformer for Audio-Visual Segmentation", "year": 2023}, {"arxiv_id": "2307.16620", "title": "Audio-Visual Segmentation by Exploring Cross-Modal Mutual Semantics", "year": 2023}, {"arxiv_id": "2305.11019", "title": "Annotation-free Audio-Visual Segmentation", "year": 2023}], "gt_label": [1, 1, 1, 1, 1, 1, 1], "date": "2023-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_310", "valid": true}
{"query": "What papers introduced more precise retrieval by decomposing the problem into QDMR format?", "cited_paper": [{"arxiv_id": "2001.11770", "title": "Break It Down: A Question Understanding Benchmark", "year": 2020}], "gt_label": [1], "date": "2020-01", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_311", "valid": true}
{"query": "Which papers have explored the Strong Growth Condition (SGC) assumption to control the rates at which the stochastic gradients decay comparing to the full gradient?", "cited_paper": [{"arxiv_id": "1810.07288", "title": "Fast and Faster Convergence of SGD for Over-Parameterized Models and an Accelerated Perceptron", "year": 2018}], "gt_label": [1], "date": "2018-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_312", "valid": true}
{"query": "What papers focus on methods treating factual knowledge as subject-relation-object tuples for transformers?", "cited_paper": [{"arxiv_id": "2202.05262", "title": "Locating and Editing Factual Associations in GPT", "year": 2022}, {"arxiv_id": "2210.07229", "title": "Mass-Editing Memory in a Transformer", "year": 2022}], "gt_label": [1, 1], "date": "2022-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_313", "valid": true}
{"query": "Could you give me some studies that use an external camera or scene depth estimation for 3D pose estimation with volumetric heatmaps?", "cited_paper": [{"arxiv_id": "2201.07929", "title": "Estimating Egocentric 3D Human Pose in the Wild with External Weak Supervision", "year": 2022}, {"arxiv_id": "2212.11684", "title": "Scene-aware Egocentric 3D Human Pose Estimation", "year": 2022}], "gt_label": [1, 1], "date": "2022-12", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_314", "valid": true}
{"query": "What research has covered the topic of fine-tuning via bootstrapping models in Large Language Models?", "cited_paper": [{"arxiv_id": "2203.14465", "title": "STaR: Bootstrapping Reasoning With Reasoning", "year": 2022}, {"arxiv_id": "2206.14858", "title": "Solving Quantitative Reasoning Problems with Language Models", "year": 2022}], "gt_label": [1, 1], "date": "2022-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_315", "valid": true}
{"query": "What studies discuss readability as a feature for article quality?", "cited_paper": [{"arxiv_id": "2310.02235", "title": "Automatic Quality Assessment of Wikipedia Articles -- A Systematic Literature Review", "year": 2023}], "gt_label": [1], "date": "2023-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_316", "valid": true}
{"query": "Could you list some works discussing Countsketch-type operators used in tensor decomposition when A has Khatri-Rao product structure?", "cited_paper": [{"arxiv_id": "1506.04448", "title": "Fast and Guaranteed Tensor Decomposition via Sketching", "year": 2015}], "gt_label": [1], "date": "2015-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_317", "valid": true}
{"query": "Can you list any studies about the trade-off between model's standard accuracy and robustness?", "cited_paper": [{"arxiv_id": "1805.12152", "title": "Robustness May Be at Odds with Accuracy", "year": 2018}, {"arxiv_id": "1706.06083", "title": "Towards Deep Learning Models Resistant to Adversarial Attacks", "year": 2017}, {"arxiv_id": "2110.13771", "title": "AugMax: Adversarial Composition of Random Augmentations for Robust Training", "year": 2021}], "gt_label": [1, 1, 1], "date": "2021-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_318", "valid": true}
{"query": "Can you cite some works about the use of chain-of-thought to generate the reasoning process by LLMs?", "cited_paper": [{"arxiv_id": "2201.11903", "title": "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models", "year": 2022}, {"arxiv_id": "2203.11171", "title": "Self-Consistency Improves Chain of Thought Reasoning in Language Models", "year": 2022}, {"arxiv_id": "2205.03401", "title": "The Unreliability of Explanations in Few-shot Prompting for Textual Reasoning", "year": 2022}, {"arxiv_id": "2205.11916", "title": "Large Language Models are Zero-Shot Reasoners", "year": 2022}, {"arxiv_id": "2211.12588", "title": "Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks", "year": 2022}], "gt_label": [1, 1, 1, 1, 1], "date": "2022-11", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_319", "valid": true}
{"query": "Which works reassessed traditional benchmarks in light of the advent of Multimodal Large Language Models?", "cited_paper": [{"arxiv_id": "1405.0312", "title": "Microsoft COCO: Common Objects in Context", "year": 2014}, {"arxiv_id": "1906.00067", "title": "OK-VQA: A Visual Question Answering Benchmark Requiring External Knowledge", "year": 2019}, {"arxiv_id": "1902.09506", "title": "GQA: A New Dataset for Real-World Visual Reasoning and Compositional Question Answering", "year": 2019}], "gt_label": [1, 1, 1], "date": "2019-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_320", "valid": true}
{"query": "What work proposed utilizing a learned adversarial distribution as noise in NCE, and developed the Adversarial Contrastive Estimation method?", "cited_paper": [{"arxiv_id": "1805.03642", "title": "Adversarial Contrastive Estimation", "year": 2018}], "gt_label": [1], "date": "2018-05", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_321", "valid": true}
{"query": "Which work indicates a method for editing pre-existing images based on instructions?", "cited_paper": [{"arxiv_id": "2211.09800", "title": "InstructPix2Pix: Learning to Follow Image Editing Instructions", "year": 2022}], "gt_label": [1], "date": "2022-11", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_322", "valid": true}
{"query": "Which studies focus on target-agnostic zero-shot object navigation tasks?", "cited_paper": [{"arxiv_id": "2103.00020", "title": "Learning Transferable Visual Models From Natural Language Supervision", "year": 2021}, {"arxiv_id": "2202.02440", "title": "Zero Experience Required: Plug & Play Modular Transfer Learning for Semantic Visual Navigation", "year": 2022}, {"arxiv_id": "2206.07423", "title": "Zero-shot object goal visual navigation", "year": 2022}], "gt_label": [1, 1, 1], "date": "2022-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_323", "valid": true}
{"query": "What studies achieved the universal models by including dihedral angles or by introducing SO(3) equivariant models?", "cited_paper": [{"arxiv_id": "2102.05013", "title": "Spherical Message Passing for 3D Graph Networks", "year": 2021}, {"arxiv_id": "1802.08219", "title": "Tensor field networks: Rotationand translation-equivariant neural networks for 3D point clouds", "year": 2018}], "gt_label": [1, 1], "date": "2021-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_324", "valid": true}
{"query": "Which research papers propose deadlock-specialized methods to solve object navigation tasks?", "cited_paper": [{"arxiv_id": "1812.00971", "title": "Learning to Learn How to Learn: Self-Adaptive Visual Navigation Using Meta-Learning", "year": 2018}, {"arxiv_id": "2007.11018", "title": "Learning Object Relation Graph and Tentative Policy for Visual Navigation", "year": 2020}], "gt_label": [1, 1], "date": "2020-07", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_325", "valid": true}
{"query": "Are there any studies that use voxel method for segmenting 3D LiDAR point clouds?", "cited_paper": [{"arxiv_id": "1711.10275", "title": "3D Semantic Segmentation with Submanifold Sparse Convolutional Networks", "year": 2017}, {"arxiv_id": "1811.04337", "title": "VV-Net: Voxel VAE Net with Group Convolutions for Point Cloud Segmentation", "year": 2018}, {"arxiv_id": "2109.05441", "title": "Cylindrical and Asymmetrical 3D Convolution Networks for LiDAR-based Perception", "year": 2021}], "gt_label": [1, 1, 1], "date": "2021-09", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_326", "valid": true}
{"query": "What work investigates how human users perceive fairness and transparency in recommender systems?", "cited_paper": [{"arxiv_id": "2103.08786", "title": "Fairness and Transparency in Recommendation: The Users' Perspective", "year": 2021}], "gt_label": [1], "date": "2021-03", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_327", "valid": true}
{"query": "Which study proposes a benchmark that covers both image and video modality to evaluate the performance of multi-modal LLMs?", "cited_paper": [{"arxiv_id": "2307.16125", "title": "SEED-Bench: Benchmarking Multimodal LLMs with Generative Comprehension", "year": 2023}], "gt_label": [1], "date": "2023-07", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_328", "valid": true}
{"query": "What works have been conducted about Electronic Health Records (EHRs) in the field of multi-modal databases?", "cited_paper": [{"arxiv_id": "2205.01290", "title": "DrugEHRQA: A Question Answering Dataset on Structured and Unstructured Electronic Health Records For Medicine Related Queries", "year": 2022}], "gt_label": [1], "date": "2022-05", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_329", "valid": true}
{"query": "Which works investigated the lower bounds of SGD-RR in the quadratic case?", "cited_paper": [{"arxiv_id": "1908.00045", "title": "How Good is SGD with Random Shuffling?", "year": 2019}, {"arxiv_id": "2106.06880", "title": "Random Shuffling Beats SGD Only After Many Epochs on Ill-Conditioned Problems", "year": 2021}], "gt_label": [1, 1], "date": "2021-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_330", "valid": true}
{"query": "Could you provide me some studies about inferring both light transport and density in portrait and face relighting?", "cited_paper": [{"arxiv_id": "2107.12351", "title": "NeLF: Neural Light-transport Field for Portrait View Synthesis and Relighting", "year": 2021}], "gt_label": [1], "date": "2021-07", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_331", "valid": true}
{"query": "Can you provide examples of research that implemented unsupervised OOD detection methods?", "cited_paper": [{"arxiv_id": "1605.07717", "title": "Deep Structured Energy Based Models for Anomaly Detection", "year": 2016}, {"arxiv_id": "1810.09136", "title": "Do Deep Generative Models Know What They Don't Know?", "year": 2018}, {"arxiv_id": "1810.01392", "title": "WAIC, but Why? Generative Ensembles for Robust Anomaly Detection", "year": 2018}, {"arxiv_id": "1903.08689", "title": "Implicit Generation and Generalization in Energy-Based Models", "year": 2019}, {"arxiv_id": "1906.02845", "title": "Likelihood Ratios for Out-of-Distribution Detection", "year": 2019}, {"arxiv_id": "1909.11480", "title": "Input complexity and out-of-distribution detection with likelihood-based generative models", "year": 2019}, {"arxiv_id": "1912.03263", "title": "Your Classifier is Secretly an Energy Based Model and You Should Treat it Like One", "year": 2019}, {"arxiv_id": "2010.03759", "title": "Energy-based Out-of-distribution Detection", "year": 2020}, {"arxiv_id": "1605.08803", "title": "Density estimation using Real NVP", "year": 2016}, {"arxiv_id": "1703.05921", "title": "Unsupervised Anomaly Detection with Generative Adversarial Networks to Guide Marker Discovery", "year": 2017}, {"arxiv_id": "1807.02588", "title": "Generative Probabilistic Novelty Detection with Adversarial Autoencoders", "year": 2018}, {"arxiv_id": "1903.08550", "title": "OCGAN: One-class Novelty Detection Using GANs with Constrained Latent Representations", "year": 2019}, {"arxiv_id": "1810.01392", "title": "WAIC, but Why? Generative Ensembles for Robust Anomaly Detection", "year": 2018}, {"arxiv_id": "1805.10917", "title": "Deep Anomaly Detection Using Geometric Transformations", "year": 2018}, {"arxiv_id": "1906.12340", "title": "Using Self-Supervised Learning Can Improve Model Robustness and Uncertainty", "year": 2019}, {"arxiv_id": "2005.02359", "title": "Classification-Based Anomaly Detection for General Data", "year": 2020}, {"arxiv_id": "2007.08176", "title": "CSI: Novelty Detection via Contrastive Learning on Distributionally Shifted Instances", "year": 2020}, {"arxiv_id": "2203.10807", "title": "ViM: Out-Of-Distribution with Virtual-logit Matching", "year": 2022}], "gt_label": [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], "date": "2022-03", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_332", "valid": true}
{"query": "Can you provide me with some works that thoroughly used geometric insights on distances to surfaces in the context of classification?", "cited_paper": [{"arxiv_id": "1608.08967", "title": "Robustness of classifiers: from adversarial to random noise", "year": 2016}, {"arxiv_id": "1809.02104", "title": "Are adversarial examples inevitable?", "year": 2018}], "gt_label": [1, 1], "date": "2018-09", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_333", "valid": true}
{"query": "What study demonstrated that dividing complex problems into simpler sub-problems improved the performance of CoT prompting?", "cited_paper": [{"arxiv_id": "2205.10625", "title": "Least-to-Most Prompting Enables Complex Reasoning in Large Language Models", "year": 2022}], "gt_label": [1], "date": "2022-05", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_334", "valid": true}
{"query": "Which paper proposed to constrain the weights in linear layers as lower triangular matrices to speed up training?", "cited_paper": [{"arxiv_id": "1907.07945", "title": "MintNet: Building Invertible Neural Networks with Masked Convolutions", "year": 2019}], "gt_label": [1], "date": "2019-07", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_335", "valid": true}
{"query": "What are the cited examples of citation-based QA systems?", "cited_paper": [{"arxiv_id": "2112.09332", "title": "WebGPT: Browser-assisted question-answering with human feedback", "year": 2021}, {"arxiv_id": "2203.11147", "title": "Teaching language models to support answers with verified quotes", "year": 2022}, {"arxiv_id": "2306.07906", "title": "WebGLM: Towards An Efficient Web-Enhanced Question Answering System with Human Preferences", "year": 2023}], "gt_label": [1, 1, 1], "date": "2023-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_336", "valid": true}
{"query": "Could you provide me some studies about HMT methods focusing on monocular images?", "cited_paper": [{"arxiv_id": "1607.08128", "title": "Keep it SMPL: Automatic Estimation of 3D Human Pose and Shape from a Single Image", "year": 2016}, {"arxiv_id": "1712.06584", "title": "End-to-end Recovery of Human Shape and Pose", "year": 2017}, {"arxiv_id": "1904.05866", "title": "Expressive Body Capture: 3D Hands, Face, and Body from a Single Image", "year": 2019}, {"arxiv_id": "1812.01601", "title": "Learning 3D Human Dynamics from Video", "year": 2018}, {"arxiv_id": "1912.05656", "title": "VIBE: Video Inference for Human Body Pose and Shape Estimation", "year": 2019}, {"arxiv_id": "2104.08527", "title": "PARE: Part Attention Regressor for 3D Human Body Estimation", "year": 2021}, {"arxiv_id": "2303.18246", "title": "3D Human Pose Estimation via Intuitive Physics", "year": 2023}, {"arxiv_id": "2203.14065", "title": "Neural MoCon: Neural Motion Control for Physically Plausible Human Motion Capture", "year": 2022}, {"arxiv_id": "2305.08590", "title": "NIKI: Neural Inverse Kinematics with Invertible Neural Networks for 3D Human Pose and Shape Estimation", "year": 2023}], "gt_label": [1, 1, 1, 1, 1, 1, 1, 1, 1], "date": "2023-05", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_337", "valid": true}
{"query": "Which research papers investigated techniques for demonstration retrieval as part of improving in-context learning?", "cited_paper": [{"arxiv_id": "2112.08633", "title": "Learning To Retrieve Prompts for In-Context Learning", "year": 2021}, {"arxiv_id": "2211.04486", "title": "Active Example Selection for In-Context Learning", "year": 2022}], "gt_label": [1, 1], "date": "2022-11", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_338", "valid": true}
{"query": "What works used these loss functions to train a non-linear independent component estimation (NICE) model on high-dimensional tasks?", "cited_paper": [{"arxiv_id": "1905.07088", "title": "Sliced Score Matching: A Scalable Approach to Density and Score Estimation", "year": 2019}, {"arxiv_id": "2007.03317", "title": "Efficient Learning of Generative Models via Finite-Difference Score Matching", "year": 2020}], "gt_label": [1, 1], "date": "2020-07", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_339", "valid": true}
{"query": "Who designed extractive summarization approaches that adopt types of hierarchical architectures?", "cited_paper": [{"arxiv_id": "1905.06566", "title": "HIBERT: Document Level Pre-training of Hierarchical Bidirectional Transformers for Document Summarization", "year": 2019}, {"arxiv_id": "2010.08242", "title": "Unsupervised Extractive Summarization by Pre-training Hierarchical Transformers", "year": 2020}, {"arxiv_id": "2203.09629", "title": "HiStruct+: Improving Extractive Text Summarization with Hierarchical Structure Information", "year": 2022}, {"arxiv_id": "1810.04805", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "year": 2018}], "gt_label": [1, 1, 1, 1], "date": "2022-03", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_340", "valid": true}
{"query": "What are the papers about the Forgetting method that considers hard samples with the most forgetting events as important?", "cited_paper": [{"arxiv_id": "1812.05159", "title": "An Empirical Study of Example Forgetting during Deep Neural Network Learning", "year": 2018}], "gt_label": [1], "date": "2018-12", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_341", "valid": true}
{"query": "Which paper devised multiple tests to examine the faithfulness of CoT?", "cited_paper": [{"arxiv_id": "2307.13702", "title": "Measuring Faithfulness in Chain-of-Thought Reasoning", "year": 2023}], "gt_label": [1], "date": "2023-07", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_342", "valid": true}
{"query": "Which are the initial research papers on generative data augmentation?", "cited_paper": [{"arxiv_id": "1711.04340", "title": "Data Augmentation Generative Adversarial Networks", "year": 2017}, {"arxiv_id": "1701.07717", "title": "Unlabeled Samples Generated by GAN Improve the Person Re-identification Baseline in vitro", "year": 2017}], "gt_label": [1, 1], "date": "2017-11", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_343", "valid": true}
{"query": "Which researcher extended the classifier approach of CCMI by applying it directly to the estimation of CMI?", "cited_paper": [{"arxiv_id": "2006.07225", "title": "Neural Estimators for Conditional Mutual Information Using Nearest Neighbors Sampling", "year": 2020}], "gt_label": [1], "date": "2020-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_344", "valid": true}
{"query": "What research introduced L2SP, a simple regularization method which minimizes the parameters between source and target models during fine-tuning?", "cited_paper": [{"arxiv_id": "1802.01483", "title": "Explicit Inductive Bias for Transfer Learning with Convolutional Networks", "year": 2018}], "gt_label": [1], "date": "2018-02", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_345", "valid": true}
{"query": "What research introduces OOD detection?", "cited_paper": [{"arxiv_id": "2110.11334", "title": "Generalized Out-of-Distribution Detection: A Survey", "year": 2021}], "gt_label": [1], "date": "2021-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_346", "valid": true}
{"query": "What research initially focused on content moderation in online social media platforms like Twitter and Reddit?", "cited_paper": [{"arxiv_id": "2005.04790", "title": "The Hateful Memes Challenge: Detecting Hate Speech in Multimodal Memes", "year": 2020}, {"arxiv_id": "2106.05664", "title": "Ruddit: Norms of Offensiveness for English Reddit Comments", "year": 2021}, {"arxiv_id": "1903.08983", "title": "SemEval-2019 Task 6: Identifying and Categorizing Offensive Language in Social Media (OffensEval)", "year": 2019}], "gt_label": [1, 1, 1], "date": "2021-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_347", "valid": true}
{"query": "Which works have been proposed for reducing the 3D annotation demand via weakly-supervised or semi-supervised learning in monocular 3D reconstruction?", "cited_paper": [{"arxiv_id": "1908.05293", "title": "Multiview-Consistent Semi-Supervised Learning for 3D Human Pose Estimation", "year": 2019}, {"arxiv_id": "2106.09614", "title": "Robust Model-based Face Reconstruction through Weakly-Supervised Outlier Segmentation", "year": 2021}, {"arxiv_id": "1908.02126", "title": "Semi-Supervised Adversarial Monocular Depth Estimation", "year": 2019}], "gt_label": [1, 1, 1], "date": "2021-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_348", "valid": true}
{"query": "What papers involve the use of an encoder-decoder network for height value regression?", "cited_paper": [{"arxiv_id": "2301.04581", "title": "Elevation Estimation-Driven Building 3D Reconstruction from Single-View Remote Sensing Imagery", "year": 2023}], "gt_label": [1], "date": "2023-01", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_349", "valid": true}
{"query": "In what work is SAM, a model that can segment any object of a given image based on visual prompts such as points and boxes, proposed?", "cited_paper": [{"arxiv_id": "2401.14159", "title": "Grounded SAM: Assembling Open-World Models for Diverse Visual Tasks", "year": 2024}], "gt_label": [1], "date": "2024-01", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_350", "valid": true}
{"query": "What works apply reinforcement learning to rule learning by training agents to search for paths in the knowledge graph?", "cited_paper": [{"arxiv_id": "1707.06690", "title": "DeepPath: A Reinforcement Learning Method for Knowledge Graph Reasoning", "year": 2017}, {"arxiv_id": "1803.06581", "title": "Variational Knowledge Graph Reasoning", "year": 2018}, {"arxiv_id": "1711.05851", "title": "Go for a Walk and Arrive at the Answer: Reasoning Over Paths in Knowledge Bases using Reinforcement Learning", "year": 2017}, {"arxiv_id": "1808.10568", "title": "Multi-Hop Knowledge Graph Reasoning with Reward Shaping", "year": 2018}, {"arxiv_id": "1802.04394", "title": "M-Walk: Learning to Walk over Graphs using Monte Carlo Tree Search", "year": 2018}], "gt_label": [1, 1, 1, 1, 1], "date": "2018-08", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_351", "valid": true}
{"query": "Could you give me examples of research on distributing matrix multiplication or more general, polynomial function computation?", "cited_paper": [{"arxiv_id": "1705.10464", "title": "Polynomial Codes: an Optimal Design for High-Dimensional Coded Matrix Multiplication", "year": 2017}, {"arxiv_id": "1512.02673", "title": "Speeding Up Distributed Machine Learning Using Codes", "year": 2015}, {"arxiv_id": "1901.10674", "title": "Universally Decodable Matrices for Distributed Matrix-Vector Multiplication", "year": 2019}, {"arxiv_id": "1907.05965", "title": "Random Khatri-Rao-Product Codes for Numerically-Stable Distributed Matrix Multiplication", "year": 2019}, {"arxiv_id": "1806.00939", "title": "Lagrange Coded Computing: Optimal Design for Resiliency, Security and Privacy", "year": 2018}, {"arxiv_id": "1801.07487", "title": "Straggler Mitigation in Distributed Matrix Multiplication: Fundamental Limits and Optimal Coding", "year": 2018}, {"arxiv_id": "2002.03515", "title": "Straggler-resistant distributed matrix computation via coding theory", "year": 2020}], "gt_label": [1, 1, 1, 1, 1, 1, 1], "date": "2020-02", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_352", "valid": true}
{"query": "Which papers incorporated task-inference methods in their research?", "cited_paper": [{"arxiv_id": "1905.06424", "title": "Meta reinforcement learning as task inference", "year": 2019}, {"arxiv_id": "1910.08348", "title": "VariBAD: A Very Good Method for Bayes-Adaptive Deep RL via Meta-Learning", "year": 2019}, {"arxiv_id": "2005.02934", "title": "Learning Adaptive Exploration Strategies in Dynamic Environments Through Informed Policy Regularization", "year": 2020}, {"arxiv_id": "2008.02790", "title": "Decoupling Exploration and Exploitation for Meta-Reinforcement Learning without Sacrifices", "year": 2020}, {"arxiv_id": "2210.11348", "title": "Hypernetworks in Meta-Reinforcement Learning", "year": 2022}], "gt_label": [1, 1, 1, 1, 1], "date": "2022-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_353", "valid": true}
{"query": "Which works are based on Inverse Dynamic Models (IDMs) in the field of imitation learning from observations?", "cited_paper": [{"arxiv_id": "1805.01954", "title": "Behavioral Cloning from Observation", "year": 2018}, {"arxiv_id": "2206.11795", "title": "Video PreTraining (VPT): Learning to Act by Watching Unlabeled Online Videos", "year": 2022}, {"arxiv_id": "1703.02018", "title": "Combining Self-Supervised Learning and Imitation for Vision-Based Rope Manipulation", "year": 2017}, {"arxiv_id": "1804.08606", "title": "Zero-Shot Visual Imitation", "year": 2018}], "gt_label": [1, 1, 1, 1], "date": "2022-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_354", "valid": true}
{"query": "What works have proposed node-to-graph Graph Contrastive Learning methods?", "cited_paper": [{"arxiv_id": "1809.10341", "title": "Deep Graph Infomax", "year": 2018}, {"arxiv_id": "2006.05582", "title": "Contrastive Multi-View Representation Learning on Graphs", "year": 2020}], "gt_label": [1, 1], "date": "2020-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_355", "valid": true}
{"query": "What research showed how underlying generative factors can be recovered given certain conditions?", "cited_paper": [{"arxiv_id": "2106.05200", "title": "Independent mechanism analysis, a new concept?", "year": 2021}], "gt_label": [1], "date": "2021-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_356", "valid": true}
{"query": "What papers are about the extensions to NeRF for better image encoding?", "cited_paper": [{"arxiv_id": "2012.02190", "title": "pixelNeRF: Neural Radiance Fields from One or Few Images", "year": 2020}], "gt_label": [1], "date": "2020-12", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_357", "valid": true}
{"query": "Are there any works about low-rank models applied for matrix completion?", "cited_paper": [{"arxiv_id": "1107.0789", "title": "Distributed Matrix Completion and Robust Factorization", "year": 2011}], "gt_label": [1], "date": "2011-07", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_358", "valid": true}
{"query": "What works applied variational autoencoders to compressing various data modalities like video or point clouds?", "cited_paper": [{"arxiv_id": "1812.00101", "title": "DVC: An End-to-end Deep Video Compression Framework", "year": 2018}, {"arxiv_id": "2204.12684", "title": "Density-preserving Deep Point Cloud Compression", "year": 2022}], "gt_label": [1, 1], "date": "2022-04", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_359", "valid": true}
{"query": "Which papers discuss the high cost of sample-based explanations that require repeated retraining?", "cited_paper": [{"arxiv_id": "1908.08619", "title": "Efficient Task-Specific Data Valuation for Nearest Neighbor Algorithms", "year": 2019}, {"arxiv_id": "2110.14049", "title": "Beta Shapley: a Unified and Noise-reduced Data Valuation Framework for Machine Learning", "year": 2021}, {"arxiv_id": "2008.03703", "title": "What Neural Networks Memorize and Why: Discovering the Long Tail via Influence Estimation", "year": 2020}], "gt_label": [1, 1, 1], "date": "2021-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_360", "valid": true}
{"query": "Are there any papers related to the application of Principal component analysis (PCA) to re-weighted samples?", "cited_paper": [{"arxiv_id": "0804.3575", "title": "Isotropic PCA and Affine-Invariant Clustering", "year": 2008}, {"arxiv_id": "1306.5825", "title": "Fourier PCA and Robust Tensor Decomposition", "year": 2013}], "gt_label": [1, 1], "date": "2013-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_361", "valid": true}
{"query": "What research papers mention the use of 3D attention-based models for instance segmentation?", "cited_paper": [{"arxiv_id": "2211.15766", "title": "Superpoint Transformer for 3D Scene Instance Segmentation", "year": 2022}], "gt_label": [1], "date": "2022-11", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_362", "valid": true}
{"query": "What research proposes the use of a meta-network to generate an image-conditioned prompt?", "cited_paper": [{"arxiv_id": "2203.05557", "title": "Conditional Prompt Learning for Vision-Language Models", "year": 2022}], "gt_label": [1], "date": "2022-03", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_363", "valid": true}
{"query": "Which work extended the Sinkhorn method to address the problems in the field of UOT?", "cited_paper": [{"arxiv_id": "2002.03293", "title": "On Unbalanced Optimal Transport: An Analysis of Sinkhorn Algorithm", "year": 2020}], "gt_label": [1], "date": "2020-02", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_364", "valid": true}
{"query": "Could you provide me some works that developed different variants of memory augmented networks?", "cited_paper": [{"arxiv_id": "2002.03519", "title": "Self-Attentive Associative Memory", "year": 2020}, {"arxiv_id": "1806.01822", "title": "Relational recurrent neural networks", "year": 2018}, {"arxiv_id": "1410.5401", "title": "Neural Turing Machines", "year": 2014}, {"arxiv_id": "1906.08862", "title": "Neural Stored-program Memory", "year": 2019}], "gt_label": [1, 1, 1, 1], "date": "2020-02", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_365", "valid": true}
{"query": "What studies utilized deep learning methods in recent advances of anomaly detection (AD)?", "cited_paper": [{"arxiv_id": "2009.11732", "title": "A Unifying Review of Deep and Shallow Anomaly Detection", "year": 2020}], "gt_label": [1], "date": "2020-09", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_366", "valid": true}
{"query": "Which papers provide studies about causative attacks on GNNs to reduce the accuracy of or intentionally change the outcome of node classification, link prediction, and graph classification?", "cited_paper": [{"arxiv_id": "1809.01093", "title": "Adversarial Attacks on Node Embeddings via Graph Poisoning", "year": 2018}, {"arxiv_id": "2006.05057", "title": "Towards More Practical Adversarial Attacks on Graph Neural Networks", "year": 2020}, {"arxiv_id": "1906.04214", "title": "Topology Attack and Defense for Graph Neural Networks: An Optimization Perspective", "year": 2019}, {"arxiv_id": "1809.01093", "title": "Adversarial Attacks on Node Embeddings via Graph Poisoning", "year": 2018}, {"arxiv_id": "1806.02371", "title": "Adversarial Attack on Graph Structured Data", "year": 2018}, {"arxiv_id": "2006.11890", "title": "Graph Backdoor", "year": 2020}], "gt_label": [1, 1, 1, 1, 1, 1], "date": "2020-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_367", "valid": true}
{"query": "Which studies propose the use of intermediate domains to mitigate domain shift?", "cited_paper": [{"arxiv_id": "2207.09778", "title": "CoSMix: Compositional Semantic Mix for Domain Adaptation in 3D LiDAR Segmentation", "year": 2022}, {"arxiv_id": "2204.01599", "title": "DODA: Data-oriented Sim-to-Real Domain Adaptation for 3D Semantic Segmentation", "year": 2022}], "gt_label": [1, 1], "date": "2022-07", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_368", "valid": true}
{"query": "Are there any research contributions to the study of the noun-based Referring Expression Comprehension model?", "cited_paper": [{"arxiv_id": "1906.01784", "title": "Learning to Compose and Reason with Language Tree Structures for Visual Grounding", "year": 2019}, {"arxiv_id": "1611.09978", "title": "Modeling Relationships in Referential Expressions with Compositional Modular Networks", "year": 2016}, {"arxiv_id": "1812.03299", "title": "Learning to Assemble Neural Module Tree Networks for Visual Grounding", "year": 2018}, {"arxiv_id": "1704.03470", "title": "Learning Two-Branch Neural Networks for Image-Text Matching Tasks", "year": 2017}, {"arxiv_id": "1909.08164", "title": "Dynamic Graph Attention for Referring Expression Comprehension", "year": 2019}, {"arxiv_id": "1801.08186", "title": "MAttNet: Modular Attention Network for Referring Expression Comprehension", "year": 2018}, {"arxiv_id": "1712.01892", "title": "Grounding Referring Expressions in Images by Variational Context", "year": 2017}, {"arxiv_id": "1711.06370", "title": "Parallel Attention: A Unified Framework for Visual Object Discovery through Dialogs and Queries", "year": 2017}, {"arxiv_id": "1812.03426", "title": "Real-Time Referring Expression Comprehension by Single-Stage Grounding Network", "year": 2018}, {"arxiv_id": "1909.07072", "title": "A Real-Time Cross-modality Correlation Filtering Method for Referring Expression Comprehension", "year": 2019}, {"arxiv_id": "1908.07129", "title": "Zero-Shot Grounding of Objects from Natural Language Queries", "year": 2019}, {"arxiv_id": "2008.01059", "title": "Improving One-stage Visual Grounding by Recursive Sub-query Construction", "year": 2020}, {"arxiv_id": "1908.06354", "title": "A Fast and Accurate One-Stage Approach to Visual Grounding", "year": 2019}], "gt_label": [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], "date": "2020-08", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_369", "valid": true}
{"query": "What works are part of the 'BLIP-family' models that adopted both ITC and ITM as training objectives?", "cited_paper": [{"arxiv_id": "2202.10401", "title": "Vision-Language Pre-Training with Triple Contrastive Learning", "year": 2022}, {"arxiv_id": "2111.08276", "title": "Multi-Grained Vision Language Pre-Training: Aligning Texts with Visual Concepts", "year": 2021}, {"arxiv_id": "2208.04060", "title": "GRIT-VLP: Grouped Mini-batch Sampling for Efficient Vision and Language Pre-training", "year": 2022}, {"arxiv_id": "2201.12086", "title": "BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation", "year": 2022}, {"arxiv_id": "2307.08504", "title": "BUS:Efficient and Effective Vision-language Pre-training with Bottom-Up Patch Summarization", "year": 2023}, {"arxiv_id": "2301.12597", "title": "BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models", "year": 2023}, {"arxiv_id": "2307.07063", "title": "Bootstrapping Vision-Language Learning with Decoupled Language Pre-training", "year": 2023}], "gt_label": [1, 1, 1, 1, 1, 1, 1], "date": "2023-07", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_370", "valid": true}
{"query": "What research papers applied attention mechanisms for single-task based deblurring?", "cited_paper": [{"arxiv_id": "2111.09881", "title": "Restormer: Efficient Transformer for High-Resolution Image Restoration", "year": 2021}], "gt_label": [1], "date": "2021-11", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_371", "valid": true}
{"query": "Could you provide me some works about mitigating simplicity bias?", "cited_paper": [{"arxiv_id": "2105.05612", "title": "Evading the Simplicity Bias: Training a Diverse Set of Models Discovers Solutions with Superior OOD Generalization", "year": 2021}], "gt_label": [1], "date": "2021-05", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_372", "valid": true}
{"query": "Are there any works debating the correlation between high attention weights and high feature importance in attention-based methods?", "cited_paper": [{"arxiv_id": "1906.03731", "title": "Is Attention Interpretable?", "year": 2019}], "gt_label": [1], "date": "2019-08", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_373", "valid": true}
{"query": "Can you mention some papers that incorporate self-attention module for image restoration due to the limited ability of CNN to model long-range dependencies?", "cited_paper": [{"arxiv_id": "2006.04139", "title": "Learning Texture Transformer Network for Image Super-Resolution", "year": 2020}, {"arxiv_id": "2108.10257", "title": "SwinIR: Image Restoration Using Swin Transformer", "year": 2021}, {"arxiv_id": "2111.09881", "title": "Restormer: Efficient Transformer for High-Resolution Image Restoration", "year": 2021}, {"arxiv_id": "2012.00364", "title": "Pre-Trained Image Processing Transformer", "year": 2020}], "gt_label": [1, 1, 1, 1], "date": "2021-11", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_374", "valid": true}
{"query": "Could you list some studies in speech tokenization that focus on semantic tokens with high correlation with phonemes?", "cited_paper": [{"arxiv_id": "2112.08352", "title": "Textless Speech-to-Speech Translation on Real Data", "year": 2021}, {"arxiv_id": "2106.07447", "title": "HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units", "year": 2021}, {"arxiv_id": "2108.06209", "title": "W2v-BERT: Combining Contrastive Learning and Masked Language Modeling for Self-Supervised Speech Pre-Training", "year": 2021}], "gt_label": [1, 1, 1], "date": "2021-12", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_375", "valid": true}
{"query": "Are there any works that propose models to solve Task-oriented Object detection and segmentation tasks?", "cited_paper": [{"arxiv_id": "2210.10775", "title": "TOIST: Task Oriented Instance Segmentation Transformer with Noun-Pronoun Distillation", "year": 2022}], "gt_label": [1], "date": "2022-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_376", "valid": true}
{"query": "Which studies addressed pretraining legal language models for Italian, Romanian, and Spanish?", "cited_paper": [{"arxiv_id": "2110.12201", "title": "Spanish Legalese Language Model and Corpora", "year": 2021}], "gt_label": [1], "date": "2021-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_377", "valid": true}
{"query": "What work proposed a similar PAC-Bayes framework where the KL divergence is replaced by a general family of Integral Probability Metrics?", "cited_paper": [{"arxiv_id": "2207.00614", "title": "Integral Probability Metrics PAC-Bayes Bounds", "year": 2022}], "gt_label": [1], "date": "2022-07", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_378", "valid": true}
{"query": "What works use traditional machine translation data like parallel data from the web for fine-tuning LLMs?", "cited_paper": [{"arxiv_id": "2305.18098", "title": "BigTranslate: Augmenting Large Language Models with Multilingual Translation Capability over 100 Languages", "year": 2023}, {"arxiv_id": "2310.13448", "title": "Steering Large Language Models for Machine Translation with Finetuning and In-Context Learning", "year": 2023}, {"arxiv_id": "2308.04948", "title": "Extrapolating Large Language Models to Non-English by Aligning Languages", "year": 2023}], "gt_label": [1, 1, 1], "date": "2023-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_379", "valid": true}
{"query": "Which work employs masked image modeling for in-context training in computer vision?", "cited_paper": [{"arxiv_id": "2212.02499", "title": "Images Speak in Images: A Generalist Painter for In-Context Visual Learning", "year": 2022}], "gt_label": [1], "date": "2022-12", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_380", "valid": true}
{"query": "What papers propose datasets that focus on testing the ability of visual question-answering (VQA) models to answer 'counterfactual' questions?", "cited_paper": [{"arxiv_id": "1712.00377", "title": "Don't Just Assume; Look and Answer: Overcoming Priors for Visual Question Answering", "year": 2017}], "gt_label": [1], "date": "2020-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_381", "valid": true}
{"query": "Can you provide the works about the large-scale web data containing billions of image-text pairs?", "cited_paper": [{"arxiv_id": "2210.08402", "title": "LAION-5B: An open large-scale dataset for training next generation image-text models", "year": 2022}, {"arxiv_id": "2102.08981", "title": "Conceptual 12M: Pushing Web-Scale Image-Text Pre-Training To Recognize Long-Tail Visual Concepts", "year": 2021}], "gt_label": [1, 1], "date": "2022-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_382", "valid": true}
{"query": "Which papers introduce generative models for LiDAR scene creation?", "cited_paper": [{"arxiv_id": "1812.01180", "title": "Deep Generative Modeling of LiDAR Data", "year": 2018}], "gt_label": [1], "date": "2018-12", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_383", "valid": true}
{"query": "Could you name some concurrent tuning-free methods for personalized visual content generation which use an image encoder for accessibility?", "cited_paper": [{"arxiv_id": "2305.10431", "title": "FastComposer: Tuning-Free Multi-Subject Image Generation with Localized Attention", "year": 2023}, {"arxiv_id": "2306.06638", "title": "Face0: Instantaneously Conditioning a Text-to-Image Model on a Face", "year": 2023}, {"arxiv_id": "2307.11410", "title": "Subject-Diffusion:Open Domain Personalized Text-to-Image Generation without Test-time Fine-tuning", "year": 2023}], "gt_label": [1, 1, 1], "date": "2023-07", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_384", "valid": true}
{"query": "Which papers analyze the convergence of Minibatch RR and Local RR, a variant of Minibatch SGD and Local SGD in Federated Learning?", "cited_paper": [{"arxiv_id": "2110.10342", "title": "Minibatch vs Local SGD with Shuffling: Tight Convergence Bounds and Beyond", "year": 2021}], "gt_label": [1], "date": "2021-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_385", "valid": true}
{"query": "What research papers used Bayesian approaches and evidential neural networks to quantify uncertainty in neural networks?", "cited_paper": [{"arxiv_id": "1703.04977", "title": "What Uncertainties Do We Need in Bayesian Deep Learning for Computer Vision?", "year": 2017}, {"arxiv_id": "1806.01768", "title": "Evidential Deep Learning to Quantify Classification Uncertainty", "year": 2018}, {"arxiv_id": "1910.02600", "title": "Deep Evidential Regression", "year": 2019}], "gt_label": [1, 1, 1], "date": "2019-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_386", "valid": true}
{"query": "Any papers that utilized the self-supervised training of transformer for vision tasks?", "cited_paper": [{"arxiv_id": "1810.04805", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "year": 2018}], "gt_label": [1], "date": "2018-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_387", "valid": true}
{"query": "Which works focus on generating code directly from natural language descriptions?", "cited_paper": [{"arxiv_id": "2002.08155", "title": "CodeBERT: A Pre-Trained Model for Programming and Natural Languages", "year": 2020}, {"arxiv_id": "2107.03374", "title": "Evaluating Large Language Models Trained on Code", "year": 2021}, {"arxiv_id": "2005.14165", "title": "Language Models are Few-Shot Learners", "year": 2020}], "gt_label": [1, 1, 1], "date": "2022-03", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_388", "valid": true}
{"query": "What studies improved GCL by introducing adaptive masking and dropping rate related to node centrality?", "cited_paper": [{"arxiv_id": "2010.14945", "title": "Graph Contrastive Learning with Adaptive Augmentation", "year": 2020}], "gt_label": [1], "date": "2020-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_389", "valid": true}
{"query": "Which works focus on 3D shape generation using variational autoencoders?", "cited_paper": [{"arxiv_id": "1712.07262", "title": "FoldingNet: Point Cloud Auto-encoder via Deep Grid Deformation", "year": 2017}, {"arxiv_id": "2103.15619", "title": "SetVAE: Learning Hierarchical Composition for Generative Modeling of Set-Structured Data", "year": 2021}], "gt_label": [1, 1], "date": "2021-03", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_390", "valid": true}
{"query": "Which studies proposed methods to introduce hard orthogonality?", "cited_paper": [{"arxiv_id": "2002.01113", "title": "Efficient Riemannian Optimization on the Stiefel Manifold via the Cayley Transform", "year": 2020}, {"arxiv_id": "1709.06079", "title": "Orthogonal Weight Normalization: Solution to Optimization over Multiple Dependent Stiefel Manifolds in Deep Neural Networks", "year": 2017}, {"arxiv_id": "2104.07167", "title": "Orthogonalizing Convolutional Layers with the Cayley Transform", "year": 2021}, {"arxiv_id": "2105.11417", "title": "Skew Orthogonal Convolutions", "year": 2021}, {"arxiv_id": "1805.10965", "title": "Lipschitz regularity of deep neural networks: analysis and efficient estimation", "year": 2018}], "gt_label": [1, 1, 1, 1, 1], "date": "2021-05", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_391", "valid": true}
{"query": "Which work provided a lower bound theory for general activation functions?", "cited_paper": [{"arxiv_id": "2006.08859", "title": "Minimum Width for Universal Approximation", "year": 2020}], "gt_label": [1], "date": "2020-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_392", "valid": true}
{"query": "Which works contribute to the advancements in vector quantization for efficient coding of information?", "cited_paper": [{"arxiv_id": "1711.00937", "title": "Neural Discrete Representation Learning", "year": 2017}, {"arxiv_id": "1710.09064", "title": "End-to-End Optimized Speech Coding with Deep Neural Networks", "year": 2017}, {"arxiv_id": "1910.06464", "title": "Low Bit-Rate Speech Coding with VQ-VAE and a WaveNet Decoder", "year": 2019}, {"arxiv_id": "2203.17263", "title": "Audio-Visual Speech Codecs: Rethinking Audio-Visual Speech Enhancement by Re-Synthesis", "year": 2022}, {"arxiv_id": "2107.03312", "title": "SoundStream: An End-to-End Neural Audio Codec", "year": 2021}], "gt_label": [1, 1, 1, 1, 1], "date": "2022-03", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_393", "valid": true}
{"query": "Could you provide me with studies that used LLMs to generate queries from a document in information retrieval?", "cited_paper": [{"arxiv_id": "2202.05144", "title": "InPars: Data Augmentation for Information Retrieval using Large Language Models", "year": 2022}], "gt_label": [1], "date": "2022-02", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_394", "valid": true}
{"query": "Which work introduced an intermediate representation to enable the weakly-supervised training for hand pose estimation?", "cited_paper": [{"arxiv_id": "1511.06728", "title": "Hand Pose Estimation through Semi-Supervised and Weakly-Supervised Learning", "year": 2015}], "gt_label": [1], "date": "2015-11", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_395", "valid": true}
{"query": "Can you give examples of studies where new knowledge base was connected with the base model to implement a retrieve for needed new knowledge to a prompt or a question?", "cited_paper": [{"arxiv_id": "2211.03318", "title": "Fixing Model Bugs with Natural Language Patches", "year": 2022}, {"arxiv_id": "2206.06520", "title": "Memory-Based Model Editing at Scale", "year": 2022}, {"arxiv_id": "2211.05110", "title": "Large Language Models with Controllable Working Memory", "year": 2022}, {"arxiv_id": "2201.06009", "title": "Memory-assisted prompt editing to improve GPT-3 after deployment", "year": 2022}], "gt_label": [1, 1, 1, 1], "date": "2022-11", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_396", "valid": true}
{"query": "Which researches have studied sarcasm in NLP?", "cited_paper": [{"arxiv_id": "1704.05579", "title": "A Large Self-Annotated Corpus for Sarcasm", "year": 2017}], "gt_label": [1], "date": "2017-04", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_397", "valid": true}
{"query": "What work suggests that direct pixel space parameterization is a key factor for the architecture transferability issue and proposes GLaD for enhancing generalization across any distillation method?", "cited_paper": [{"arxiv_id": "2305.01649", "title": "Generalizing Dataset Distillation via Deep Generative Prior", "year": 2023}], "gt_label": [1], "date": "2023-05", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_398", "valid": true}
{"query": "What papers discuss research on post-hoc watermarking techniques involving synonym replacement or paraphrasing?", "cited_paper": [{"arxiv_id": "2305.05773", "title": "DeepTextMark: A Deep Learning-Driven Text Watermarking Approach for Identifying Large Language Model Generated Text", "year": 2023}], "gt_label": [1], "date": "2023-05", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_399", "valid": true}
{"query": "Could you provide me some works that studied spatial reasoning limitations in generative VLMs?", "cited_paper": [{"arxiv_id": "2212.10015", "title": "Benchmarking Spatial Relationships in Text-to-Image Generation", "year": 2022}, {"arxiv_id": "2202.04053", "title": "DALL-Eval: Probing the Reasoning Skills and Social Biases of Text-to-Image Generation Models", "year": 2022}], "gt_label": [1, 1], "date": "2022-12", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_400", "valid": true}
{"query": "What studies applied TracIn for measuring training data importance?", "cited_paper": [{"arxiv_id": "2002.08484", "title": "Estimating Training Data Influence by Tracing Gradient Descent", "year": 2020}], "gt_label": [1], "date": "2021-02", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_401", "valid": true}
{"query": "What research proposes a method for complex edits such as geometry deformation and texture swapping, filling, and painting in NeRF editing?", "cited_paper": [{"arxiv_id": "2207.11911", "title": "NeuMesh: Learning Disentangled Neural Mesh-based Implicit Field for Geometry and Texture Editing", "year": 2022}], "gt_label": [1], "date": "2022-07", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_402", "valid": true}
{"query": "What studies have focused on adapting foundation models, specifically through techniques such as prompt-tuning or fine-tuning with residual connections?", "cited_paper": [{"arxiv_id": "2109.01134", "title": "Learning to Prompt for Vision-Language Models", "year": 2021}], "gt_label": [1], "date": "2021-09", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_403", "valid": true}
{"query": "Which work generated images containing specific text based on a large number of image-text pairs to improve the text generation capability of diffusion models?", "cited_paper": [{"arxiv_id": "2212.10562", "title": "Character-Aware Models Improve Visual Text Rendering", "year": 2022}], "gt_label": [1], "date": "2022-12", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_404", "valid": true}
{"query": "Which works have used ensemble predictions from clients models on an unlabeled dataset to guide the training of the server model?", "cited_paper": [{"arxiv_id": "2009.01974", "title": "FedBE: Making Bayesian Model Ensemble Applicable to Federated Learning", "year": 2020}, {"arxiv_id": "1910.03581", "title": "FedMD: Heterogenous Federated Learning via Model Distillation", "year": 2019}], "gt_label": [1, 1], "date": "2020-09", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_405", "valid": true}
{"query": "Can you provide me with research that focused on manipulating LLMs internal representations to guide them towards factuality during inference-time intervention?", "cited_paper": [{"arxiv_id": "2306.03341", "title": "Inference-Time Intervention: Eliciting Truthful Answers from a Language Model", "year": 2023}, {"arxiv_id": "2309.03883", "title": "DoLa: Decoding by Contrasting Layers Improves Factuality in Large Language Models", "year": 2023}, {"arxiv_id": "2210.15097", "title": "Contrastive Decoding: Open-ended Text Generation as Optimization", "year": 2022}, {"arxiv_id": "2312.15710", "title": "Alleviating Hallucinations of Large Language Models through Induced Hallucinations", "year": 2023}], "gt_label": [1, 1, 1, 1], "date": "2023-12", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_406", "valid": true}
{"query": "Which studies employed meta-learning loops to reduce the fitting times during encoding in INR-based compression methods?", "cited_paper": [{"arxiv_id": "2201.12904", "title": "COIN++: Neural Compression Across Modalities", "year": 2022}, {"arxiv_id": "2205.08957", "title": "Meta-Learning Sparse Compression Networks", "year": 2022}], "gt_label": [1, 1], "date": "2022-05", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_407", "valid": true}
{"query": "What studies introduced various regularization techniques to maximize the utility of sparse input views for scene reconstruction?", "cited_paper": [{"arxiv_id": "2104.00677", "title": "Putting NeRF on a Diet: Semantically Consistent Few-Shot View Synthesis", "year": 2021}, {"arxiv_id": "2112.15399", "title": "InfoNeRF: Ray Entropy Minimization for Few-Shot Neural Volume Rendering", "year": 2021}, {"arxiv_id": "2103.00020", "title": "Learning Transferable Visual Models From Natural Language Supervision", "year": 2021}, {"arxiv_id": "2304.13386", "title": "VGOS: Voxel Grid Optimization for View Synthesis from Sparse Inputs", "year": 2023}, {"arxiv_id": "2303.07418", "title": "FreeNeRF: Improving Few-shot Neural Rendering with Free Frequency Regularization", "year": 2023}], "gt_label": [1, 1, 1, 1, 1], "date": "2023-04", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_408", "valid": true}
{"query": "Could you provide me some works which decouples the continuous decision-making process into two steps?", "cited_paper": [{"arxiv_id": "2108.01176", "title": "Hierarchical Representations and Explicit Memory: Learning Effective Navigation Policies on 3D Scene Graphs using Graph Neural Networks", "year": 2021}], "gt_label": [1], "date": "2021-08", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_409", "valid": true}
{"query": "Which research works propose different selection methods for scans, regions, points, or boxes to be labeled during training?", "cited_paper": [{"arxiv_id": "2303.05886", "title": "Bi3D: Bi-domain Active Learning for Cross-domain 3D Object Detection", "year": 2023}, {"arxiv_id": "2107.11769", "title": "ReDAL: Region-based and Diversity-aware Active Learning for Point Cloud Semantic Segmentation", "year": 2021}, {"arxiv_id": "2210.08064", "title": "LESS: Label-Efficient Semantic Segmentation for LiDAR Point Clouds", "year": 2022}, {"arxiv_id": "2301.09249", "title": "Exploring Active 3D Object Detection from a Generalization Perspective", "year": 2023}], "gt_label": [1, 1, 1, 1], "date": "2023-03", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_410", "valid": true}
{"query": "What research papers are about physics-simulator-based methods in VR HMD settings?", "cited_paper": [{"arxiv_id": "2306.05666", "title": "QuestEnvSim: Environment-Aware Simulated Motion Tracking from Sparse Sensors", "year": 2023}, {"arxiv_id": "2209.09391", "title": "QuestSim: Human Motion Tracking from Sparse Sensors with Simulated Avatars", "year": 2022}, {"arxiv_id": "2108.10470", "title": "Isaac Gym: High Performance GPU-Based Physics Simulation For Robot Learning", "year": 2021}], "gt_label": [1, 1, 1], "date": "2023-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_411", "valid": true}
{"query": "Which papers describe a knowledge graph as a type of a heterogeneous graph?", "cited_paper": [{"arxiv_id": "2011.14867", "title": "A Survey on Heterogeneous Graph Embedding: Methods, Techniques, Applications and Sources", "year": 2020}], "gt_label": [1], "date": "2020-11", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_412", "valid": true}
{"query": "Could you name few works that employed the residual approach to study the effect of syntactic and semantic properties on brain alignment?", "cited_paper": [{"arxiv_id": "2212.08094", "title": "Joint processing of linguistic properties in brains and language models", "year": 2022}], "gt_label": [1], "date": "2022-12", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_413", "valid": true}
{"query": "Which works established the SustainBench consisting of 15 public datasets covering sustainable development goals?", "cited_paper": [{"arxiv_id": "2111.04724", "title": "SustainBench: Benchmarks for Monitoring the Sustainable Development Goals with Machine Learning", "year": 2021}], "gt_label": [1], "date": "2021-11", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_414", "valid": true}
{"query": "Any works about the effectiveness of knowledge distillation in semi-supervised learning?", "cited_paper": [{"arxiv_id": "2210.06711", "title": "Weighted Distillation with Unlabeled Examples", "year": 2022}], "gt_label": [1], "date": "2022-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_415", "valid": true}
{"query": "Which papers extended IPS and SNIPS methods to implicit feedback data?", "cited_paper": [{"arxiv_id": "1909.03601", "title": "Unbiased Recommender Learning from Missing-Not-At-Random Implicit Feedback", "year": 2019}], "gt_label": [1], "date": "2019-09", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_416", "valid": true}
{"query": "Which works are considered as the first to propose the task of Visual Question Answering (VQA)?", "cited_paper": [{"arxiv_id": "1505.02074", "title": "Exploring Models and Data for Image Question Answering", "year": 2015}, {"arxiv_id": "1505.00468", "title": "VQA: Visual Question Answering", "year": 2015}], "gt_label": [1, 1], "date": "2015-05", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_417", "valid": true}
{"query": "Could you provide me some studies about speaker identification in manga?", "cited_paper": [{"arxiv_id": "2306.17469", "title": "Manga109Dialog: A Large-scale Dialogue Dataset for Comics Speaker Detection", "year": 2023}], "gt_label": [1], "date": "2023-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_418", "valid": true}
{"query": "What studies propose hierarchical classifiers for CNN-based deep models?", "cited_paper": [{"arxiv_id": "1709.09890", "title": "B-CNN: Branch Convolutional Neural Network for Hierarchical Classification", "year": 2017}, {"arxiv_id": "1906.01536", "title": "Visual Tree Convolutional Neural Network in Image Classification", "year": 2019}, {"arxiv_id": "1604.06119", "title": "Network of Experts for Large-Scale Image Categorization", "year": 2016}], "gt_label": [1, 1, 1], "date": "2019-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_419", "valid": true}
{"query": "What works explored perturbations on different granularities in various aspects of NLP tasks?", "cited_paper": [{"arxiv_id": "1711.04903", "title": "Robust Multilingual Part-of-Speech Tagging via Adversarial Training", "year": 2017}, {"arxiv_id": "1909.11764", "title": "FreeLB: Enhanced Adversarial Training for Natural Language Understanding", "year": 2019}, {"arxiv_id": "2010.02329", "title": "InfoBERT: Improving Robustness of Language Models from An Information Theoretic Perspective", "year": 2020}, {"arxiv_id": "2211.10738", "title": "Knowledge Graph Contrastive Learning Based on Relation-Symmetrical Structure", "year": 2022}], "gt_label": [1, 1, 1, 1], "date": "2022-11", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_420", "valid": true}
{"query": "Which papers have evaluated explanation benchmarks with correlation to system performance or human understanding of decisions?", "cited_paper": [], "gt_label": [], "date": "2018-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_421", "valid": true}
{"query": "What initiatives were mentioned as related to open science community initiatives in language modeling?", "cited_paper": [{"arxiv_id": "2212.04960", "title": "BigScience: A Case Study in the Social Construction of a Multilingual Large Language Model", "year": 2022}, {"arxiv_id": "2402.00838", "title": "OLMo: Accelerating the Science of Language Models", "year": 2024}, {"arxiv_id": "2402.00159", "title": "Dolma: an Open Corpus of Three Trillion Tokens for Language Model Pretraining Research", "year": 2024}], "gt_label": [1, 1, 1], "date": "2024-02", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_422", "valid": true}
{"query": "What papers utilize VAEs, normalizing flows, reinforcement learning, optimal transport and diffusion models for the task of predicting the 3D structure of molecules given a molecular graph?", "cited_paper": [{"arxiv_id": "1909.11459", "title": "A Generative Model for Molecular Distance Geometry", "year": 2019}, {"arxiv_id": "2011.12747", "title": "Symmetry-Aware Actor-Critic for 3D Molecular Design", "year": 2020}, {"arxiv_id": "2203.02923", "title": "GeoDiff: a Geometric Diffusion Model for Molecular Conformation Generation", "year": 2022}], "gt_label": [1, 1, 1], "date": "2022-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_423", "valid": true}
{"query": "Could you provide me some studies that used the theory of universal learning in their work?", "cited_paper": [{"arxiv_id": "2011.04483", "title": "A Theory of Universal Learning", "year": 2020}, {"arxiv_id": "2208.14615", "title": "Fine-Grained Distribution-Dependent Learning Curves", "year": 2022}, {"arxiv_id": "2210.02297", "title": "Multiclass Learnability Beyond the PAC Framework: Universal Rates and Partial Concept Classes", "year": 2022}], "gt_label": [1, 1, 1], "date": "2022-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_424", "valid": true}
{"query": "What papers discuss about generating a reasoning process to enhance interpretability and extra supervision for answer generation?", "cited_paper": [{"arxiv_id": "1705.04146", "title": "Program Induction by Rationale Generation : Learning to Solve and Explain Algebraic Word Problems", "year": 2017}], "gt_label": [1], "date": "2017-05", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_425", "valid": true}
{"query": "What research papers have performed theoretical studies on dataset shifts, specifically covariate shifts and label shifts?", "cited_paper": [], "gt_label": [], "date": "2012-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_426", "valid": true}
{"query": "Which work proposed a meta-learning approach to align the linguistic spaces, enabling zero-shot and few-shot generalization?", "cited_paper": [{"arxiv_id": "2310.12794", "title": "Are Structural Concepts Universal in Transformer Language Models? Towards Interpretable Cross-Lingual Generalization", "year": 2023}], "gt_label": [1], "date": "2023-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_427", "valid": true}
{"query": "Could you list some works that generalise conformal prediction beyond the i.i.d. data setting?", "cited_paper": [{"arxiv_id": "2103.03323", "title": "Distribution-free uncertainty quantification for classification under label shift", "year": 2021}], "gt_label": [1], "date": "2021-03", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_428", "valid": true}
{"query": "Could you provide me some works that use prompt-based learning based on seq2seq models to solve ARA as a text-to-text generative task?", "cited_paper": [{"arxiv_id": "2302.13139", "title": "Prompt-based Learning for Text Readability Assessment", "year": 2023}], "gt_label": [1], "date": "2023-02", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_429", "valid": true}
{"query": "Which research works address prompt engineering for LLM performance?", "cited_paper": [{"arxiv_id": "2203.11364", "title": "An Information-theoretic Approach to Prompt Engineering Without Ground Truth Labels", "year": 2022}, {"arxiv_id": "2102.09690", "title": "Calibrate Before Use: Improving Few-Shot Performance of Language Models", "year": 2021}, {"arxiv_id": "2105.11447", "title": "True Few-Shot Learning with Language Models", "year": 2021}], "gt_label": [1, 1, 1], "date": "2022-03", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_430", "valid": true}
{"query": "What references discuss the use of Differential Privacy in the context of machine learning to protect training data?", "cited_paper": [{"arxiv_id": "1607.00133", "title": "Deep Learning with Differential Privacy", "year": 2016}, {"arxiv_id": "1802.06739", "title": "Differentially Private Generative Adversarial Network", "year": 2018}], "gt_label": [1, 1], "date": "2018-02", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_431", "valid": true}
{"query": "What papers explored downsizing frame resolution as a solution to GPU memory constraints?", "cited_paper": [{"arxiv_id": "2103.13137", "title": "Learning Salient Boundary Feature for Anchor-free Temporal Action Localization", "year": 2021}, {"arxiv_id": "2207.10448", "title": "An Efficient Spatio-Temporal Pyramid Transformer for Action Detection", "year": 2022}], "gt_label": [1, 1], "date": "2022-07", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_432", "valid": true}
{"query": "Can you tell me about the works that fine-tune language like models with privacy guarantees?", "cited_paper": [{"arxiv_id": "2110.05679", "title": "Large Language Models Can Be Strong Differentially Private Learners", "year": 2021}, {"arxiv_id": "2110.06500", "title": "Differentially Private Fine-tuning of Language Models", "year": 2021}], "gt_label": [1, 1], "date": "2021-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_433", "valid": true}
{"query": "What papers relate to training an additional router model to integrate multiple LLMs into one framework?", "cited_paper": [{"arxiv_id": "2306.02561", "title": "LLM-Blender: Ensembling Large Language Models with Pairwise Ranking and Generative Fusion", "year": 2023}, {"arxiv_id": "2311.08692", "title": "Routing to the Expert: Efficient Reward-guided Ensemble of Large Language Models", "year": 2023}], "gt_label": [1, 1], "date": "2023-11", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_434", "valid": true}
{"query": "What research used Householder orthogonal decomposition to achieve strict matrix orthogonality in neural networks?", "cited_paper": [{"arxiv_id": "2009.13977", "title": "What if Neural Networks had SVDs?", "year": 2020}], "gt_label": [1], "date": "2020-09", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_435", "valid": true}
{"query": "Could you provide me some works updating evaluation tasks?", "cited_paper": [{"arxiv_id": "2204.01906", "title": "Dynatask: A Framework for Creating Dynamic AI Benchmark Tasks", "year": 2022}], "gt_label": [1], "date": "2022-04", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_436", "valid": true}
{"query": "What studies provide a discussion on the importance of the cut distance and homomorphism counts in the graph learning context?", "cited_paper": [{"arxiv_id": "1802.08876", "title": "Lovsz Meets Weisfeiler and Leman", "year": 2018}, {"arxiv_id": "2003.12590", "title": "word2vec, node2vec, graph2vec, X2vec: Towards a Theory of Vector Embeddings of Structured Data", "year": 2020}, {"arxiv_id": "2005.01214", "title": "Graph Homomorphism Convolution", "year": 2020}], "gt_label": [1, 1, 1], "date": "2020-05", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_437", "valid": true}
{"query": "Could you provide me some studies about the utilization of the OT map in domain adaptation?", "cited_paper": [{"arxiv_id": "1507.00504", "title": "Optimal Transport for Domain Adaptation", "year": 2015}], "gt_label": [1], "date": "2015-07", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_438", "valid": true}
{"query": "Could you provide me some references about neural audio codecs?", "cited_paper": [{"arxiv_id": "2107.03312", "title": "SoundStream: An End-to-End Neural Audio Codec", "year": 2021}, {"arxiv_id": "2210.13438", "title": "High Fidelity Neural Audio Compression", "year": 2022}], "gt_label": [1, 1], "date": "2022-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_439", "valid": true}
{"query": "What works are about discrete prompts in Large Language Models (LLMs)?", "cited_paper": [{"arxiv_id": "2005.14165", "title": "Language Models are Few-Shot Learners", "year": 2020}, {"arxiv_id": "2012.15723", "title": "Making Pre-trained Language Models Better Few-shot Learners", "year": 2020}, {"arxiv_id": "2101.06804", "title": "What Makes Good In-Context Examples for GPT-$3$?", "year": 2021}, {"arxiv_id": "2010.15980", "title": "AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts", "year": 2020}], "gt_label": [1, 1, 1, 1], "date": "2021-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_440", "valid": true}
{"query": "What works provide datasets specifically designed for collaborative perception?", "cited_paper": [{"arxiv_id": "2202.08449", "title": "V2X-Sim: Multi-Agent Collaborative Perception Dataset and Benchmark for Autonomous Driving", "year": 2022}, {"arxiv_id": "2209.12836", "title": "Where2comm: Communication-Efficient Collaborative Perception via Spatial Confidence Maps", "year": 2022}, {"arxiv_id": "2204.05575", "title": "DAIR-V2X: A Large-Scale Dataset for Vehicle-Infrastructure Cooperative 3D Object Detection", "year": 2022}, {"arxiv_id": "2305.05938", "title": "V2X-Seq: A Large-Scale Sequential Dataset for Vehicle-Infrastructure Cooperative Perception and Forecasting", "year": 2023}, {"arxiv_id": "2303.07601", "title": "V2V4Real: A Real-world Large-scale Dataset for Vehicle-to-Vehicle Cooperative Perception", "year": 2023}], "gt_label": [1, 1, 1, 1, 1], "date": "2023-05", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_441", "valid": true}
{"query": "What works utilized ResNet as a backbone network for visual object tracking?", "cited_paper": [{"arxiv_id": "2103.15436", "title": "Transformer Tracking", "year": 2021}, {"arxiv_id": "2103.17154", "title": "Learning Spatio-Temporal Transformer for Visual Tracking", "year": 2021}], "gt_label": [1, 1], "date": "2021-03", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_442", "valid": true}
{"query": "Can you provide studies that detect visual objects and match ROI embeddings with textual embeddings?", "cited_paper": [{"arxiv_id": "2012.15409", "title": "UNIMO: Towards Unified-Modal Understanding and Generation via Cross-Modal Contrastive Learning", "year": 2020}], "gt_label": [1], "date": "2020-12", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_443", "valid": true}
{"query": "What works discuss extensions to hierarchical datasets for differential privacy marginals, such as geographic level and household composition?", "cited_paper": [{"arxiv_id": "2204.08986", "title": "The 2020 Census Disclosure Avoidance System TopDown Algorithm", "year": 2022}, {"arxiv_id": "1804.00370", "title": "Differentially Private Hierarchical Count-of-Counts Histograms", "year": 2018}, {"arxiv_id": "2206.05942", "title": "Private Synthetic Data with Hierarchical Structure", "year": 2022}], "gt_label": [1, 1, 1], "date": "2022-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_444", "valid": true}
{"query": "What studies propose augmenting techniques for improving the transferability of adversarial examples?", "cited_paper": [{"arxiv_id": "1603.05027", "title": "Identity Mappings in Deep Residual Networks", "year": 2016}, {"arxiv_id": "1803.06978", "title": "Improving Transferability of Adversarial Examples with Input Diversity", "year": 2018}, {"arxiv_id": "1904.02884", "title": "Evading Defenses to Transferable Adversarial Examples by Translation-Invariant Attacks", "year": 2019}, {"arxiv_id": "1707.07397", "title": "Synthesizing Robust Adversarial Examples", "year": 2017}, {"arxiv_id": "1908.06281", "title": "Nesterov Accelerated Gradient and Scale Invariance for Adversarial Attacks", "year": 2019}, {"arxiv_id": "2102.00436", "title": "Admix: Enhancing the Transferability of Adversarial Attacks", "year": 2021}], "gt_label": [1, 1, 1, 1, 1, 1], "date": "2021-02", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_445", "valid": true}
{"query": "Could you provide some studies about language-guided Diffusion Models?", "cited_paper": [{"arxiv_id": "2112.10741", "title": "GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models", "year": 2021}, {"arxiv_id": "2204.06125", "title": "Hierarchical Text-Conditional Image Generation with CLIP Latents", "year": 2022}], "gt_label": [1, 1], "date": "2022-04", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_446", "valid": true}
{"query": "What papers are about CLIP variants with focus on performance and efficiency improvements?", "cited_paper": [{"arxiv_id": "2112.12750", "title": "SLIP: Self-supervision meets Language-Image Pre-training", "year": 2021}, {"arxiv_id": "2208.12262", "title": "MaskCLIP: Masked Self-Distillation Advances Contrastive Language-Image Pretraining", "year": 2022}, {"arxiv_id": "2110.05208", "title": "Supervision Exists Everywhere: A Data Efficient Contrastive Language-Image Pre-training Paradigm", "year": 2021}, {"arxiv_id": "2212.00794", "title": "Scaling Language-Image Pre-training via Masking", "year": 2022}, {"arxiv_id": "2212.08653", "title": "Attentive Mask CLIP", "year": 2022}], "gt_label": [1, 1, 1, 1, 1], "date": "2022-12", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_447", "valid": true}
{"query": "Can you name a few works that research personalized models in pFL using client-side local distillation?", "cited_paper": [{"arxiv_id": "2111.02862", "title": "Parameterized Knowledge Transfer for Personalized Federated Learning", "year": 2021}, {"arxiv_id": "2107.13892", "title": "QuPeD: Quantized Personalization via Distillation with Applications to Federated Learning", "year": 2021}], "gt_label": [1, 1], "date": "2021-11", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_448", "valid": true}
{"query": "Which works adapt the preconditioning of on-policy, linear, least-squares forms of TD for nonlinear function approximation?", "cited_paper": [{"arxiv_id": "1910.05405", "title": "Zap Q-Learning With Nonlinear Function Approximation", "year": 2019}, {"arxiv_id": "2007.02786", "title": "TDprop: Does Jacobi Preconditioning Help Temporal Difference Learning?", "year": 2020}], "gt_label": [1, 1], "date": "2020-07", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_449", "valid": true}
{"query": "Could you provide me some studies that popularized membership inference attacks (MIA) as a practical means to demonstrate leakage of private information in Machine Learning?", "cited_paper": [{"arxiv_id": "2112.03570", "title": "Membership Inference Attacks From First Principles", "year": 2021}, {"arxiv_id": "1610.05820", "title": "Membership Inference Attacks against Machine Learning Models", "year": 2016}, {"arxiv_id": "2012.07805", "title": "Extracting Training Data from Large Language Models", "year": 2020}], "gt_label": [1, 1, 1], "date": "2021-12", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_450", "valid": true}
{"query": "What works are about masking and scaled cosine error usage focused on feature reconstruction in generative self-supervised learning?", "cited_paper": [{"arxiv_id": "2205.10803", "title": "GraphMAE: Self-Supervised Masked Graph Autoencoders", "year": 2022}], "gt_label": [1], "date": "2022-05", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_451", "valid": true}
{"query": "Could you provide me some works that proposed approaches to increase the context length by making the attention mechanism more scalable?", "cited_paper": [{"arxiv_id": "2004.05150", "title": "Longformer: The Long-Document Transformer", "year": 2020}, {"arxiv_id": "2110.08499", "title": "PRIMERA: Pyramid-based Masked Sentence Pre-training for Multi-document Summarization", "year": 2021}, {"arxiv_id": "2105.04371", "title": "Poolingformer: Long Document Modeling with Pooling Attention", "year": 2021}, {"arxiv_id": "2001.04451", "title": "Reformer: The Efficient Transformer", "year": 2020}, {"arxiv_id": "2310.05869", "title": "HyperAttention: Long-context Attention in Near-Linear Time", "year": 2023}], "gt_label": [1, 1, 1, 1, 1], "date": "2023-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_452", "valid": true}
{"query": "Could you mention studies that adopt controlling character scripts for plot development?", "cited_paper": [{"arxiv_id": "2112.06953", "title": "Controlled Cue Generation for Play Scripts", "year": 2021}, {"arxiv_id": "2010.05230", "title": "Controllable Multi-Character Psychology-Oriented Story Generation", "year": 2020}], "gt_label": [1, 1], "date": "2021-12", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_453", "valid": true}
{"query": "Which works demonstrate applying vision-language models to tasks such as visual question answering and object detection?", "cited_paper": [{"arxiv_id": "1909.11059", "title": "Unified Vision-Language Pre-Training for Image Captioning and VQA", "year": 2019}, {"arxiv_id": "2106.02636", "title": "MERLOT: Multimodal Neural Script Knowledge Models", "year": 2021}, {"arxiv_id": "2109.10282", "title": "TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models", "year": 2021}, {"arxiv_id": "2109.10852", "title": "Pix2seq: A Language Modeling Framework for Object Detection", "year": 2021}], "gt_label": [1, 1, 1, 1], "date": "2021-09", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_454", "valid": true}
{"query": "What are some research papers that focus on unsupervised anomaly detection methods?", "cited_paper": [{"arxiv_id": "1802.09088", "title": "Adversarially Learned One-Class Classifier for Novelty Detection", "year": 2018}, {"arxiv_id": "1904.02639", "title": "Memorizing Normality to Detect Anomaly: Memory-augmented Deep Autoencoder for Unsupervised Anomaly Detection", "year": 2019}, {"arxiv_id": "1911.02357", "title": "Uninformed Students: Student-Teacher Anomaly Detection with Discriminative Latent Embeddings", "year": 2019}, {"arxiv_id": "2104.04015", "title": "CutPaste: Self-Supervised Learning for Anomaly Detection and Localization", "year": 2021}, {"arxiv_id": "1911.10676", "title": "Attribute Restoration Framework for Anomaly Detection", "year": 2019}, {"arxiv_id": "2107.12571", "title": "CFLOW-AD: Real-Time Unsupervised Anomaly Detection with Localization via Conditional Normalizing Flows", "year": 2021}, {"arxiv_id": "2201.10703", "title": "Anomaly Detection via Reverse Distillation from One-Class Embedding", "year": 2022}, {"arxiv_id": "2106.08265", "title": "Towards Total Recall in Industrial Anomaly Detection", "year": 2021}, {"arxiv_id": "2011.11108", "title": "Multiresolution Knowledge Distillation for Anomaly Detection", "year": 2020}], "gt_label": [1, 1, 1, 1, 1, 1, 1, 1, 1], "date": "2022-01", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_455", "valid": true}
{"query": "What studies have focused on generative self-supervised learning in graph representation learning?", "cited_paper": [{"arxiv_id": "2205.10803", "title": "GraphMAE: Self-Supervised Masked Graph Autoencoders", "year": 2022}, {"arxiv_id": "2006.15437", "title": "GPT-GNN: Generative Pre-Training of Graph Neural Networks", "year": 2020}], "gt_label": [1, 1], "date": "2022-05", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_456", "valid": true}
{"query": "What works have proposed the use of Large Language Models (LLMs) as rewards through fine-tuning them on extensive user data in Reinforcement Learning from Human Feedback (RLHF)?", "cited_paper": [{"arxiv_id": "2203.02155", "title": "Training language models to follow instructions with human feedback", "year": 2022}, {"arxiv_id": "2204.05862", "title": "Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback", "year": 2022}], "gt_label": [1, 1], "date": "2022-04", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_457", "valid": true}
{"query": "Which works have presented different views and understandings on the concept of memorization in LMs?", "cited_paper": [{"arxiv_id": "2210.17546", "title": "Preventing Verbatim Memorization in Language Models Gives a False Sense of Privacy", "year": 2022}, {"arxiv_id": "2112.12938", "title": "Counterfactual Memorization in Neural Language Models", "year": 2021}, {"arxiv_id": "2304.11158", "title": "Emergent and Predictable Memorization in Large Language Models", "year": 2023}], "gt_label": [1, 1, 1], "date": "2023-04", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_458", "valid": true}
{"query": "What papers observed the failings of existing AD methods in detecting anomalies when distribution shifts occur?", "cited_paper": [{"arxiv_id": "2206.15476", "title": "AnoShift: A Distribution Shift Benchmark for Unsupervised Anomaly Detection", "year": 2022}], "gt_label": [1], "date": "2022-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_459", "valid": true}
{"query": "What papers discuss the techniques that have been incorporated into previous Seq2Seq GEC models?", "cited_paper": [{"arxiv_id": "2105.13318", "title": "Synthetic Data Generation for Grammatical Error Correction with Tagged Corruption Models", "year": 2021}, {"arxiv_id": "2008.02976", "title": "Data Weighted Training Strategies for Grammatical Error Correction", "year": 2020}, {"arxiv_id": "2311.11813", "title": "Efficient Grammatical Error Correction Via Multi-Task Training and Optimized Training Schedule", "year": 2023}], "gt_label": [1, 1, 1], "date": "2023-11", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_460", "valid": true}
{"query": "Any works discuss post-hoc calibration methods dealing poorly with over-confident predictions in domain-shift scenarios?", "cited_paper": [{"arxiv_id": "2012.10988", "title": "Post-hoc Uncertainty Calibration for Domain Drift Scenarios", "year": 2020}], "gt_label": [1], "date": "2020-12", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_461", "valid": true}
{"query": "What paper considered generating adversarial perturbations for training with an auxiliary network?", "cited_paper": [{"arxiv_id": "2002.11821", "title": "Improving Robustness of Deep-Learning-Based Image Reconstruction", "year": 2020}], "gt_label": [1], "date": "2020-02", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_462", "valid": true}
{"query": "Which research works apply slot attention to the domain of novel view synthesis?", "cited_paper": [{"arxiv_id": "2206.06922", "title": "Object Scene Representation Transformer", "year": 2022}], "gt_label": [1], "date": "2022-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_463", "valid": true}
{"query": "Can you provide some references that introduced and developed the concept of Split Learning with regards to data protection?", "cited_paper": [{"arxiv_id": "1812.00564", "title": "Split learning for health: Distributed deep learning without sharing raw patient data", "year": 2018}, {"arxiv_id": "2011.14818", "title": "Advancements of federated learning towards privacy preservation: from federated learning to split learning", "year": 2020}, {"arxiv_id": "2004.12088", "title": "SplitFed: When Federated Learning Meets Split Learning", "year": 2020}], "gt_label": [1, 1, 1], "date": "2020-11", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_464", "valid": true}
{"query": "Which paper reports that existing adaptations of foundation models for AD may generalize poorly to specific domains not covered in their massive training samples?", "cited_paper": [{"arxiv_id": "2205.11474", "title": "Exposing Outlier Exposure: What Can Be Learned From Few, One, and Zero Outlier Images", "year": 2022}], "gt_label": [1], "date": "2022-05", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_465", "valid": true}
{"query": "Could you provide the papers that discussed the models capable of addressing real-scenario compositional reasoning?", "cited_paper": [{"arxiv_id": "2305.12599", "title": "Abstract Meaning Representation-Based Logic-Driven Data Augmentation for Logical Reasoning", "year": 2023}, {"arxiv_id": "2306.15273", "title": "IDOL: Indicator-oriented Logic Pre-training for Logical Reasoning", "year": 2023}, {"arxiv_id": "2305.13718", "title": "Exploring Self-supervised Logic-enhanced Training for Large Language Models", "year": 2023}, {"arxiv_id": "2203.00357", "title": "MERIt: Meta-Path Guided Contrastive Learning for Logical Reasoning", "year": 2022}, {"arxiv_id": "2207.01450", "title": "Discourse-Aware Graph Networks for Textual Logical Reasoning", "year": 2022}, {"arxiv_id": "2103.14349", "title": "DAGN: Discourse-Aware Graph Network for Logical Reasoning", "year": 2021}], "gt_label": [1, 1, 1, 1, 1, 1], "date": "2023-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_466", "valid": true}
{"query": "Any works about GEM benchmark that assesses models on 40 language generation tasks?", "cited_paper": [{"arxiv_id": "2102.01672", "title": "The GEM Benchmark: Natural Language Generation, its Evaluation and Metrics", "year": 2021}], "gt_label": [1], "date": "2021-02", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_467", "valid": true}
{"query": "Can you provide some works about improving mathematical reasoning potentials by further training the generators with feedback from reward models?", "cited_paper": [{"arxiv_id": "2211.14275", "title": "Solving math word problems with processand outcome-based feedback", "year": 2022}, {"arxiv_id": "2311.05821", "title": "Let's Reinforce Step by Step", "year": 2023}], "gt_label": [1, 1], "date": "2023-11", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_468", "valid": true}
{"query": "Which paper introduced the concept of implicit data augmentation in the context of image classification?", "cited_paper": [{"arxiv_id": "1909.12220", "title": "Implicit Semantic Data Augmentation for Deep Networks", "year": 2019}], "gt_label": [1], "date": "2019-09", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_469", "valid": true}
{"query": "What works explored video textures as a kind of texture in moving scenes?", "cited_paper": [{"arxiv_id": "2104.02687", "title": "Strumming to the Beat: Audio-Conditioned Contrastive Video Textures", "year": 2021}], "gt_label": [1], "date": "2021-04", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_470", "valid": true}
{"query": "Could you provide me with the work providing theoretical explanation about the credibility of generated pseudolabels in comparison to original labels?", "cited_paper": [{"arxiv_id": "2010.03622", "title": "Theoretical Analysis of Self-Training with Deep Networks on Unlabeled Data", "year": 2020}], "gt_label": [1], "date": "2020-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_471", "valid": true}
{"query": "Which papers talk about external tools called by a large language model to perform mathematical operations?", "cited_paper": [{"arxiv_id": "2304.09102", "title": "Solving Math Word Problems by Combining Language Models With Symbolic Solvers", "year": 2023}], "gt_label": [1], "date": "2023-04", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_472", "valid": true}
{"query": "What works have focused on the interactions between group fairness and differential privacy?", "cited_paper": [{"arxiv_id": "1905.12101", "title": "Differential Privacy Has Disparate Impact on Model Accuracy", "year": 2019}], "gt_label": [1], "date": "2019-05", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_473", "valid": true}
{"query": "What work is partially related to the proposed method in this paper and concerns the estimation of CT models with latent variables?", "cited_paper": [], "gt_label": [], "date": "2019-02", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_474", "valid": true}
{"query": "Which work utilize OOD data for training-time regularization?", "cited_paper": [{"arxiv_id": "1812.04606", "title": "Deep Anomaly Detection with Outlier Exposure", "year": 2018}, {"arxiv_id": "1908.04951", "title": "Unsupervised Out-of-Distribution Detection by Maximum Classifier Discrepancy", "year": 2019}, {"arxiv_id": "2202.01197", "title": "VOS: Learning What You Don't Know by Virtual Outlier Synthesis", "year": 2022}, {"arxiv_id": "2108.11941", "title": "Semantically Coherent Out-of-Distribution Detection", "year": 2021}], "gt_label": [1, 1, 1, 1], "date": "2022-02", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_475", "valid": true}
{"query": "Can you name the work that applied self-supervised pre-trained features to detect instances in driving scenarios?", "cited_paper": [{"arxiv_id": "2210.04458", "title": "OGC: Unsupervised 3D Object Segmentation from Rigid Dynamics of Point Clouds", "year": 2022}], "gt_label": [1], "date": "2022-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_476", "valid": true}
{"query": "Which works explored generating photo-realistic sign language videos using GANs or diffusion models?", "cited_paper": [{"arxiv_id": "2203.15354", "title": "Signing at Scale: Learning to Co-Articulate Signs for Large-Scale Photo-Realistic Sign Language Production", "year": 2022}, {"arxiv_id": "2308.16082", "title": "SignDiff: Diffusion Model for American Sign Language Production", "year": 2023}, {"arxiv_id": "2312.12917", "title": "Sign Language Production with Latent Motion Transformer", "year": 2023}], "gt_label": [1, 1, 1], "date": "2023-12", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_477", "valid": true}
{"query": "What works adapted random Fourier features to graphs and proposed a sampling-based variant of the global alignment graph kernel?", "cited_paper": [{"arxiv_id": "1911.11119", "title": "Scalable Global Alignment Graph Kernel Using Random Features: From Node Embedding to Graph Embedding", "year": 2019}], "gt_label": [1], "date": "2019-11", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_478", "valid": true}
{"query": "Could you provide studies that propose two-stage training to improve the prediction performance for commonsense reasoning tasks?", "cited_paper": [{"arxiv_id": "1906.02361", "title": "Explain Yourself! Leveraging Language Models for Commonsense Reasoning", "year": 2019}], "gt_label": [1], "date": "2019-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_479", "valid": true}
{"query": "Which work proposes CCMI and an estimator for the KL-divergence in the context of Conditional Mutual Information (CMI)?", "cited_paper": [{"arxiv_id": "1906.01824", "title": "CCMI : Classifier based Conditional Mutual Information Estimation", "year": 2019}], "gt_label": [1], "date": "2019-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_480", "valid": true}
{"query": "Who applied a contrastive loss on a supervised setting in a multi-view rendering-based method?", "cited_paper": [{"arxiv_id": "1706.04496", "title": "Learning Local Shape Descriptors from Part Correspondences With Multi-view Convolutional Networks", "year": 2017}], "gt_label": [1], "date": "2017-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_481", "valid": true}
{"query": "What works introduce task-specific gating networks in the sparse-MoE framework?", "cited_paper": [{"arxiv_id": "2210.14793", "title": "M$^3$ViT: Mixture-of-Experts Vision Transformer for Efficient Multi-task Learning with Model-Accelerator Co-design", "year": 2022}, {"arxiv_id": "2307.15324", "title": "TaskExpert: Dynamically Assembling Multi-Task Representations with Memorial Mixture-of-Experts", "year": 2023}, {"arxiv_id": "2106.03760", "title": "DSelect-k: Differentiable Selection in the Mixture of Experts with Applications to Multi-Task Learning", "year": 2021}], "gt_label": [1, 1, 1], "date": "2023-07", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_482", "valid": true}
{"query": "What works utilize point methods for segmenting 3D LiDAR point clouds?", "cited_paper": [{"arxiv_id": "1706.02413", "title": "PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space", "year": 2017}, {"arxiv_id": "1807.06288", "title": "PointSeg: Real-Time Semantic Segmentation Based on 3D LiDAR Point Cloud", "year": 2018}], "gt_label": [1, 1], "date": "2018-07", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_483", "valid": true}
{"query": "What are the studies that focus on improving the quality of annotations by including model adversaries into the annotation rounds?", "cited_paper": [{"arxiv_id": "1910.14599", "title": "Adversarial NLI: A New Benchmark for Natural Language Understanding", "year": 2019}], "gt_label": [1], "date": "2019-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_484", "valid": true}
{"query": "Could you list the works using normalized flows for 3D shape generation?", "cited_paper": [{"arxiv_id": "1906.12320", "title": "PointFlow: 3D Point Cloud Generation with Continuous Normalizing Flows", "year": 2019}, {"arxiv_id": "2006.04604", "title": "SoftFlow: Probabilistic Framework for Normalizing Flow on Manifolds", "year": 2020}, {"arxiv_id": "2007.10170", "title": "Discrete Point Flow Networks for Efficient Point Cloud Generation", "year": 2020}], "gt_label": [1, 1, 1], "date": "2020-07", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_485", "valid": true}
{"query": "Can you suggest some researches that consider graph to be fully-connected when the underlying connectivity structure is unknown?", "cited_paper": [{"arxiv_id": "1706.03762", "title": "Attention Is All You Need", "year": 2017}, {"arxiv_id": "1706.06122", "title": "VAIN: Attentional Multi-agent Predictive Modeling", "year": 2017}], "gt_label": [1, 1], "date": "2017-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_486", "valid": true}
{"query": "What works have proposed alternative diffusion processes closely related to Gaussian diffusion?", "cited_paper": [{"arxiv_id": "2208.09392", "title": "Cold Diffusion: Inverting Arbitrary Image Transforms Without Noise", "year": 2022}, {"arxiv_id": "2209.05557", "title": "Blurring Diffusion Models", "year": 2022}], "gt_label": [1, 1], "date": "2022-09", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_487", "valid": true}
{"query": "Could you tell me if there are any studies that propose Neural Radiance Fields for novel-view synthesis?", "cited_paper": [{"arxiv_id": "2003.08934", "title": "NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis", "year": 2020}], "gt_label": [1], "date": "2020-03", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_488", "valid": true}
{"query": "Could you provide some examples of datasets that involved scenarios with only a single API call?", "cited_paper": [{"arxiv_id": "2306.05301", "title": "ToolAlpaca: Generalized Tool Learning for Language Models with 3000 Simulated Cases", "year": 2023}, {"arxiv_id": "2305.15334", "title": "Gorilla: Large Language Model Connected with Massive APIs", "year": 2023}, {"arxiv_id": "2305.16504", "title": "On the Tool Manipulation Capability of Open-source Large Language Models", "year": 2023}], "gt_label": [1, 1, 1], "date": "2023-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_489", "valid": true}
{"query": "Which works have incorporated equivariances into CNPs, but still suffer from the same scaling issues?", "cited_paper": [{"arxiv_id": "1910.13556", "title": "Convolutional Conditional Neural Processes", "year": 2019}, {"arxiv_id": "2203.08775", "title": "Practical Conditional Neural Processes Via Tractable Dependent Predictions", "year": 2022}, {"arxiv_id": "2102.08759", "title": "Group Equivariant Conditional Neural Processes", "year": 2021}, {"arxiv_id": "2002.12880", "title": "Generalizing Convolutional Neural Networks for Equivariance to Lie Groups on Arbitrary Continuous Data", "year": 2020}, {"arxiv_id": "2011.12916", "title": "Equivariant Learning of Stochastic Fields: Gaussian Processes and Steerable Conditional Neural Processes", "year": 2020}], "gt_label": [1, 1, 1, 1, 1], "date": "2022-03", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_490", "valid": true}
{"query": "Which research is based on the task of RGBD-based 6D pose estimation in point cloud registration?", "cited_paper": [{"arxiv_id": "1712.07629", "title": "SuperPoint: Self-Supervised Interest Point Detection and Description", "year": 2017}, {"arxiv_id": "2307.15514", "title": "Revisiting Fully Convolutional Geometric Features for Object 6D Pose Estimation", "year": 2023}, {"arxiv_id": "2103.00937", "title": "OMNet: Learning Overlapping Mask for Partial-to-Partial Point Cloud Registration", "year": 2021}, {"arxiv_id": "2203.14517", "title": "REGTR: End-to-end Point Cloud Correspondences with Transformers", "year": 2022}], "gt_label": [1, 1, 1, 1], "date": "2023-07", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_491", "valid": true}
{"query": "Which works discussed adjusting quantization error in Post-training Quantization (PTQ) for Language Model (LLM)?", "cited_paper": [{"arxiv_id": "2210.17323", "title": "GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers", "year": 2022}, {"arxiv_id": "2307.13304", "title": "QuIP: 2-Bit Quantization of Large Language Models With Guarantees", "year": 2023}], "gt_label": [1, 1], "date": "2023-07", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_492", "valid": true}
{"query": "What research demonstrates the application of contrastive learning in text and image domains?", "cited_paper": [{"arxiv_id": "2002.05709", "title": "A Simple Framework for Contrastive Learning of Visual Representations", "year": 2020}, {"arxiv_id": "2103.00020", "title": "Learning Transferable Visual Models From Natural Language Supervision", "year": 2021}], "gt_label": [1, 1], "date": "2021-03", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_493", "valid": true}
{"query": "Which works are about Seq2Seq models that have demonstrated high performance in Grammar Error Correction (GEC)?", "cited_paper": [{"arxiv_id": "1910.13461", "title": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension", "year": 2019}, {"arxiv_id": "1910.10683", "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer", "year": 2019}, {"arxiv_id": "1804.05940", "title": "Approaching Neural Grammatical Error Correction as a Low-Resource Machine Translation Task", "year": 2018}, {"arxiv_id": "1907.01256", "title": "A Neural Grammatical Error Correction System Built On Better Pre-training and Sequential Transfer Learning", "year": 2019}, {"arxiv_id": "1903.00138", "title": "Improving Grammatical Error Correction via Pre-Training a Copy-Augmented Architecture with Unlabeled Data", "year": 2019}, {"arxiv_id": "2005.11849", "title": "Stronger Baselines for Grammatical Error Correction Using Pretrained Encoder-Decoder Model", "year": 2020}], "gt_label": [1, 1, 1, 1, 1, 1], "date": "2020-05", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_494", "valid": true}
{"query": "Which studies showed that when the Polyak-Lojasiewicz condition is replaced by the weak PL condition, PG methods can also achieve linear convergence?", "cited_paper": [{"arxiv_id": "2302.01734", "title": "Stochastic Policy Gradient Methods: Improved Sample Complexity for Fisher-non-degenerate Policies", "year": 2023}], "gt_label": [1], "date": "2023-02", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_495", "valid": true}
{"query": "What studies focus on human-curated multilingual examples?", "cited_paper": [{"arxiv_id": "2304.07327", "title": "OpenAssistant Conversations -- Democratizing Large Language Model Alignment", "year": 2023}, {"arxiv_id": "2304.07327", "title": "OpenAssistant Conversations -- Democratizing Large Language Model Alignment", "year": 2023}], "gt_label": [1, 1], "date": "2023-04", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_496", "valid": true}
{"query": "Which studies developed protein sequence design models using a BERT-style generative framework?", "cited_paper": [{"arxiv_id": "2006.15222", "title": "BERTology Meets Biology: Interpreting Attention in Protein Language Models", "year": 2020}], "gt_label": [1], "date": "2020-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_497", "valid": true}
{"query": "What research combines the property of Kronecker products with other techniques to produce accurate and updatable Kronecker sketching methods?", "cited_paper": [{"arxiv_id": "2209.04876", "title": "Subquadratic Kronecker Regression with Applications to Tensor Decomposition", "year": 2022}], "gt_label": [1], "date": "2022-09", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_498", "valid": true}
{"query": "Are there studies that allow for non-stationary environments but only explore regret to the best arm in hindsight?", "cited_paper": [], "gt_label": [], "date": "2009-07", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_499", "valid": true}
{"query": "Could you provide some studies where the idea of gating has been used in designing GNNs?", "cited_paper": [{"arxiv_id": "1711.07553", "title": "Residual Gated Graph ConvNets", "year": 2017}, {"arxiv_id": "1511.05493", "title": "Gated Graph Sequence Neural Networks", "year": 2015}, {"arxiv_id": "1803.07294", "title": "GaAN: Gated Attention Networks for Learning on Large and Spatiotemporal Graphs", "year": 2018}], "gt_label": [1, 1, 1], "date": "2018-03", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_500", "valid": true}
{"query": "Which works focus on improving efficiency of LLM inference using parallelism methods such as pipeline parallelism and tensor parallelism?", "cited_paper": [{"arxiv_id": "1909.08053", "title": "Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism", "year": 2019}, {"arxiv_id": "1811.06965", "title": "GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism", "year": 2018}, {"arxiv_id": "2110.14883", "title": "Colossal-AI: A Unified Deep Learning System For Large-Scale Parallel Training", "year": 2021}], "gt_label": [1, 1, 1], "date": "2021-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_501", "valid": true}
{"query": "What papers presented the first framework of explaining GNN predictions?", "cited_paper": [{"arxiv_id": "1903.03894", "title": "GNNExplainer: Generating Explanations for Graph Neural Networks", "year": 2019}], "gt_label": [1], "date": "2019-03", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_502", "valid": true}
{"query": "Which research introduced CodeBLEU, which adds terms to measure Abstract Syntax Tree and data-flow similarity?", "cited_paper": [{"arxiv_id": "2009.10297", "title": "CodeBLEU: a Method for Automatic Evaluation of Code Synthesis", "year": 2020}], "gt_label": [1], "date": "2020-09", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_503", "valid": true}
{"query": "What paper has worked on improving the explainability of the information retrieval approach by inferring an adjacency matrix?", "cited_paper": [{"arxiv_id": "2104.07302", "title": "TransferNet: An Effective and Transparent Framework for Multi-hop Question Answering over Relation Graph", "year": 2021}], "gt_label": [1], "date": "2021-04", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_504", "valid": true}
{"query": "What studies focused on prompting methods to elicit the mathematical reasoning abilities of LLMs?", "cited_paper": [{"arxiv_id": "2201.11903", "title": "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models", "year": 2022}, {"arxiv_id": "2308.04371", "title": "Cumulative Reasoning with Large Language Models", "year": 2023}, {"arxiv_id": "2210.00720", "title": "Complexity-Based Prompting for Multi-Step Reasoning", "year": 2022}], "gt_label": [1, 1, 1], "date": "2023-08", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_505", "valid": true}
{"query": "What research papers used a pooling layer and MLP classifier to predict the response length?", "cited_paper": [{"arxiv_id": "1802.06901", "title": "Deterministic Non-Autoregressive Neural Sequence Modeling by Iterative Refinement", "year": 2018}, {"arxiv_id": "1909.02480", "title": "FlowSeq: Non-Autoregressive Conditional Sequence Generation with Generative Flow", "year": 2019}, {"arxiv_id": "2009.07177", "title": "Iterative Refinement in the Continuous Space for Non-Autoregressive Neural Machine Translation", "year": 2020}, {"arxiv_id": "1908.07181", "title": "Latent-Variable Non-Autoregressive Neural Machine Translation with Deterministic Inference Using a Delta Posterior", "year": 2019}], "gt_label": [1, 1, 1, 1], "date": "2020-09", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_506", "valid": true}
{"query": "Could you give me examples of works that designed model aggregation schemes in the context of Federated Learning?", "cited_paper": [{"arxiv_id": "1909.06335", "title": "Measuring the Effects of Non-Identical Data Distribution for Federated Visual Classification", "year": 2019}, {"arxiv_id": "2007.07481", "title": "Tackling the Objective Inconsistency Problem in Heterogeneous Federated Optimization", "year": 2020}, {"arxiv_id": "2002.06440", "title": "Federated Learning with Matched Averaging", "year": 2020}, {"arxiv_id": "2102.07623", "title": "FedBN: Federated Learning on Non-IID Features via Local Batch Normalization", "year": 2021}], "gt_label": [1, 1, 1, 1], "date": "2021-02", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_507", "valid": true}
{"query": "What works used the ideas in neural ODEs and extended them to normalizing flows to efficiently model arbitrary probability distributions?", "cited_paper": [{"arxiv_id": "1912.02762", "title": "Normalizing Flows for Probabilistic Modeling and Inference", "year": 2019}, {"arxiv_id": "1810.01367", "title": "FFJORD: Free-form Continuous Dynamics for Scalable Reversible Generative Models", "year": 2018}], "gt_label": [1, 1], "date": "2019-12", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_508", "valid": true}
{"query": "Which research found Gaussian noise addition beneficial for corruption robustness in classification?", "cited_paper": [{"arxiv_id": "2001.06057", "title": "A simple way to make neural networks robust against diverse image corruptions", "year": 2020}], "gt_label": [1], "date": "2020-01", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_509", "valid": true}
{"query": "Which research works have contributed to the domain of TEE-based Private Learning for privacy-preserving machine learning?", "cited_paper": [{"arxiv_id": "2101.08204", "title": "secureTF: A Secure TensorFlow Framework", "year": 2021}], "gt_label": [1], "date": "2021-01", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_510", "valid": true}
{"query": "What works are related to the applications of Latent Diffusion Models?", "cited_paper": [{"arxiv_id": "2112.10752", "title": "High-Resolution Image Synthesis with Latent Diffusion Models", "year": 2021}], "gt_label": [1], "date": "2021-12", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_511", "valid": true}
{"query": "Which work is similar to the proposed work in that they both index and shuffle the slot description in natural language?", "cited_paper": [{"arxiv_id": "2201.08904", "title": "Description-Driven Task-Oriented Dialog Modeling", "year": 2022}], "gt_label": [1], "date": "2022-01", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_512", "valid": true}
{"query": "What work required actual calls to a real API to solve its problems, contrasting with other works that simulated API calls?", "cited_paper": [{"arxiv_id": "2307.16789", "title": "ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs", "year": 2023}, {"arxiv_id": "2305.16504", "title": "On the Tool Manipulation Capability of Open-source Large Language Models", "year": 2023}], "gt_label": [1, 1], "date": "2023-07", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_513", "valid": true}
{"query": "Could you provide me some works which discuss about Non-Gaussian Component Analysis (NGCA)?", "cited_paper": [{"arxiv_id": "1704.01041", "title": "Polynomial Time and Sample Complexity for Non-Gaussian Component Analysis: Spectral Methods", "year": 2017}], "gt_label": [1], "date": "2017-04", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_514", "valid": true}
{"query": "What papers have focused on the subset of medical visual question answering that deals with image-based EHR QA?", "cited_paper": [{"arxiv_id": "2302.09636", "title": "Interpretable Medical Image Visual Question Answering via Multi-Modal Relationship Graph Learning", "year": 2023}], "gt_label": [1], "date": "2023-02", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_515", "valid": true}
{"query": "Could you provide me some studies that applies text prompting approach in multi-modal scenarios?", "cited_paper": [{"arxiv_id": "2109.01134", "title": "Learning to Prompt for Vision-Language Models", "year": 2021}, {"arxiv_id": "2203.05557", "title": "Conditional Prompt Learning for Vision-Language Models", "year": 2022}], "gt_label": [1, 1], "date": "2022-03", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_516", "valid": true}
{"query": "What studies integrate detection results as supplementary information for Seq2Seq correction models?", "cited_paper": [{"arxiv_id": "2005.00987", "title": "Encoder-Decoder Models Can Benefit from Pre-trained Masked Language Models in Grammatical Error Correction", "year": 2020}], "gt_label": [1], "date": "2020-05", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_517", "valid": true}
{"query": "Is there any work that used zero-shot group equivariance in partially observable Markov decision processes?", "cited_paper": [{"arxiv_id": "2210.12124", "title": "Equivariant Networks for Zero-Shot Coordination", "year": 2022}], "gt_label": [1], "date": "2022-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_518", "valid": true}
{"query": "What research papers discuss the use of diffusion models for high-fidelity image synthesis?", "cited_paper": [{"arxiv_id": "2105.05233", "title": "Diffusion Models Beat GANs on Image Synthesis", "year": 2021}, {"arxiv_id": "2209.07162", "title": "Brain Imaging Generation with Latent Diffusion Models", "year": 2022}, {"arxiv_id": "2211.07804", "title": "Diffusion Models for Medical Image Analysis: A Comprehensive Survey", "year": 2022}], "gt_label": [1, 1, 1], "date": "2022-12", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_519", "valid": true}
{"query": "Can you cite some early studies that investigated adversarial training in the computer vision domain?", "cited_paper": [{"arxiv_id": "1611.01236", "title": "Adversarial Machine Learning at Scale", "year": 2016}, {"arxiv_id": "1706.06083", "title": "Towards Deep Learning Models Resistant to Adversarial Attacks", "year": 2017}], "gt_label": [1, 1], "date": "2017-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_520", "valid": true}
{"query": "Which papers introduced learning-augmented algorithms for weighted paging?", "cited_paper": [{"arxiv_id": "2011.09076", "title": "Learning-Augmented Weighted Paging", "year": 2020}], "gt_label": [1], "date": "2020-11", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_521", "valid": true}
{"query": "Which papers discuss the application of diffusion models in the field of image reconstruction from fMRI?", "cited_paper": [{"arxiv_id": "2305.18274", "title": "Reconstructing the Mind's Eye: fMRI-to-Image with Contrastive Learning and Diffusion Priors", "year": 2023}], "gt_label": [1], "date": "2023-05", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_522", "valid": true}
{"query": "Which research papers used the RealNews dataset for pretraining?", "cited_paper": [{"arxiv_id": "1905.12616", "title": "Defending Against Neural Fake News", "year": 2019}, {"arxiv_id": "1909.08053", "title": "Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism", "year": 2019}, {"arxiv_id": "1907.11692", "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach", "year": 2019}], "gt_label": [1, 1, 1], "date": "2019-09", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_523", "valid": true}
{"query": "What references propose LLM-Blender, a method to rank and fuse generations from different models?", "cited_paper": [{"arxiv_id": "2306.02561", "title": "LLM-Blender: Ensembling Large Language Models with Pairwise Ranking and Generative Fusion", "year": 2023}], "gt_label": [1], "date": "2023-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_524", "valid": true}
{"query": "What are some works that have designed methods to learn the geocentric pose of buildings in off-nadir images for monocular height estimation?", "cited_paper": [{"arxiv_id": "2007.00729", "title": "Learning Geocentric Object Pose in Oblique Monocular Images", "year": 2020}], "gt_label": [1], "date": "2020-07", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_525", "valid": true}
{"query": "Could you name a few works that provide documentation guidelines for NLP and ML datasets, models, and systems?", "cited_paper": [{"arxiv_id": "1810.03993", "title": "Model Cards for Model Reporting", "year": 2018}, {"arxiv_id": "1803.09010", "title": "Datasheets for Datasets", "year": 2018}], "gt_label": [1, 1], "date": "2024-05", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_526", "valid": true}
{"query": "What works propose weighting strategies in the fusion of multiple views considering the view quality?", "cited_paper": [{"arxiv_id": "2103.07738", "title": "Reconsidering Representation Alignment for Multi-view Clustering", "year": 2021}], "gt_label": [1], "date": "2021-03", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_527", "valid": true}
{"query": "Which works utilize spatio-temporal LSTM for action recognition?", "cited_paper": [{"arxiv_id": "1706.08276", "title": "Skeleton-Based Action Recognition Using Spatio-Temporal LSTM Network with Trust Gates", "year": 2017}], "gt_label": [1], "date": "2017-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_528", "valid": true}
{"query": "Give me the examples of papers where surrogate gradient estimation of the firing function in SNNs has been studied.", "cited_paper": [{"arxiv_id": "1705.07565", "title": "Learning to Prune Deep Neural Networks via Layer-wise Optimal Brain Surgeon", "year": 2017}], "gt_label": [1], "date": "2017-05", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_529", "valid": true}
{"query": "What works focused on calibration in generative question answering?", "cited_paper": [{"arxiv_id": "2012.00955", "title": "How Can We Know When Language Models Know? On the Calibration of Language Models for Question Answering", "year": 2020}], "gt_label": [1], "date": "2020-12", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_530", "valid": true}
{"query": "Are there papers that tried to use internal solver heuristics to control the learned dynamics of Neural Differential Equations?", "cited_paper": [{"arxiv_id": "2105.03918", "title": "Opening the Blackbox: Accelerating Neural Differential Equations by Regularizing Internal Solver Heuristics", "year": 2021}], "gt_label": [1], "date": "2021-05", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_531", "valid": true}
{"query": "Could you mention some papers that have discussed knowledge distillation in meta-learning?", "cited_paper": [{"arxiv_id": "1503.02531", "title": "Distilling the Knowledge in a Neural Network", "year": 2015}, {"arxiv_id": "2107.00197", "title": "Few-Shot Learning with a Strong Teacher", "year": 2021}], "gt_label": [1, 1], "date": "2021-07", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_532", "valid": true}
{"query": "Did any research propose canonicalization-based methods to construct equivariant networks out of non-equivariant backbones?", "cited_paper": [{"arxiv_id": "2211.06489", "title": "Equivariance with Learned Canonicalization Functions", "year": 2022}], "gt_label": [1], "date": "2022-11", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_533", "valid": true}
{"query": "Which works have been particularly focused on introducing the concept of quantile temporal-difference learning?", "cited_paper": [{"arxiv_id": "1710.10044", "title": "Distributional Reinforcement Learning with Quantile Regression", "year": 2017}, {"arxiv_id": "2301.04462", "title": "An Analysis of Quantile Temporal-Difference Learning", "year": 2023}, {"arxiv_id": "1806.06923", "title": "Implicit Quantile Networks for Distributional Reinforcement Learning", "year": 2018}, {"arxiv_id": "1911.02140", "title": "Fully Parameterized Quantile Function for Distributional Reinforcement Learning", "year": 2019}], "gt_label": [1, 1, 1, 1], "date": "2023-01", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_534", "valid": true}
{"query": "What works have studied the generalization from training data to test data both theoretically and practically?", "cited_paper": [{"arxiv_id": "1409.1556", "title": "Very Deep Convolutional Networks for Large-Scale Image Recognition", "year": 2014}], "gt_label": [1], "date": "2014-09", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_535", "valid": true}
{"query": "What papers require an L2-norm bound on the error of the linear approximation of Qt?", "cited_paper": [{"arxiv_id": "2209.15382", "title": "Linear Convergence for Natural Policy Gradient with Log-linear Policy Parametrization", "year": 2022}], "gt_label": [1], "date": "2022-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_536", "valid": true}
{"query": "What works examined bandits and global optimization with neural function approximation?", "cited_paper": [{"arxiv_id": "1911.04462", "title": "Neural Contextual Bandits with UCB-based Exploration", "year": 2019}, {"arxiv_id": "2010.00827", "title": "Neural Thompson Sampling", "year": 2020}, {"arxiv_id": "2210.06850", "title": "Sample-Then-Optimize Batch Neural Thompson Sampling", "year": 2022}], "gt_label": [1, 1, 1], "date": "2022-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_537", "valid": true}
{"query": "What papers discuss the use of data augmentation or mixup to prevent robust overfittings?", "cited_paper": [{"arxiv_id": "2002.11569", "title": "Overfitting in adversarially robust deep learning", "year": 2020}, {"arxiv_id": "2312.07392", "title": "ReRoGCRL: Representation-based Robustness in Goal-Conditioned Reinforcement Learning", "year": 2023}], "gt_label": [1, 1], "date": "2023-12", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_538", "valid": true}
{"query": "Which studies focus on efficient FL by deploying model ensemble and sub-parameter sharing?", "cited_paper": [{"arxiv_id": "2001.01523", "title": "Think Locally, Act Globally: Federated Learning with Local and Global Representations", "year": 2020}], "gt_label": [1], "date": "2020-01", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_539", "valid": true}
{"query": "Could you provide me with studies that have demonstrated the importance of relationships between entities in deep learning?", "cited_paper": [{"arxiv_id": "1911.06962", "title": "Inductive Relation Prediction by Subgraph Reasoning", "year": 2019}, {"arxiv_id": "2306.05689", "title": "Single-Stage Visual Relationship Learning using Conditional Queries", "year": 2023}, {"arxiv_id": "1706.01427", "title": "A simple neural network module for relational reasoning", "year": 2017}, {"arxiv_id": "1711.11575", "title": "Relation Networks for Object Detection", "year": 2017}, {"arxiv_id": "1904.05068", "title": "Relational Knowledge Distillation", "year": 2019}, {"arxiv_id": "1711.06025", "title": "Learning to Compare: Relation Network for Few-Shot Learning", "year": 2017}], "gt_label": [1, 1, 1, 1, 1, 1], "date": "2023-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_540", "valid": true}
{"query": "Are there any studies about ControlNet for image editing by providing reference images?", "cited_paper": [{"arxiv_id": "2302.05543", "title": "Adding Conditional Control to Text-to-Image Diffusion Models", "year": 2023}], "gt_label": [1], "date": "2023-02", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_541", "valid": true}
{"query": "Could you provide me some papers that added inductive bias into the neural network policy or learning algorithm?", "cited_paper": [{"arxiv_id": "1602.02867", "title": "Value Iteration Networks", "year": 2016}, {"arxiv_id": "2102.07456", "title": "Neuro-algorithmic Policies enable Fast Combinatorial Generalization", "year": 2021}], "gt_label": [1, 1], "date": "2021-02", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_542", "valid": true}
{"query": "What work conducted further pre-training and instruction tuning on a speech dataset of semantic tokens?", "cited_paper": [{"arxiv_id": "2305.11000", "title": "SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities", "year": 2023}], "gt_label": [1], "date": "2023-05", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_543", "valid": true}
{"query": "What paper proposed the utilization of the in-context learning method to revise the output of LLMs with demonstrations extracted from the corpus based on similarity?", "cited_paper": [{"arxiv_id": "2305.12740", "title": "Can We Edit Factual Knowledge by In-Context Learning?", "year": 2023}], "gt_label": [1], "date": "2023-05", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_544", "valid": true}
{"query": "Could you provide research on enhancing in-context learning's capability in vision?", "cited_paper": [{"arxiv_id": "2304.04748", "title": "Exploring Effective Factors for Improving Visual In-Context Learning", "year": 2023}, {"arxiv_id": "2306.01667", "title": "Towards In-context Scene Understanding", "year": 2023}], "gt_label": [1, 1], "date": "2023-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_545", "valid": true}
{"query": "Which works have extended prototypical networks to few-shot anomaly detection?", "cited_paper": [{"arxiv_id": "1906.00820", "title": "One-Way Prototypical Networks", "year": 2019}, {"arxiv_id": "2204.01905", "title": "Learning to Adapt to Domain Shifts with Few-shot Samples in Anomalous Sound Detection", "year": 2022}], "gt_label": [1, 1], "date": "2022-04", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_546", "valid": true}
{"query": "What study observed that neural-network based deblurring is sensitive to adversarial perturbations despite being trained with Jittering?", "cited_paper": [{"arxiv_id": "2210.02502", "title": "On Adversarial Robustness of Deep Image Deblurring", "year": 2022}], "gt_label": [1], "date": "2022-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_547", "valid": true}
{"query": "Could you provide me the papers which applied representations learned by large-scale models for semantic correspondence?", "cited_paper": [{"arxiv_id": "2112.05143", "title": "GAN-Supervised Dense Visual Alignment", "year": 2021}, {"arxiv_id": "2104.14294", "title": "Emerging Properties in Self-Supervised Vision Transformers", "year": 2021}, {"arxiv_id": "2112.05814", "title": "Deep ViT Features as Dense Visual Descriptors", "year": 2021}], "gt_label": [1, 1, 1], "date": "2021-12", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_548", "valid": true}
{"query": "What research have made progress in predicting low-energy conformations given molecular graphs?", "cited_paper": [{"arxiv_id": "2203.02923", "title": "GeoDiff: a Geometric Diffusion Model for Molecular Conformation Generation", "year": 2022}], "gt_label": [1], "date": "2022-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_549", "valid": true}
{"query": "What papers explored vision-based UI Agents for web or mobile?", "cited_paper": [{"arxiv_id": "2306.00245", "title": "From Pixels to UI Actions: Learning to Follow Instructions via Graphical User Interfaces", "year": 2023}, {"arxiv_id": "2309.11436", "title": "You Only Look at Screens: Multimodal Chain-of-Action Agents", "year": 2023}], "gt_label": [1, 1], "date": "2023-09", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_550", "valid": true}
{"query": "What papers studied methods that employed the use of motion in audio-visual learning?", "cited_paper": [{"arxiv_id": "1611.05358", "title": "Lip Reading Sentences in the Wild", "year": 2016}, {"arxiv_id": "1804.03619", "title": "Looking to Listen at the Cocktail Party: A Speaker-Independent Audio-Visual Model for Speech Separation", "year": 2018}, {"arxiv_id": "1804.03641", "title": "Audio-Visual Scene Analysis with Self-Supervised Multisensory Features", "year": 2018}, {"arxiv_id": "1904.05979", "title": "The Sound of Motions", "year": 2019}, {"arxiv_id": "2211.03019", "title": "Hear The Flow: Optical Flow-Based Self-Supervised Visual Sound Source Localization", "year": 2022}, {"arxiv_id": "2211.08367", "title": "FlowGrad: Using Motion for Visual Sound Source Localization", "year": 2022}], "gt_label": [1, 1, 1, 1, 1, 1], "date": "2022-11", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_551", "valid": true}
{"query": "Which works used image generation models to create synthetic images for classification tasks?", "cited_paper": [{"arxiv_id": "2210.07574", "title": "Is synthetic data from generative models ready for image recognition?", "year": 2022}, {"arxiv_id": "2304.08466", "title": "Synthetic Data from Diffusion Models Improves ImageNet Classification", "year": 2023}, {"arxiv_id": "2302.02503", "title": "Leaving Reality to Imagination: Robust Classification via Generated Datasets", "year": 2023}], "gt_label": [1, 1, 1], "date": "2023-04", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_552", "valid": true}
{"query": "Which papers propose the use of generative adversarial networks (GANs) and counterfactuals to augment training sets as dataset-level mitigation strategies against bias amplification?", "cited_paper": [{"arxiv_id": "2012.01469", "title": "Fair Attribute Classification through Latent Space De-biasing", "year": 2020}, {"arxiv_id": "2004.06524", "title": "Contrastive Examples for Addressing the Tyranny of the Majority", "year": 2020}, {"arxiv_id": "1909.12434", "title": "Learning the Difference that Makes a Difference with Counterfactually-Augmented Data", "year": 2019}, {"arxiv_id": "2012.10040", "title": "Robustness to Spurious Correlations in Text Classification via Automatically Generated Counterfactuals", "year": 2020}], "gt_label": [1, 1, 1, 1], "date": "2020-12", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_553", "valid": true}
{"query": "Could you point to the literature that discusses the KL variation in relation to Proximal Policy Optimization?", "cited_paper": [], "gt_label": [], "date": "2017-07", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_554", "valid": true}
{"query": "What studies have presented connections between RNNs and early versions of GNNs?", "cited_paper": [], "gt_label": [], "date": "2018-12", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_555", "valid": true}
{"query": "What paper proposed the integration of a generative adversarial network (GAN) framework for solving the primal formulation of unbalanced Monge OT?", "cited_paper": [{"arxiv_id": "1810.11447", "title": "Scalable Unbalanced Optimal Transport using Generative Adversarial Networks", "year": 2018}], "gt_label": [1], "date": "2018-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_556", "valid": true}
{"query": "Which paper presented a modification to the decoder that allows SimulST using the wait-k policy and a fixed pre-decision module?", "cited_paper": [{"arxiv_id": "2011.00033", "title": "Streaming Simultaneous Speech Translation with Augmented Memory Transformer", "year": 2020}], "gt_label": [1], "date": "2020-11", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_557", "valid": true}
{"query": "Are there any papers about controllable Diffusion Models?", "cited_paper": [{"arxiv_id": "2302.05543", "title": "Adding Conditional Control to Text-to-Image Diffusion Models", "year": 2023}], "gt_label": [1], "date": "2023-02", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_558", "valid": true}
{"query": "Which paper discusses the usage of Logical Neural Networks in the Neuro-Symbolic RL framework?", "cited_paper": [{"arxiv_id": "2110.10963", "title": "Neuro-Symbolic Reinforcement Learning with First-Order Logic", "year": 2021}, {"arxiv_id": "2006.13155", "title": "Logical Neural Networks", "year": 2020}], "gt_label": [1, 1], "date": "2021-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_559", "valid": true}
{"query": "Are there any papers that explain the reasoning behind the phenomenon of arithmetic operations, such as linear analogies, revealing semantic meaning?", "cited_paper": [{"arxiv_id": "1810.04882", "title": "Towards Understanding Linear Word Analogies", "year": 2018}], "gt_label": [1], "date": "2018-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_560", "valid": true}
{"query": "Which works indicated models trained with their methods yield less satisfactory results in comparison to the researcher's approach?", "cited_paper": [{"arxiv_id": "1905.07088", "title": "Sliced Score Matching: A Scalable Approach to Density and Score Estimation", "year": 2019}, {"arxiv_id": "2007.03317", "title": "Efficient Learning of Generative Models via Finite-Difference Score Matching", "year": 2020}], "gt_label": [1, 1], "date": "2020-07", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_561", "valid": true}
{"query": "Which works achieve cross-modal interaction by matching the visual tokens of fixed patches and textual tokens?", "cited_paper": [{"arxiv_id": "2111.07783", "title": "FILIP: Fine-grained Interactive Language-Image Pre-Training", "year": 2021}], "gt_label": [1], "date": "2021-11", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_562", "valid": true}
{"query": "What is the paper that launched SemEval-2020 Task 1 on Unsupervised Lexical Semantic Change Detection?", "cited_paper": [{"arxiv_id": "2007.11464", "title": "SemEval-2020 Task 1: Unsupervised Lexical Semantic Change Detection", "year": 2020}], "gt_label": [1], "date": "2020-07", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_563", "valid": true}
{"query": "Which papers discuss the evaluation of synthetic images in the medical domain?", "cited_paper": [{"arxiv_id": "2102.08921", "title": "How Faithful is your Synthetic Data? Sample-level Metrics for Evaluating and Auditing Generative Models", "year": 2021}, {"arxiv_id": "1612.00542", "title": "Breast Mass Classification from Mammograms using Deep Convolutional Neural Networks", "year": 2016}, {"arxiv_id": "1706.08500", "title": "GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium", "year": 2017}, {"arxiv_id": "1806.00035", "title": "Assessing Generative Models via Precision and Recall", "year": 2018}], "gt_label": [1, 1, 1, 1], "date": "2021-02", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_564", "valid": true}
{"query": "Which work initiated the research agenda of replicable algorithm design?", "cited_paper": [{"arxiv_id": "2201.08430", "title": "Reproducibility in Learning", "year": 2022}], "gt_label": [1], "date": "2022-01", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_565", "valid": true}
{"query": "What are some of the selected works that involve research on implicit modeling related to LazyGNN?", "cited_paper": [{"arxiv_id": "1806.07366", "title": "Neural Ordinary Differential Equations", "year": 2018}, {"arxiv_id": "1908.06315", "title": "Implicit Deep Learning", "year": 2019}, {"arxiv_id": "1909.01377", "title": "Deep Equilibrium Models", "year": 2019}, {"arxiv_id": "2009.06211", "title": "Implicit Graph Neural Networks", "year": 2020}], "gt_label": [1, 1, 1, 1], "date": "2020-09", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_566", "valid": true}
{"query": "Which studies focused on improving the transformer-based 2D-to-3D pose lifting method with the per joint temporal characteristics and frequency domain feature?", "cited_paper": [{"arxiv_id": "2203.00859", "title": "MixSTE: Seq2seq Mixed Spatio-Temporal Encoder for 3D Human Pose Estimation in Video", "year": 2022}, {"arxiv_id": "2303.17472", "title": "PoseFormerV2: Exploring Frequency Domain for Efficient and Robust 3D Human Pose Estimation", "year": 2023}], "gt_label": [1, 1], "date": "2023-03", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_567", "valid": true}
{"query": "What work highlighted that using synthetic samples for augmented data can result in performance degradation?", "cited_paper": [{"arxiv_id": "1807.09499", "title": "How good is my GAN?", "year": 2018}], "gt_label": [1], "date": "2018-07", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_568", "valid": true}
{"query": "Which studies discuss diffusion-based text-to-image models in synthetic face generation?", "cited_paper": [{"arxiv_id": "2112.10752", "title": "High-Resolution Image Synthesis with Latent Diffusion Models", "year": 2021}, {"arxiv_id": "2204.06125", "title": "Hierarchical Text-Conditional Image Generation with CLIP Latents", "year": 2022}], "gt_label": [1, 1], "date": "2022-04", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_569", "valid": true}
{"query": "Which works proposed IL+RL methods that are based on including prior data in the replay buffer for a value-based approach?", "cited_paper": [{"arxiv_id": "1709.10089", "title": "Overcoming Exploration in Reinforcement Learning with Demonstrations", "year": 2017}, {"arxiv_id": "1707.08817", "title": "Leveraging Demonstrations for Deep Reinforcement Learning on Robotics Problems with Sparse Rewards", "year": 2017}], "gt_label": [1, 1], "date": "2017-09", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_570", "valid": true}
{"query": "Any papers around which indicate the application of Visual-Language Modeling in various scenarios?", "cited_paper": [{"arxiv_id": "2211.10277", "title": "Task Residual for Tuning Vision-Language Models", "year": 2022}, {"arxiv_id": "2309.13625", "title": "GraphAdapter: Tuning Vision-Language Models With Dual Knowledge Graph", "year": 2023}, {"arxiv_id": "2308.10916", "title": "Diffusion Model as Representation Learner", "year": 2023}, {"arxiv_id": "2312.12768", "title": "Mutual-modality Adversarial Attack with Semantic Perturbation", "year": 2023}, {"arxiv_id": "2312.00858", "title": "DeepCache: Accelerating Diffusion Models for Free", "year": 2023}], "gt_label": [1, 1, 1, 1, 1], "date": "2023-12", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_571", "valid": true}
{"query": "What works have uncovered inherent challenges in RLHF?", "cited_paper": [{"arxiv_id": "2307.15217", "title": "Open Problems and Fundamental Limitations of Reinforcement Learning from Human Feedback", "year": 2023}, {"arxiv_id": "2310.13595", "title": "The History and Risks of Reinforcement Learning and Human Feedback", "year": 2023}], "gt_label": [1, 1], "date": "2023-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_572", "valid": true}
{"query": "Which papers propose to apply modern variance reduction techniques to efficiently solve the regression problem?", "cited_paper": [{"arxiv_id": "1703.00102", "title": "SARAH: A Novel Method for Machine Learning Problems Using Stochastic Recursive Gradient", "year": 2017}, {"arxiv_id": "1905.10018", "title": "Momentum-Based Variance Reduction in Non-Convex SGD", "year": 2019}, {"arxiv_id": "2008.10898", "title": "PAGE: A Simple and Optimal Probabilistic Gradient Estimator for Nonconvex Optimization", "year": 2020}, {"arxiv_id": "2211.07937", "title": "An Improved Analysis of (Variance-Reduced) Policy Gradient and Natural Policy Gradient Methods", "year": 2022}], "gt_label": [1, 1, 1, 1], "date": "2022-11", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_573", "valid": true}
{"query": "Which studies have developed supervised disentanglement metrics for latent spaces?", "cited_paper": [{"arxiv_id": "1802.05983", "title": "Disentangling by Factorising", "year": 2018}], "gt_label": [1], "date": "2018-02", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_574", "valid": true}
{"query": "Which works suggest that careful engineering of the provided prompts can influence LLMs behavior?", "cited_paper": [{"arxiv_id": "2102.07350", "title": "Prompt Programming for Large Language Models: Beyond the Few-Shot Paradigm", "year": 2021}, {"arxiv_id": "2010.15980", "title": "AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts", "year": 2020}], "gt_label": [1, 1], "date": "2021-02", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_575", "valid": true}
{"query": "Can you provide me the study of safe RL with linear function approximation?", "cited_paper": [{"arxiv_id": "2106.06239", "title": "Safe Reinforcement Learning with Linear Function Approximation", "year": 2021}], "gt_label": [1], "date": "2021-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_576", "valid": true}
{"query": "Which works address moment matching approaches to domain adaptation?", "cited_paper": [{"arxiv_id": "2007.08702", "title": "DACS: Domain Adaptation via Cross-domain Mixed Sampling", "year": 2020}, {"arxiv_id": "1507.00504", "title": "Optimal Transport for Domain Adaptation", "year": 2015}], "gt_label": [1, 1], "date": "2020-07", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_577", "valid": true}
{"query": "Which studies are about Large Language Models (LLMs)?", "cited_paper": [{"arxiv_id": "2203.15556", "title": "Training Compute-Optimal Large Language Models", "year": 2022}, {"arxiv_id": "2005.14165", "title": "Language Models are Few-Shot Learners", "year": 2020}, {"arxiv_id": "2204.02311", "title": "PaLM: Scaling Language Modeling with Pathways", "year": 2022}], "gt_label": [1, 1, 1], "date": "2022-04", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_578", "valid": true}
{"query": "What papers discussed the technique of randomized smoothing for obtaining robust classifiers?", "cited_paper": [{"arxiv_id": "1902.02918", "title": "Certified Adversarial Robustness via Randomized Smoothing", "year": 2019}, {"arxiv_id": "2206.10550", "title": "(Certified!!) Adversarial Robustness for Free!", "year": 2022}], "gt_label": [1, 1], "date": "2022-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_579", "valid": true}
{"query": "Which paper established the theoretical framework for supervised adversarial training (sup-AT)?", "cited_paper": [{"arxiv_id": "1706.06083", "title": "Towards Deep Learning Models Resistant to Adversarial Attacks", "year": 2017}], "gt_label": [1], "date": "2017-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_580", "valid": true}
{"query": "Can you list some works that used Image-text pre-training for Vision and Language tasks?", "cited_paper": [{"arxiv_id": "1908.07490", "title": "LXMERT: Learning Cross-Modality Encoder Representations from Transformers", "year": 2019}, {"arxiv_id": "1909.11740", "title": "UNITER: UNiversal Image-TExt Representation Learning", "year": 2019}, {"arxiv_id": "2101.00529", "title": "VinVL: Revisiting Visual Representations in Vision-Language Models", "year": 2021}, {"arxiv_id": "2102.02779", "title": "Unifying Vision-and-Language Tasks via Text Generation", "year": 2021}, {"arxiv_id": "2111.12233", "title": "Scaling Up Vision-Language Pre-training for Image Captioning", "year": 2021}], "gt_label": [1, 1, 1, 1, 1], "date": "2021-11", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_581", "valid": true}
{"query": "What works predict 3D keypoints on the object from which the pose can be extracted?", "cited_paper": [{"arxiv_id": "1911.04231", "title": "PVN3D: A Deep Point-wise 3D Keypoints Voting Network for 6DoF Pose Estimation", "year": 2019}], "gt_label": [1], "date": "2019-11", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_582", "valid": true}
{"query": "Which papers focus on refining the latent dynamics model learning by proposing a joint learning scheme?", "cited_paper": [{"arxiv_id": "1912.01603", "title": "Dream to Control: Learning Behaviors by Latent Imagination", "year": 2019}, {"arxiv_id": "2010.02193", "title": "Mastering Atari with Discrete World Models", "year": 2020}], "gt_label": [1, 1], "date": "2020-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_583", "valid": true}
{"query": "Which research papers extend the distillation process to train NeRF for the 2D-to-3D task?", "cited_paper": [{"arxiv_id": "2303.14184", "title": "Make-It-3D: High-Fidelity 3D Creation from A Single Image with Diffusion Prior", "year": 2023}, {"arxiv_id": "2306.17843", "title": "Magic123: One Image to High-Quality 3D Object Generation Using Both 2D and 3D Diffusion Priors", "year": 2023}], "gt_label": [1, 1], "date": "2023-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_584", "valid": true}
{"query": "Could you provide me some studies about medical anomaly detection?", "cited_paper": [{"arxiv_id": "2008.03632", "title": "Encoding Structure-Texture Relation with P-Net for Anomaly Detection in Retinal Images", "year": 2020}, {"arxiv_id": "2110.01761", "title": "Proxy-bridged Image Reconstruction Network for Anomaly Detection in Medical Images", "year": 2021}, {"arxiv_id": "2306.11876", "title": "BMAD: Benchmarks for Medical Anomaly Detection", "year": 2023}, {"arxiv_id": "2210.04227", "title": "Dual-distribution discrepancy with self-supervised refinement for anomaly detection in medical images", "year": 2022}, {"arxiv_id": "2308.01639", "title": "Multi-scale Cross-restoration Framework for Electrocardiogram Anomaly Detection", "year": 2023}], "gt_label": [1, 1, 1, 1, 1], "date": "2023-08", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_585", "valid": true}
{"query": "Could you provide me some works about the successful use of Vision-language models in various downstream tasks?", "cited_paper": [{"arxiv_id": "2103.00020", "title": "Learning Transferable Visual Models From Natural Language Supervision", "year": 2021}, {"arxiv_id": "2112.04482", "title": "FLAVA: A Foundational Language And Vision Alignment Model", "year": 2021}, {"arxiv_id": "2010.00747", "title": "Contrastive Learning of Medical Visual Representations from Paired Images and Text", "year": 2020}, {"arxiv_id": "2111.11432", "title": "Florence: A New Foundation Model for Computer Vision", "year": 2021}, {"arxiv_id": "2102.05918", "title": "Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision", "year": 2021}, {"arxiv_id": "2204.14198", "title": "Flamingo: a Visual Language Model for Few-Shot Learning", "year": 2022}], "gt_label": [1, 1, 1, 1, 1, 1], "date": "2022-04", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_586", "valid": true}
{"query": "What studies focused on bounding the GE specifically for deep iterative recovery algorithms?", "cited_paper": [{"arxiv_id": "2010.15658", "title": "Compressive Sensing and Neural Networks from a Statistical Learning Perspective", "year": 2020}], "gt_label": [1], "date": "2021-12", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_587", "valid": true}
{"query": "What research work highlighted ALBEF's incorporation of the ITC loss and in-batch hard negative sampling strategy for ITM?", "cited_paper": [{"arxiv_id": "2107.07651", "title": "Align before Fuse: Vision and Language Representation Learning with Momentum Distillation", "year": 2021}], "gt_label": [1], "date": "2021-07", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_588", "valid": true}
{"query": "Which papers proposed a hyper neural network in meta-learning?", "cited_paper": [{"arxiv_id": "1807.01613", "title": "Conditional Neural Processes", "year": 2018}, {"arxiv_id": "1703.00837", "title": "Meta Networks", "year": 2017}], "gt_label": [1, 1], "date": "2018-07", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_589", "valid": true}
{"query": "Could you provide me research about data-driven methods generating 2D motions based on facial keypoints or movement frequencies?", "cited_paper": [{"arxiv_id": "1801.09092", "title": "Interactive Generative Adversarial Networks for Facial Expression Generation in Dyadic Interactions", "year": 2018}], "gt_label": [1], "date": "2018-01", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_590", "valid": true}
{"query": "What papers inspired the logical inductive bias of the study?", "cited_paper": [{"arxiv_id": "1805.10872", "title": "DeepProbLog: Neural Probabilistic Logic Programming", "year": 2018}, {"arxiv_id": "1711.04574", "title": "Learning Explanatory Rules from Noisy Data", "year": 2017}], "gt_label": [1, 1], "date": "2018-05", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_591", "valid": true}
{"query": "Which research decomposes features for occupancy segmentation into a 3D space?", "cited_paper": [{"arxiv_id": "2302.07817", "title": "Tri-Perspective View for Vision-Based 3D Semantic Occupancy Prediction", "year": 2023}], "gt_label": [1], "date": "2023-02", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_592", "valid": true}
{"query": "What research proposed building an offline memory bank or a backbone with reversible modules to address GPU memory constraints for TAD?", "cited_paper": [{"arxiv_id": "2204.01680", "title": "TALLFormer: Temporal Action Localization with a Long-memory Transformer", "year": 2022}, {"arxiv_id": "2211.14053", "title": "Re^2TAL: Rewiring Pretrained Video Backbones for Reversible Temporal Action Localization", "year": 2022}], "gt_label": [1, 1], "date": "2022-11", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_593", "valid": true}
{"query": "What work proposed E(3) Equivariant Diffusion Models (EDM) for molecular design?", "cited_paper": [{"arxiv_id": "2203.17003", "title": "Equivariant Diffusion for Molecule Generation in 3D", "year": 2022}], "gt_label": [1], "date": "2022-03", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_594", "valid": true}
{"query": "Which works have been proposed to solve image restoration problems using CNN?", "cited_paper": [{"arxiv_id": "1906.12021", "title": "Densely Residual Laplacian Super-Resolution", "year": 2019}, {"arxiv_id": "2110.03680", "title": "Burst Image Restoration and Enhancement", "year": 2021}, {"arxiv_id": "2003.06792", "title": "Learning Enriched Features for Real Image Restoration and Enhancement", "year": 2020}, {"arxiv_id": "2102.02808", "title": "Multi-Stage Progressive Image Restoration", "year": 2021}, {"arxiv_id": "2205.05996", "title": "Blueprint Separable Residual Network for Efficient Image Super-Resolution", "year": 2022}, {"arxiv_id": "2104.09497", "title": "Attention in Attention Network for Image Super-Resolution", "year": 2021}], "gt_label": [1, 1, 1, 1, 1, 1], "date": "2022-05", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_595", "valid": true}
{"query": "Can you name the works jointly consider bandit PCA and its rank-1 special cases?", "cited_paper": [{"arxiv_id": "1902.03035", "title": "Bandit Principal Component Analysis", "year": 2019}, {"arxiv_id": "2106.01660", "title": "Bandit Phase Retrieval", "year": 2021}], "gt_label": [1, 1], "date": "2021-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_596", "valid": true}
{"query": "What studies have proposed GAN-based methods to learn OT plans?", "cited_paper": [{"arxiv_id": "1406.2661", "title": "Generative Adversarial Networks", "year": 2014}, {"arxiv_id": "2003.06635", "title": "Large-Scale Optimal Transport via Adversarial Training with Cycle-Consistency", "year": 2020}, {"arxiv_id": "1905.00158", "title": "On Scalable and Efficient Computation of Large Scale Optimal Transport", "year": 2019}, {"arxiv_id": "2202.07965", "title": "GAN Estimation of Lipschitz Optimal Transport Maps", "year": 2022}], "gt_label": [1, 1, 1, 1], "date": "2022-02", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_597", "valid": true}
{"query": "Could you list some prior works that proposed special-purpose significance tests for different conditions in Machine Learning?", "cited_paper": [{"arxiv_id": "1709.09500", "title": "Replicability Analysis for Natural Language Processing: Testing Significance with Multiple Datasets", "year": 2017}], "gt_label": [1], "date": "2017-09", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_598", "valid": true}
{"query": "Could you provide me some studies about implementing Knowledge Distillation (KD) in various research fields?", "cited_paper": [{"arxiv_id": "1503.02531", "title": "Distilling the Knowledge in a Neural Network", "year": 2015}, {"arxiv_id": "1412.6550", "title": "FitNets: Hints for Thin Deep Nets", "year": 2014}, {"arxiv_id": "2204.06986", "title": "Cross-Image Relational Knowledge Distillation for Semantic Segmentation", "year": 2022}, {"arxiv_id": "1909.10351", "title": "TinyBERT: Distilling BERT for Natural Language Understanding", "year": 2019}, {"arxiv_id": "2104.02096", "title": "Compressing Visual-linguistic Model via Knowledge Distillation", "year": 2021}, {"arxiv_id": "2204.10496", "title": "Multimodal Adaptive Distillation for Leveraging Unimodal Encoders for Vision-Language Tasks", "year": 2022}], "gt_label": [1, 1, 1, 1, 1, 1], "date": "2022-04", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_599", "valid": true}
{"query": "Which works consider data augmentation as a viable option for improving NLI models?", "cited_paper": [{"arxiv_id": "1808.08609", "title": "Adversarially Regularising Neural NLI Models to Integrate Logical Background Knowledge", "year": 2018}, {"arxiv_id": "2101.00288", "title": "Polyjuice: Generating Counterfactuals for Explaining, Evaluating, and Improving Models", "year": 2021}, {"arxiv_id": "2203.12942", "title": "Generating Data to Mitigate Spurious Correlations in Natural Language Inference Datasets", "year": 2022}], "gt_label": [1, 1, 1], "date": "2022-03", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_600", "valid": true}
{"query": "What works apply score functions such as probability-based method, logit-based method, and feature-based method for out-of-distribution detection in computer vision?", "cited_paper": [{"arxiv_id": "1610.02136", "title": "A Baseline for Detecting Misclassified and Out-of-Distribution Examples in Neural Networks", "year": 2016}, {"arxiv_id": "1911.11132", "title": "Scaling Out-of-Distribution Detection for Real-World Settings", "year": 2019}, {"arxiv_id": "2110.00218", "title": "On the Importance of Gradients for Detecting Distributional Shifts in the Wild", "year": 2021}, {"arxiv_id": "1706.02690", "title": "Enhancing The Reliability of Out-of-distribution Image Detection in Neural Networks", "year": 2017}, {"arxiv_id": "2111.12797", "title": "ReAct: Out-of-distribution Detection With Rectified Activations", "year": 2021}, {"arxiv_id": "2204.06507", "title": "Out-of-Distribution Detection with Deep Nearest Neighbors", "year": 2022}, {"arxiv_id": "2010.03759", "title": "Energy-based Out-of-distribution Detection", "year": 2020}, {"arxiv_id": "1911.11132", "title": "Scaling Out-of-Distribution Detection for Real-World Settings", "year": 2019}, {"arxiv_id": "2111.12797", "title": "ReAct: Out-of-distribution Detection With Rectified Activations", "year": 2021}, {"arxiv_id": "1807.03888", "title": "A Simple Unified Framework for Detecting Out-of-Distribution Samples and Adversarial Attacks", "year": 2018}, {"arxiv_id": "2203.10807", "title": "ViM: Out-Of-Distribution with Virtual-logit Matching", "year": 2022}], "gt_label": [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], "date": "2022-04", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_601", "valid": true}
{"query": "Could you provide me some works about association methods utilized for solving object navigation tasks?", "cited_paper": [{"arxiv_id": "1909.04306", "title": "Bayesian Relational Memory for Semantic Visual Navigation", "year": 2019}, {"arxiv_id": "1810.06543", "title": "Visual Semantic Navigation using Scene Priors", "year": 2018}], "gt_label": [1, 1], "date": "2019-09", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_602", "valid": true}
{"query": "Any studies about the Process Reward Model (PRM) and its comparison with the Outcome Reward Model (ORM)?", "cited_paper": [{"arxiv_id": "2305.20050", "title": "Let's Verify Step by Step", "year": 2023}], "gt_label": [1], "date": "2023-05", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_603", "valid": true}
{"query": "Which papers describes the approach of using a small set of high-quality human-written translations or a set of translation instructions for fine-tuning LLMs in Machine Translation?", "cited_paper": [{"arxiv_id": "2305.15083", "title": "Eliciting the Translation Ability of Large Language Models via Multilingual Finetuning with Translation Instructions", "year": 2023}, {"arxiv_id": "2307.04408", "title": "TIM: Teaching Large Language Models to Translate with Comparison", "year": 2023}], "gt_label": [1, 1], "date": "2023-07", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_604", "valid": true}
{"query": "What paper proposed a method that leverages a font-adaptive neural network and a color-preserving model for scene text editing?", "cited_paper": [{"arxiv_id": "1903.01192", "title": "STEFANN: Scene Text Editor using Font Adaptive Neural Network", "year": 2019}], "gt_label": [1], "date": "2019-03", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_605", "valid": true}
{"query": "Which works related to ccbo utilize an acquisition function in constrained BO methods?", "cited_paper": [{"arxiv_id": "1403.5607", "title": "Bayesian Optimization with Unknown Constraints", "year": 2014}, {"arxiv_id": "1406.2541", "title": "Predictive Entropy Search for Efficient Global Optimization of Black-box Functions", "year": 2014}], "gt_label": [1, 1], "date": "2017-09", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_606", "valid": true}
{"query": "Which studies applied self-supervised contrastive learning methods?", "cited_paper": [{"arxiv_id": "1805.01978", "title": "Unsupervised Feature Learning via Non-Parametric Instance-level Discrimination", "year": 2018}, {"arxiv_id": "1911.05722", "title": "Momentum Contrast for Unsupervised Visual Representation Learning", "year": 2019}, {"arxiv_id": "2002.05709", "title": "A Simple Framework for Contrastive Learning of Visual Representations", "year": 2020}, {"arxiv_id": "2006.10029", "title": "Big Self-Supervised Models are Strong Semi-Supervised Learners", "year": 2020}], "gt_label": [1, 1, 1, 1], "date": "2020-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_607", "valid": true}
{"query": "Which papers developed algorithms that can achieve a linear speedup for nonconvex-strongly-concave optimization problems in federated learning?", "cited_paper": [{"arxiv_id": "2203.04850", "title": "Federated Minimax Optimization: Improved Convergence Analyses and Algorithms", "year": 2022}, {"arxiv_id": "2005.02426", "title": "Communication-Efficient Distributed Stochastic AUC Maximization with Deep Neural Networks", "year": 2020}], "gt_label": [1, 1], "date": "2022-03", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_608", "valid": true}
{"query": "What efforts have been made in the development of Tree-of-Thought prompting in the context of Large Language Models?", "cited_paper": [{"arxiv_id": "2305.10601", "title": "Tree of Thoughts: Deliberate Problem Solving with Large Language Models", "year": 2023}], "gt_label": [1], "date": "2023-05", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_609", "valid": true}
{"query": "What work employs kernel ridge-regression with NTK to formulate dataset distillation?", "cited_paper": [{"arxiv_id": "1806.07572", "title": "Neural Tangent Kernel: Convergence and Generalization in Neural Networks", "year": 2018}], "gt_label": [1], "date": "2018-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_610", "valid": true}
{"query": "What study refers to the annotation's issue of hypotheses alone being highly predictive of the label?", "cited_paper": [{"arxiv_id": "1803.02324", "title": "Annotation Artifacts in Natural Language Inference Data", "year": 2018}], "gt_label": [1], "date": "2018-03", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_611", "valid": true}
{"query": "Which works discuss the area of test-time adaptation?", "cited_paper": [{"arxiv_id": "2006.10963", "title": "Evaluating Prediction-Time Batch Normalization for Robustness under Covariate Shift", "year": 2020}, {"arxiv_id": "2302.05155", "title": "TTN: A Domain-Shift Aware Batch Normalization in Test-Time Adaptation", "year": 2023}, {"arxiv_id": "2006.10726", "title": "Tent: Fully Test-time Adaptation by Entropy Minimization", "year": 2020}, {"arxiv_id": "2207.11707", "title": "Improving Test-Time Adaptation via Shift-agnostic Weight Regularization and Nearest Source Prototypes", "year": 2022}], "gt_label": [1, 1, 1, 1], "date": "2023-02", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_612", "valid": true}
{"query": "Could we name some research that utilized specifying noise for ensuring the coherent fluency between clips under different text commands?", "cited_paper": [{"arxiv_id": "2212.04248", "title": "Talking Head Generation with Probabilistic Audio-to-Visual Diffusion Priors", "year": 2022}], "gt_label": [1], "date": "2022-12", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_613", "valid": true}
{"query": "Could you give me some works that proposed methods of approximate inference or posterior sampling that could be used for a design framework?", "cited_paper": [{"arxiv_id": "2106.07635", "title": "Variational Causal Networks: Approximate Bayesian Inference over Causal Structures", "year": 2021}, {"arxiv_id": "2112.02761", "title": "BCD Nets: Scalable Variational Approaches for Bayesian Causal Discovery", "year": 2021}, {"arxiv_id": "2202.13903", "title": "Bayesian Structure Learning with Generative Flow Networks", "year": 2022}], "gt_label": [1, 1, 1], "date": "2022-11", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_614", "valid": true}
{"query": "What studies use instance slots in their models and solve their routing problem through mixture models with amortized variational inference?", "cited_paper": [{"arxiv_id": "1606.06724", "title": "Tagger: Deep Unsupervised Perceptual Grouping", "year": 2016}, {"arxiv_id": "1903.00450", "title": "Multi-Object Representation Learning with Iterative Variational Inference", "year": 2019}], "gt_label": [1, 1], "date": "2019-03", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_615", "valid": true}
{"query": "Any works targeting the enhancement of LLMs factuality?", "cited_paper": [{"arxiv_id": "2309.14525", "title": "Aligning Large Multimodal Models with Factually Augmented RLHF", "year": 2023}, {"arxiv_id": "2305.11206", "title": "LIMA: Less Is More for Alignment", "year": 2023}, {"arxiv_id": "2305.20050", "title": "Let's Verify Step by Step", "year": 2023}, {"arxiv_id": "2302.12813", "title": "Check Your Facts and Try Again: Improving Large Language Models with External Knowledge and Automated Feedback", "year": 2023}, {"arxiv_id": "2305.13269", "title": "Chain-of-Knowledge: Grounding Large Language Models via Dynamic Knowledge Adapting over Heterogeneous Sources", "year": 2023}, {"arxiv_id": "2212.10511", "title": "When Not to Trust Language Models: Investigating Effectiveness of Parametric and Non-Parametric Memories", "year": 2022}, {"arxiv_id": "2307.03987", "title": "A Stitch in Time Saves Nine: Detecting and Mitigating Hallucinations of LLMs by Validating Low-Confidence Generation", "year": 2023}], "gt_label": [1, 1, 1, 1, 1, 1, 1], "date": "2023-09", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_616", "valid": true}
{"query": "Which work propose to learn a score-based Average Thresholded Confidence (ATC) by leveraging the softmax probability of a CNN classifiers?", "cited_paper": [{"arxiv_id": "2201.04234", "title": "Leveraging Unlabeled Data to Predict Out-of-Distribution Performance", "year": 2022}], "gt_label": [1], "date": "2022-01", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_617", "valid": true}
{"query": "Which studies showed the recovery of samples from a training dataset using the gradients generated during training?", "cited_paper": [{"arxiv_id": "1906.08935", "title": "Deep Leakage from Gradients", "year": 2019}], "gt_label": [1], "date": "2019-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_618", "valid": true}
{"query": "What studies have applied parallel atrous convolutions in their method for deblurring?", "cited_paper": [{"arxiv_id": "2108.09108", "title": "Single Image Defocus Deblurring Using Kernel-Sharing Parallel Atrous Convolutions", "year": 2021}], "gt_label": [1], "date": "2021-08", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_619", "valid": true}
{"query": "What papers are about white-box attacks, a type of adversarial attacks where the adversary has complete access to the model parameters?", "cited_paper": [{"arxiv_id": "1312.6199", "title": "Intriguing properties of neural networks", "year": 2013}, {"arxiv_id": "1608.04644", "title": "Towards Evaluating the Robustness of Neural Networks", "year": 2016}, {"arxiv_id": "1802.00420", "title": "Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples", "year": 2018}], "gt_label": [1, 1, 1], "date": "2018-02", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_620", "valid": true}
{"query": "Could you name the studies that implemented recurrent neural networks (RNN) for improving real-time performance in full-body motion estimation?", "cited_paper": [{"arxiv_id": "1810.04703", "title": "Deep Inertial Poser: Learning to Reconstruct Human Pose from Sparse Inertial Measurements in Real Time", "year": 2018}], "gt_label": [1], "date": "2018-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_621", "valid": true}
{"query": "What papers have demonstrated the efficacy of label smoothing in both visual and language domains?", "cited_paper": [{"arxiv_id": "1512.00567", "title": "Rethinking the Inception Architecture for Computer Vision", "year": 2015}, {"arxiv_id": "1906.02629", "title": "When Does Label Smoothing Help?", "year": 2019}, {"arxiv_id": "2001.01900", "title": "Regularization via Structural Label Smoothing", "year": 2020}, {"arxiv_id": "2009.06432", "title": "Adaptive Label Smoothing", "year": 2020}, {"arxiv_id": "2210.13459", "title": "Adaptive Label Smoothing with Self-Knowledge in Natural Language Generation", "year": 2022}], "gt_label": [1, 1, 1, 1, 1], "date": "2022-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_622", "valid": true}
{"query": "In what studies were Error Imputation-based (EIB) unbiased learning method derived?", "cited_paper": [], "gt_label": [], "date": "2012-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_623", "valid": true}
{"query": "What papers discuss the use of Wikipedia as a multilingual dataset for pretraining language models?", "cited_paper": [{"arxiv_id": "1810.04805", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "year": 2018}, {"arxiv_id": "1909.08053", "title": "Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism", "year": 2019}, {"arxiv_id": "1910.10683", "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer", "year": 2019}, {"arxiv_id": "2005.14165", "title": "Language Models are Few-Shot Learners", "year": 2020}], "gt_label": [1, 1, 1, 1], "date": "2020-05", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_624", "valid": true}
{"query": "What papers have studied the use of reinforcement learning or evolutionary algorithms in neural architecture search (NAS)?", "cited_paper": [{"arxiv_id": "1611.01578", "title": "Neural Architecture Search with Reinforcement Learning", "year": 2016}, {"arxiv_id": "1611.02167", "title": "Designing Neural Network Architectures using Reinforcement Learning", "year": 2016}, {"arxiv_id": "1708.05552", "title": "Practical Block-wise Neural Network Architecture Generation", "year": 2017}, {"arxiv_id": "1703.01041", "title": "Large-Scale Evolution of Image Classifiers", "year": 2017}, {"arxiv_id": "1711.00436", "title": "Hierarchical Representations for Efficient Architecture Search", "year": 2017}, {"arxiv_id": "1804.09081", "title": "Efficient Multi-objective Neural Architecture Search via Lamarckian Evolution", "year": 2018}, {"arxiv_id": "1802.01548", "title": "Regularized Evolution for Image Classifier Architecture Search", "year": 2018}], "gt_label": [1, 1, 1, 1, 1, 1, 1], "date": "2018-04", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_625", "valid": true}
{"query": "Which studies achieved success in self-supervised representation learning through contrastive learning?", "cited_paper": [{"arxiv_id": "2002.05709", "title": "A Simple Framework for Contrastive Learning of Visual Representations", "year": 2020}, {"arxiv_id": "1911.05722", "title": "Momentum Contrast for Unsupervised Visual Representation Learning", "year": 2019}, {"arxiv_id": "2003.04297", "title": "Improved Baselines with Momentum Contrastive Learning", "year": 2020}, {"arxiv_id": "2007.00224", "title": "Debiased Contrastive Learning", "year": 2020}, {"arxiv_id": "2104.14294", "title": "Emerging Properties in Self-Supervised Vision Transformers", "year": 2021}], "gt_label": [1, 1, 1, 1, 1], "date": "2021-04", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_626", "valid": true}
{"query": "What research efforts have been made to improve efficient INT8 quantisation?", "cited_paper": [{"arxiv_id": "2206.01861", "title": "ZeroQuant: Efficient and Affordable Post-Training Quantization for Large-Scale Transformers", "year": 2022}, {"arxiv_id": "2208.07339", "title": "LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale", "year": 2022}, {"arxiv_id": "2211.10438", "title": "SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models", "year": 2022}], "gt_label": [1, 1, 1], "date": "2022-11", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_627", "valid": true}
{"query": "Which study first introduced the Dynamic routing in capsule networks?", "cited_paper": [{"arxiv_id": "1710.09829", "title": "Dynamic Routing Between Capsules", "year": 2017}], "gt_label": [1], "date": "2017-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_628", "valid": true}
{"query": "What papers discuss the influences of multiple training examples on a model's prediction?", "cited_paper": [{"arxiv_id": "1905.13289", "title": "On the Accuracy of Influence Functions for Measuring Group Effects", "year": 2019}], "gt_label": [1], "date": "2019-05", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_629", "valid": true}
{"query": "What works revealed that the expressiveness of MPNNs and k-GNNs is bounded by k-WL?", "cited_paper": [{"arxiv_id": "1810.00826", "title": "How Powerful are Graph Neural Networks?", "year": 2018}, {"arxiv_id": "1810.02244", "title": "Weisfeiler and Leman Go Neural: Higher-order Graph Neural Networks", "year": 2018}], "gt_label": [1, 1], "date": "2018-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_630", "valid": true}
{"query": "Which works enhanced diffusion based on an initial input graph in latent graph and topology inference?", "cited_paper": [{"arxiv_id": "1911.05485", "title": "Diffusion Improves Graph Learning", "year": 2019}, {"arxiv_id": "2006.05205", "title": "On the Bottleneck of Graph Neural Networks and its Practical Implications", "year": 2020}], "gt_label": [1, 1], "date": "2020-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_631", "valid": true}
{"query": "What works proposed methods of improving the generation quality for SLP using adversarial training, mixture density networks, and dictionary representations?", "cited_paper": [{"arxiv_id": "2008.12405", "title": "Adversarial Training for Multi-Channel Sign Language Production", "year": 2020}, {"arxiv_id": "2107.11317", "title": "Mixed SIGNals: Sign Language Production via a Mixture of Motion Primitives", "year": 2021}, {"arxiv_id": "2203.15354", "title": "Signing at Scale: Learning to Co-Articulate Signs for Large-Scale Photo-Realistic Sign Language Production", "year": 2022}], "gt_label": [1, 1, 1], "date": "2022-03", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_632", "valid": true}
{"query": "Which work drove the success of VLMs by training transformers on large scale image-text pairs data using contrastive learning?", "cited_paper": [{"arxiv_id": "2103.00020", "title": "Learning Transferable Visual Models From Natural Language Supervision", "year": 2021}, {"arxiv_id": "2102.05918", "title": "Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision", "year": 2021}], "gt_label": [1, 1], "date": "2021-03", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_633", "valid": true}
{"query": "Could you name some methods that require a neural network forward pass to get embeddings?", "cited_paper": [{"arxiv_id": "1708.00489", "title": "Active Learning for Convolutional Neural Networks: A Core-Set Approach", "year": 2017}], "gt_label": [1], "date": "2022-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_634", "valid": true}
{"query": "Which works proposed variations of non-local blocks for aggregating long-range context in semantic segmentation models?", "cited_paper": [{"arxiv_id": "1809.02983", "title": "Dual Attention Network for Scene Segmentation", "year": 2018}, {"arxiv_id": "1809.00916", "title": "OCNet: Object Context Network for Scene Parsing", "year": 2018}, {"arxiv_id": "1811.11721", "title": "CCNet: Criss-Cross Attention for Semantic Segmentation", "year": 2018}, {"arxiv_id": "1711.07971", "title": "Non-local Neural Networks", "year": 2017}], "gt_label": [1, 1, 1, 1], "date": "2018-11", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_635", "valid": true}
{"query": "In what work was mentioned the fine-tuning of GPT-3 (175B) for answering open-domain questions?", "cited_paper": [{"arxiv_id": "2112.09332", "title": "WebGPT: Browser-assisted question-answering with human feedback", "year": 2021}], "gt_label": [1], "date": "2021-12", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_636", "valid": true}
{"query": "In which work were pretrained language models augmented with a mechanism to directly attend to a single context image?", "cited_paper": [{"arxiv_id": "2204.14198", "title": "Flamingo: a Visual Language Model for Few-Shot Learning", "year": 2022}], "gt_label": [1], "date": "2022-04", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_637", "valid": true}
{"query": "What works developed local methods for finding meaningful latent perturbations?", "cited_paper": [{"arxiv_id": "2103.17249", "title": "StyleCLIP: Text-Driven Manipulation of StyleGAN Imagery", "year": 2021}, {"arxiv_id": "2008.02401", "title": "StyleFlow: Attribute-conditioned Exploration of StyleGAN-Generated Images using Conditional Continuous Normalizing Flows", "year": 2020}, {"arxiv_id": "2106.04488", "title": "Low-Rank Subspaces in GANs", "year": 2021}, {"arxiv_id": "2106.06959", "title": "Do Not Escape From the Manifold: Discovering the Local Coordinates on the Latent Space of GANs", "year": 2021}], "gt_label": [1, 1, 1, 1], "date": "2021-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_638", "valid": true}
{"query": "Which papers made contributions in designing hierarchical Transformer architectures for document classification?", "cited_paper": [{"arxiv_id": "1910.10781", "title": "Hierarchical Transformers for Long Document Classification", "year": 2019}, {"arxiv_id": "2204.06683", "title": "Revisiting Transformer-based Models for Long Document Classification", "year": 2022}], "gt_label": [1, 1], "date": "2022-04", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_639", "valid": true}
{"query": "Could you provide me some studies that proposed strategies to mitigate biases in NLI models?", "cited_paper": [{"arxiv_id": "1908.10763", "title": "Unlearn Dataset Bias in Natural Language Inference by Fitting the Residual", "year": 2019}, {"arxiv_id": "1909.03683", "title": "Don't Take the Easy Way Out: Ensemble Based Methods for Avoiding Known Dataset Biases", "year": 2019}, {"arxiv_id": "1909.06321", "title": "End-to-End Bias Mitigation by Modelling Biases in Corpora", "year": 2019}], "gt_label": [1, 1, 1], "date": "2019-09", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_640", "valid": true}
{"query": "Which papers focus on combining vision and language inputs in an embodied setting with the goal of direct action prediction?", "cited_paper": [{"arxiv_id": "2209.04899", "title": "Instruction-driven history-aware policies for robotic manipulations", "year": 2022}, {"arxiv_id": "2209.05451", "title": "Perceiver-Actor: A Multi-Task Transformer for Robotic Manipulation", "year": 2022}, {"arxiv_id": "2109.12098", "title": "CLIPort: What and Where Pathways for Robotic Manipulation", "year": 2021}, {"arxiv_id": "2106.03427", "title": "Hierarchical Task Learning from Language Instructions with Unified Transformers and Self-Monitoring", "year": 2021}, {"arxiv_id": "2202.02005", "title": "BC-Z: Zero-Shot Task Generalization with Robotic Imitation Learning", "year": 2022}, {"arxiv_id": "2109.01115", "title": "Learning Language-Conditioned Robot Behavior from Offline Data and Crowd-Sourced Annotation", "year": 2021}, {"arxiv_id": "2210.06407", "title": "Interactive Language: Talking to Robots in Real Time", "year": 2022}, {"arxiv_id": "2212.06817", "title": "RT-1: Robotics Transformer for Real-World Control at Scale", "year": 2022}], "gt_label": [1, 1, 1, 1, 1, 1, 1, 1], "date": "2022-12", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_641", "valid": true}
{"query": "Which papers explored enhancing the accuracy of responses by concurrently generating reasoning processes while producing answers?", "cited_paper": [{"arxiv_id": "2201.11903", "title": "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models", "year": 2022}, {"arxiv_id": "2309.15402", "title": "Navigate through Enigmatic Labyrinth A Survey of Chain of Thought Reasoning: Advances, Frontiers and Future", "year": 2023}], "gt_label": [1, 1], "date": "2023-09", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_642", "valid": true}
{"query": "Which papers have conducted a sublinear convergence analysis of softmax tabular policies?", "cited_paper": [{"arxiv_id": "1909.02769", "title": "Adaptive Trust Region Policy Optimization: Global Convergence and Faster Rates for Regularized MDPs", "year": 2019}, {"arxiv_id": "1908.00261", "title": "On the Theory of Policy Gradient Methods: Optimality, Approximation, and Distribution Shift", "year": 2019}, {"arxiv_id": "2102.09318", "title": "Finite-Sample Analysis of Off-Policy Natural Actor-Critic Algorithm", "year": 2021}, {"arxiv_id": "2201.07443", "title": "On the Convergence Rates of Policy Gradient Methods", "year": 2022}, {"arxiv_id": "1901.11275", "title": "A Theory of Regularized Markov Decision Processes", "year": 2019}], "gt_label": [1, 1, 1, 1, 1], "date": "2022-01", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_643", "valid": true}
{"query": "Which study proposed OpenSeg, a technique for fine-tuning a model using class-agnostic masks and image-text pair data?", "cited_paper": [{"arxiv_id": "2112.12143", "title": "Scaling Open-Vocabulary Image Segmentation with Image-Level Labels", "year": 2021}, {"arxiv_id": "2102.05918", "title": "Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision", "year": 2021}], "gt_label": [1, 1], "date": "2021-12", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_644", "valid": true}
{"query": "What are the works that handled volumetric radiative decomposition?", "cited_paper": [{"arxiv_id": "2007.09892", "title": "Deep Reflectance Volumes: Relightable Reconstructions from Multi-View Photometric Images", "year": 2020}, {"arxiv_id": "2011.12490", "title": "DeRF: Decomposed Radiance Fields", "year": 2020}, {"arxiv_id": "2106.01970", "title": "NeRFactor: Neural Factorization of Shape and Reflectance Under an Unknown Illumination", "year": 2021}, {"arxiv_id": "2112.03907", "title": "Ref-NeRF: Structured View-Dependent Appearance for Neural Radiance Fields", "year": 2021}], "gt_label": [1, 1, 1, 1], "date": "2021-12", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_645", "valid": true}
{"query": "Which studies discuss hallucinations in LLMs?", "cited_paper": [{"arxiv_id": "2310.05470", "title": "Generative Judge for Evaluating Alignment", "year": 2023}, {"arxiv_id": "2309.01219", "title": "Siren's Song in the AI Ocean: A Survey on Hallucination in Large Language Models", "year": 2023}, {"arxiv_id": "2401.01313", "title": "A Comprehensive Survey of Hallucination Mitigation Techniques in Large Language Models", "year": 2024}], "gt_label": [1, 1, 1], "date": "2024-01", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_646", "valid": true}
{"query": "What papers have been written on incorporating unsafe prompt detection into online ChatBot and LLM-integrated applications?", "cited_paper": [{"arxiv_id": "2302.07842", "title": "Augmented Language Models: a Survey", "year": 2023}, {"arxiv_id": "2302.04761", "title": "Toolformer: Language Models Can Teach Themselves to Use Tools", "year": 2023}, {"arxiv_id": "2303.17580", "title": "HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face", "year": 2023}], "gt_label": [1, 1, 1], "date": "2023-03", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_647", "valid": true}
{"query": "What studies have used prototypical networks and neural processes in the field of efficient meta-learning?", "cited_paper": [{"arxiv_id": "1703.05175", "title": "Prototypical Networks for Few-shot Learning", "year": 2017}, {"arxiv_id": "2107.01105", "title": "Memory Efficient Meta-Learning with Large Images", "year": 2021}, {"arxiv_id": "1807.01622", "title": "Neural Processes", "year": 2018}, {"arxiv_id": "1807.01613", "title": "Conditional Neural Processes", "year": 2018}, {"arxiv_id": "1903.11907", "title": "Meta-Learning surrogate models for sequential decision making", "year": 2019}], "gt_label": [1, 1, 1, 1, 1], "date": "2021-07", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_648", "valid": true}
{"query": "Which papers discuss about equivariant neural networks for voxel grids with respect to voxel and point cloud representations?", "cited_paper": [{"arxiv_id": "1807.02547", "title": "3D Steerable CNNs: Learning Rotationally Equivariant Features in Volumetric Data", "year": 2018}, {"arxiv_id": "2303.00351", "title": "Leveraging SO(3)-steerable convolutions for pose-robust semantic segmentation in 3D medical data", "year": 2023}, {"arxiv_id": "2111.07383", "title": "Sparse Steerable Convolutions: An Efficient Learning of SE(3)-Equivariant Features for Estimation and Tracking of Object Poses in 3D Space", "year": 2021}], "gt_label": [1, 1, 1], "date": "2023-03", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_649", "valid": true}
{"query": "Which works are about the sampling-based uncertainty estimation methods?", "cited_paper": [{"arxiv_id": "1612.01474", "title": "Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles", "year": 2016}, {"arxiv_id": "2002.06715", "title": "BatchEnsemble: An Alternative Approach to Efficient Ensemble and Lifelong Learning", "year": 2020}, {"arxiv_id": "2006.13570", "title": "Hyperparameter Ensembles for Robustness and Uncertainty Quantification", "year": 2020}, {"arxiv_id": "1506.02142", "title": "Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning", "year": 2015}, {"arxiv_id": "1505.05424", "title": "Weight Uncertainty in Neural Networks", "year": 2015}, {"arxiv_id": "1902.02476", "title": "A Simple Baseline for Bayesian Uncertainty in Deep Learning", "year": 2019}], "gt_label": [1, 1, 1, 1, 1, 1], "date": "2020-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_650", "valid": true}
{"query": "Which works have explored white-box detection methods involving watermarks in LLM-generated texts?", "cited_paper": [{"arxiv_id": "2303.07205", "title": "The Science of Detecting LLM-Generated Texts", "year": 2023}], "gt_label": [1], "date": "2023-05", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_651", "valid": true}
{"query": "Could you give me examples of studies that made significant progress in multimodal response generation?", "cited_paper": [{"arxiv_id": "2201.07520", "title": "CM3: A Causal Masked Multimodal Model of the Internet", "year": 2022}, {"arxiv_id": "2307.10802", "title": "Meta-Transformer: A Unified Framework for Multimodal Learning", "year": 2023}], "gt_label": [1, 1], "date": "2023-07", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_652", "valid": true}
{"query": "What papers have proposed methods to learn representations that are invariant to image distractors such as background colour?", "cited_paper": [{"arxiv_id": "2003.06016", "title": "Invariant Causal Prediction for Block MDPs", "year": 2020}, {"arxiv_id": "2006.10742", "title": "Learning Invariant Representations for Reinforcement Learning without Reconstruction", "year": 2020}, {"arxiv_id": "2102.07097", "title": "Domain Adversarial Reinforcement Learning", "year": 2021}, {"arxiv_id": "2106.04379", "title": "Learning Markov State Abstractions for Deep Reinforcement Learning", "year": 2021}], "gt_label": [1, 1, 1, 1], "date": "2021-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_653", "valid": true}
{"query": "Which paper provides a discussion on the many-body representation hypothesis in context of voxel and point cloud representations?", "cited_paper": [], "gt_label": [], "date": "2020-12", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_654", "valid": true}
{"query": "What is the first trial on instruction-following LMMs?", "cited_paper": [{"arxiv_id": "2203.02155", "title": "Training language models to follow instructions with human feedback", "year": 2022}], "gt_label": [1], "date": "2022-03", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_655", "valid": true}
{"query": "What research work incorporated attention into the capsule routing via a non-iterative feed-forward operation?", "cited_paper": [{"arxiv_id": "1907.01750", "title": "Attention routing between capsules", "year": 2019}], "gt_label": [1], "date": "2019-07", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_656", "valid": true}
{"query": "Which works were mentioned in relation to the use of synthetic captions generated using BLIP and ranked using CLIP models?", "cited_paper": [{"arxiv_id": "2201.12086", "title": "BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation", "year": 2022}, {"arxiv_id": "2103.00020", "title": "Learning Transferable Visual Models From Natural Language Supervision", "year": 2021}], "gt_label": [1, 1], "date": "2022-01", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_657", "valid": true}
{"query": "Which research suggested that jittering can enhance worst-case robustness?", "cited_paper": [{"arxiv_id": "2011.04268", "title": "Solving Inverse Problems With Deep Neural Networks -- Robustness Included?", "year": 2020}], "gt_label": [1], "date": "2020-11", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_658", "valid": true}
{"query": "Which papers explored generating responses to queries using multi-modal knowledge sources?", "cited_paper": [{"arxiv_id": "2109.00590", "title": "WebQA: Multihop and Multimodal QA", "year": 2021}, {"arxiv_id": "2210.02928", "title": "MuRAG: Multimodal Retrieval-Augmented Generator for Open Question Answering over Images and Text", "year": 2022}, {"arxiv_id": "2204.11677", "title": "Conversational Question Answering on Heterogeneous Sources", "year": 2022}, {"arxiv_id": "2209.09513", "title": "Learn to Explain: Multimodal Reasoning via Thought Chains for Science Question Answering", "year": 2022}, {"arxiv_id": "2104.06039", "title": "MultiModalQA: Complex Question Answering over Text, Tables and Images", "year": 2021}, {"arxiv_id": "2304.13559", "title": "Towards Multi-Modal DBMSs for Seamless Querying of Texts and Tables", "year": 2023}, {"arxiv_id": "2206.01347", "title": "MultiHiertt: Numerical Reasoning over Multi Hierarchical Tabular and Textual Data", "year": 2022}], "gt_label": [1, 1, 1, 1, 1, 1, 1], "date": "2023-04", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_659", "valid": true}
{"query": "Could you tell me about some research papers that have used a 3D native pipeline for diffusion-based text-to-3D work?", "cited_paper": [{"arxiv_id": "2305.02463", "title": "Shap-E: Generating Conditional 3D Implicit Functions", "year": 2023}, {"arxiv_id": "2212.08751", "title": "Point-E: A System for Generating 3D Point Clouds from Complex Prompts", "year": 2022}, {"arxiv_id": "2306.07349", "title": "ATT3D: Amortized Text-to-3D Object Synthesis", "year": 2023}, {"arxiv_id": "2212.04493", "title": "SDFusion: Multimodal 3D Shape Completion, Reconstruction, and Generation", "year": 2022}, {"arxiv_id": "2304.06714", "title": "Single-Stage Diffusion NeRF: A Unified Approach to 3D Generation and Reconstruction", "year": 2023}, {"arxiv_id": "2303.05371", "title": "3DGen: Triplane Latent Diffusion for Textured Mesh Generation", "year": 2023}, {"arxiv_id": "2307.05445", "title": "AutoDecoding Latent 3D Diffusion Models", "year": 2023}], "gt_label": [1, 1, 1, 1, 1, 1, 1], "date": "2023-07", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_660", "valid": true}
{"query": "What studies improve the approximation factor to 3/4, and then to 3/4+o(1)?", "cited_paper": [], "gt_label": [], "date": "2023-03", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_661", "valid": true}
{"query": "What previous studies explored the trade-off between communication and straggler resiliency in Gradient Coding?", "cited_paper": [{"arxiv_id": "1802.03475", "title": "Communication-Computation Efficient Gradient Coding", "year": 2018}, {"arxiv_id": "2005.07184", "title": "Communication-Efficient Gradient Coding for Straggler Mitigation in Distributed Learning", "year": 2020}], "gt_label": [1, 1], "date": "2020-05", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_662", "valid": true}
{"query": "What papers in the field of NLP have explored the topic of hate/offense or toxicity?", "cited_paper": [{"arxiv_id": "1703.04009", "title": "Automated Hate Speech Detection and the Problem of Offensive Language", "year": 2017}], "gt_label": [1], "date": "2017-03", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_663", "valid": true}
{"query": "What are the early studies on Random Reshuffling (SGD-RR) that proposed upper bounds for strongly convex and twice-smooth objectives?", "cited_paper": [], "gt_label": [], "date": "2018-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_664", "valid": true}
{"query": "What works proposed the approach of embedding images and text into a shared space?", "cited_paper": [{"arxiv_id": "1406.5679", "title": "Deep Fragment Embeddings for Bidirectional Image Sentence Mapping", "year": 2014}, {"arxiv_id": "1410.1090", "title": "Explain Images with Multimodal Recurrent Neural Networks", "year": 2014}], "gt_label": [1, 1], "date": "2014-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_665", "valid": true}
{"query": "What work introduced rotating features to complex-valued activations by extending Convolutional Auto-Encoders?", "cited_paper": [{"arxiv_id": "2306.00600", "title": "Rotating Features for Object Discovery", "year": 2023}], "gt_label": [1], "date": "2023-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_666", "valid": true}
{"query": "What papers explored spatio-temporal information in visual object tracking?", "cited_paper": [{"arxiv_id": "2304.14394", "title": "Unified Sequence-to-Sequence Learning for Singleand Multi-Modal Visual Object Tracking", "year": 2023}, {"arxiv_id": "2301.10938", "title": "Compact Transformer Tracker with Correlative Masked Modeling", "year": 2023}, {"arxiv_id": "2203.11082", "title": "MixFormer: End-to-End Tracking with Iterative Mixed Attention", "year": 2022}], "gt_label": [1, 1, 1], "date": "2023-04", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_667", "valid": true}
{"query": "What works introduced the prompts paradigm to Vision Transformer?", "cited_paper": [{"arxiv_id": "2203.12119", "title": "Visual Prompt Tuning", "year": 2022}, {"arxiv_id": "2203.17274", "title": "Exploring Visual Prompts for Adapting Large-Scale Models", "year": 2022}], "gt_label": [1, 1], "date": "2022-03", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_668", "valid": true}
{"query": "Could you provide me some studies about the use of transformers in visual object tracking?", "cited_paper": [{"arxiv_id": "1706.03762", "title": "Attention Is All You Need", "year": 2017}, {"arxiv_id": "2103.15436", "title": "Transformer Tracking", "year": 2021}, {"arxiv_id": "2103.17154", "title": "Learning Spatio-Temporal Transformer for Visual Tracking", "year": 2021}, {"arxiv_id": "2112.00995", "title": "SwinTrack: A Simple and Strong Baseline for Transformer Tracking", "year": 2021}, {"arxiv_id": "2203.11082", "title": "MixFormer: End-to-End Tracking with Iterative Mixed Attention", "year": 2022}, {"arxiv_id": "2203.11991", "title": "Joint Feature Learning and Relation Modeling for Tracking: A One-Stream Framework", "year": 2022}, {"arxiv_id": "2203.01666", "title": "Correlation-Aware Deep Tracking", "year": 2022}], "gt_label": [1, 1, 1, 1, 1, 1, 1], "date": "2022-03", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_669", "valid": true}
{"query": "What studies exploit depth maps for view-morphing to augment sparse inputs?", "cited_paper": [{"arxiv_id": "2210.04214", "title": "VM-NeRF: Tackling Sparsity in NeRF with View Morphing", "year": 2022}], "gt_label": [1], "date": "2022-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_670", "valid": true}
{"query": "Could you provide some of the initial explorations on LLMs, which involve prompting methods and model variants?", "cited_paper": [{"arxiv_id": "2302.04166", "title": "GPTScore: Evaluate as You Desire", "year": 2023}, {"arxiv_id": "2302.14520", "title": "Large Language Models Are State-of-the-Art Evaluators of Translation Quality", "year": 2023}, {"arxiv_id": "2303.04048", "title": "Is ChatGPT a Good NLG Evaluator? A Preliminary Study", "year": 2023}, {"arxiv_id": "2303.16634", "title": "G-Eval: NLG Evaluation using GPT-4 with Better Human Alignment", "year": 2023}], "gt_label": [1, 1, 1, 1], "date": "2023-03", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_671", "valid": true}
{"query": "What works propose optimizing a surrogate loss function to enhance stability in learning?", "cited_paper": [{"arxiv_id": "1502.05477", "title": "Trust Region Policy Optimization", "year": 2015}], "gt_label": [1], "date": "2017-07", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_672", "valid": true}
{"query": "Are there papers that analyze the lower bounds of PFL?", "cited_paper": [{"arxiv_id": "2002.07839", "title": "Is Local SGD Better than Minibatch SGD?", "year": 2020}, {"arxiv_id": "2006.04735", "title": "Minibatch vs Local SGD for Heterogeneous Distributed Learning", "year": 2020}, {"arxiv_id": "2110.10342", "title": "Minibatch vs Local SGD with Shuffling: Tight Convergence Bounds and Beyond", "year": 2021}], "gt_label": [1, 1, 1], "date": "2021-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_673", "valid": true}
{"query": "What studies used distillation techniques for 'gisting' to make shorter prompts?", "cited_paper": [{"arxiv_id": "2304.08467", "title": "Learning to Compress Prompts with Gist Tokens", "year": 2023}], "gt_label": [1], "date": "2023-04", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_674", "valid": true}
{"query": "Could you provide me some studies about probing-based methods for factuality detection in LLM ?", "cited_paper": [{"arxiv_id": "1610.01644", "title": "Understanding intermediate layers using linear classifier probes", "year": 2016}, {"arxiv_id": "2310.02207", "title": "Language Models Represent Space and Time", "year": 2023}, {"arxiv_id": "2207.05221", "title": "Language Models (Mostly) Know What They Know", "year": 2022}, {"arxiv_id": "2304.13734", "title": "The Internal State of an LLM Knows When It's Lying", "year": 2023}, {"arxiv_id": "2310.01405", "title": "Representation Engineering: A Top-Down Approach to AI Transparency", "year": 2023}], "gt_label": [1, 1, 1, 1, 1], "date": "2024-07", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_675", "valid": true}
{"query": "What are the references that discuss the estimation of Q functions and learned transition models under epistemic uncertainty?", "cited_paper": [{"arxiv_id": "1602.04621", "title": "Deep Exploration via Bootstrapped DQN", "year": 2016}, {"arxiv_id": "2010.14497", "title": "Conservative Safety Critics for Exploration", "year": 2020}, {"arxiv_id": "1805.12114", "title": "Deep Reinforcement Learning in a Handful of Trials using Probabilistic Dynamics Models", "year": 2018}, {"arxiv_id": "1802.10592", "title": "Model-Ensemble Trust-Region Policy Optimization", "year": 2018}, {"arxiv_id": "1906.08253", "title": "When to Trust Your Model: Model-Based Policy Optimization", "year": 2019}, {"arxiv_id": "2004.07804", "title": "A Game Theoretic Framework for Model Based Reinforcement Learning", "year": 2020}, {"arxiv_id": "2201.09802", "title": "Constrained Policy Optimization via Bayesian World Models", "year": 2022}], "gt_label": [1, 1, 1, 1, 1, 1, 1], "date": "2022-01", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_676", "valid": true}
{"query": "Could you provide me some researches that develop learning-augmented algorithms for metrical task systems?", "cited_paper": [], "gt_label": [], "date": "2020-03", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_677", "valid": true}
{"query": "Which studies highlighted the vulnerability of contrastive learning to adversarial attack in downstream classification tasks?", "cited_paper": [{"arxiv_id": "2010.12050", "title": "Contrastive Learning with Adversarial Examples", "year": 2020}, {"arxiv_id": "2006.07589", "title": "Adversarial Self-Supervised Contrastive Learning", "year": 2020}], "gt_label": [1, 1], "date": "2020-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_678", "valid": true}
{"query": "Which research utilized the reverse KLD to improve the accuracy of language generation in the MINILLM?", "cited_paper": [{"arxiv_id": "2306.08543", "title": "MiniLLM: Knowledge Distillation of Large Language Models", "year": 2023}], "gt_label": [1], "date": "2023-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_679", "valid": true}
{"query": "What works use MIAs to assess whether a given data point was used within the prompt prepended to the inputs of a trained LLM?", "cited_paper": [{"arxiv_id": "2112.03570", "title": "Membership Inference Attacks From First Principles", "year": 2021}, {"arxiv_id": "1610.05820", "title": "Membership Inference Attacks against Machine Learning Models", "year": 2016}], "gt_label": [1, 1], "date": "2021-12", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_680", "valid": true}
{"query": "Could you provide studies that use diffusion models in relation to computer vision problems?", "cited_paper": [{"arxiv_id": "2111.15640", "title": "Diffusion Autoencoders: Toward a Meaningful and Decodable Representation", "year": 2021}, {"arxiv_id": "2105.14257", "title": "Diffusion-Based Representation Learning", "year": 2021}, {"arxiv_id": "2112.03126", "title": "Label-Efficient Semantic Segmentation with Diffusion Models", "year": 2021}], "gt_label": [1, 1, 1], "date": "2021-12", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_681", "valid": true}
{"query": "Which papers propose methods to predict 6D pose of objects in an image to find applications in fields like robotics, autonomous vehicles, and microscopy?", "cited_paper": [{"arxiv_id": "1809.10790", "title": "Deep Object Pose Estimation for Semantic Robotic Grasping of Household Objects", "year": 2018}, {"arxiv_id": "2203.08138", "title": "CryoAI: Amortized Inference of Poses for Ab Initio Reconstruction of 3D Molecular Volumes from Real Cryo-EM Images", "year": 2022}], "gt_label": [1, 1], "date": "2022-03", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_682", "valid": true}
{"query": "What are the works that addressed the differences between individual annotators or the group-level attributes of annotators by adding individual layers?", "cited_paper": [{"arxiv_id": "2110.05719", "title": "Dealing with Disagreements: Looking Beyond the Majority Vote in Subjective Annotations", "year": 2021}, {"arxiv_id": "2202.02950", "title": "Jury Learning: Integrating Dissenting Voices into Machine Learning Models", "year": 2022}, {"arxiv_id": "2305.06626", "title": "When the Majority is Wrong: Modeling Annotator Disagreement for Subjective Tasks", "year": 2023}], "gt_label": [1, 1, 1], "date": "2023-05", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_683", "valid": true}
{"query": "Who has recently explored an instruction-based text embedder?", "cited_paper": [{"arxiv_id": "2212.09741", "title": "One Embedder, Any Task: Instruction-Finetuned Text Embeddings", "year": 2022}], "gt_label": [1], "date": "2022-12", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_684", "valid": true}
{"query": "Which studies align the text with a paired image in the embedding space in visual-language learning?", "cited_paper": [{"arxiv_id": "2103.00020", "title": "Learning Transferable Visual Models From Natural Language Supervision", "year": 2021}, {"arxiv_id": "2303.17569", "title": "Iterative Prompt Learning for Unsupervised Backlit Image Enhancement", "year": 2023}], "gt_label": [1, 1], "date": "2023-03", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_685", "valid": true}
{"query": "Are there any works that focus on image inpainting methods that don't require finetuning?", "cited_paper": [{"arxiv_id": "2206.02779", "title": "Blended Latent Diffusion", "year": 2022}, {"arxiv_id": "2111.14818", "title": "Blended Diffusion for Text-driven Editing of Natural Images", "year": 2021}], "gt_label": [1, 1], "date": "2022-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_686", "valid": true}
{"query": "What works used model's weights to identify parts of the training dataset that influenced the model?", "cited_paper": [{"arxiv_id": "2205.12600", "title": "ORCA: Interpreting Prompted Language Models via Locating Supporting Data Evidence in the Ocean of Pretraining Data", "year": 2022}], "gt_label": [1], "date": "2023-08", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_687", "valid": true}
{"query": "What studies demonstrate that a simple image classifier trained on a specific CNN generator is able to generalize well to unseen architectures?", "cited_paper": [{"arxiv_id": "1912.11035", "title": "CNN-generated images are surprisingly easy to spot... for now", "year": 2019}], "gt_label": [1], "date": "2019-12", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_688", "valid": true}
{"query": "What paper introduced a method for identifying close and robust counterfactuals which use interval neural networks?", "cited_paper": [{"arxiv_id": "2208.14878", "title": "Formalising the Robustness of Counterfactual Explanations for Neural Networks", "year": 2022}], "gt_label": [1], "date": "2022-08", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_689", "valid": true}
{"query": "Are there any studies focussed on managing false negatives in contrastive learning, particularly for the vision domain?", "cited_paper": [{"arxiv_id": "2101.05068", "title": "Probabilistic Embeddings for Cross-Modal Retrieval", "year": 2021}, {"arxiv_id": "1906.04402", "title": "Polysemous Visual-Semantic Embedding for Cross-Modal Retrieval", "year": 2019}, {"arxiv_id": "2107.07651", "title": "Align before Fuse: Vision and Language Representation Learning with Momentum Distillation", "year": 2021}, {"arxiv_id": "2208.04060", "title": "GRIT-VLP: Grouped Mini-batch Sampling for Efficient Vision and Language Pre-training", "year": 2022}, {"arxiv_id": "2011.11765", "title": "Boosting Contrastive Self-Supervised Learning with False Negative Cancellation", "year": 2020}, {"arxiv_id": "2007.00224", "title": "Debiased Contrastive Learning", "year": 2020}, {"arxiv_id": "2106.03719", "title": "Incremental False Negative Detection for Contrastive Learning", "year": 2021}, {"arxiv_id": "2010.04592", "title": "Contrastive Learning with Hard Negative Samples", "year": 2020}], "gt_label": [1, 1, 1, 1, 1, 1, 1, 1], "date": "2022-08", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_690", "valid": true}
{"query": "Which study represents the embedding-based method in the method of jointly learning the logical rule form and the weights in a differentiable manner?", "cited_paper": [{"arxiv_id": "1412.6575", "title": "Embedding Entities and Relations for Learning and Inference in Knowledge Bases", "year": 2014}], "gt_label": [1], "date": "2014-12", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_691", "valid": true}
{"query": "What studies focus on developing an estimator for model's classifier performance on unlabeled data from unknown distributions in the target domain?", "cited_paper": [{"arxiv_id": "2007.02915", "title": "Are Labels Always Necessary for Classifier Accuracy Evaluation?", "year": 2020}, {"arxiv_id": "2201.04234", "title": "Leveraging Unlabeled Data to Predict Out-of-Distribution Performance", "year": 2022}, {"arxiv_id": "2202.05834", "title": "Predicting Out-of-Distribution Error with the Projection Norm", "year": 2022}, {"arxiv_id": "2207.07065", "title": "On the Strong Correlation Between Model Invariance and Generalization", "year": 2022}, {"arxiv_id": "2107.03315", "title": "Predicting with Confidence on Unseen Distributions", "year": 2021}, {"arxiv_id": "2106.05961", "title": "What Does Rotation Prediction Tell Us about Classifier Accuracy under Varying Testing Environments?", "year": 2021}], "gt_label": [1, 1, 1, 1, 1, 1], "date": "2022-07", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_692", "valid": true}
{"query": "Which studies discuss using modifications like larger/smaller learning rates and regularization-based methods for enhancing Fine-Tuning's performance?", "cited_paper": [{"arxiv_id": "1911.03437", "title": "SMART: Robust and Efficient Fine-Tuning for Pre-trained Natural Language Models through Principled Regularized Optimization", "year": 2019}], "gt_label": [1], "date": "2019-11", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_693", "valid": true}
{"query": "What research studies proposed improved concentration coefficients than AMPO?", "cited_paper": [], "gt_label": [], "date": "2022-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_694", "valid": true}
{"query": "Which studies developed physics-based LiDAR simulators?", "cited_paper": [{"arxiv_id": "1711.03938", "title": "CARLA: An Open Urban Driving Simulator", "year": 2017}], "gt_label": [1], "date": "2017-11", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_695", "valid": true}
{"query": "Can you name some studies that focused on cross-style or zero-shot classification in NLP?", "cited_paper": [{"arxiv_id": "1912.10165", "title": "Zero-shot Text Classification With Generative Language Models", "year": 2019}, {"arxiv_id": "1911.03663", "title": "Style is NOT a single variable: Case Studies for Cross-Style Language Understanding", "year": 2019}], "gt_label": [1, 1], "date": "2019-12", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_696", "valid": true}
{"query": "Which works used models structured as an RNN in meta-RL methods?", "cited_paper": [{"arxiv_id": "1611.02779", "title": "RL$^2$: Fast Reinforcement Learning via Slow Reinforcement Learning", "year": 2016}, {"arxiv_id": "1611.05763", "title": "Learning to reinforcement learn", "year": 2016}, {"arxiv_id": "2110.05038", "title": "Recurrent Model-Free RL Can Be a Strong Baseline for Many POMDPs", "year": 2021}], "gt_label": [1, 1, 1], "date": "2021-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_697", "valid": true}
{"query": "Any studies tackling the challenge of Uncertainty Estimation in specific NLP tasks such as paraphrase detection and natural language inference?", "cited_paper": [{"arxiv_id": "2003.07892", "title": "Calibration of Pre-trained Transformers", "year": 2020}], "gt_label": [1], "date": "2020-03", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_698", "valid": true}
{"query": "Which works explored the offline label shift problem in the domain adaptation literature and estimating mixture proportions of different classes in unlabeled data?", "cited_paper": [{"arxiv_id": "1802.03916", "title": "Detecting and Correcting for Label Shift with Black Box Predictors", "year": 2018}, {"arxiv_id": "2003.07554", "title": "A Unified View of Label Shift Estimation", "year": 2020}, {"arxiv_id": "2111.00980", "title": "Mixture Proportion Estimation and PU Learning: A Modern Approach", "year": 2021}, {"arxiv_id": "2207.13048", "title": "Domain Adaptation under Open Set Label Shift", "year": 2022}, {"arxiv_id": "2207.13179", "title": "Unsupervised Learning under Latent Label Shift", "year": 2022}], "gt_label": [1, 1, 1, 1, 1], "date": "2022-07", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_699", "valid": true}
{"query": "What studies introduced improvements or advancements in stereo egocentric setups?", "cited_paper": [{"arxiv_id": "2208.01633", "title": "UnrealEgo: A New Dataset for Robust Egocentric 3D Human Motion Capture", "year": 2022}, {"arxiv_id": "2309.11962", "title": "Ego3DPose: Capturing 3D Cues from Binocular Egocentric Views", "year": 2023}], "gt_label": [1, 1], "date": "2023-09", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_700", "valid": true}
{"query": "Which paper introduces the MoTIF dataset with a large number of task demonstrations?", "cited_paper": [{"arxiv_id": "2104.08560", "title": "Mobile App Tasks with Iterative Feedback (MoTIF): Addressing Task Feasibility in Interactive Visual Environments", "year": 2021}], "gt_label": [1], "date": "2021-04", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_701", "valid": true}
{"query": "Which papers propose creating synthetic data using copulas for answering marginal queries?", "cited_paper": [{"arxiv_id": "1902.01499", "title": "Differentially Private Release of High-Dimensional Datasets using the Gaussian Copula", "year": 2019}], "gt_label": [1], "date": "2019-02", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_702", "valid": true}
{"query": "Which studies evaluate the LMs success at performing multi-hop inferences with the edited information?", "cited_paper": [{"arxiv_id": "2305.14795", "title": "MQuAKE: Assessing Knowledge Editing in Language Models via Multi-Hop Questions", "year": 2023}, {"arxiv_id": "2307.12976", "title": "Evaluating the Ripple Effects of Knowledge Editing in Language Models", "year": 2023}], "gt_label": [1, 1], "date": "2023-08", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_703", "valid": true}
{"query": "Which research work provided both sublinear and linear convergence analysis of natural policy gradient (NPG) with softmax tabular policies or with log-linear policies?", "cited_paper": [{"arxiv_id": "2201.07443", "title": "On the Convergence Rates of Policy Gradient Methods", "year": 2022}, {"arxiv_id": "2209.15382", "title": "Linear Convergence for Natural Policy Gradient with Log-linear Policy Parametrization", "year": 2022}], "gt_label": [1, 1], "date": "2022-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_704", "valid": true}
{"query": "Which research work offered theoretical analyses on the issue of balancing the generator and discriminator in GAN training?", "cited_paper": [{"arxiv_id": "1703.00573", "title": "Generalization and Equilibrium in Generative Adversarial Nets (GANs)", "year": 2017}, {"arxiv_id": "1806.10586", "title": "Approximability of Discriminators Implies Diversity in GANs", "year": 2018}], "gt_label": [1, 1], "date": "2018-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_705", "valid": true}
{"query": "Which studies decomposed the problem into a tree or constructed a reasoning graph instead?", "cited_paper": [{"arxiv_id": "2311.13982", "title": "Probabilistic Tree-of-thought Reasoning for Answering Knowledge-intensive Complex Questions", "year": 2023}, {"arxiv_id": "2311.09762", "title": "Graph Elicitation for Guiding Multi-Step Reasoning in Large Language Models", "year": 2023}], "gt_label": [1, 1], "date": "2023-11", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_706", "valid": true}
{"query": "Which studies discuss the threat of gradient inversion to Federated Learning (FL)?", "cited_paper": [{"arxiv_id": "2110.09074", "title": "Towards General Deep Leakage in Federated Learning", "year": 2021}, {"arxiv_id": "2106.06089", "title": "Gradient Disaggregation: Breaking Privacy in Federated Learning by Reconstructing the User Participant Matrix", "year": 2021}], "gt_label": [1, 1], "date": "2021-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_707", "valid": true}
{"query": "Any works that applied reinforcement learning (RL) and planning algorithms for code generation by formulating the code generation problem as a sequential decision-making problem?", "cited_paper": [{"arxiv_id": "1805.04276", "title": "Leveraging Grammar and Reinforcement Learning for Neural Program Synthesis", "year": 2018}], "gt_label": [1], "date": "2018-05", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_708", "valid": true}
{"query": "Can you indicate some studies addressing security and safety concerns in the deployment of multi-modal models in real-world applications such as autonomous driving?", "cited_paper": [{"arxiv_id": "2008.11351", "title": "SNE-RoadSeg: Incorporating Surface Normal Information into Semantic Segmentation for Accurate Freespace Detection", "year": 2020}, {"arxiv_id": "2203.08195", "title": "DeepFusion: Lidar-Camera Deep Fusion for Multi-Modal 3D Object Detection", "year": 2022}, {"arxiv_id": "1612.07695", "title": "MultiNet: Real-time Joint Semantic Reasoning for Autonomous Driving", "year": 2016}, {"arxiv_id": "2204.05513", "title": "End-to-end Autonomous Driving with Semantic Depth Cloud Mapping and Multi-agent", "year": 2022}], "gt_label": [1, 1, 1, 1], "date": "2022-04", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_709", "valid": true}
{"query": "Which papers have focused on highlighting decision words as a method for explaining predictions of neural NLP systems?", "cited_paper": [{"arxiv_id": "1707.01943", "title": "A causal framework for explaining the predictions of black-box sequence-to-sequence models", "year": 2017}, {"arxiv_id": "1906.03731", "title": "Is Attention Interpretable?", "year": 2019}], "gt_label": [1, 1], "date": "2019-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_710", "valid": true}
{"query": "Which work introduced the General Language Understanding Evaluation (GLUE) benchmark in NLP?", "cited_paper": [{"arxiv_id": "1804.07461", "title": "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding", "year": 2018}], "gt_label": [1], "date": "2018-04", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_711", "valid": true}
{"query": "Which papers attempted to handle the problem of GAN compression through the use of pruning-based methods?", "cited_paper": [{"arxiv_id": "1907.10804", "title": "Co-Evolutionary Compression for Unpaired Image Translation", "year": 2019}, {"arxiv_id": "2103.03467", "title": "Teachers Do More Than Teach: Compressing Image-to-Image Models", "year": 2021}], "gt_label": [1, 1], "date": "2021-03", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_712", "valid": true}
{"query": "What paper proposed the method ExpertPrompting to improve the reasoning capabilities of LLMs by generating expert-level responses?", "cited_paper": [{"arxiv_id": "2305.14688", "title": "ExpertPrompting: Instructing Large Language Models to be Distinguished Experts", "year": 2023}], "gt_label": [1], "date": "2023-05", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_713", "valid": true}
{"query": "Could you provide me the study that proposed the relative gradient method for optimizing flow-based models with arbitrary linear transformations?", "cited_paper": [{"arxiv_id": "2006.15090", "title": "Relative gradient optimization of the Jacobian term in unsupervised deep learning", "year": 2020}], "gt_label": [1], "date": "2020-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_714", "valid": true}
{"query": "What research aimed to explain collaboration mechanism in a social psychology view?", "cited_paper": [{"arxiv_id": "2310.02124", "title": "Exploring Collaboration Mechanisms for LLM Agents: A Social Psychology View", "year": 2023}], "gt_label": [1], "date": "2023-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_715", "valid": true}
{"query": "What works have been done on graph neural networks (GNNs) that are used for graph classification problems?", "cited_paper": [{"arxiv_id": "1609.02907", "title": "Semi-Supervised Classification with Graph Convolutional Networks", "year": 2016}, {"arxiv_id": "1710.10903", "title": "Graph Attention Networks", "year": 2017}, {"arxiv_id": "1704.01212", "title": "Neural Message Passing for Quantum Chemistry", "year": 2017}, {"arxiv_id": "1905.05178", "title": "Graph U-Nets", "year": 2019}], "gt_label": [1, 1, 1, 1], "date": "2019-05", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_716", "valid": true}
{"query": "What are the examples of works on text-to-image diffusion models?", "cited_paper": [{"arxiv_id": "2112.10741", "title": "GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models", "year": 2021}, {"arxiv_id": "2205.11487", "title": "Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding", "year": 2022}, {"arxiv_id": "2204.06125", "title": "Hierarchical Text-Conditional Image Generation with CLIP Latents", "year": 2022}, {"arxiv_id": "2112.10752", "title": "High-Resolution Image Synthesis with Latent Diffusion Models", "year": 2021}, {"arxiv_id": "2204.02849", "title": "KNN-Diffusion: Image Generation via Large-Scale Retrieval", "year": 2022}], "gt_label": [1, 1, 1, 1, 1], "date": "2022-05", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_717", "valid": true}
{"query": "Which works are about extending instruction finetuned datasets outside of English through translation?", "cited_paper": [{"arxiv_id": "2205.15960", "title": "NusaX: Multilingual Parallel Sentiment Dataset for 10 Indonesian Local Languages", "year": 2022}], "gt_label": [1], "date": "2022-05", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_718", "valid": true}
{"query": "Can you provide some works that have incorporated human prior models like SMPL and imGHUM for text-driven 3D human generation?", "cited_paper": [{"arxiv_id": "2108.10842", "title": "imGHUM: Implicit Generative Models of 3D Human Shape and Articulated Pose", "year": 2021}, {"arxiv_id": "2210.04888", "title": "EVA3D: Compositional 3D Human Generation from 2D Image Collections", "year": 2022}, {"arxiv_id": "2304.00916", "title": "DreamAvatar: Text-and-Shape Guided 3D Human Avatar Generation via Diffusion Models", "year": 2023}, {"arxiv_id": "2303.17606", "title": "AvatarCraft: Transforming Text into Neural Human Avatars with Parameterized Shape and Pose Control", "year": 2023}, {"arxiv_id": "2306.09864", "title": "AvatarBooth: High-Quality and Customizable 3D Human Avatar Generation", "year": 2023}, {"arxiv_id": "2309.07125", "title": "Text-Guided Generation and Editing of Compositional 3D Avatars", "year": 2023}, {"arxiv_id": "2304.13348", "title": "TextDeformer: Geometry Manipulation using Text Guidance", "year": 2023}, {"arxiv_id": "2303.01311", "title": "Zero-Shot Text-to-Parameter Translation for Game Character Auto-Creation", "year": 2023}, {"arxiv_id": "2203.13161", "title": "Learning Hierarchical Cross-Modal Association for Co-Speech Gesture Generation", "year": 2022}], "gt_label": [1, 1, 1, 1, 1, 1, 1, 1, 1], "date": "2023-09", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_719", "valid": true}
{"query": "Could you provide some studies discussing the high-level connection between stability, online learnability, and differential privacy?", "cited_paper": [{"arxiv_id": "1411.2664", "title": "Preserving Statistical Validity in Adaptive Data Analysis", "year": 2014}, {"arxiv_id": "1511.02513", "title": "Algorithmic Stability for Adaptive Data Analysis", "year": 2015}, {"arxiv_id": "2006.13508", "title": "A Limitation of the PAC-Bayes Framework", "year": 2020}], "gt_label": [1, 1, 1], "date": "2020-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_720", "valid": true}
{"query": "Can you provide me with studies that explored variants of the Cross Entropy (CE) loss to improve discriminative power of learned feature representations of data?", "cited_paper": [{"arxiv_id": "1503.03832", "title": "FaceNet: A Unified Embedding for Face Recognition and Clustering", "year": 2015}, {"arxiv_id": "1707.07391", "title": "Contrastive-center loss for deep neural networks", "year": 2017}], "gt_label": [1, 1], "date": "2017-07", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_721", "valid": true}
{"query": "What studies does AMPO recovers the best-known convergence rates in both the tabular and non-tabular setting?", "cited_paper": [{"arxiv_id": "1901.11275", "title": "A Theory of Regularized Markov Decision Processes", "year": 2019}, {"arxiv_id": "2201.07443", "title": "On the Convergence Rates of Policy Gradient Methods", "year": 2022}, {"arxiv_id": "2206.00833", "title": "Finite-Time Analysis of Entropy-Regularized Neural Natural Actor-Critic Algorithm", "year": 2022}, {"arxiv_id": "2209.15382", "title": "Linear Convergence for Natural Policy Gradient with Log-linear Policy Parametrization", "year": 2022}], "gt_label": [1, 1, 1, 1], "date": "2022-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_722", "valid": true}
{"query": "What works contribute to the theoretical study of linear MDPs and linear convergence theory of AMPO?", "cited_paper": [{"arxiv_id": "1907.05388", "title": "Provably Efficient Reinforcement Learning with Linear Function Approximation", "year": 2019}], "gt_label": [1], "date": "2019-07", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_723", "valid": true}
{"query": "What papers propose code filtering strategies involving code execution with given or generated test cases?", "cited_paper": [{"arxiv_id": "2204.08373", "title": "Learning to Execute Actions or Ask Clarification Questions", "year": 2022}, {"arxiv_id": "2207.10397", "title": "CodeT: Code Generation with Generated Tests", "year": 2022}, {"arxiv_id": "2309.17272", "title": "Enhancing Large Language Models in Coding Through Multi-Perspective Self-Consistency", "year": 2023}], "gt_label": [1, 1, 1], "date": "2023-09", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_724", "valid": true}
{"query": "Which work propose Gaussian posterior approximation method (SWAG) based on the first two moments of SGD iterations explicitly for model calibration or uncertainty quantification?", "cited_paper": [{"arxiv_id": "1902.02476", "title": "A Simple Baseline for Bayesian Uncertainty in Deep Learning", "year": 2019}], "gt_label": [1], "date": "2019-02", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_725", "valid": true}
{"query": "What recent works have focused on the intersection of FL and DG?", "cited_paper": [{"arxiv_id": "2103.06030", "title": "FedDG: Federated Domain Generalization on Medical Image Segmentation via Episodic Learning in Continuous Frequency Space", "year": 2021}, {"arxiv_id": "2210.00912", "title": "Federated Domain Generalization for Image Recognition via Cross-Client Style Transfer", "year": 2022}], "gt_label": [1, 1], "date": "2022-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_726", "valid": true}
{"query": "Which study first proposed A-MTRL?", "cited_paper": [{"arxiv_id": "2202.00911", "title": "Active Multi-Task Representation Learning", "year": 2022}], "gt_label": [1], "date": "2022-02", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_727", "valid": true}
{"query": "Which studies focused on dynamic regret for non-stationary tabular MDPs?", "cited_paper": [{"arxiv_id": "2006.14389", "title": "Reinforcement Learning for Non-Stationary Markov Decision Processes: The Blessing of (More) Optimism", "year": 2020}, {"arxiv_id": "2007.00148", "title": "Dynamic Regret of Policy Optimization in Non-stationary Environments", "year": 2020}], "gt_label": [1, 1], "date": "2020-07", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_728", "valid": true}
{"query": "Which research shows that heuristic classification improves downstream few-shot performance for GLaM?", "cited_paper": [{"arxiv_id": "2112.06905", "title": "GLaM: Efficient Scaling of Language Models with Mixture-of-Experts", "year": 2021}], "gt_label": [1], "date": "2021-12", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_729", "valid": true}
{"query": "What are the studies that are based on anonymous temporal random walks for temporal graph learning?", "cited_paper": [{"arxiv_id": "2101.05974", "title": "Inductive Representation Learning in Temporal Networks via Causal Anonymous Walks", "year": 2021}], "gt_label": [1], "date": "2021-01", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_730", "valid": true}
{"query": "What study showcased improvement in results by jointly generating the 3D conformation and the connectivity graph of molecules?", "cited_paper": [{"arxiv_id": "2302.09048", "title": "MiDi: Mixed Graph and 3D Denoising Diffusion for Molecule Generation", "year": 2023}], "gt_label": [1], "date": "2023-02", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_731", "valid": true}
{"query": "In which study did the experiments use results or hyperparameters from the original papers, while affording extra computation to tune the RNNs on each benchmark?", "cited_paper": [{"arxiv_id": "2110.05038", "title": "Recurrent Model-Free RL Can Be a Strong Baseline for Many POMDPs", "year": 2021}], "gt_label": [1], "date": "2021-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_732", "valid": true}
{"query": "Can you name the works that introduced neural models in the development of code search models?", "cited_paper": [{"arxiv_id": "2107.00992", "title": "Multimodal Representation for Neural Code Search", "year": 2021}, {"arxiv_id": "1905.03813", "title": "When Deep Learning Met Code Search", "year": 2019}, {"arxiv_id": "1909.09436", "title": "CodeSearchNet Challenge: Evaluating the State of Semantic Code Search", "year": 2019}], "gt_label": [1, 1, 1], "date": "2021-07", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_733", "valid": true}
{"query": "Which research work focused on convergence analysis using variants of PMD methods for the linear MDP setting?", "cited_paper": [{"arxiv_id": "2103.12923", "title": "Cautiously Optimistic Policy Optimization and Exploration with Linear Function Approximation", "year": 2021}, {"arxiv_id": "2110.11280", "title": "Actor-critic is implicitly biased towards high entropy optimal policies", "year": 2021}], "gt_label": [1, 1], "date": "2021-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_734", "valid": true}
{"query": "Which studies use integer quantisation for accelerated 8-bit inference?", "cited_paper": [{"arxiv_id": "1712.05877", "title": "Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference", "year": 2017}], "gt_label": [1], "date": "2017-12", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_735", "valid": true}
{"query": "Which works have conducted studies on video action detection with datasets that only include single-person videos?", "cited_paper": [{"arxiv_id": "1212.0402", "title": "UCF101: A Dataset of 101 Human Actions Classes From Videos in The Wild", "year": 2012}], "gt_label": [1], "date": "2012-12", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_736", "valid": true}
{"query": "What are some recent variants that have improved on optimization time in novel-view synthesis?", "cited_paper": [{"arxiv_id": "2112.05131", "title": "Plenoxels: Radiance Fields without Neural Networks", "year": 2021}, {"arxiv_id": "2103.14024", "title": "PlenOctrees for Real-time Rendering of Neural Radiance Fields", "year": 2021}, {"arxiv_id": "2201.05989", "title": "Instant Neural Graphics Primitives with a Multiresolution Hash Encoding", "year": 2022}, {"arxiv_id": "2308.04079", "title": "3D Gaussian Splatting for Real-Time Radiance Field Rendering", "year": 2023}], "gt_label": [1, 1, 1, 1], "date": "2023-08", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_737", "valid": true}
{"query": "What papers study the Constrained MDP (CMDP) framework in the field of safe reinforcement learning?", "cited_paper": [{"arxiv_id": "2003.02189", "title": "Exploration-Exploitation in Constrained MDPs", "year": 2020}, {"arxiv_id": "2006.12136", "title": "Safe Reinforcement Learning via Curriculum Induction", "year": 2020}, {"arxiv_id": "2009.11348", "title": "A Sample-Efficient Algorithm for Episodic Finite-Horizon MDP with Constraints", "year": 2020}, {"arxiv_id": "2106.02684", "title": "Learning Policies with Zero or Bounded Constraint Violation for Constrained MDPs", "year": 2021}, {"arxiv_id": "2206.11889", "title": "Provably Efficient Model-Free Constrained RL with Linear Function Approximation", "year": 2022}], "gt_label": [1, 1, 1, 1, 1], "date": "2022-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_738", "valid": true}
{"query": "What work generates 3D models based on text prompts by optimizing the CLIP distance between the CLIP text embedding and NeRF renderings?", "cited_paper": [{"arxiv_id": "2112.01455", "title": "Zero-Shot Text-Guided Object Generation with Dream Fields", "year": 2021}], "gt_label": [1], "date": "2021-12", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_739", "valid": true}
{"query": "Which papers propose neural volumetric relighting approaches in face relighting?", "cited_paper": [{"arxiv_id": "2104.04638", "title": "Pixel Codec Avatars", "year": 2021}, {"arxiv_id": "1910.11791", "title": "Self-supervised Learning of Detailed 3D Face Reconstruction", "year": 2019}, {"arxiv_id": "2203.16681", "title": "Face Relighting with Geometrically Consistent Shadows", "year": 2022}], "gt_label": [1, 1, 1], "date": "2022-03", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_740", "valid": true}
{"query": "What studies are about unsupervised domain adaptation?", "cited_paper": [{"arxiv_id": "1901.05335", "title": "A review of domain adaptation without target labels", "year": 2019}], "gt_label": [1], "date": "2019-01", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_741", "valid": true}
{"query": "What papers cover autoregressive models where bonds are added using separate algorithms after all atoms are generated?", "cited_paper": [], "gt_label": [], "date": "2019-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_742", "valid": true}
{"query": "Can you give me some works that utilized model mixtures for personalized Federated Learning?", "cited_paper": [{"arxiv_id": "2012.08565", "title": "Personalized Federated Learning with First Order Model Optimization", "year": 2020}, {"arxiv_id": "2012.04221", "title": "Ditto: Fair and Robust Federated Learning Through Personalization", "year": 2020}], "gt_label": [1, 1], "date": "2020-12", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_743", "valid": true}
{"query": "What papers discussed approaches for personalized image synthesis?", "cited_paper": [{"arxiv_id": "2008.00951", "title": "Encoding in Style: a StyleGAN Encoder for Image-to-Image Translation", "year": 2020}, {"arxiv_id": "2208.01618", "title": "An Image is Worth One Word: Personalizing Text-to-Image Generation using Textual Inversion", "year": 2022}, {"arxiv_id": "2208.12242", "title": "DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation", "year": 2022}, {"arxiv_id": "2212.04488", "title": "Multi-Concept Customization of Text-to-Image Diffusion", "year": 2022}, {"arxiv_id": "2302.13848", "title": "ELITE: Encoding Visual Concepts into Textual Embeddings for Customized Text-to-Image Generation", "year": 2023}, {"arxiv_id": "2303.09319", "title": "Unified Multi-Modal Latent Diffusion for Joint Subject and Text Conditional Image Generation", "year": 2023}, {"arxiv_id": "2304.00186", "title": "Subject-driven Text-to-Image Generation via Apprenticeship Learning", "year": 2023}], "gt_label": [1, 1, 1, 1, 1, 1, 1], "date": "2023-04", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_744", "valid": true}
{"query": "Can you cite the research that showed the use of ControlNet in fine-tuning image diffusion models based on various secondary inputs?", "cited_paper": [{"arxiv_id": "2302.05543", "title": "Adding Conditional Control to Text-to-Image Diffusion Models", "year": 2023}, {"arxiv_id": "2211.13752", "title": "Sketch-Guided Text-to-Image Diffusion Models", "year": 2022}], "gt_label": [1, 1], "date": "2023-02", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_745", "valid": true}
{"query": "Could you provide me some works that have discussed uncertainty quantification in molecule property prediction?", "cited_paper": [{"arxiv_id": "2005.10036", "title": "Uncertainty Quantification Using Neural Networks for Molecular Property Prediction", "year": 2020}], "gt_label": [1], "date": "2020-05", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_746", "valid": true}
{"query": "What studies have extended the zero-shot learning capability of CLIP to semantic segmentation?", "cited_paper": [{"arxiv_id": "2112.01518", "title": "DenseCLIP: Language-Guided Dense Prediction with Context-Aware Prompting", "year": 2021}, {"arxiv_id": "2212.03588", "title": "ZegCLIP: Towards Adapting CLIP for Zero-shot Semantic Segmentation", "year": 2022}], "gt_label": [1, 1], "date": "2022-12", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_747", "valid": true}
{"query": "What references detail the creation of the Large Language Models (LLMs)?", "cited_paper": [{"arxiv_id": "2005.14165", "title": "Language Models are Few-Shot Learners", "year": 2020}, {"arxiv_id": "2211.09085", "title": "Galactica: A Large Language Model for Science", "year": 2022}, {"arxiv_id": "2302.13971", "title": "LLaMA: Open and Efficient Foundation Language Models", "year": 2023}, {"arxiv_id": "2210.02414", "title": "GLM-130B: An Open Bilingual Pre-trained Model", "year": 2022}], "gt_label": [1, 1, 1, 1], "date": "2023-02", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_748", "valid": true}
{"query": "What works involve training of DPMs with advanced sampling strategies to improve inference speed and reduce training costs?", "cited_paper": [{"arxiv_id": "2010.02502", "title": "Denoising Diffusion Implicit Models", "year": 2020}, {"arxiv_id": "2106.00132", "title": "On Fast Sampling of Diffusion Probabilistic Models", "year": 2021}, {"arxiv_id": "2104.02600", "title": "Noise Estimation for Generative Diffusion Models", "year": 2021}], "gt_label": [1, 1, 1], "date": "2021-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_749", "valid": true}
{"query": "Which research defined a faithful explanation as one that accurately represents the true reasoning process behind the models prediction?", "cited_paper": [{"arxiv_id": "2004.03685", "title": "Towards Faithfully Interpretable NLP Systems: How should we define and evaluate faithfulness?", "year": 2020}], "gt_label": [1], "date": "2020-04", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_750", "valid": true}
{"query": "Can you give an example of a work that evaluates LLMs' reasoning ability using text generation instead of multiple choices?", "cited_paper": [{"arxiv_id": "2103.03874", "title": "Measuring Mathematical Problem Solving With the MATH Dataset", "year": 2021}], "gt_label": [1], "date": "2021-03", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_751", "valid": true}
{"query": "Could you provide me some works that apply graph-based models for pose estimation?", "cited_paper": [{"arxiv_id": "2108.07181", "title": "Learning Skeletal Graph Neural Networks for Hard 3D Pose Estimation", "year": 2021}, {"arxiv_id": "2307.05853", "title": "GLA-GCN: Global-local Adaptive Graph Convolutional Network for 3D Human Pose Estimation from Monocular Video", "year": 2023}, {"arxiv_id": "1801.07455", "title": "Spatial Temporal Graph Convolutional Networks for Skeleton-Based Action Recognition", "year": 2018}], "gt_label": [1, 1, 1], "date": "2023-07", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_752", "valid": true}
{"query": "What studies provide general overviews on using computational models from NLP and machine learning to measure readability?", "cited_paper": [{"arxiv_id": "2105.00973", "title": "Trends, Limitations and Open Challenges in Automatic Readability Assessment Research", "year": 2021}], "gt_label": [1], "date": "2021-05", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_753", "valid": true}
{"query": "In which works generating sparse graphs resulted in more computationally tractable solutions?", "cited_paper": [{"arxiv_id": "1802.04687", "title": "Neural Relational Inference for Interacting Systems", "year": 2018}, {"arxiv_id": "1909.03211", "title": "Measuring and Relieving the Over-smoothing Problem for Graph Neural Networks from the Topological View", "year": 2019}], "gt_label": [1, 1], "date": "2019-09", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_754", "valid": true}
{"query": "Which studies adapt dynamic skeletal graphs with action-specific edges for pose estimation?", "cited_paper": [{"arxiv_id": "2307.05853", "title": "GLA-GCN: Global-local Adaptive Graph Convolutional Network for 3D Human Pose Estimation from Monocular Video", "year": 2023}, {"arxiv_id": "1801.07455", "title": "Spatial Temporal Graph Convolutional Networks for Skeleton-Based Action Recognition", "year": 2018}], "gt_label": [1, 1], "date": "2023-07", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_755", "valid": true}
{"query": "What works covered the application of pseudo labeling?", "cited_paper": [{"arxiv_id": "2108.10743", "title": "DeepPanoContext: Panoramic 3D Scene Understanding with Holistic Scene Context Graph and Relation-based Optimization", "year": 2021}, {"arxiv_id": "2104.13613", "title": "Domain Adaptive Semantic Segmentation with Self-Supervised Depth Estimation", "year": 2021}, {"arxiv_id": "1707.09465", "title": "Curriculum Domain Adaptation for Semantic Segmentation of Urban Scenes", "year": 2017}], "gt_label": [1, 1, 1], "date": "2021-08", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_756", "valid": true}
{"query": "What works focus on improving privacy cost by assuming sub-Gaussian beauty of the underlying distribution in private mean estimation?", "cited_paper": [{"arxiv_id": "1902.04495", "title": "The Cost of Privacy: Optimal Rates of Convergence for Parameter Estimation with Differential Privacy", "year": 2019}, {"arxiv_id": "1805.00216", "title": "Privately Learning High-Dimensional Distributions", "year": 2018}, {"arxiv_id": "1906.02830", "title": "Average-Case Averages: Private Algorithms for Smooth Sensitivity and Mean Estimation", "year": 2019}, {"arxiv_id": "2006.06618", "title": "CoinPress: Practical Private Mean and Covariance Estimation", "year": 2020}], "gt_label": [1, 1, 1, 1], "date": "2020-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_757", "valid": true}
{"query": "What works have been done to improve the optimization-based pipeline in novel view synthesis with sparse input views?", "cited_paper": [{"arxiv_id": "2112.00724", "title": "RegNeRF: Regularizing Neural Radiance Fields for View Synthesis from Sparse Inputs", "year": 2021}, {"arxiv_id": "2211.11738", "title": "SPARF: Neural Radiance Fields from Sparse and Noisy Poses", "year": 2022}, {"arxiv_id": "2302.12231", "title": "DiffusioNeRF: Regularizing Neural Radiance Fields with Denoising Diffusion Models", "year": 2023}], "gt_label": [1, 1, 1], "date": "2023-02", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_758", "valid": true}
{"query": "Which works discuss the zero-violation approaches, where methods are initialized in the feasible region, and only updated in ways that are guaranteed not to leave the feasible region?", "cited_paper": [{"arxiv_id": "2106.02684", "title": "Learning Policies with Zero or Bounded Constraint Violation for Constrained MDPs", "year": 2021}], "gt_label": [1], "date": "2021-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_759", "valid": true}
{"query": "Can you provide me with some studies about the generation of robust counterfactuals?", "cited_paper": [{"arxiv_id": "2102.13620", "title": "Towards Robust and Reliable Algorithmic Recourse", "year": 2021}, {"arxiv_id": "2110.03109", "title": "Consistent Counterfactuals for Deep Models", "year": 2021}, {"arxiv_id": "2207.02739", "title": "Robust Counterfactual Explanations for Tree-Based Ensembles", "year": 2022}, {"arxiv_id": "2208.14878", "title": "Formalising the Robustness of Counterfactual Explanations for Neural Networks", "year": 2022}], "gt_label": [1, 1, 1, 1], "date": "2022-08", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_760", "valid": true}
{"query": "What studies are focused on the KD strategy, aimed at training a smaller student model with the guidance of a teacher model?", "cited_paper": [{"arxiv_id": "1503.02531", "title": "Distilling the Knowledge in a Neural Network", "year": 2015}, {"arxiv_id": "2306.08543", "title": "MiniLLM: Knowledge Distillation of Large Language Models", "year": 2023}, {"arxiv_id": "2208.10160", "title": "PANDA: Prompt Transfer Meets Knowledge Distillation for Efficient Model Adaptation", "year": 2022}, {"arxiv_id": "2110.13398", "title": "Unified Instance and Knowledge Alignment Pretraining for Aspect-based Sentiment Analysis", "year": 2021}], "gt_label": [1, 1, 1, 1], "date": "2023-07", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_761", "valid": true}
{"query": "What research added supervision during training to bypass the impossibility of learning disentangled representations from independent and identically distributed (i.i.d) data?", "cited_paper": [{"arxiv_id": "1811.12359", "title": "Challenging Common Assumptions in the Unsupervised Learning of Disentangled Representations", "year": 2018}], "gt_label": [1], "date": "2018-11", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_762", "valid": true}
{"query": "Are there any papers exploring disentanglement in diffusion models?", "cited_paper": [{"arxiv_id": "2210.10960", "title": "Diffusion Models already have a Semantic Latent Space", "year": 2022}, {"arxiv_id": "2111.15640", "title": "Diffusion Autoencoders: Toward a Meaningful and Decodable Representation", "year": 2021}, {"arxiv_id": "2212.08698", "title": "Uncovering the Disentanglement Capability in Text-to-Image Diffusion Models", "year": 2022}, {"arxiv_id": "2306.00983", "title": "StyleDrop: Text-to-Image Generation in Any Style", "year": 2023}, {"arxiv_id": "2309.01770", "title": "StyleAdapter: A Unified Stylized Image Generation Model", "year": 2023}, {"arxiv_id": "2308.07863", "title": "StyleDiffusion: Controllable Disentangled Style Transfer via Diffusion Models", "year": 2023}], "gt_label": [1, 1, 1, 1, 1, 1], "date": "2023-09", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_763", "valid": true}
{"query": "Which works introduced multi-task representation learning techniques in natural language domain?", "cited_paper": [{"arxiv_id": "2005.14165", "title": "Language Models are Few-Shot Learners", "year": 2020}], "gt_label": [1], "date": "2020-05", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_764", "valid": true}
{"query": "Which study was dedicated Multi-LLM Debate, which uses multiple LLM instances to propose and debate responses?", "cited_paper": [{"arxiv_id": "2305.14325", "title": "Improving Factuality and Reasoning in Language Models through Multiagent Debate", "year": 2023}], "gt_label": [1], "date": "2023-05", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_765", "valid": true}
{"query": "Any works are benchmarked on datasets with limited number of object instances for 6D Pose Estimation?", "cited_paper": [{"arxiv_id": "1711.00199", "title": "PoseCNN: A Convolutional Neural Network for 6D Object Pose Estimation in Cluttered Scenes", "year": 2017}], "gt_label": [1], "date": "2017-11", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_766", "valid": true}
{"query": "Could you provide me studies about RNN-based approaches for short-term future prediction in 3D human pose forecasting?", "cited_paper": [{"arxiv_id": "1508.00271", "title": "Recurrent Network Models for Human Dynamics", "year": 2015}, {"arxiv_id": "1511.05298", "title": "Structural-RNN: Deep Learning on Spatio-Temporal Graphs", "year": 2015}, {"arxiv_id": "1705.02445", "title": "On human motion prediction using recurrent neural networks", "year": 2017}, {"arxiv_id": "1910.09070", "title": "Structured Prediction Helps 3D Human Motion Modelling", "year": 2019}, {"arxiv_id": "1810.09676", "title": "Action-Agnostic Human Pose Forecasting", "year": 2018}, {"arxiv_id": "1909.03449", "title": "Imitation Learning for Human Pose Prediction", "year": 2019}, {"arxiv_id": "1809.03036", "title": "A Neural Temporal Model for Human Motion Prediction", "year": 2018}, {"arxiv_id": "1805.06485", "title": "QuaterNet: A Quaternion-based Recurrent Model for Human Motion", "year": 2018}], "gt_label": [1, 1, 1, 1, 1, 1, 1, 1], "date": "2019-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_767", "valid": true}
{"query": "What work specified the target styles in the instructions as constraints to improve controlled text generation?", "cited_paper": [{"arxiv_id": "2304.14293", "title": "Controlled Text Generation with Natural Language Instructions", "year": 2023}], "gt_label": [1], "date": "2023-04", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_768", "valid": true}
{"query": "Could you give me studies that focus on two-stage models for diffusion models?", "cited_paper": [{"arxiv_id": "2104.07636", "title": "Image Super-Resolution via Iterative Refinement", "year": 2021}, {"arxiv_id": "2112.10752", "title": "High-Resolution Image Synthesis with Latent Diffusion Models", "year": 2021}, {"arxiv_id": "2307.01952", "title": "SDXL: Improving Latent Diffusion Models for High-Resolution Image Synthesis", "year": 2023}, {"arxiv_id": "2305.07015", "title": "Exploiting Diffusion Prior for Real-World Image Super-Resolution", "year": 2023}], "gt_label": [1, 1, 1, 1], "date": "2023-07", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_769", "valid": true}
{"query": "Which papers explore the utility of tool-calling in benchmarks for machine translation?", "cited_paper": [{"arxiv_id": "1910.06204", "title": "Estimating post-editing effort: a study on human judgements, task-based and reference-based metrics of MT quality", "year": 2019}, {"arxiv_id": "1910.07475", "title": "MLQA: Evaluating Cross-lingual Extractive Question Answering", "year": 2019}], "gt_label": [1, 1], "date": "2019-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_770", "valid": true}
{"query": "Which studies can be mentioned as significant advancements in text-to-image models?", "cited_paper": [{"arxiv_id": "2112.10741", "title": "GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models", "year": 2021}, {"arxiv_id": "2204.06125", "title": "Hierarchical Text-Conditional Image Generation with CLIP Latents", "year": 2022}, {"arxiv_id": "2205.11487", "title": "Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding", "year": 2022}, {"arxiv_id": "2112.10752", "title": "High-Resolution Image Synthesis with Latent Diffusion Models", "year": 2021}], "gt_label": [1, 1, 1, 1], "date": "2022-05", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_771", "valid": true}
{"query": "What papers are about translating natural language utterances into query language through semantic parsing?", "cited_paper": [{"arxiv_id": "2305.03111", "title": "Can LLM Already Serve as A Database Interface? A BIg Bench for Large-Scale Database Grounded Text-to-SQLs", "year": 2023}, {"arxiv_id": "1809.08887", "title": "Spider: A Large-Scale Human-Labeled Dataset for Complex and Cross-Domain Semantic Parsing and Text-to-SQL Task", "year": 2018}, {"arxiv_id": "1709.00103", "title": "Seq2SQL: Generating Structured Queries from Natural Language using Reinforcement Learning", "year": 2017}], "gt_label": [1, 1, 1], "date": "2023-05", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_772", "valid": true}
{"query": "Who first observed grokking for algorithmic datasets?", "cited_paper": [{"arxiv_id": "2201.02177", "title": "Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets", "year": 2022}], "gt_label": [1], "date": "2022-01", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_773", "valid": true}
{"query": "Can you name the studies propagating the use of 3D human models for sign language understanding and production tasks?", "cited_paper": [{"arxiv_id": "1904.05866", "title": "Expressive Body Capture: 3D Hands, Face, and Body from a Single Image", "year": 2019}, {"arxiv_id": "2308.09305", "title": "Human Part-wise 3D Motion Context Learning for Sign Language Recognition", "year": 2023}], "gt_label": [1, 1], "date": "2023-08", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_774", "valid": true}
{"query": "What research involve the use of training trajectories in minimizing surrogate models during dataset distillation?", "cited_paper": [{"arxiv_id": "2203.11932", "title": "Dataset Distillation by Matching Training Trajectories", "year": 2022}, {"arxiv_id": "2207.09639", "title": "DC-BENCH: Dataset Condensation Benchmark", "year": 2022}, {"arxiv_id": "2211.11004", "title": "Minimizing the Accumulated Trajectory Error to Improve Dataset Distillation", "year": 2022}, {"arxiv_id": "2211.10586", "title": "Scaling Up Dataset Distillation to ImageNet-1K with Constant Memory", "year": 2022}, {"arxiv_id": "2301.07014", "title": "Dataset Distillation: A Comprehensive Review", "year": 2023}, {"arxiv_id": "2310.05773", "title": "Towards Lossless Dataset Distillation via Difficulty-Aligned Trajectory Matching", "year": 2023}], "gt_label": [1, 1, 1, 1, 1, 1], "date": "2023-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_775", "valid": true}
{"query": "Who employed the concept of acoustic tokens preserving all information of the audio with RVQ for raw audio reconstruction?", "cited_paper": [{"arxiv_id": "2107.03312", "title": "SoundStream: An End-to-End Neural Audio Codec", "year": 2021}, {"arxiv_id": "2210.13438", "title": "High Fidelity Neural Audio Compression", "year": 2022}], "gt_label": [1, 1], "date": "2022-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_776", "valid": true}
{"query": "Could you provide me some works particularly related to quantifying reproducibility in Machine Learning?", "cited_paper": [{"arxiv_id": "2103.07929", "title": "A Systematic Review of Reproducibility Research in Natural Language Processing", "year": 2021}, {"arxiv_id": "1711.10337", "title": "Are GANs Created Equal? A Large-Scale Study", "year": 2017}, {"arxiv_id": "1709.06560", "title": "Deep Reinforcement Learning that Matters", "year": 2017}], "gt_label": [1, 1, 1], "date": "2021-03", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_777", "valid": true}
{"query": "Which researches utilize a symmetry deformation module to learn the reconstruction and compute dense correspondence in the context of learning on point clouds?", "cited_paper": [{"arxiv_id": "2012.15638", "title": "CorrNet3D: Unsupervised End-to-end Learning of Dense Correspondence for 3D Point Clouds", "year": 2020}], "gt_label": [1], "date": "2020-12", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_778", "valid": true}
{"query": "Are there any papers that stated that the performance of in-context learning is sensitive to the input of pre-training language models?", "cited_paper": [{"arxiv_id": "1909.00505", "title": "Commonsense Knowledge Mining from Pretrained Models", "year": 2019}, {"arxiv_id": "1911.12543", "title": "How Can We Know What Language Models Know?", "year": 2019}], "gt_label": [1, 1], "date": "2019-11", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_779", "valid": true}
{"query": "What studies explored the use of hypernetworks in meta-RL?", "cited_paper": [{"arxiv_id": "2210.11348", "title": "Hypernetworks in Meta-Reinforcement Learning", "year": 2022}, {"arxiv_id": "2101.04750", "title": "Linear Representation Meta-Reinforcement Learning for Instant Adaptation", "year": 2021}, {"arxiv_id": "2106.06842", "title": "Recomposing the Reinforcement Learning Building Blocks with Hypernetworks", "year": 2021}], "gt_label": [1, 1, 1], "date": "2022-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_780", "valid": true}
{"query": "Any papers introduced approaches that combine learning with classical graph planning for generalization across various planning domains?", "cited_paper": [{"arxiv_id": "1706.04317", "title": "Schema Networks: Zero-shot Transfer with a Generative Causal Model of Intuitive Physics", "year": 2017}, {"arxiv_id": "1709.04271", "title": "Action Schema Networks: Generalised Policies with Deep Learning", "year": 2017}, {"arxiv_id": "2005.02305", "title": "Generalized Planning With Deep Reinforcement Learning", "year": 2020}], "gt_label": [1, 1, 1], "date": "2020-05", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_781", "valid": true}
{"query": "Which works proposed to extend Large Language Models to other modalities, such as audio?", "cited_paper": [{"arxiv_id": "2305.10790", "title": "Listen, Think, and Understand", "year": 2023}, {"arxiv_id": "2305.11000", "title": "SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities", "year": 2023}], "gt_label": [1, 1], "date": "2023-05", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_782", "valid": true}
{"query": "What study proposed the refinement of avatar generation through coarse-to-fine and multi-box training for higher-quality avatar?", "cited_paper": [{"arxiv_id": "2303.17606", "title": "AvatarCraft: Transforming Text into Neural Human Avatars with Parameterized Shape and Pose Control", "year": 2023}], "gt_label": [1], "date": "2023-03", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_783", "valid": true}
{"query": "What studies propose to train such model on large-scale image-text data, enabling it to complete various instructions about images?", "cited_paper": [{"arxiv_id": "2310.03744", "title": "Improved Baselines with Visual Instruction Tuning", "year": 2023}, {"arxiv_id": "2304.10592", "title": "MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models", "year": 2023}, {"arxiv_id": "2304.15010", "title": "LLaMA-Adapter V2: Parameter-Efficient Visual Instruction Model", "year": 2023}, {"arxiv_id": "2304.14178", "title": "mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality", "year": 2023}], "gt_label": [1, 1, 1, 1], "date": "2023-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_784", "valid": true}
{"query": "Which papers argue for a soft-constraint approach to Constrained Reinforcement Learning (CRL)?", "cited_paper": [{"arxiv_id": "2010.06324", "title": "Balancing Constraints and Rewards with Meta-Gradient D4PG", "year": 2020}, {"arxiv_id": "2003.11881", "title": "An empirical investigation of the challenges of real-world reinforcement learning", "year": 2020}], "gt_label": [1, 1], "date": "2020-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_785", "valid": true}
{"query": "Which papers proposed random sketching operators used in solving overdetermined least squares problems?", "cited_paper": [{"arxiv_id": "1411.4357", "title": "Sketching as a Tool for Numerical Linear Algebra", "year": 2014}], "gt_label": [1], "date": "2014-11", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_786", "valid": true}
{"query": "Which research leveraged the memory of users feedback to generate prompt for LLMs?", "cited_paper": [{"arxiv_id": "2201.06009", "title": "Memory-assisted prompt editing to improve GPT-3 after deployment", "year": 2022}], "gt_label": [1], "date": "2022-01", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_787", "valid": true}
{"query": "Which papers extended BERT paradigm to areas like point cloud, audio, and video perception?", "cited_paper": [{"arxiv_id": "2203.06604", "title": "Masked Autoencoders for Point Cloud Self-supervised Learning", "year": 2022}, {"arxiv_id": "2104.01778", "title": "AST: Audio Spectrogram Transformer", "year": 2021}, {"arxiv_id": "2203.12602", "title": "VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training", "year": 2022}, {"arxiv_id": "2307.10802", "title": "Meta-Transformer: A Unified Framework for Multimodal Learning", "year": 2023}], "gt_label": [1, 1, 1, 1], "date": "2023-07", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_788", "valid": true}
{"query": "What works proposed regularization methods refining parameters or features without explicitly maintaining source knowledge?", "cited_paper": [{"arxiv_id": "1606.02819", "title": "Low-shot Visual Recognition by Shrinking and Hallucinating Features", "year": 2016}, {"arxiv_id": "1908.05997", "title": "Regularizing CNN Transfer Learning with Randomised Regression", "year": 2019}], "gt_label": [1, 1], "date": "2019-08", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_789", "valid": true}
{"query": "What works tried to amalgamate the search query and all the candidates together as input for retrieval tasks?", "cited_paper": [{"arxiv_id": "2306.17563", "title": "Large Language Models are Effective Text Rankers with Pairwise Ranking Prompting", "year": 2023}], "gt_label": [1], "date": "2023-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_790", "valid": true}
{"query": "Any studies about the difficulty in tuning the rate of the second timescale in GTD-style approaches?", "cited_paper": [{"arxiv_id": "2106.05012", "title": "Bayesian Bellman Operators", "year": 2021}], "gt_label": [1], "date": "2021-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_791", "valid": true}
{"query": "Could you tell me the papers which demonstrated that heuristically filtered data improves T5?", "cited_paper": [{"arxiv_id": "1910.10683", "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer", "year": 2019}], "gt_label": [1], "date": "2019-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_792", "valid": true}
{"query": "What research work introduced the PMD algorithm that was strictly limited to the tabular setting?", "cited_paper": [{"arxiv_id": "2201.07443", "title": "On the Convergence Rates of Policy Gradient Methods", "year": 2022}], "gt_label": [1], "date": "2022-01", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_793", "valid": true}
{"query": "Which work introduced the concept of oracle complexity separation?", "cited_paper": [], "gt_label": [], "date": "2020-02", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_794", "valid": true}
{"query": "Could you provide the studies about empirical development of federated learning in large-scale deep learning?", "cited_paper": [{"arxiv_id": "1602.05629", "title": "Communication-Efficient Learning of Deep Networks from Decentralized Data", "year": 2016}, {"arxiv_id": "1410.7455", "title": "Parallel training of DNNs with Natural Gradient and Parameter Averaging", "year": 2014}, {"arxiv_id": "1507.01239", "title": "Experiments on Parallel Training of Deep Neural Network using Model Averaging", "year": 2015}], "gt_label": [1, 1, 1], "date": "2016-02", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_795", "valid": true}
{"query": "Could you provide me some works that studied the design of U-Nets and their connection to wavelets?", "cited_paper": [{"arxiv_id": "1805.07071", "title": "Multi-level Wavelet-CNN for Image Restoration", "year": 2018}], "gt_label": [1], "date": "2018-05", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_796", "valid": true}
{"query": "What studies achieved local alignment by exploiting the fine-grained relation between visual objects and textual words?", "cited_paper": [{"arxiv_id": "1909.11740", "title": "UNITER: UNiversal Image-TExt Representation Learning", "year": 2019}, {"arxiv_id": "2004.06165", "title": "Oscar: Object-Semantics Aligned Pre-training for Vision-Language Tasks", "year": 2020}, {"arxiv_id": "2012.15409", "title": "UNIMO: Towards Unified-Modal Understanding and Generation via Cross-Modal Contrastive Learning", "year": 2020}, {"arxiv_id": "2107.14572", "title": "Product1M: Towards Weakly Supervised Instance-Level Product Retrieval via Cross-modal Pretraining", "year": 2021}, {"arxiv_id": "2102.03334", "title": "ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision", "year": 2021}, {"arxiv_id": "2111.07783", "title": "FILIP: Fine-grained Interactive Language-Image Pre-Training", "year": 2021}, {"arxiv_id": "2109.01949", "title": "Improving Joint Learning of Chest X-Ray and Radiology Report by Word Region Alignment", "year": 2021}, {"arxiv_id": "2210.06044", "title": "Multi-Granularity Cross-modal Alignment for Generalized Medical Visual Representation Learning", "year": 2022}], "gt_label": [1, 1, 1, 1, 1, 1, 1, 1], "date": "2022-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_797", "valid": true}
{"query": "Could you provide me some studies where DPMs were used for text-to-image synthesis?", "cited_paper": [{"arxiv_id": "2206.00386", "title": "DiVAE: Photorealistic Images Synthesis with Denoising Diffusion Decoder", "year": 2022}, {"arxiv_id": "2207.13038", "title": "Text-Guided Synthesis of Artistic Images with Retrieval-Augmented Diffusion Models", "year": 2022}, {"arxiv_id": "2205.11487", "title": "Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding", "year": 2022}], "gt_label": [1, 1, 1], "date": "2022-07", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_798", "valid": true}
{"query": "Which papers discuss the improvements in performance across a wide range of NLP tasks due to the Transformer architecture?", "cited_paper": [{"arxiv_id": "1706.03762", "title": "Attention Is All You Need", "year": 2017}, {"arxiv_id": "2203.02155", "title": "Training language models to follow instructions with human feedback", "year": 2022}, {"arxiv_id": "2212.08073", "title": "Constitutional AI: Harmlessness from AI Feedback", "year": 2022}, {"arxiv_id": "2204.02311", "title": "PaLM: Scaling Language Modeling with Pathways", "year": 2022}, {"arxiv_id": "2307.09288", "title": "Llama 2: Open Foundation and Fine-Tuned Chat Models", "year": 2023}, {"arxiv_id": "2302.13971", "title": "LLaMA: Open and Efficient Foundation Language Models", "year": 2023}, {"arxiv_id": "1910.10683", "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer", "year": 2019}], "gt_label": [1, 1, 1, 1, 1, 1, 1], "date": "2023-07", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_799", "valid": true}
{"query": "Which works discusses about exploiting additional structures found in large classes of non-monotone VIPs?", "cited_paper": [{"arxiv_id": "2009.09623", "title": "The Complexity of Constrained Min-Max Optimization", "year": 2020}], "gt_label": [1], "date": "2021-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_800", "valid": true}
{"query": "Which works are mentioned in the context of benchmarks for assessing multimodal capabilities?", "cited_paper": [{"arxiv_id": "2105.04165", "title": "Inter-GPS: Interpretable Geometry Problem Solving with Formal Language and Symbolic Reasoning", "year": 2021}, {"arxiv_id": "2105.14517", "title": "GeoQA: A Geometric Question Answering Benchmark Towards Multimodal Numerical Reasoning", "year": 2021}, {"arxiv_id": "2212.02746", "title": "UniGeo: Unifying Geometry Logical Reasoning via Reformulating Mathematical Expression", "year": 2022}, {"arxiv_id": "2209.09513", "title": "Learn to Explain: Multimodal Reasoning via Thought Chains for Science Question Answering", "year": 2022}, {"arxiv_id": "2311.16502", "title": "MMMU: A Massive Multi-discipline Multimodal Understanding and Reasoning Benchmark for Expert AGI", "year": 2023}, {"arxiv_id": "2401.11944", "title": "CMMMU: A Chinese Massive Multi-discipline Multimodal Understanding Benchmark", "year": 2024}, {"arxiv_id": "2401.14011", "title": "CMMU: A Benchmark for Chinese Multi-modal Multi-type Question Understanding and Reasoning", "year": 2024}, {"arxiv_id": "2310.02255", "title": "MathVista: Evaluating Mathematical Reasoning of Foundation Models in Visual Contexts", "year": 2023}], "gt_label": [1, 1, 1, 1, 1, 1, 1, 1], "date": "2024-01", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_801", "valid": true}
{"query": "Can you provide studies about perturbation methods like RISE and SHAP?", "cited_paper": [{"arxiv_id": "1806.07421", "title": "RISE: Randomized Input Sampling for Explanation of Black-box Models", "year": 2018}, {"arxiv_id": "1705.07874", "title": "A Unified Approach to Interpreting Model Predictions", "year": 2017}], "gt_label": [1, 1], "date": "2018-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_802", "valid": true}
{"query": "What papers provide evidence of semantic information encoded into the activations of a neural network?", "cited_paper": [{"arxiv_id": "2106.00737", "title": "Implicit Representations of Meaning in Neural Language Models", "year": 2021}, {"arxiv_id": "1606.05378", "title": "Simpler Context-Dependent Logical Forms via Model Projections", "year": 2016}], "gt_label": [1, 1], "date": "2021-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_803", "valid": true}
{"query": "Which works applied alignment approaches to capture the domain invariant characteristics of images?", "cited_paper": [{"arxiv_id": "1711.03213", "title": "CyCADA: Cycle-Consistent Adversarial Domain Adaptation", "year": 2017}, {"arxiv_id": "1904.10620", "title": "Bidirectional Learning for Domain Adaptation of Semantic Segmentation", "year": 2019}, {"arxiv_id": "1712.00479", "title": "Image to Image Translation for Domain Adaptation", "year": 2017}], "gt_label": [1, 1, 1], "date": "2019-04", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_804", "valid": true}
{"query": "Which papers address the use of generative models, such as normalizing flows, Variational Autoencoders (VAE), and diffusion models, for VR HMD?", "cited_paper": [{"arxiv_id": "2203.05789", "title": "FLAG: Flow-based 3D Avatar Generation from Sparse Observations", "year": 2022}, {"arxiv_id": "2304.08577", "title": "Avatars Grow Legs: Generating Smooth Human Motion from Sparse Tracking Inputs with Diffusion Model", "year": 2023}], "gt_label": [1, 1], "date": "2023-04", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_805", "valid": true}
{"query": "Who projected the first method for distinguishing the neurons ability based on the neurons activation value?", "cited_paper": [{"arxiv_id": "2211.07349", "title": "Finding Skill Neurons in Pre-trained Transformer-based Language Models", "year": 2022}], "gt_label": [1], "date": "2022-11", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_806", "valid": true}
{"query": "Which papers have studied algorithmic fairness, specifically with regards to improving model performance at a group level?", "cited_paper": [{"arxiv_id": "2206.02058", "title": "When Personalization Harms: Reconsidering the Use of Group Attributes in Prediction", "year": 2022}], "gt_label": [1], "date": "2022-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_807", "valid": true}
{"query": "What are the studies that used external knowledge for reference retrieval to detect hallucinations?", "cited_paper": [{"arxiv_id": "2209.10063", "title": "Generate rather than Retrieve: Large Language Models are Strong Context Generators", "year": 2022}, {"arxiv_id": "2307.11019", "title": "Investigating the Factual Knowledge Boundary of Large Language Models with Retrieval Augmentation", "year": 2023}], "gt_label": [1, 1], "date": "2023-07", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_808", "valid": true}
{"query": "Which studies focused on detecting contamination in models without accessing their training data, primarily using output probabilities?", "cited_paper": [{"arxiv_id": "2310.16789", "title": "Detecting Pretraining Data from Large Language Models", "year": 2023}, {"arxiv_id": "2310.17623", "title": "Proving Test Set Contamination in Black Box Language Models", "year": 2023}, {"arxiv_id": "2402.08100", "title": "Investigating the Impact of Data Contamination of Large Language Models in Text-to-SQL Translation", "year": 2024}], "gt_label": [1, 1, 1], "date": "2024-02", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_809", "valid": true}
{"query": "What works have made advancements in using deep neural networks for real-space electronic systems?", "cited_paper": [], "gt_label": [], "date": "2019-09", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_810", "valid": true}
{"query": "What are the studies that used various motion representations in prediction tasks?", "cited_paper": [{"arxiv_id": "1803.06951", "title": "Deja Vu: Motion Prediction in Static Images", "year": 2018}, {"arxiv_id": "1712.04109", "title": "Im2Flow: Motion Hallucination from Static Images for Action Recognition", "year": 2017}, {"arxiv_id": "1609.02612", "title": "Generating Videos with Scene Dynamics", "year": 2016}, {"arxiv_id": "1606.07873", "title": "An Uncertain Future: Forecasting from Static Images using Variational Autoencoders", "year": 2016}, {"arxiv_id": "1807.09245", "title": "Visual Dynamics: Stochastic Future Generation via Layered Cross Convolutional Networks", "year": 2018}], "gt_label": [1, 1, 1, 1, 1], "date": "2018-07", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_811", "valid": true}
{"query": "Which papers propose solutions to multi-dataset object detection challenges?", "cited_paper": [{"arxiv_id": "2008.06614", "title": "Object Detection with a Unified Label Space from Multiple Datasets", "year": 2020}, {"arxiv_id": "2001.04621", "title": "Cross-dataset Training for Class Increasing Object Detection", "year": 2020}], "gt_label": [1, 1], "date": "2020-08", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_812", "valid": true}
{"query": "What works focus on animating input source image through explicit or implicit image-based rendering according to motion derived from external sources?", "cited_paper": [{"arxiv_id": "2003.00196", "title": "First Order Motion Model for Image Animation", "year": 2020}, {"arxiv_id": "2104.11280", "title": "Motion Representations for Articulated Animation", "year": 2021}, {"arxiv_id": "1812.08861", "title": "Animating Arbitrary Objects via Deep Motion Transfer", "year": 2018}, {"arxiv_id": "2203.09043", "title": "Latent Image Animator: Learning to Animate Images via Latent Space Navigation", "year": 2022}, {"arxiv_id": "2304.06025", "title": "DreamPose: Fashion Image-to-Video Synthesis via Stable Diffusion", "year": 2023}], "gt_label": [1, 1, 1, 1, 1], "date": "2023-04", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_813", "valid": true}
{"query": "Could you provide me some studies about efficient exploration strategies in RL that are based on starting exploration from specific states?", "cited_paper": [{"arxiv_id": "1812.03381", "title": "Learning Montezuma's Revenge from a Single Demonstration", "year": 2018}, {"arxiv_id": "1607.05077", "title": "Playing Atari Games with Deep Reinforcement Learning and Human Checkpoint Replay", "year": 2016}, {"arxiv_id": "1804.02717", "title": "DeepMimic: Example-Guided Deep Reinforcement Learning of Physics-Based Character Skills", "year": 2018}, {"arxiv_id": "1709.10089", "title": "Overcoming Exploration in Reinforcement Learning with Demonstrations", "year": 2017}, {"arxiv_id": "1707.05300", "title": "Reverse Curriculum Generation for Reinforcement Learning", "year": 2017}, {"arxiv_id": "1805.07470", "title": "Solving the Rubik's Cube Without Human Knowledge", "year": 2018}, {"arxiv_id": "1806.06161", "title": "BaRC: Backward Reachability Curriculum for Robotic Reinforcement Learning", "year": 2018}, {"arxiv_id": "1807.06919", "title": "Backplay: \"Man muss immer umkehren\"", "year": 2018}], "gt_label": [1, 1, 1, 1, 1, 1, 1, 1], "date": "2020-04", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_814", "valid": true}
{"query": "Which studies explored methods to combine multiple concepts or attributes in personalization?", "cited_paper": [{"arxiv_id": "2303.11305", "title": "SVDiff: Compact Parameter Space for Diffusion Fine-Tuning", "year": 2023}, {"arxiv_id": "2212.04488", "title": "Multi-Concept Customization of Text-to-Image Diffusion", "year": 2022}, {"arxiv_id": "2305.16311", "title": "Break-A-Scene: Extracting Multiple Concepts from a Single Image", "year": 2023}, {"arxiv_id": "2306.00983", "title": "StyleDrop: Text-to-Image Generation in Any Style", "year": 2023}, {"arxiv_id": "2305.16225", "title": "ProSpect: Prompt Spectrum for Attribute-Aware Personalization of Diffusion Models", "year": 2023}, {"arxiv_id": "2305.18203", "title": "Concept Decomposition for Visual Exploration and Inspiration", "year": 2023}], "gt_label": [1, 1, 1, 1, 1, 1], "date": "2023-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_815", "valid": true}
{"query": "Which paper demonstrates mBERTs ability to learn multilingual representations, enabling cross-lingual transfer for languages with different scripts?", "cited_paper": [{"arxiv_id": "1906.01502", "title": "How multilingual is Multilingual BERT?", "year": 2019}], "gt_label": [1], "date": "2019-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_816", "valid": true}
{"query": "Could you mention some works that focused on temporal or geographical contexts that can change the answer to the same question? ", "cited_paper": [{"arxiv_id": "2109.06157", "title": "SituatedQA: Incorporating Extra-Linguistic Contexts into QA", "year": 2021}, {"arxiv_id": "2205.11388", "title": "StreamingQA: A Benchmark for Adaptation to New Knowledge over Time in Question Answering Models", "year": 2022}], "gt_label": [1, 1], "date": "2022-05", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_817", "valid": true}
{"query": "Could you name some video datasets that provide 2D keypoints with temporal information?", "cited_paper": [{"arxiv_id": "1710.10000", "title": "PoseTrack: A Benchmark for Human Pose Estimation and Tracking", "year": 2017}], "gt_label": [1], "date": "2017-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_818", "valid": true}
{"query": "Which works are related to the study of partial client participation in the context of PFL?", "cited_paper": [{"arxiv_id": "1907.02189", "title": "On the Convergence of FedAvg on Non-IID Data", "year": 2019}, {"arxiv_id": "2101.11203", "title": "Achieving Linear Speedup with Partial Worker Participation in Non-IID Federated Learning", "year": 2021}, {"arxiv_id": "2205.13648", "title": "A Unified Analysis of Federated Learning with Arbitrary Client Participation", "year": 2022}], "gt_label": [1, 1, 1], "date": "2022-05", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_819", "valid": true}
{"query": "Which works focus on the development of multi-modal visual foundation models?", "cited_paper": [{"arxiv_id": "2103.00020", "title": "Learning Transferable Visual Models From Natural Language Supervision", "year": 2021}, {"arxiv_id": "2102.05918", "title": "Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision", "year": 2021}, {"arxiv_id": "2111.11432", "title": "Florence: A New Foundation Model for Computer Vision", "year": 2021}, {"arxiv_id": "2208.10442", "title": "Image as a Foreign Language: BEiT Pretraining for All Vision and Vision-Language Tasks", "year": 2022}, {"arxiv_id": "2204.14198", "title": "Flamingo: a Visual Language Model for Few-Shot Learning", "year": 2022}], "gt_label": [1, 1, 1, 1, 1], "date": "2022-08", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_820", "valid": true}
{"query": "What papers have considered a similar optimization approach as used in TTOpt?", "cited_paper": [{"arxiv_id": "2206.05077", "title": "Tensor Train for Global Optimization Problems in Robotics", "year": 2022}], "gt_label": [1], "date": "2022-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_821", "valid": true}
{"query": "Any studies focusing on attribute-aware personalization?", "cited_paper": [{"arxiv_id": "2305.16225", "title": "ProSpect: Prompt Spectrum for Attribute-Aware Personalization of Diffusion Models", "year": 2023}], "gt_label": [1], "date": "2023-05", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_822", "valid": true}
{"query": "Could you provide me any research about AUM which identifies data by computing the Area Under the Margin?", "cited_paper": [{"arxiv_id": "2001.10528", "title": "Identifying Mislabeled Data using the Area Under the Margin Ranking", "year": 2020}], "gt_label": [1], "date": "2020-01", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_823", "valid": true}
{"query": "Which research work highlighted the integration of elements such as emotion, semantics in ESC systems?", "cited_paper": [{"arxiv_id": "2305.03296", "title": "TransESC: Smoothing Emotional Support Conversation via Turn-Level State Transition", "year": 2023}], "gt_label": [1], "date": "2023-05", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_824", "valid": true}
{"query": "What studies incorporated adversarial training in self-supervised settings?", "cited_paper": [{"arxiv_id": "2010.13337", "title": "Robust Pre-Training by Adversarial Contrastive Learning", "year": 2020}, {"arxiv_id": "2006.07589", "title": "Adversarial Self-Supervised Contrastive Learning", "year": 2020}, {"arxiv_id": "2010.12050", "title": "Contrastive Learning with Adversarial Examples", "year": 2020}], "gt_label": [1, 1, 1], "date": "2020-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_825", "valid": true}
{"query": "Which works considered the impact of attacks on multi-modal auto-driving systems?", "cited_paper": [{"arxiv_id": "2304.14614", "title": "Fusion is Not Enough: Single Modal Attacks on Fusion Models for 3D Object Detection", "year": 2023}], "gt_label": [1], "date": "2023-04", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_826", "valid": true}
{"query": "Could you list the works that performed self-adversarial training by dual-BN technique or used pretrained models to generate pseudo-labels?", "cited_paper": [{"arxiv_id": "2010.13337", "title": "Robust Pre-Training by Adversarial Contrastive Learning", "year": 2020}, {"arxiv_id": "1911.09665", "title": "Adversarial Examples Improve Image Recognition", "year": 2019}, {"arxiv_id": "2111.01124", "title": "When Does Contrastive Learning Preserve Adversarial Robustness from Pretraining to Finetuning?", "year": 2021}, {"arxiv_id": "2207.10899", "title": "Decoupled Adversarial Contrastive Learning for Self-supervised Adversarial Robustness", "year": 2022}], "gt_label": [1, 1, 1, 1], "date": "2022-07", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_827", "valid": true}
{"query": "What works have proposed decoupling the environmental exploration from navigation to the target?", "cited_paper": [{"arxiv_id": "2209.08803", "title": "Zero-shot Active Visual Search (ZAVIS): Intelligent Object Search for Robotic Assistants", "year": 2022}], "gt_label": [1], "date": "2022-09", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_828", "valid": true}
{"query": "Which papers applied open-loop imitation learning to predict driving behaviors of other road users in autonomous driving?", "cited_paper": [{"arxiv_id": "1812.03079", "title": "ChauffeurNet: Learning to Drive by Imitating the Best and Synthesizing the Worst", "year": 2018}, {"arxiv_id": "1910.05449", "title": "MultiPath: Multiple Probabilistic Anchor Trajectory Hypotheses for Behavior Prediction", "year": 2019}, {"arxiv_id": "1911.00997", "title": "Multiple Futures Prediction", "year": 2019}, {"arxiv_id": "2007.13732", "title": "Learning Lane Graph Representations for Motion Forecasting", "year": 2020}, {"arxiv_id": "2106.08417", "title": "Scene Transformer: A unified architecture for predicting multiple agent trajectories", "year": 2021}, {"arxiv_id": "2108.09640", "title": "DenseTNT: End-to-end Trajectory Prediction from Dense Goal Sets", "year": 2021}, {"arxiv_id": "2207.05844", "title": "Wayformer: Motion Forecasting via Simple & Efficient Attention Networks", "year": 2022}], "gt_label": [1, 1, 1, 1, 1, 1, 1], "date": "2022-07", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_829", "valid": true}
{"query": "Which work has proposed replacing ODE solvers in the forward with a Taylor-Lagrange expansion to improve training and prediction times?", "cited_paper": [{"arxiv_id": "2201.05715", "title": "Taylor-Lagrange Neural Ordinary Differential Equations: Toward Fast Training and Evaluation of Neural ODEs", "year": 2022}], "gt_label": [1], "date": "2022-01", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_830", "valid": true}
{"query": "What works discuss the use of white-box knowledge distillation in language learning models?", "cited_paper": [{"arxiv_id": "1503.02531", "title": "Distilling the Knowledge in a Neural Network", "year": 2015}, {"arxiv_id": "2308.07633", "title": "A Survey on Model Compression for Large Language Models", "year": 2023}], "gt_label": [1, 1], "date": "2023-08", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_831", "valid": true}
{"query": "Which studies proposed activation functions that encourage unit-variance activations and gradients?", "cited_paper": [{"arxiv_id": "1706.02515", "title": "Self-Normalizing Neural Networks", "year": 2017}], "gt_label": [1], "date": "2017-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_832", "valid": true}
{"query": "What are the works or models related to synthetic face generation using 2D-based models?", "cited_paper": [{"arxiv_id": "1812.04948", "title": "A Style-Based Generator Architecture for Generative Adversarial Networks", "year": 2018}, {"arxiv_id": "2006.06676", "title": "Training Generative Adversarial Networks with Limited Data", "year": 2020}, {"arxiv_id": "2106.12423", "title": "Alias-Free Generative Adversarial Networks", "year": 2021}], "gt_label": [1, 1, 1], "date": "2021-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_833", "valid": true}
{"query": "Any works that use Vision Transformers (ViT) in place of the traditional convolutional backbone?", "cited_paper": [{"arxiv_id": "2012.15840", "title": "Rethinking Semantic Segmentation from a Sequence-to-Sequence Perspective with Transformers", "year": 2020}, {"arxiv_id": "2105.05633", "title": "Segmenter: Transformer for Semantic Segmentation", "year": 2021}], "gt_label": [1, 1], "date": "2021-05", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_834", "valid": true}
{"query": "What research in Natural Language Processing focuses on tasks like common-sense reasoning and sentiment analysis for Uncertainty Estimation?", "cited_paper": [{"arxiv_id": "2210.04714", "title": "Uncertainty Quantification with Pre-trained Language Models: A Large-Scale Empirical Analysis", "year": 2022}], "gt_label": [1], "date": "2022-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_835", "valid": true}
{"query": "Which papers presented the concept of Counterfactual examples (CFEs) as minimal modifications required to elicit a different outcome?", "cited_paper": [{"arxiv_id": "1912.04930", "title": "The Hidden Assumptions Behind Counterfactual Explanations and Principal Reasons", "year": 2019}], "gt_label": [1], "date": "2019-12", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_836", "valid": true}
{"query": "What is the reference for studies that highlighted the overdependence of existing multimodal pretraining methods on well-aligned multimodal sample pairs/tuples?", "cited_paper": [{"arxiv_id": "2206.06488", "title": "Multimodal Learning with Transformers: A Survey", "year": 2022}], "gt_label": [1], "date": "2022-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_837", "valid": true}
{"query": "Which works try to determine the explicit adversarial distributions for adversarial attack?", "cited_paper": [{"arxiv_id": "1706.06083", "title": "Towards Deep Learning Models Resistant to Adversarial Attacks", "year": 2017}, {"arxiv_id": "2002.05999", "title": "Adversarial Distributional Training for Robust Deep Learning", "year": 2020}, {"arxiv_id": "1905.00441", "title": "NATTACK: Learning the Distributions of Adversarial Examples for an Improved Black-Box Attack on Deep Neural Networks", "year": 2019}], "gt_label": [1, 1, 1], "date": "2020-02", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_838", "valid": true}
{"query": "Could you give me examples of research that have investigated the memorization behavior of pre-trained LMs?", "cited_paper": [{"arxiv_id": "2012.07805", "title": "Extracting Training Data from Large Language Models", "year": 2020}, {"arxiv_id": "2202.06539", "title": "Deduplicating Training Data Mitigates Privacy Risks in Language Models", "year": 2022}, {"arxiv_id": "2107.06499", "title": "Deduplicating Training Data Makes Language Models Better", "year": 2021}, {"arxiv_id": "2202.07646", "title": "Quantifying Memorization Across Neural Language Models", "year": 2022}], "gt_label": [1, 1, 1, 1], "date": "2022-02", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_839", "valid": true}
{"query": "Could you provide me some works focused on simplifying models by mapping between SEM of different levels of abstractions?", "cited_paper": [{"arxiv_id": "1812.03789", "title": "Abstracting Causal Models", "year": 2018}], "gt_label": [1], "date": "2018-12", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_840", "valid": true}
{"query": "What were the initial works that discussed Generative Adversarial Networks (GANs)?", "cited_paper": [{"arxiv_id": "2203.00667", "title": "Generative Adversarial Networks", "year": 2022}], "gt_label": [1], "date": "2022-03", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_841", "valid": true}
{"query": "What research papers have been reporting on demographically imbalanced datasets, particularly lacking representation of females and individuals with darker skin tones?", "cited_paper": [{"arxiv_id": "1912.07726", "title": "Towards Fairer Datasets: Filtering and Balancing the Distribution of the People Subtree in the ImageNet Hierarchy", "year": 2019}, {"arxiv_id": "2106.08503", "title": "Understanding and Evaluating Racial Biases in Image Captioning", "year": 2021}, {"arxiv_id": "1905.01347", "title": "Auditing ImageNet: Towards a Model-driven Framework for Annotating Demographic Attributes of Large-Scale Image Datasets", "year": 2019}, {"arxiv_id": "2012.05345", "title": "Data and its (dis)contents: A survey of dataset development and use in machine learning research", "year": 2020}], "gt_label": [1, 1, 1, 1], "date": "2021-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_842", "valid": true}
{"query": "Which studies talked about distilling reasoning capabilities from a stronger teacher to a weaker student in student-teacher frameworks?", "cited_paper": [{"arxiv_id": "2212.08410", "title": "Teaching Small Language Models to Reason", "year": 2022}, {"arxiv_id": "2301.12726", "title": "Specializing Smaller Language Models towards Multi-Step Reasoning", "year": 2023}, {"arxiv_id": "2212.10071", "title": "Large Language Models Are Reasoning Teachers", "year": 2022}], "gt_label": [1, 1, 1], "date": "2023-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_843", "valid": true}
{"query": "Can you cite a study that applied QAT-based KD for LLMs and considered the risk of overfitting?", "cited_paper": [{"arxiv_id": "2308.06744", "title": "Token-Scaled Logit Distillation for Ternary Weight Generative Language Models", "year": 2023}], "gt_label": [1], "date": "2023-08", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_844", "valid": true}
{"query": "What works leverage consistency-based or verification-based approach to improve the reasoning capacity of LLMs?", "cited_paper": [{"arxiv_id": "2203.11171", "title": "Self-Consistency Improves Chain of Thought Reasoning in Language Models", "year": 2022}, {"arxiv_id": "2311.07099", "title": "Explanation-aware Soft Ensemble Empowers Large Language Model In-context Learning", "year": 2023}, {"arxiv_id": "2306.03872", "title": "Deductive Verification of Chain-of-Thought Reasoning", "year": 2023}, {"arxiv_id": "2303.17651", "title": "Self-Refine: Iterative Refinement with Self-Feedback", "year": 2023}], "gt_label": [1, 1, 1, 1], "date": "2023-11", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_845", "valid": true}
{"query": "Which research introduced MMDBs, which allow seamless querying of text and tables using SQL?", "cited_paper": [{"arxiv_id": "2304.13559", "title": "Towards Multi-Modal DBMSs for Seamless Querying of Texts and Tables", "year": 2023}], "gt_label": [1], "date": "2023-04", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_846", "valid": true}
{"query": "What studies propose methods to achieve face capture in the multi-view setup using novel neural BRDF?", "cited_paper": [{"arxiv_id": "2303.14092", "title": "NeuFace: Realistic 3D Neural Face Rendering from Multi-view Images", "year": 2023}], "gt_label": [1], "date": "2023-03", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_847", "valid": true}
{"query": "Which studies focus on the evaluation of the reasoning ability of large language models (LLMs)?", "cited_paper": [{"arxiv_id": "2110.14168", "title": "Training Verifiers to Solve Math Word Problems", "year": 2021}, {"arxiv_id": "2009.03300", "title": "Measuring Massive Multitask Language Understanding", "year": 2020}, {"arxiv_id": "2103.03874", "title": "Measuring Mathematical Problem Solving With the MATH Dataset", "year": 2021}, {"arxiv_id": "2305.14010", "title": "IfQA: A Dataset for Open-domain Question Answering under Counterfactual Presuppositions", "year": 2023}, {"arxiv_id": "2307.02477", "title": "Reasoning or Reciting? Exploring the Capabilities and Limitations of Language Models Through Counterfactual Tasks", "year": 2023}, {"arxiv_id": "1712.00377", "title": "Don't Just Assume; Look and Answer: Overcoming Priors for Visual Question Answering", "year": 2017}], "gt_label": [1, 1, 1, 1, 1, 1], "date": "2023-07", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_848", "valid": true}
{"query": "What are the papers on the proposal-based methods used for 3D instance segmentation?", "cited_paper": [{"arxiv_id": "1812.03320", "title": "GSPN: Generative Shape Proposal Network for 3D Instance Segmentation in Point Cloud", "year": 2018}, {"arxiv_id": "1906.01140", "title": "Learning Object Bounding Boxes for 3D Instance Segmentation on Point Clouds", "year": 2019}, {"arxiv_id": "2003.13867", "title": "3D-MPA: Multi Proposal Aggregation for 3D Semantic Instance Segmentation", "year": 2020}, {"arxiv_id": "2007.09860", "title": "Learning Gaussian Instance Segmentation in Point Clouds", "year": 2020}], "gt_label": [1, 1, 1, 1], "date": "2020-07", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_849", "valid": true}
{"query": "Which papers have demonstrated the disentanglement achieved by pre-trained GANs?", "cited_paper": [{"arxiv_id": "1809.11096", "title": "Large Scale GAN Training for High Fidelity Natural Image Synthesis", "year": 2018}, {"arxiv_id": "2106.12423", "title": "Alias-Free Generative Adversarial Networks", "year": 2021}, {"arxiv_id": "1812.04948", "title": "A Style-Based Generator Architecture for Generative Adversarial Networks", "year": 2018}, {"arxiv_id": "1912.04958", "title": "Analyzing and Improving the Image Quality of StyleGAN", "year": 2019}], "gt_label": [1, 1, 1, 1], "date": "2021-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_850", "valid": true}
{"query": "What studies proposed to adopt convolutional layers with masked kernels to accelerate the computation of Jacobian determinants?", "cited_paper": [{"arxiv_id": "1901.11137", "title": "Emerging Convolutions for Generative Normalizing Flows", "year": 2019}, {"arxiv_id": "1902.04208", "title": "MaCow: Masked Convolutional Generative Flow", "year": 2019}], "gt_label": [1, 1], "date": "2019-02", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_851", "valid": true}
{"query": "Can you point out the study wherein a generative data augmentation method was devised to simultaneously train GANs and classifiers?", "cited_paper": [{"arxiv_id": "1710.10564", "title": "A Bayesian Data Augmentation Approach for Learning Deep Models", "year": 2017}], "gt_label": [1], "date": "2017-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_852", "valid": true}
{"query": "Which works advocate CNNs as multi-scale architectures?", "cited_paper": [{"arxiv_id": "2006.08656", "title": "Multiscale Deep Equilibrium Models", "year": 2020}], "gt_label": [1], "date": "2020-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_853", "valid": true}
{"query": "What studies focused on imitation learning?", "cited_paper": [{"arxiv_id": "1606.03476", "title": "Generative Adversarial Imitation Learning", "year": 2016}, {"arxiv_id": "1911.02256", "title": "A Divergence Minimization Perspective on Imitation Learning Methods", "year": 2019}, {"arxiv_id": "1905.12888", "title": "Imitation Learning as $f$-Divergence Minimization", "year": 2019}], "gt_label": [1, 1, 1], "date": "2019-11", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_854", "valid": true}
{"query": "Which studies proposed novel-view synthesis approaches aggregating image features from aligned pixels without 3D representations?", "cited_paper": [{"arxiv_id": "2207.10662", "title": "Generalizable Patch-Based Neural Rendering", "year": 2022}, {"arxiv_id": "2207.13298", "title": "Is Attention All That NeRF Needs?", "year": 2022}], "gt_label": [1, 1], "date": "2022-07", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_855", "valid": true}
{"query": "What work proposed utilizing a separate model sequentially for each channel via UAE in reconstruction-based techniques?", "cited_paper": [{"arxiv_id": "2109.11428", "title": "An Evaluation of Anomaly Detection and Diagnosis in Multivariate Time Series", "year": 2021}], "gt_label": [1], "date": "2021-09", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_856", "valid": true}
{"query": "Which research study introduced neural ODEs in CT model estimation?", "cited_paper": [{"arxiv_id": "1806.07366", "title": "Neural Ordinary Differential Equations", "year": 2018}], "gt_label": [1], "date": "2018-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_857", "valid": true}
{"query": "What research has been done that integrates the MVS approach and two-view stereo matching for geometry estimation?", "cited_paper": [{"arxiv_id": "2111.13539", "title": "GeoNeRF: Generalizing NeRF with Geometry Priors", "year": 2021}], "gt_label": [1], "date": "2021-11", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_858", "valid": true}
{"query": "What are the commonly used graph augmentation methods based on random modification of graph structures or features?", "cited_paper": [{"arxiv_id": "1706.02216", "title": "Inductive Representation Learning on Large Graphs", "year": 2017}, {"arxiv_id": "2009.10564", "title": "GraphCrop: Subgraph Cropping for Graph Classification", "year": 2020}, {"arxiv_id": "2010.13902", "title": "Graph Contrastive Learning with Augmentations", "year": 2020}, {"arxiv_id": "1907.10903", "title": "DropEdge: Towards Deep Graph Convolutional Networks on Node Classification", "year": 2019}, {"arxiv_id": "2109.01116", "title": "An Empirical Study of Graph Contrastive Learning", "year": 2021}], "gt_label": [1, 1, 1, 1, 1], "date": "2021-09", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_859", "valid": true}
{"query": "Which works proposed faster sampling strategies for diffusion models?", "cited_paper": [{"arxiv_id": "2010.02502", "title": "Denoising Diffusion Implicit Models", "year": 2020}, {"arxiv_id": "2106.00132", "title": "On Fast Sampling of Diffusion Probabilistic Models", "year": 2021}], "gt_label": [1, 1], "date": "2021-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_860", "valid": true}
{"query": "Which works used autoencoder or density-based models in AD?", "cited_paper": [{"arxiv_id": "1806.04972", "title": "Unsupervised Detection of Lesions in Brain MRI using constrained adversarial auto-encoders", "year": 2018}, {"arxiv_id": "1703.05921", "title": "Unsupervised Anomaly Detection with Generative Adversarial Networks to Guide Marker Discovery", "year": 2017}], "gt_label": [1, 1], "date": "2018-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_861", "valid": true}
{"query": "Which studies have directly used LLMs such as GPT-3 for computer vision tasks?", "cited_paper": [{"arxiv_id": "2211.11559", "title": "Visual Programming: Compositional visual reasoning without training", "year": 2022}, {"arxiv_id": "2303.08128", "title": "ViperGPT: Visual Inference via Python Execution for Reasoning", "year": 2023}], "gt_label": [1, 1], "date": "2023-03", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_862", "valid": true}
{"query": "Can you point to the study that used TTOpt for optimizing the weights of neural network in reinforcement learning problems?", "cited_paper": [{"arxiv_id": "2205.00293", "title": "TTOpt: A Maximum Volume Quantized Tensor Train-based Optimization and its Application to Reinforcement Learning", "year": 2022}], "gt_label": [1], "date": "2022-05", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_863", "valid": true}
{"query": "Which works used variance reduction methods such as SARAH, SPIDER and STORM to improve computational complexity?", "cited_paper": [{"arxiv_id": "1703.00102", "title": "SARAH: A Novel Method for Machine Learning Problems Using Stochastic Recursive Gradient", "year": 2017}, {"arxiv_id": "1807.01695", "title": "SPIDER: Near-Optimal Non-Convex Optimization via Stochastic Path Integrated Differential Estimator", "year": 2018}, {"arxiv_id": "1905.10018", "title": "Momentum-Based Variance Reduction in Non-Convex SGD", "year": 2019}, {"arxiv_id": "2008.10847", "title": "Solving Stochastic Compositional Optimization is Nearly as Easy as Solving Stochastic Optimization", "year": 2020}, {"arxiv_id": "2006.10138", "title": "An Online Method for A Class of Distributionally Robust Optimization with Non-Convex Objectives", "year": 2020}], "gt_label": [1, 1, 1, 1, 1], "date": "2020-08", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_864", "valid": true}
{"query": "Which studies have explored in-context learning where an LLM improves at a given task after being provided with task-relevant demonstrations?", "cited_paper": [{"arxiv_id": "2005.14165", "title": "Language Models are Few-Shot Learners", "year": 2020}], "gt_label": [1], "date": "2020-05", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_865", "valid": true}
{"query": "Which works discussed the use of image augmentations to improve robustness of representations in RL?", "cited_paper": [{"arxiv_id": "2004.14990", "title": "Reinforcement Learning with Augmented Data", "year": 2020}, {"arxiv_id": "2004.13649", "title": "Image Augmentation Is All You Need: Regularizing Deep Reinforcement Learning from Pixels", "year": 2020}, {"arxiv_id": "2011.13389", "title": "Generalization in Reinforcement Learning by Soft Data Augmentation", "year": 2020}], "gt_label": [1, 1, 1], "date": "2021-07", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_866", "valid": true}
{"query": "What papers offered proof for the lower bounds of SGD-RR?", "cited_paper": [{"arxiv_id": "1908.00045", "title": "How Good is SGD with Random Shuffling?", "year": 2019}, {"arxiv_id": "2106.06880", "title": "Random Shuffling Beats SGD Only After Many Epochs on Ill-Conditioned Problems", "year": 2021}, {"arxiv_id": "2002.10400", "title": "Closing the convergence gap of SGD without replacement", "year": 2020}, {"arxiv_id": "2303.07160", "title": "Tighter Lower Bounds for Shuffling SGD: Random Permutations and Beyond", "year": 2023}], "gt_label": [1, 1, 1, 1], "date": "2023-03", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_867", "valid": true}
{"query": "What works focus on the theoretical aspects of provable P-MTRL?", "cited_paper": [{"arxiv_id": "2006.11650", "title": "On the Theory of Transfer Learning: The Importance of Task Diversity", "year": 2020}, {"arxiv_id": "2002.11684", "title": "Provable Meta-Learning of Linear Representations", "year": 2020}, {"arxiv_id": "2002.09434", "title": "Few-Shot Learning via Learning the Representation, Provably", "year": 2020}, {"arxiv_id": "2105.08306", "title": "Sample Efficient Linear Meta-Learning by Alternating Minimization", "year": 2021}, {"arxiv_id": "2102.07078", "title": "Exploiting Shared Representations for Personalized Federated Learning", "year": 2021}, {"arxiv_id": "2105.14989", "title": "Representation Learning Beyond Linear Prediction Functions", "year": 2021}], "gt_label": [1, 1, 1, 1, 1, 1], "date": "2021-05", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_868", "valid": true}
{"query": "What are the studies that employed alignment training strategy and directly optimized LLMs to produce factual statements?", "cited_paper": [{"arxiv_id": "2212.10560", "title": "Self-Instruct: Aligning Language Models with Self-Generated Instructions", "year": 2022}, {"arxiv_id": "2203.02155", "title": "Training language models to follow instructions with human feedback", "year": 2022}], "gt_label": [1, 1], "date": "2022-12", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_869", "valid": true}
{"query": "Could you provide me some examples of works constructing an atlas of maps from 2 to n?", "cited_paper": [{"arxiv_id": "2103.16942", "title": "Neural Surface Maps", "year": 2021}, {"arxiv_id": "2205.02904", "title": "Neural Jacobian Fields: Learning Intrinsic Mappings of Arbitrary Meshes", "year": 2022}], "gt_label": [1, 1], "date": "2022-05", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_870", "valid": true}
{"query": "What are some of the works that explored the use of generative models in developing novel molecules with variational autoencoder and reinforcement learning?", "cited_paper": [{"arxiv_id": "1802.04364", "title": "Junction Tree Variational Autoencoder for Molecular Graph Generation", "year": 2018}, {"arxiv_id": "2211.16508", "title": "Reinforced Genetic Algorithm for Structure-based Drug Design", "year": 2022}, {"arxiv_id": "1806.02473", "title": "Graph Convolutional Policy Network for Goal-Directed Molecular Graph Generation", "year": 2018}, {"arxiv_id": "1704.07555", "title": "Molecular De Novo Design through Deep Reinforcement Learning", "year": 2017}], "gt_label": [1, 1, 1, 1], "date": "2022-11", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_871", "valid": true}
{"query": "Which papers does the approach Viewset Diffusion come from?", "cited_paper": [{"arxiv_id": "2306.07881", "title": "Viewset Diffusion: (0-)Image-Conditioned 3D Generative Models from 2D Data", "year": 2023}], "gt_label": [1], "date": "2023-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_872", "valid": true}
{"query": "What paper has proposed the idea of using environment interactions to learn the dynamics model?", "cited_paper": [{"arxiv_id": "1809.01999", "title": "Recurrent World Models Facilitate Policy Evolution", "year": 2018}], "gt_label": [1], "date": "2018-09", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_873", "valid": true}
{"query": "Which research proposes Differential Privacy (DP) and DPSGD algorithm for private stochastic gradient descent?", "cited_paper": [{"arxiv_id": "1607.00133", "title": "Deep Learning with Differential Privacy", "year": 2016}], "gt_label": [1], "date": "2016-07", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_874", "valid": true}
{"query": "In what studies did researchers match the lower bounds of SGD-RR with its upper bounds for both strongly convex and general convex cases?", "cited_paper": [{"arxiv_id": "2303.07160", "title": "Tighter Lower Bounds for Shuffling SGD: Random Permutations and Beyond", "year": 2023}, {"arxiv_id": "2006.05988", "title": "Random Reshuffling: Simple Analysis with Vast Improvements", "year": 2020}], "gt_label": [1, 1], "date": "2023-03", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_875", "valid": true}
{"query": "What work proposed forgetting score as a difficulty-based metric for sample importance in CoreSet selection?", "cited_paper": [{"arxiv_id": "1812.05159", "title": "An Empirical Study of Example Forgetting during Deep Neural Network Learning", "year": 2018}], "gt_label": [1], "date": "2018-12", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_876", "valid": true}
{"query": "Could you provide me some works about data-driven methods in VR HMD settings?", "cited_paper": [{"arxiv_id": "2203.05789", "title": "FLAG: Flow-based 3D Avatar Generation from Sparse Observations", "year": 2022}, {"arxiv_id": "2304.08577", "title": "Avatars Grow Legs: Generating Smooth Human Motion from Sparse Tracking Inputs with Diffusion Model", "year": 2023}, {"arxiv_id": "2308.11261", "title": "HMD-NeMo: Online 3D Avatar Motion Generation From Sparse Observations", "year": 2023}, {"arxiv_id": "2207.13784", "title": "AvatarPoser: Articulated Full-Body Pose Tracking from Sparse Motion Sensing", "year": 2022}, {"arxiv_id": "2308.08855", "title": "Realistic Full-Body Tracking from Sparse Observations via Joint-Level Modeling", "year": 2023}], "gt_label": [1, 1, 1, 1, 1], "date": "2023-08", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_877", "valid": true}
{"query": "Can you name the study that reported high success rates on few-shot learning tasks using a text-to-image generative model pre-trained on extensive external datasets?", "cited_paper": [{"arxiv_id": "2210.07574", "title": "Is synthetic data from generative models ready for image recognition?", "year": 2022}], "gt_label": [1], "date": "2022-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_878", "valid": true}
{"query": "Could you list the studies that proved the upper bounds of SGD-RR?", "cited_paper": [{"arxiv_id": "1903.01463", "title": "SGD without Replacement: Sharper Rates for General Smooth Convex Functions", "year": 2019}, {"arxiv_id": "2006.05988", "title": "Random Reshuffling: Simple Analysis with Vast Improvements", "year": 2020}], "gt_label": [1, 1], "date": "2020-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_879", "valid": true}
{"query": "Which papers discuss empirical defenses that are used in defending against adversarial attacks?", "cited_paper": [{"arxiv_id": "1904.12843", "title": "Adversarial Training for Free!", "year": 2019}, {"arxiv_id": "2001.03994", "title": "Fast is better than free: Revisiting adversarial training", "year": 2020}, {"arxiv_id": "1706.06083", "title": "Towards Deep Learning Models Resistant to Adversarial Attacks", "year": 2017}, {"arxiv_id": "1906.04691", "title": "On Single Source Robustness in Deep Fusion Models", "year": 2019}, {"arxiv_id": "2206.12714", "title": "Defending Multimodal Fusion Models against Single-Source Adversaries", "year": 2022}, {"arxiv_id": "2104.02000", "title": "Can audio-visual integration strengthen robustness under multimodal attacks?", "year": 2021}], "gt_label": [1, 1, 1, 1, 1, 1], "date": "2022-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_880", "valid": true}
{"query": "What works propose regularization of the embedding space for regression?", "cited_paper": [{"arxiv_id": "2103.13629", "title": "Learning Probabilistic Ordinal Embeddings for Uncertainty-Aware Regression", "year": 2021}, {"arxiv_id": "2301.08915", "title": "Improving Deep Regression with Ordinal Entropy", "year": 2023}, {"arxiv_id": "2102.09554", "title": "Delving into Deep Imbalanced Regression", "year": 2021}, {"arxiv_id": "2205.15236", "title": "RankSim: Ranking Similarity Regularization for Deep Imbalanced Regression", "year": 2022}], "gt_label": [1, 1, 1, 1], "date": "2023-01", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_881", "valid": true}
{"query": "Could you provide me some works about sequence-based generative models for protein sequence design?", "cited_paper": [{"arxiv_id": "2205.13760", "title": "Tranception: protein fitness prediction with autoregressive transformers and inference-time retrieval", "year": 2022}], "gt_label": [1], "date": "2022-05", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_882", "valid": true}
{"query": "Could you list some research works that studied Unsupervised Domain Adaptation (UDA) in the context of mitigating domain shift for semantic segmentation?", "cited_paper": [{"arxiv_id": "1910.13049", "title": "Category Anchor-Guided Unsupervised Domain Adaptation for Semantic Segmentation", "year": 2019}, {"arxiv_id": "1909.13589", "title": "Domain Adaptation for Semantic Segmentation with Maximum Squares Loss", "year": 2019}, {"arxiv_id": "2111.14887", "title": "DAFormer: Improving Network Architectures and Training Strategies for Domain-Adaptive Semantic Segmentation", "year": 2021}, {"arxiv_id": "1904.01886", "title": "DADA: Depth-aware Domain Adaptation in Semantic Segmentation", "year": 2019}, {"arxiv_id": "2009.12518", "title": "Unsupervised Model Adaptation for Continual Semantic Segmentation", "year": 2020}, {"arxiv_id": "1811.12833", "title": "ADVENT: Adversarial Entropy Minimization for Domain Adaptation in Semantic Segmentation", "year": 2018}, {"arxiv_id": "2004.07703", "title": "Unsupervised Intra-domain Adaptation for Semantic Segmentation through Self-Supervision", "year": 2020}, {"arxiv_id": "2105.00097", "title": "Self-supervised Augmentation Consistency for Adapting Semantic Segmentation", "year": 2021}, {"arxiv_id": "2111.11629", "title": "Uncertainty-Aware Deep Co-training for Semi-supervised Medical Image Segmentation", "year": 2021}, {"arxiv_id": "2307.12574", "title": "A Good Student is Cooperative and Reliable: CNN-Transformer Collaborative Learning for Semantic Segmentation", "year": 2023}, {"arxiv_id": "2209.02178", "title": "Transformer-CNN Cohort: Semi-supervised Semantic Segmentation by the Best of Both Students", "year": 2022}, {"arxiv_id": "2310.07265", "title": "Distilling Efficient Vision Transformers from CNNs for Semantic Segmentation", "year": 2023}, {"arxiv_id": "2310.02296", "title": "CLIP Is Also a Good Teacher: A New Learning Framework for Inductive Zero-shot Semantic Segmentation", "year": 2023}], "gt_label": [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], "date": "2023-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_883", "valid": true}
{"query": "In which references was ARA formulated as a ranking problem instead of a classification task?", "cited_paper": [{"arxiv_id": "2203.07450", "title": "A Neural Pairwise Ranking Model for Readability Assessment", "year": 2022}], "gt_label": [1], "date": "2022-03", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_884", "valid": true}
{"query": "Could you provide me some studies about unsupervised concept learning methods in concept-based models?", "cited_paper": [{"arxiv_id": "2108.11761", "title": "A Framework for Learning Ante-hoc Explainable Models via Concepts", "year": 2021}], "gt_label": [1], "date": "2021-08", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_885", "valid": true}
{"query": "What studies evaluated the reasoning ability of LLMs from several perspectives, including mathematical reasoning, common-sense reasoning, logical reasoning, and domain-specific reasoning?", "cited_paper": [{"arxiv_id": "2307.03109", "title": "A Survey on Evaluation of Large Language Models", "year": 2023}, {"arxiv_id": "2304.06364", "title": "AGIEval: A Human-Centric Benchmark for Evaluating Foundation Models", "year": 2023}, {"arxiv_id": "2302.04023", "title": "A Multitask, Multilingual, Multimodal Evaluation of ChatGPT on Reasoning, Hallucination, and Interactivity", "year": 2023}, {"arxiv_id": "2310.09107", "title": "GLoRE: Evaluating Logical Reasoning of Large Language Models", "year": 2023}], "gt_label": [1, 1, 1, 1], "date": "2023-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_886", "valid": true}
{"query": "Which papers explore data augmentation techniques to improve task performance in low-resource languages?", "cited_paper": [{"arxiv_id": "2112.02721", "title": "NL-Augmenter: A Framework for Task-Sensitive Natural Language Augmentation", "year": 2021}], "gt_label": [1], "date": "2021-12", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_887", "valid": true}
{"query": "What works exist on using large-scale datasets for vision-language pre-training?", "cited_paper": [{"arxiv_id": "2102.08981", "title": "Conceptual 12M: Pushing Web-Scale Image-Text Pre-Training To Recognize Long-Tail Visual Concepts", "year": 2021}, {"arxiv_id": "2111.12233", "title": "Scaling Up Vision-Language Pre-training for Image Captioning", "year": 2021}, {"arxiv_id": "2205.14100", "title": "GIT: A Generative Image-to-text Transformer for Vision and Language", "year": 2022}, {"arxiv_id": "2102.05918", "title": "Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision", "year": 2021}], "gt_label": [1, 1, 1, 1], "date": "2022-05", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_888", "valid": true}
{"query": "Could you provide examples of participatory data creation initiatives focused specifically on NLP?", "cited_paper": [], "gt_label": [], "date": "2007-04", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_889", "valid": true}
{"query": "Could you provide me some works that attempted to address the problem of experimental design for causal discovery for continuous variables?", "cited_paper": [{"arxiv_id": "1910.03962", "title": "Optimal experimental design via Bayesian optimization: active causal structure learning for Gaussian process networks", "year": 2019}, {"arxiv_id": "2206.02063", "title": "Active Bayesian Causal Inference", "year": 2022}, {"arxiv_id": "1703.02645", "title": "Cost-Optimal Learning of Causal Graphs", "year": 2017}, {"arxiv_id": "1810.11867", "title": "Experimental Design for Cost-Aware Learning of Causal Graphs", "year": 2018}, {"arxiv_id": "2205.10083", "title": "A Unified Experiment Design Approach for Cyclic and Acyclic Causal Models", "year": 2022}, {"arxiv_id": "1709.03625", "title": "Budgeted Experiment Design for Causal Structure Learning", "year": 2017}, {"arxiv_id": "2211.13715", "title": "Trust Your $$: Gradient-based Intervention Targeting for Causal Discovery", "year": 2022}, {"arxiv_id": "2109.02429", "title": "Learning Neural Causal Models with Active Interventions", "year": 2021}], "gt_label": [1, 1, 1, 1, 1, 1, 1, 1], "date": "2022-11", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_890", "valid": true}
{"query": "What works cover the topic of in-situ adjustment for accuracy-robustness in AI platforms?", "cited_paper": [{"arxiv_id": "2110.10232", "title": "Test time Adaptation through Perturbation Robustness", "year": 2021}, {"arxiv_id": "2202.13711", "title": "Evaluating the Adversarial Robustness of Adaptive Test-time Defenses", "year": 2022}, {"arxiv_id": "1712.00673", "title": "Towards Robust Neural Networks via Random Self-ensemble", "year": 2017}, {"arxiv_id": "2010.11828", "title": "Once-for-All Adversarial Training: In-Situ Tradeoff between Robustness and Accuracy for Free", "year": 2020}], "gt_label": [1, 1, 1, 1], "date": "2022-02", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_891", "valid": true}
{"query": "What research showcases the potential of pre-trained diffusion models in generating diverse multi-view images?", "cited_paper": [{"arxiv_id": "2303.11328", "title": "Zero-1-to-3: Zero-shot One Image to 3D Object", "year": 2023}, {"arxiv_id": "2306.07881", "title": "Viewset Diffusion: (0-)Image-Conditioned 3D Generative Models from 2D Data", "year": 2023}, {"arxiv_id": "2309.03453", "title": "SyncDreamer: Generating Multiview-consistent Images from a Single-view Image", "year": 2023}, {"arxiv_id": "2310.03020", "title": "Consistent-1-to-3: Consistent Image to 3D View Synthesis via Geometry-aware Diffusion Models", "year": 2023}], "gt_label": [1, 1, 1, 1], "date": "2023-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_892", "valid": true}
{"query": "What studies highlight a connection between BERT model activations and 3D color space?", "cited_paper": [{"arxiv_id": "2109.06129", "title": "Can Language Models Encode Perceptual Structure Without Grounding? A Case Study in Color", "year": 2021}], "gt_label": [1], "date": "2021-09", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_893", "valid": true}
{"query": "Which study showed a relationship between the diversity and fidelity of synthetic samples and test accuracies when applied to generative data augmentation?", "cited_paper": [{"arxiv_id": "1912.11597", "title": "Effective Data Augmentation with Multi-Domain Learning GANs", "year": 2019}], "gt_label": [1], "date": "2019-12", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_894", "valid": true}
{"query": "In which study did the author apply TTOpt to the QUBO problem?", "cited_paper": [{"arxiv_id": "2205.04490", "title": "Are Quantum Computers Practical Yet? A Case for Feature Selection in Recommender Systems using Tensor Networks", "year": 2022}], "gt_label": [1], "date": "2022-05", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_895", "valid": true}
{"query": "Which paper popularized sinusoidal networks for implicit modelling tasks?", "cited_paper": [], "gt_label": [], "date": "2007-04", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_896", "valid": true}
{"query": "Could you give me an example of a study that adapts a voxelized structure to protein targets?", "cited_paper": [], "gt_label": [], "date": "2021-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_897", "valid": true}
{"query": "Could you provide me some works regarding GNNs used in accurately reproducing molecular force fields?", "cited_paper": [], "gt_label": [], "date": "2019-02", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_898", "valid": true}
{"query": "Which papers discuss the integration of image-based vision models with Large Language Models for multimodal understanding?", "cited_paper": [{"arxiv_id": "2204.14198", "title": "Flamingo: a Visual Language Model for Few-Shot Learning", "year": 2022}, {"arxiv_id": "2301.12597", "title": "BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models", "year": 2023}, {"arxiv_id": "2304.10592", "title": "MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models", "year": 2023}, {"arxiv_id": "2304.08485", "title": "Visual Instruction Tuning", "year": 2023}, {"arxiv_id": "2305.06500", "title": "InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning", "year": 2023}, {"arxiv_id": "2305.06355", "title": "VideoChat: Chat-Centric Video Understanding", "year": 2023}], "gt_label": [1, 1, 1, 1, 1, 1], "date": "2023-05", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_899", "valid": true}
{"query": "Which works have explored the application of user instructions to determine sentence relationships and fine-tune clusters?", "cited_paper": [{"arxiv_id": "2305.14871", "title": "ClusterLLM: Large Language Models as a Guide for Text Clustering", "year": 2023}], "gt_label": [1], "date": "2023-05", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_900", "valid": true}
{"query": "What research papers proposed the sampling-free uncertainty estimation methods?", "cited_paper": [{"arxiv_id": "2010.10474", "title": "Towards Maximizing the Representation Gap between In-Domain & Out-of-Distribution Examples", "year": 2020}, {"arxiv_id": "2006.04183", "title": "Uncertainty-Aware Deep Classifiers using Generative Models", "year": 2020}, {"arxiv_id": "2110.14012", "title": "Graph Posterior Network: Bayesian Predictive Uncertainty for Node Classification", "year": 2021}, {"arxiv_id": "2006.10108", "title": "Simple and Principled Uncertainty Estimation with Deterministic Deep Learning via Distance Awareness", "year": 2020}, {"arxiv_id": "2102.11409", "title": "On Feature Collapse and Deep Kernel Learning for Single Forward Pass Uncertainty", "year": 2021}, {"arxiv_id": "2003.02037", "title": "Uncertainty Estimation Using a Single Deep Deterministic Neural Network", "year": 2020}, {"arxiv_id": "1911.05503", "title": "Uncertainty on Asynchronous Time Event Prediction", "year": 2019}, {"arxiv_id": "1807.00263", "title": "Accurate Uncertainties for Deep Learning Using Calibrated Regression", "year": 2018}, {"arxiv_id": "2007.01458", "title": "Confidence-Aware Learning for Deep Neural Networks", "year": 2020}, {"arxiv_id": "2006.10288", "title": "Individual Calibration with Randomized Forecasting", "year": 2020}, {"arxiv_id": "1905.06023", "title": "Distribution Calibration for Regression", "year": 2019}, {"arxiv_id": "2003.06820", "title": "Intra Order-preserving Functions for Calibration of Multi-Class Neural Networks", "year": 2020}, {"arxiv_id": "1611.00448", "title": "Natural-Parameter Networks: A Class of Probabilistic Neural Networks", "year": 2016}, {"arxiv_id": "1908.00598", "title": "Sampling-free Epistemic Uncertainty Estimation Using Approximated Variance Propagation", "year": 2019}, {"arxiv_id": "2012.03085", "title": "Graph Mixture Density Networks", "year": 2020}], "gt_label": [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], "date": "2021-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_901", "valid": true}
{"query": "Could you provide me with some studies that employed distributed methods to improve scalability of large-scale Graph Neural Networks?", "cited_paper": [{"arxiv_id": "1905.07953", "title": "Cluster-GCN: An Efficient Algorithm for Training Deep and Large Graph Convolutional Networks", "year": 2019}, {"arxiv_id": "2211.00216", "title": "Distributed Graph Neural Network Training: A Survey", "year": 2022}], "gt_label": [1, 1], "date": "2022-11", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_902", "valid": true}
{"query": "What papers discus coupling of large language models with image captioning models to enhance image descriptions?", "cited_paper": [{"arxiv_id": "2303.06594", "title": "ChatGPT Asks, BLIP-2 Answers: Automatic Questioning Towards Enriched Visual Descriptions", "year": 2023}], "gt_label": [1], "date": "2023-03", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_903", "valid": true}
{"query": "Could you provide examples of studies where the feed-forward designs are used by learning reliable representations from data?", "cited_paper": [{"arxiv_id": "2103.15595", "title": "MVSNeRF: Fast Generalizable Radiance Field Reconstruction from Multi-View Stereo", "year": 2021}, {"arxiv_id": "2111.13539", "title": "GeoNeRF: Generalizing NeRF with Geometry Priors", "year": 2021}, {"arxiv_id": "2107.13421", "title": "Neural Rays for Occlusion-aware Image-based Rendering", "year": 2021}], "gt_label": [1, 1, 1], "date": "2021-11", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_904", "valid": true}
{"query": "Which papers focus on improving the computational efficiency of NeRF by using a hybrid representation?", "cited_paper": [{"arxiv_id": "2112.07945", "title": "Efficient Geometry-aware 3D Generative Adversarial Networks", "year": 2021}, {"arxiv_id": "2203.09517", "title": "TensoRF: Tensorial Radiance Fields", "year": 2022}, {"arxiv_id": "2201.05989", "title": "Instant Neural Graphics Primitives with a Multiresolution Hash Encoding", "year": 2022}, {"arxiv_id": "2301.10241", "title": "K-Planes: Explicit Radiance Fields in Space, Time, and Appearance", "year": 2023}, {"arxiv_id": "2301.09632", "title": "HexPlane: A Fast Representation for Dynamic Scenes", "year": 2023}], "gt_label": [1, 1, 1, 1, 1], "date": "2023-01", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_905", "valid": true}
{"query": "Which research proposed an information flow type system that could be used for automatic marginalization of discrete random variables?", "cited_paper": [{"arxiv_id": "2010.11887", "title": "Conditional independence by typing", "year": 2020}], "gt_label": [1], "date": "2020-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_906", "valid": true}
{"query": "Could you provide me some works that improved neural scene representations methods by optimizing implementations to take maximum advantage of acceleration hardware with efficient compressed field representations?", "cited_paper": [{"arxiv_id": "2201.05989", "title": "Instant Neural Graphics Primitives with a Multiresolution Hash Encoding", "year": 2022}, {"arxiv_id": "2208.00277", "title": "MobileNeRF: Exploiting the Polygon Rasterization Pipeline for Efficient Neural Field Rendering on Mobile Architectures", "year": 2022}, {"arxiv_id": "2203.09517", "title": "TensoRF: Tensorial Radiance Fields", "year": 2022}, {"arxiv_id": "2112.07945", "title": "Efficient Geometry-aware 3D Generative Adversarial Networks", "year": 2021}, {"arxiv_id": "2301.09632", "title": "HexPlane: A Fast Representation for Dynamic Scenes", "year": 2023}, {"arxiv_id": "2301.10241", "title": "K-Planes: Explicit Radiance Fields in Space, Time, and Appearance", "year": 2023}], "gt_label": [1, 1, 1, 1, 1, 1], "date": "2023-01", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_907", "valid": true}
{"query": "Which work is most closely related to the current research and provides an alternative proof of the equivalence between TV indistinguishability, replicability, and differential privacy?", "cited_paper": [{"arxiv_id": "2303.12921", "title": "Stability is Stable: Connections between Replicability, Privacy, and Adaptive Generalization", "year": 2023}], "gt_label": [1], "date": "2023-03", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_908", "valid": true}
{"query": "What projects used this strategy to the high-order method?", "cited_paper": [], "gt_label": [], "date": "2022-02", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_909", "valid": true}
{"query": "Could you provide me some studies that apply slot attention to visual question answering?", "cited_paper": [{"arxiv_id": "2012.15814", "title": "Language-Mediated, Object-Centric Representation Learning", "year": 2020}], "gt_label": [1], "date": "2020-12", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_910", "valid": true}
{"query": "What works explored the memory footprint and access pattern of voxel representations compared to point cloud representations?", "cited_paper": [{"arxiv_id": "1907.03739", "title": "Point-Voxel CNN for Efficient 3D Deep Learning", "year": 2019}], "gt_label": [1], "date": "2019-07", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_911", "valid": true}
{"query": "Which work devises a pipeline for generating synthetic data for evaluating the compositional reasoning ability of VQA models?", "cited_paper": [{"arxiv_id": "1612.06890", "title": "CLEVR: A Diagnostic Dataset for Compositional Language and Elementary Visual Reasoning", "year": 2016}], "gt_label": [1], "date": "2016-12", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_912", "valid": true}
{"query": "Where can I find details about the ChatGLM architecture?", "cited_paper": [{"arxiv_id": "2210.02414", "title": "GLM-130B: An Open Bilingual Pre-trained Model", "year": 2022}], "gt_label": [1], "date": "2022-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_913", "valid": true}
{"query": "What studies provide bounds for linear TD in both the i. i. d. data setting and a correlated data setting?", "cited_paper": [{"arxiv_id": "1806.02450", "title": "A Finite Time Analysis of Temporal Difference Learning With Linear Function Approximation", "year": 2018}], "gt_label": [1], "date": "2018-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_914", "valid": true}
{"query": "What papers focused on addressing extant limitations within LLMs evaluators such as factuality, interpretability and position bias?", "cited_paper": [{"arxiv_id": "2305.14251", "title": "FActScore: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation", "year": 2023}, {"arxiv_id": "2305.17926", "title": "Large Language Models are not Fair Evaluators", "year": 2023}], "gt_label": [1, 1], "date": "2023-05", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_915", "valid": true}
{"query": "Which works are related to motion generation based on 3D face coefficients?", "cited_paper": [{"arxiv_id": "2206.12837", "title": "Perceptual Conversational Head Generation with Regularized Driver and Enhanced Renderer", "year": 2022}, {"arxiv_id": "2204.08451", "title": "Learning to Listen: Modeling Non-Deterministic Dyadic Facial Motion", "year": 2022}, {"arxiv_id": "2308.16635", "title": "MFR-Net: Multi-faceted Responsive Listening Head Generation via Denoising Diffusion Model", "year": 2023}], "gt_label": [1, 1, 1], "date": "2023-08", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_916", "valid": true}
{"query": "What study showed RNNs to be a competitive baseline in meta-RL?", "cited_paper": [{"arxiv_id": "2110.05038", "title": "Recurrent Model-Free RL Can Be a Strong Baseline for Many POMDPs", "year": 2021}], "gt_label": [1], "date": "2021-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_917", "valid": true}
{"query": "Can you name the studies which provide solutions to the high cost associated with neural network-based wave function models?", "cited_paper": [{"arxiv_id": "2110.05064", "title": "Ab-Initio Potential Energy Surfaces by Pairing GNNs with Neural Wave Functions", "year": 2021}, {"arxiv_id": "2205.14962", "title": "Sampling-free Inference for Ab-Initio Potential Energy Surface Networks", "year": 2022}], "gt_label": [1, 1], "date": "2022-05", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_918", "valid": true}
{"query": "Which study incorporated self-attention and Gumbel subset sampling in point-based methodologies to enhance recognition tasks in Point Cloud processing?", "cited_paper": [{"arxiv_id": "1904.03375", "title": "Modeling Point Clouds with Self-Attention and Gumbel Subset Sampling", "year": 2019}], "gt_label": [1], "date": "2019-04", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_919", "valid": true}
{"query": "Which works focus on denoising diffusion probabilistic models?", "cited_paper": [{"arxiv_id": "2006.11239", "title": "Denoising Diffusion Probabilistic Models", "year": 2020}, {"arxiv_id": "2010.02502", "title": "Denoising Diffusion Implicit Models", "year": 2020}, {"arxiv_id": "2105.05233", "title": "Diffusion Models Beat GANs on Image Synthesis", "year": 2021}, {"arxiv_id": "2112.10752", "title": "High-Resolution Image Synthesis with Latent Diffusion Models", "year": 2021}], "gt_label": [1, 1, 1, 1], "date": "2021-12", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_920", "valid": true}
{"query": "Could you provide me some studies about unstructured pruning in deep learning?", "cited_paper": [{"arxiv_id": "1803.03635", "title": "The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks", "year": 2018}], "gt_label": [1], "date": "2018-03", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_921", "valid": true}
{"query": "Any works about leveraging a multi-task framework to generate NLEs and labels simultaneously?", "cited_paper": [{"arxiv_id": "2010.04119", "title": "Leakage-Adjusted Simulatability: Can Models Generate Non-Trivial Explanations of Their Behavior in Natural Language?", "year": 2020}], "gt_label": [1], "date": "2020-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_922", "valid": true}
{"query": "Are there any studies that optimize the process of dataset condensation?", "cited_paper": [{"arxiv_id": "1911.02590", "title": "Optimizing Millions of Hyperparameters by Implicit Differentiation", "year": 2019}, {"arxiv_id": "2011.00050", "title": "Dataset Meta-Learning from Kernel Ridge-Regression", "year": 2020}, {"arxiv_id": "2107.13034", "title": "Dataset Distillation with Infinitely Wide Convolutional Networks", "year": 2021}, {"arxiv_id": "2212.14032", "title": "On Implicit Bias in Overparameterized Bilevel Optimization", "year": 2022}, {"arxiv_id": "2206.00719", "title": "Dataset Distillation using Neural Feature Regression", "year": 2022}, {"arxiv_id": "2210.12067", "title": "Efficient Dataset Distillation Using Random Feature Approximation", "year": 2022}, {"arxiv_id": "2212.06152", "title": "Accelerating Dataset Distillation via Model Augmentation", "year": 2022}, {"arxiv_id": "2211.10586", "title": "Scaling Up Dataset Distillation to ImageNet-1K with Constant Memory", "year": 2022}], "gt_label": [1, 1, 1, 1, 1, 1, 1, 1], "date": "2023-02", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_923", "valid": true}
{"query": "Which works are examples of pretrained Large Language Models?", "cited_paper": [{"arxiv_id": "2005.14165", "title": "Language Models are Few-Shot Learners", "year": 2020}, {"arxiv_id": "2302.13971", "title": "LLaMA: Open and Efficient Foundation Language Models", "year": 2023}, {"arxiv_id": "2205.01068", "title": "OPT: Open Pre-trained Transformer Language Models", "year": 2022}], "gt_label": [1, 1, 1], "date": "2023-02", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_924", "valid": true}
{"query": "Which studies trained across multiple vision tasks and solved them simultaneously?", "cited_paper": [{"arxiv_id": "2108.11353", "title": "Multi-Task Self-Training for Learning General Representations", "year": 2021}, {"arxiv_id": "2111.08687", "title": "INTERN: A New Learning Paradigm Towards General Vision", "year": 2021}], "gt_label": [1, 1], "date": "2021-11", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_925", "valid": true}
{"query": "Could you mention some studies that focused on generating adversarial examples designed to induce LLMs to generate harmful or non-factual content?", "cited_paper": [{"arxiv_id": "2307.15043", "title": "Universal and Transferable Adversarial Attacks on Aligned Language Models", "year": 2023}, {"arxiv_id": "2304.08979", "title": "In ChatGPT We Trust? Measuring and Characterizing the Reliability of ChatGPT", "year": 2023}], "gt_label": [1, 1], "date": "2023-07", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_926", "valid": true}
{"query": "Which studies have been conducted on diffusion models performing conditional image generation with guiding input channels?", "cited_paper": [{"arxiv_id": "2102.12092", "title": "Zero-Shot Text-to-Image Generation", "year": 2021}, {"arxiv_id": "2104.07636", "title": "Image Super-Resolution via Iterative Refinement", "year": 2021}, {"arxiv_id": "2111.05826", "title": "Palette: Image-to-Image Diffusion Models", "year": 2021}, {"arxiv_id": "2205.12952", "title": "Pretraining is All You Need for Image-to-Image Translation", "year": 2022}, {"arxiv_id": "2111.15640", "title": "Diffusion Autoencoders: Toward a Meaningful and Decodable Representation", "year": 2021}, {"arxiv_id": "2207.12598", "title": "Classifier-Free Diffusion Guidance", "year": 2022}], "gt_label": [1, 1, 1, 1, 1, 1], "date": "2022-07", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_927", "valid": true}
{"query": "Could you provide me a work that extends the decomposition of 3D feature volumes to 4D dynamic scenes?", "cited_paper": [{"arxiv_id": "2301.09632", "title": "HexPlane: A Fast Representation for Dynamic Scenes", "year": 2023}], "gt_label": [1], "date": "2023-01", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_928", "valid": true}
{"query": "What is a study that proposes diverse ensembles as a measure of sample importance?", "cited_paper": [{"arxiv_id": "2110.05922", "title": "Trivial or impossible -- dichotomous data difficulty masks model differences (on ImageNet and beyond)", "year": 2021}], "gt_label": [1], "date": "2021-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_929", "valid": true}
{"query": "What work focused on learning a weighting over the tasks in A-MTRL?", "cited_paper": [{"arxiv_id": "2105.14095", "title": "Weighted Training for Cross-Task Learning", "year": 2021}], "gt_label": [1], "date": "2021-05", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_930", "valid": true}
{"query": "Which pieces of research output a dense representation of the objects coordinate space?", "cited_paper": [{"arxiv_id": "1901.02970", "title": "Normalized Object Coordinate Space for Category-Level 6D Object Pose and Size Estimation", "year": 2019}, {"arxiv_id": "1902.11020", "title": "DPOD: 6D Pose Object Detector and Refiner", "year": 2019}], "gt_label": [1, 1], "date": "2019-02", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_931", "valid": true}
{"query": "Which work investigated the impact of weight oscillation on quantization-aware training, and rooted it in depth-wise convolution (DW-Conv) and batch normalization (BN) layers?", "cited_paper": [{"arxiv_id": "2203.11086", "title": "Overcoming Oscillations in Quantization-Aware Training", "year": 2022}], "gt_label": [1], "date": "2022-03", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_932", "valid": true}
{"query": "Any works about TracIn being a promising approach for training data attribution?", "cited_paper": [{"arxiv_id": "2002.08484", "title": "Estimating Training Data Influence by Tracing Gradient Descent", "year": 2020}], "gt_label": [1], "date": "2020-02", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_933", "valid": true}
{"query": "Could you provide me some works that allow flexible learning and combination of attributes from different concepts?", "cited_paper": [{"arxiv_id": "2305.16225", "title": "ProSpect: Prompt Spectrum for Attribute-Aware Personalization of Diffusion Models", "year": 2023}], "gt_label": [1], "date": "2023-05", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_934", "valid": true}
{"query": "Could you provide the research that proposed an open-book QA model conditions answer generation upon newly-retrieved documents?", "cited_paper": [{"arxiv_id": "2005.11401", "title": "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks", "year": 2020}], "gt_label": [1], "date": "2020-05", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_935", "valid": true}
{"query": "Which papers discussed the use of audio as auxiliary inputs in image editing?", "cited_paper": [{"arxiv_id": "2211.12194", "title": "SadTalker: Learning Realistic 3D Motion Coefficients for Stylized Audio-Driven Single Image Talking Face Animation", "year": 2022}], "gt_label": [1], "date": "2022-11", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_936", "valid": true}
{"query": "What work presents the graphlet kernel and similar fixed pattern-based approaches which only count subgraphs up to size around 5?", "cited_paper": [{"arxiv_id": "2006.09252", "title": "Improving Graph Neural Network Expressivity via Subgraph Isomorphism Counting", "year": 2020}], "gt_label": [1], "date": "2020-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_937", "valid": true}
{"query": "What studies detailed the success of Mixture of Experts in the fields of computer vision and natural language processing?", "cited_paper": [{"arxiv_id": "1806.01531", "title": "Deep Mixture of Experts via Shallow Embedding", "year": 2018}, {"arxiv_id": "2106.05974", "title": "Scaling Vision with Sparse Mixture of Experts", "year": 2021}, {"arxiv_id": "2109.02008", "title": "Cross-token Modeling with Conditional Computation", "year": 2021}, {"arxiv_id": "1701.06538", "title": "Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer", "year": 2017}, {"arxiv_id": "2110.03742", "title": "Beyond Distillation: Task-level Mixture-of-Experts for Efficient Inference", "year": 2021}, {"arxiv_id": "2101.03961", "title": "Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity", "year": 2021}, {"arxiv_id": "2112.06905", "title": "GLaM: Efficient Scaling of Language Models with Mixture-of-Experts", "year": 2021}, {"arxiv_id": "2202.08906", "title": "ST-MoE: Designing Stable and Transferable Sparse Expert Models", "year": 2022}], "gt_label": [1, 1, 1, 1, 1, 1, 1, 1], "date": "2022-02", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_938", "valid": true}
{"query": "Who introduced the Set Abstraction module in point-based methodologies for enhancing Point Cloud processing?", "cited_paper": [{"arxiv_id": "1706.02413", "title": "PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space", "year": 2017}], "gt_label": [1], "date": "2017-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_939", "valid": true}
{"query": "What works are about prompting, which refers to providing manual instructions or fine-tuning task-specific tokens for a desired behavior from large language models?", "cited_paper": [{"arxiv_id": "1911.12543", "title": "How Can We Know What Language Models Know?", "year": 2019}, {"arxiv_id": "2010.15980", "title": "AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts", "year": 2020}], "gt_label": [1, 1], "date": "2021-07", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_940", "valid": true}
{"query": "What research papers have been published around privacy leakage in prompt-based learning?", "cited_paper": [{"arxiv_id": "1909.00505", "title": "Commonsense Knowledge Mining from Pretrained Models", "year": 2019}, {"arxiv_id": "1911.12543", "title": "How Can We Know What Language Models Know?", "year": 2019}, {"arxiv_id": "1909.01066", "title": "Language Models as Knowledge Bases?", "year": 2019}], "gt_label": [1, 1, 1], "date": "2019-11", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_941", "valid": true}
{"query": "Which studies incurred linear convergence guarantees for NPG with log-linear policies by adding entropy regularization?", "cited_paper": [], "gt_label": [], "date": "2021-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_942", "valid": true}
{"query": "Which researches have focused on HMT techniques using sparse signals from HMD or wearable IMU sensors?", "cited_paper": [{"arxiv_id": "2105.04605", "title": "TransPose: Real-time 3D Human Translation and Pose Estimation with Six Inertial Sensors", "year": 2021}, {"arxiv_id": "2203.08528", "title": "Physical Inertial Poser (PIP): Physics-aware Real-time Human Motion Tracking from Sparse Inertial Sensors", "year": 2022}, {"arxiv_id": "2207.13784", "title": "AvatarPoser: Articulated Full-Body Pose Tracking from Sparse Motion Sensing", "year": 2022}, {"arxiv_id": "2203.05789", "title": "FLAG: Flow-based 3D Avatar Generation from Sparse Observations", "year": 2022}, {"arxiv_id": "2304.08577", "title": "Avatars Grow Legs: Generating Smooth Human Motion from Sparse Tracking Inputs with Diffusion Model", "year": 2023}], "gt_label": [1, 1, 1, 1, 1], "date": "2023-04", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_943", "valid": true}
{"query": "Which papers outline criteria for useful benchmarks for natural language understanding?", "cited_paper": [{"arxiv_id": "2104.02145", "title": "What Will it Take to Fix Benchmarking in Natural Language Understanding?", "year": 2021}], "gt_label": [1], "date": "2021-04", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_944", "valid": true}
{"query": "Could you provide me some works about the progress in graph neural networks that enhance the usage of 2D molecular graph representation?", "cited_paper": [{"arxiv_id": "1805.09076", "title": "Constrained Graph Variational Autoencoders for Molecule Design", "year": 2018}, {"arxiv_id": "2001.09382", "title": "GraphAF: a Flow-based Autoregressive Model for Molecular Graph Generation", "year": 2020}, {"arxiv_id": "1802.04364", "title": "Junction Tree Variational Autoencoder for Molecular Graph Generation", "year": 2018}, {"arxiv_id": "1806.02473", "title": "Graph Convolutional Policy Network for Goal-Directed Molecular Graph Generation", "year": 2018}, {"arxiv_id": "1810.08678", "title": "Optimization of Molecules via Deep Reinforcement Learning", "year": 2018}], "gt_label": [1, 1, 1, 1, 1], "date": "2020-01", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_945", "valid": true}
{"query": "Which papers propose models that reason about orientation uncertainty by predicting the parameters of von Mises, Fisher, and Bingham distributions?", "cited_paper": [{"arxiv_id": "1805.03430", "title": "Deep Directional Statistics: Pose Estimation with Uncertainty Quantification", "year": 2018}], "gt_label": [1], "date": "2020-12", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_946", "valid": true}
{"query": "Which studies contain pre-training of embeddings with privileged information in task-inference methods?", "cited_paper": [{"arxiv_id": "2008.02790", "title": "Decoupling Exploration and Exploitation for Meta-Reinforcement Learning without Sacrifices", "year": 2020}], "gt_label": [1], "date": "2020-08", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_947", "valid": true}
{"query": "What papers talk about style/data-augmentation in DG strategies?", "cited_paper": [{"arxiv_id": "2104.02008", "title": "Domain Generalization with MixStyle", "year": 2021}, {"arxiv_id": "2202.03958", "title": "Uncertainty Modeling for Out-of-Distribution Generalization", "year": 2022}, {"arxiv_id": "2203.07740", "title": "Exact Feature Distribution Matching for Arbitrary Style Transfer and Domain Generalization", "year": 2022}, {"arxiv_id": "2007.03304", "title": "Learning to Generate Novel Domains for Domain Generalization", "year": 2020}], "gt_label": [1, 1, 1, 1], "date": "2022-03", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_948", "valid": true}
{"query": "What research discussed that the regret bounds couldn't improve beyond the guarantees for the original E2D method?", "cited_paper": [], "gt_label": [], "date": "2021-12", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_949", "valid": true}
{"query": "Which works explore the establishment of clustering pseudo labels for achieving end-to-end multi-view clustering?", "cited_paper": [{"arxiv_id": "2103.15069", "title": "Self-supervised Discriminative Feature Learning for Deep Multi-view Clustering", "year": 2021}], "gt_label": [1], "date": "2021-03", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_950", "valid": true}
{"query": "What studies designed multi-agent reinforcement learning methods to train agents for Nash equilibrium in common-payoff games?", "cited_paper": [{"arxiv_id": "2110.14555", "title": "V-Learning -- A Simple, Efficient, Decentralized Algorithm for Multiagent RL", "year": 2021}], "gt_label": [1], "date": "2021-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_951", "valid": true}
{"query": "Which studies discussed architectures for predicting drug properties from SMILES/SELFIES strings in drug discovery?", "cited_paper": [], "gt_label": [], "date": "2019-05", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_952", "valid": true}
{"query": "What work describes Trans-MM, an interpretation method for Transformer-based architectures?", "cited_paper": [{"arxiv_id": "2103.15679", "title": "Generic Attention-model Explainability for Interpreting Bi-Modal and Encoder-Decoder Transformers", "year": 2021}], "gt_label": [1], "date": "2021-03", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_953", "valid": true}
{"query": "Which works divide the GEC task into detection and correction processes?", "cited_paper": [{"arxiv_id": "1607.06153", "title": "Compositional Sequence Labeling Models for Error Detection in Learner Writing", "year": 2016}, {"arxiv_id": "1906.06593", "title": "Context is Key: Grammatical Error Detection with Contextual Word Representations", "year": 2019}], "gt_label": [1, 1], "date": "2019-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_954", "valid": true}
{"query": "What works proposed a framework that jointly infers logical forms and direct answers?", "cited_paper": [{"arxiv_id": "2210.00063", "title": "DecAF: Joint Decoding of Answers and Logical Forms for Question Answering over Knowledge Bases", "year": 2022}], "gt_label": [1], "date": "2022-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_955", "valid": true}
{"query": "Could you provide me some works that use recurrent neural networks and an attention mechanism for routing information in 'sequential slots'?", "cited_paper": [{"arxiv_id": "1603.08575", "title": "Attend, Infer, Repeat: Fast Scene Understanding with Generative Models", "year": 2016}, {"arxiv_id": "1806.01794", "title": "Sequential Attend, Infer, Repeat: Generative Modelling of Moving Objects", "year": 2018}, {"arxiv_id": "1910.05231", "title": "R-SQAIR: Relational Sequential Attend, Infer, Repeat", "year": 2019}], "gt_label": [1, 1, 1], "date": "2019-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_956", "valid": true}
{"query": "Which papers indicated that LLMs have recently advanced the state-of-the-art performance across many NLP tasks?", "cited_paper": [{"arxiv_id": "2305.10403", "title": "PaLM 2 Technical Report", "year": 2023}, {"arxiv_id": "2303.08774", "title": "GPT-4 Technical Report", "year": 2023}, {"arxiv_id": "2204.02311", "title": "PaLM: Scaling Language Modeling with Pathways", "year": 2022}, {"arxiv_id": "2302.13971", "title": "LLaMA: Open and Efficient Foundation Language Models", "year": 2023}, {"arxiv_id": "2307.09288", "title": "Llama 2: Open Foundation and Fine-Tuned Chat Models", "year": 2023}], "gt_label": [1, 1, 1, 1, 1], "date": "2023-07", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_957", "valid": true}
{"query": "What studies are associated with the problem of finetuning the entire model in hierarchical classification, which requires substantial data and is computationally inefficient?", "cited_paper": [{"arxiv_id": "1709.09890", "title": "B-CNN: Branch Convolutional Neural Network for Hierarchical Classification", "year": 2017}, {"arxiv_id": "1906.01536", "title": "Visual Tree Convolutional Neural Network in Image Classification", "year": 2019}, {"arxiv_id": "1604.06119", "title": "Network of Experts for Large-Scale Image Categorization", "year": 2016}, {"arxiv_id": "2007.09898", "title": "Solving Long-tailed Recognition with Deep Realistic Taxonomic Classifier", "year": 2020}], "gt_label": [1, 1, 1, 1], "date": "2020-07", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_958", "valid": true}
{"query": "Which work decomposes the problem of task automation into action phrase-extraction and grounding stages?", "cited_paper": [{"arxiv_id": "2005.03776", "title": "Mapping Natural Language Instructions to Mobile UI Action Sequences", "year": 2020}], "gt_label": [1], "date": "2020-05", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_959", "valid": true}
{"query": "What studies introduced templates for multimodal ICL to improve ICL performance?", "cited_paper": [{"arxiv_id": "2310.00647", "title": "Beyond Task Performance: Evaluating and Reducing the Flaws of Large Multimodal Models with In-Context Learning", "year": 2023}], "gt_label": [1], "date": "2023-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_960", "valid": true}
{"query": "Which papers focused on 'blind source separation' in the context of Independent Component Analysis?", "cited_paper": [{"arxiv_id": "2106.09620", "title": "Disentangling Identifiable Features from Noisy Data with Structured Nonlinear ICA", "year": 2021}, {"arxiv_id": "2303.16535", "title": "Nonlinear Independent Component Analysis for Principled Disentanglement in Unsupervised Deep Learning", "year": 2023}], "gt_label": [1, 1], "date": "2023-03", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_961", "valid": true}
{"query": "Could you provide me some works about the concept of data pruning in the context of sparse data selection?", "cited_paper": [{"arxiv_id": "1812.05159", "title": "An Empirical Study of Example Forgetting during Deep Neural Network Learning", "year": 2018}, {"arxiv_id": "2008.03703", "title": "What Neural Networks Memorize and Why: Discovering the Long Tail via Influence Estimation", "year": 2020}], "gt_label": [1, 1], "date": "2022-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_962", "valid": true}
{"query": "Which works detailed an algorithm for approximate ridge leverage-score sampling for the Khatri-Rao product?", "cited_paper": [{"arxiv_id": "2202.04515", "title": "Leverage Score Sampling for Tensor Product Matrices in Input Sparsity Time", "year": 2022}], "gt_label": [1], "date": "2022-02", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_963", "valid": true}
{"query": "What studies used diffusion models in the context of video?", "cited_paper": [{"arxiv_id": "2210.02303", "title": "Imagen Video: High Definition Video Generation with Diffusion Models", "year": 2022}, {"arxiv_id": "2209.14792", "title": "Make-A-Video: Text-to-Video Generation without Text-Video Data", "year": 2022}], "gt_label": [1, 1], "date": "2022-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_964", "valid": true}
{"query": "What work originally proposed chain-of-thought (CoT) prompting for LLMs?", "cited_paper": [{"arxiv_id": "2201.11903", "title": "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models", "year": 2022}], "gt_label": [1], "date": "2022-01", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_965", "valid": true}
{"query": "What research works are based on the deep subspace multi-view clustering and deep graph multi-view clustering?", "cited_paper": [{"arxiv_id": "1804.06498", "title": "Deep Multimodal Subspace Clustering Networks", "year": 2018}], "gt_label": [1], "date": "2018-04", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_966", "valid": true}
{"query": "Which paper mentioned that the sum of all leverage scores is the rank of the matrix?", "cited_paper": [{"arxiv_id": "1411.4357", "title": "Sketching as a Tool for Numerical Linear Algebra", "year": 2014}], "gt_label": [1], "date": "2014-11", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_967", "valid": true}
{"query": "What papers discuss the role of textual prompts in delivering contextual information necessary for in-context learning?", "cited_paper": [{"arxiv_id": "2211.01910", "title": "Large Language Models Are Human-Level Prompt Engineers", "year": 2022}], "gt_label": [1], "date": "2022-11", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_968", "valid": true}
{"query": "Could you provide me some works about different framings of the sparse coding problem?", "cited_paper": [{"arxiv_id": "0812.1869", "title": "Convex Sparse Matrix Factorizations", "year": 2008}], "gt_label": [1], "date": "2008-12", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_969", "valid": true}
{"query": "Any studies about allocation algorithms for subadditive valuations that guarantee 1/2121/2-envy-freeness ex-ante?", "cited_paper": [{"arxiv_id": "2304.03706", "title": "Breaking the Envy Cycle: Best-of-Both-Worlds Guarantees for Subadditive Valuations", "year": 2023}], "gt_label": [1], "date": "2023-04", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_970", "valid": true}
{"query": "What research papers have developed free-form text conditioned generation for motion?", "cited_paper": [{"arxiv_id": "2204.14109", "title": "TEMOS: Generating diverse human motions from textual descriptions", "year": 2022}, {"arxiv_id": "2103.14675", "title": "Synthesis of Compositional Animations from Textual Descriptions", "year": 2021}, {"arxiv_id": "2209.00349", "title": "FLAME: Free-form Language-based Motion Synthesis & Editing", "year": 2022}], "gt_label": [1, 1, 1], "date": "2022-09", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_971", "valid": true}
{"query": "What studies consider multi-task surrogate models in the context of safe bo?", "cited_paper": [{"arxiv_id": "1509.01066", "title": "Safe Controller Optimization for Quadrotors with Gaussian Processes", "year": 2015}, {"arxiv_id": "1602.04450", "title": "Bayesian Optimization with Safety Constraints: Safe and Automatic Parameter Tuning in Robotics", "year": 2016}, {"arxiv_id": "1902.03229", "title": "Adaptive and Safe Bayesian Optimization in High Dimensions via One-Dimensional Subspaces", "year": 2019}, {"arxiv_id": "1806.07555", "title": "Stagewise Safe Bayesian Optimization with Gaussian Processes", "year": 2018}], "gt_label": [1, 1, 1, 1], "date": "2019-02", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_972", "valid": true}
{"query": "Could you provide me some works on SlipCover?", "cited_paper": [{"arxiv_id": "1309.2080", "title": "Structure Learning of Probabilistic Logic Programs by Searching the Clause Space", "year": 2013}], "gt_label": [1], "date": "2013-09", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_973", "valid": true}
{"query": "Which works have incorporated equivariances into neural network models?", "cited_paper": [{"arxiv_id": "1802.03690", "title": "On the Generalization of Equivariance and Convolution in Neural Networks to the Action of Compact Groups", "year": 2018}, {"arxiv_id": "2006.15107", "title": "Building powerful and equivariant graph neural networks with structural message-passing", "year": 2020}], "gt_label": [1, 1], "date": "2020-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_974", "valid": true}
{"query": "What studies have utilized image sequences derived from videos for evaluation of MLLMs?", "cited_paper": [{"arxiv_id": "2311.14906", "title": "AutoEval-Video: An Automatic Benchmark for Assessing Large Vision Language Models in Open-Ended Video Question Answering", "year": 2023}], "gt_label": [1], "date": "2023-11", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_975", "valid": true}
{"query": "Which studies discussed 3D diffusion models related to point clouds?", "cited_paper": [{"arxiv_id": "2104.03670", "title": "3D Shape Generation and Completion through Point-Voxel Diffusion", "year": 2021}, {"arxiv_id": "2103.01458", "title": "Diffusion Probabilistic Models for 3D Point Cloud Generation", "year": 2021}, {"arxiv_id": "2302.10668", "title": "$PC^2$: Projection-Conditioned Point Cloud Diffusion for Single-Image 3D Reconstruction", "year": 2023}, {"arxiv_id": "2303.05916", "title": "GECCO: Geometrically-Conditioned Point Diffusion Models", "year": 2023}], "gt_label": [1, 1, 1, 1], "date": "2023-03", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_976", "valid": true}
{"query": "Are there any research outputs revealing the ambiguous benefits of techniques aiming at enhancing the reasoning capability of LLMs?", "cited_paper": [{"arxiv_id": "2310.01798", "title": "Large Language Models Cannot Self-Correct Reasoning Yet", "year": 2023}], "gt_label": [1], "date": "2023-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_977", "valid": true}
{"query": "Which research proposed the Knowledge-Guided Policy Network?", "cited_paper": [{"arxiv_id": "2002.07418", "title": "KoGuN: Accelerating Deep Reinforcement Learning via Integrating Human Suboptimal Knowledge", "year": 2020}], "gt_label": [1], "date": "2020-02", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_978", "valid": true}
{"query": "What work develops the Functional Mirror Ascent Policy Gradient (FMA-PG)?", "cited_paper": [], "gt_label": [], "date": "2021-08", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_979", "valid": true}
{"query": "Which papers present the use of differentiable material point method simulation in co-optimizing soft robots?", "cited_paper": [{"arxiv_id": "1810.01054", "title": "ChainQueen: A Real-Time Differentiable Physical Simulator for Soft Robotics", "year": 2018}], "gt_label": [1], "date": "2018-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_980", "valid": true}
{"query": "What are some examples of studies that introduced Transformer techniques to improve computer vision systems?", "cited_paper": [{"arxiv_id": "2010.11929", "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale", "year": 2020}, {"arxiv_id": "2204.01697", "title": "MaxViT: Multi-Axis Vision Transformer", "year": 2022}, {"arxiv_id": "2303.06908", "title": "CrossFormer++: A Versatile Vision Transformer Hinging on Cross-scale Attention", "year": 2023}, {"arxiv_id": "2106.02277", "title": "Glance-and-Gaze Vision Transformer", "year": 2021}], "gt_label": [1, 1, 1, 1], "date": "2023-03", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_981", "valid": true}
{"query": "Which research papers introduced prompts paradigm to language models?", "cited_paper": [], "gt_label": [], "date": "2021-07", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_982", "valid": true}
{"query": "What is the initial study that proposed adversarial training as a method for enhancing model robustness?", "cited_paper": [{"arxiv_id": "1412.6572", "title": "Explaining and Harnessing Adversarial Examples", "year": 2014}], "gt_label": [1], "date": "2014-12", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_983", "valid": true}
{"query": "Which studies developed 3D pretraining methods for semantic and instance segmentation that use instance discrimination based on different camera views?", "cited_paper": [{"arxiv_id": "2007.10985", "title": "PointContrast: Unsupervised Pre-training for 3D Point Cloud Understanding", "year": 2020}, {"arxiv_id": "2012.09165", "title": "Exploring Data-Efficient 3D Scene Understanding with Contrastive Scene Contexts", "year": 2020}], "gt_label": [1, 1], "date": "2020-12", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_984", "valid": true}
{"query": "What work provided data for evaluating model behavior in more naturalistic, 3D environments?", "cited_paper": [{"arxiv_id": "2106.08261", "title": "Physion: Evaluating Physical Prediction from Vision in Humans and Machines", "year": 2021}], "gt_label": [1], "date": "2021-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_985", "valid": true}
{"query": "Who proposed efficient variants of the SM loss called the SSM and FDSSM objectives?", "cited_paper": [{"arxiv_id": "1905.07088", "title": "Sliced Score Matching: A Scalable Approach to Density and Score Estimation", "year": 2019}, {"arxiv_id": "2007.03317", "title": "Efficient Learning of Generative Models via Finite-Difference Score Matching", "year": 2020}], "gt_label": [1, 1], "date": "2020-07", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_986", "valid": true}
{"query": "Could you provide me some papers about the work proposing a unified multimodal encoder to align all modalities with language?", "cited_paper": [{"arxiv_id": "2305.04160", "title": "X-LLM: Bootstrapping Advanced Large Language Models by Treating Multi-Modalities as Foreign Languages", "year": 2023}, {"arxiv_id": "2305.16103", "title": "ChatBridge: Bridging Modalities with Large Language Model as a Language Catalyst", "year": 2023}, {"arxiv_id": "2309.16058", "title": "AnyMAL: An Efficient and Scalable Any-Modality Augmented Language Model", "year": 2023}, {"arxiv_id": "2305.16355", "title": "PandaGPT: One Model To Instruction-Follow Them All", "year": 2023}, {"arxiv_id": "2309.03905", "title": "ImageBind-LLM: Multi-modality Instruction Tuning", "year": 2023}], "gt_label": [1, 1, 1, 1, 1], "date": "2023-09", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_987", "valid": true}
{"query": "Which works focus on detecting LLM-generated text using classifiers?", "cited_paper": [{"arxiv_id": "2304.14106", "title": "ChatLog: Carefully Evaluating the Evolution of ChatGPT Across Time", "year": 2023}, {"arxiv_id": "2301.07597", "title": "How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection", "year": 2023}, {"arxiv_id": "2301.11305", "title": "DetectGPT: Zero-Shot Machine-Generated Text Detection using Probability Curvature", "year": 2023}], "gt_label": [1, 1, 1], "date": "2023-04", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_988", "valid": true}
{"query": "What papers show that LLM hidden states can effectively represent aspects of texts such as space, time, and honesty?", "cited_paper": [{"arxiv_id": "2310.02207", "title": "Language Models Represent Space and Time", "year": 2023}, {"arxiv_id": "2310.01405", "title": "Representation Engineering: A Top-Down Approach to AI Transparency", "year": 2023}], "gt_label": [1, 1], "date": "2023-10", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_989", "valid": true}
{"query": "What works used the reparameterisation implied by unit scaling?", "cited_paper": [{"arxiv_id": "1806.07572", "title": "Neural Tangent Kernel: Convergence and Generalization in Neural Networks", "year": 2018}, {"arxiv_id": "2203.03466", "title": "Tensor Programs V: Tuning Large Neural Networks via Zero-Shot Hyperparameter Transfer", "year": 2022}], "gt_label": [1, 1], "date": "2022-03", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_990", "valid": true}
{"query": "Which work developed stochastic compositional gradient descent (SCGD) to solve the problem in Stochastic Compositional Optimization?", "cited_paper": [], "gt_label": [], "date": "2014-11", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_991", "valid": true}
{"query": "What studies propose methods for Partial Model Personalization by training personalized feature extractors?", "cited_paper": [{"arxiv_id": "2102.07078", "title": "Exploiting Shared Representations for Personalized Federated Learning", "year": 2021}], "gt_label": [1], "date": "2021-02", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_992", "valid": true}
{"query": "What research works have taken on the challenge of improving efficiency and scalability of large-scale GNNs through sampling methods?", "cited_paper": [{"arxiv_id": "1706.02216", "title": "Inductive Representation Learning on Large Graphs", "year": 2017}, {"arxiv_id": "1801.10247", "title": "FastGCN: Fast Learning with Graph Convolutional Networks via Importance Sampling", "year": 2018}, {"arxiv_id": "1911.07323", "title": "Layer-Dependent Importance Sampling for Training Deep and Large Graph Convolutional Networks", "year": 2019}, {"arxiv_id": "2106.05609", "title": "GNNAutoScale: Scalable and Expressive Graph Neural Networks via Historical Embeddings", "year": 2021}, {"arxiv_id": "2206.07161", "title": "GraphFM: Improving Large-Scale GNN Training via Feature Momentum", "year": 2022}], "gt_label": [1, 1, 1, 1, 1], "date": "2022-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_993", "valid": true}
{"query": "Which studies discuss about the concept of adversarial robustness for classifiers?", "cited_paper": [{"arxiv_id": "1608.08967", "title": "Robustness of classifiers: from adversarial to random noise", "year": 2016}], "gt_label": [1], "date": "2016-08", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_994", "valid": true}
{"query": "Which work introduced the Fisher-Rao norm as a crucial geometric complexity measure?", "cited_paper": [{"arxiv_id": "1711.01530", "title": "Fisher-Rao Metric, Geometry, and Complexity of Neural Networks", "year": 2017}], "gt_label": [1], "date": "2017-11", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_995", "valid": true}
{"query": "What works have used the concept of dimensionality reduction in diffusion models?", "cited_paper": [{"arxiv_id": "2112.10752", "title": "High-Resolution Image Synthesis with Latent Diffusion Models", "year": 2021}, {"arxiv_id": "2301.11093", "title": "Simple diffusion: End-to-end diffusion for high resolution images", "year": 2023}, {"arxiv_id": "2208.05003", "title": "Wavelet Score-Based Generative Modeling", "year": 2022}], "gt_label": [1, 1, 1], "date": "2023-01", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_996", "valid": true}
{"query": "What work has been done to establish sublinear convergence rates for two-layer neural PPO and NAC?", "cited_paper": [{"arxiv_id": "1909.01150", "title": "Neural Policy Gradient Methods: Global Optimality and Rates of Convergence", "year": 2019}, {"arxiv_id": "2206.00833", "title": "Finite-Time Analysis of Entropy-Regularized Neural Natural Actor-Critic Algorithm", "year": 2022}], "gt_label": [1, 1], "date": "2022-06", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_997", "valid": true}
{"query": "What papers study the frameworks for mitigating hallucinations?", "cited_paper": [{"arxiv_id": "2310.00754", "title": "Analyzing and Mitigating Object Hallucination in Large Vision-Language Models", "year": 2023}, {"arxiv_id": "2312.01701", "title": "Mitigating Fine-Grained Hallucination by Fine-Tuning Large Vision-Language Models with Caption Rewrites", "year": 2023}, {"arxiv_id": "2311.16922", "title": "Mitigating Object Hallucinations in Large Vision-Language Models through Visual Contrastive Decoding", "year": 2023}, {"arxiv_id": "2302.09236", "title": "Scalable Prompt Generation for Semi-supervised Learning with Language Models", "year": 2023}, {"arxiv_id": "2311.16479", "title": "Mitigating Hallucination in Visual Language Models with Visual Supervision", "year": 2023}, {"arxiv_id": "2312.06968", "title": "Hallucination Augmented Contrastive Learning for Multimodal Large Language Model", "year": 2023}, {"arxiv_id": "2311.17911", "title": "OPERA: Alleviating Hallucination in Multi-Modal Large Language Models via Over-Trust Penalty and Retrospection-Allocation", "year": 2023}, {"arxiv_id": "2311.13614", "title": "HalluciDoctor: Mitigating Hallucinatory Toxicity in Visual Instruction Data", "year": 2023}, {"arxiv_id": "2311.16839", "title": "Beyond Hallucinations: Enhancing LVLMs through Hallucination-Aware Direct Preference Optimization", "year": 2023}], "gt_label": [1, 1, 1, 1, 1, 1, 1, 1, 1], "date": "2023-12", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_998", "valid": true}
{"query": "Which works have utilized image features from frozen vision models for multi-modal language models?", "cited_paper": [{"arxiv_id": "2010.11929", "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale", "year": 2020}, {"arxiv_id": "2103.00020", "title": "Learning Transferable Visual Models From Natural Language Supervision", "year": 2021}], "gt_label": [1, 1], "date": "2021-03", "source": "PASA_AutoScholar", "qid": "AutoScholarQuery_dev_999", "valid": true}
{"query": "Give me papers which show that using a smaller dataset in large language model pre-training can result in better models than using bigger datasets.", "cited_paper": [{"arxiv_id": "2309.04564", "title": "When Less is More: Investigating Data Pruning for Pretraining LLMs at Scale", "year": 2023}, {"arxiv_id": "2402.09668", "title": "How to Train Data-Efficient LLMs", "year": 2024}, {"arxiv_id": "2107.06499", "title": "Deduplicating Training Data Makes Language Models Better", "year": 2021}, {"arxiv_id": "2307.08701", "title": "AlpaGasus: Training A Better Alpaca with Fewer Data", "year": 2023}, {"arxiv_id": "2305.02301", "title": "Distilling Step-by-Step! Outperforming Larger Language Models with Less Training Data and Smaller Model Sizes", "year": 2023}, {"arxiv_id": "2402.04333", "title": "LESS: Selecting Influential Data for Targeted Instruction Tuning", "year": 2024}, {"arxiv_id": "2210.10951", "title": "Automatic Document Selection for Efficient Encoder Pretraining", "year": 2022}, {"arxiv_id": "2305.12816", "title": "Farewell to Aimless Large-scale Pretraining: Influential Subset Selection for Language Model", "year": 2023}, {"arxiv_id": "2409.17312", "title": "BabyLlama-2: Ensemble-Distilled Models Consistently Outperform Teachers With Limited Data", "year": 2024}], "gt_label": [1, 1, 1, 1, 1, 1, 1, 1, 1], "date": "2024-09", "source": "PASA_RealScholar", "qid": "RealScholarQuery_0", "valid": true}
{"query": "Give me papers that share some insights about how large language models gain in-context learning capability in the process of pre-training.\n", "cited_paper": [{"arxiv_id": "2306.15063", "title": "Pretraining task diversity and the emergence of non-Bayesian in-context learning for regression", "year": 2023}, {"arxiv_id": "2310.10616", "title": "How Do Transformers Learn In-Context Beyond Simple Functions? A Case Study on Learning with Representations", "year": 2023}, {"arxiv_id": "2205.05055", "title": "Data Distributional Properties Drive Emergent In-Context Learning in Transformers", "year": 2022}, {"arxiv_id": "2310.17086", "title": "Transformers Learn to Achieve Second-Order Convergence Rates for In-Context Linear Regression", "year": 2023}, {"arxiv_id": "2212.10559", "title": "Why Can GPT Learn In-Context? Language Models Implicitly Perform Gradient Descent as Meta-Optimizers", "year": 2022}, {"arxiv_id": "2310.10638", "title": "In-context Pretraining: Language Modeling Beyond Document Boundaries", "year": 2023}, {"arxiv_id": "2305.09137", "title": "Pre-Training to Learn in Context", "year": 2023}, {"arxiv_id": "2212.07677", "title": "Transformers learn in-context by gradient descent", "year": 2022}, {"arxiv_id": "2310.08540", "title": "Do pretrained Transformers Learn In-Context by Gradient Descent?", "year": 2023}, {"arxiv_id": "2312.03002", "title": "The mechanistic basis of data dependence and abrupt learning in an in-context classification task", "year": 2023}, {"arxiv_id": "2305.12766", "title": "Explaining Emergent In-Context Learning as Kernel Regression", "year": 2023}, {"arxiv_id": "2402.15607", "title": "How Do Nonlinear Transformers Learn and Generalize in In-Context Learning?", "year": 2024}, {"arxiv_id": "2405.11751", "title": "Asymptotic theory of in-context learning by linear attention", "year": 2024}, {"arxiv_id": "2305.17040", "title": "A Mechanism for Sample-Efficient In-Context Learning for Sparse Retrieval Tasks", "year": 2023}, {"arxiv_id": "2402.01258", "title": "Transformers Learn Nonlinear Features In Context: Nonconvex Mean-field Dynamics on the Attention Landscape", "year": 2024}, {"arxiv_id": "2111.02080", "title": "An Explanation of In-context Learning as Implicit Bayesian Inference", "year": 2021}, {"arxiv_id": "2305.19420", "title": "What and How does In-Context Learning Learn? Bayesian Model Averaging, Parameterization, and Generalization", "year": 2023}, {"arxiv_id": "2409.09281", "title": "Language Models \"Grok\" to Copy", "year": 2024}, {"arxiv_id": "2306.04637", "title": "Transformers as Statisticians: Provable In-Context Learning with In-Context Algorithm Selection", "year": 2023}, {"arxiv_id": "2306.15091", "title": "Understanding In-Context Learning via Supportive Pretraining Data", "year": 2023}, {"arxiv_id": "2210.05675", "title": "Transformers generalize differently from information stored in context vs in weights", "year": 2022}, {"arxiv_id": "2005.14165", "title": "Language Models are Few-Shot Learners", "year": 2020}, {"arxiv_id": "2406.14022", "title": "Investigating the Pre-Training Dynamics of In-Context Learning: Task Recognition vs. Task Learning", "year": 2024}, {"arxiv_id": "2311.08360", "title": "The Transient Nature of Emergent In-Context Learning in Transformers", "year": 2023}, {"arxiv_id": "2303.07895", "title": "The Learnability of In-Context Learning", "year": 2023}, {"arxiv_id": "2303.07971", "title": "A Theory of Emergent In-Context Learning as Implicit Structure Induction", "year": 2023}, {"arxiv_id": "2402.12530", "title": "Parallel Structures in Pre-training Data Yield In-Context Learning", "year": 2024}], "gt_label": [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], "date": "2024-09", "source": "PASA_RealScholar", "qid": "RealScholarQuery_1", "valid": true}
{"query": "List all papers that use autoregressive transformer to generate videos.", "cited_paper": [{"arxiv_id": "2312.14125", "title": "VideoPoet: A Large Language Model for Zero-Shot Video Generation", "year": 2023}, {"arxiv_id": "2206.04003", "title": "Patch-based Object-centric Transformers for Efficient Video Generation", "year": 2022}, {"arxiv_id": "2406.10981", "title": "ViD-GPT: Introducing GPT-style Autoregressive Generation in Video Diffusion Models", "year": 2024}, {"arxiv_id": "2402.15391", "title": "Genie: Generative Interactive Environments", "year": 2024}, {"arxiv_id": "2107.09240", "title": "Generative Video Transformer: Can Objects be the Words?", "year": 2021}, {"arxiv_id": "2006.10704", "title": "Latent Video Transformer", "year": 2020}, {"arxiv_id": "2209.07143", "title": "HARP: Autoregressive Latent Video Prediction with High-Fidelity Image Generator", "year": 2022}, {"arxiv_id": "2405.15223", "title": "iVideoGPT: Interactive VideoGPTs are Scalable World Models", "year": 2024}, {"arxiv_id": "1912.12180", "title": "Axial Attention in Multidimensional Transformers", "year": 2019}, {"arxiv_id": "2104.14806", "title": "GODIVA: Generating Open-DomaIn Videos from nAtural Descriptions", "year": 2021}, {"arxiv_id": "2212.06026", "title": "Video Prediction by Efficient Transformers", "year": 2022}, {"arxiv_id": "2210.02396", "title": "Temporally Consistent Transformers for Video Generation", "year": 2022}, {"arxiv_id": "2203.09457", "title": "Look Outside the Room: Synthesizing A Consistent Long-Term 3D Scene Video from A Single Image", "year": 2022}, {"arxiv_id": "2203.15836", "title": "VPTR: Efficient Transformers for Video Prediction", "year": 2022}, {"arxiv_id": "2205.15868", "title": "CogVideo: Large-scale Pretraining for Text-to-Video Generation via Transformers", "year": 2022}, {"arxiv_id": "2207.09814", "title": "NUWA-Infinity: Autoregressive over Autoregressive Generation for Infinite Visual Synthesis", "year": 2022}, {"arxiv_id": "2104.10157", "title": "VideoGPT: Video Generation using VQ-VAE and Transformers", "year": 2021}, {"arxiv_id": "2402.14797", "title": "Snap Video: Scaled Spatiotemporal Transformers for Text-to-Video Synthesis", "year": 2024}, {"arxiv_id": "2406.09455", "title": "Pandora: Towards General World Model with Natural Language Actions and Video States", "year": 2024}, {"arxiv_id": "1906.02634", "title": "Scaling Autoregressive Video Models", "year": 2019}], "gt_label": [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], "date": "2024-09", "source": "PASA_RealScholar", "qid": "RealScholarQuery_2", "valid": true}
{"query": "I am looking for research papers on the construction of multimodal foundation models that support both visual and audio inputs. These models should be pre-trained on large-scale datasets, including visual, audio, and audio-visual data. Please exclude survey papers.", "cited_paper": [{"arxiv_id": "2201.02639", "title": "MERLOT Reserve: Neural Script Knowledge through Vision and Language and Sound", "year": 2022}, {"arxiv_id": "2403.15377", "title": "InternVideo2: Scaling Foundation Models for Multimodal Video Understanding", "year": 2024}, {"arxiv_id": "2104.11178", "title": "VATT: Transformers for Multimodal Self-Supervised Learning from Raw Video, Audio and Text", "year": 2021}, {"arxiv_id": "2310.05863", "title": "Fine-grained Audio-Visual Joint Representations for Multimodal Large Language Models", "year": 2023}, {"arxiv_id": "2304.08345", "title": "VALOR: Vision-Audio-Language Omni-Perception Pretraining Model and Dataset", "year": 2023}, {"arxiv_id": "2305.18500", "title": "VAST: A Vision-Audio-Subtitle-Text Omni-Modality Foundation Model and Dataset", "year": 2023}, {"arxiv_id": "2211.11275", "title": "VATLM: Visual-Audio-Text Pre-Training with Unified Masked Prediction for Speech Representation Learning", "year": 2022}, {"arxiv_id": "2401.12264", "title": "CoAVT: A Cognition-Inspired Unified Audio-Visual-Text Pre-Training Model for Multimodal Processing", "year": 2024}, {"arxiv_id": "2306.02858", "title": "Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding", "year": 2023}, {"arxiv_id": "2312.14125", "title": "VideoPoet: A Large Language Model for Zero-Shot Video Generation", "year": 2023}, {"arxiv_id": "2310.01852", "title": "LanguageBind: Extending Video-Language Pretraining to N-modality by Language-based Semantic Alignment", "year": 2023}, {"arxiv_id": "2406.15704", "title": "video-SALMONN: Speech-Enhanced Audio-Visual Large Language Models", "year": 2024}, {"arxiv_id": "1705.08168", "title": "Look, Listen and Learn", "year": 2017}, {"arxiv_id": "2306.09093", "title": "Macaw-LLM: Multi-Modal Language Modeling with Image, Audio, Video, and Text Integration", "year": 2023}, {"arxiv_id": "2408.05211", "title": "VITA: Towards Open-Source Interactive Omni Multimodal LLM", "year": 2024}, {"arxiv_id": "2305.12311", "title": "i-Code V2: An Autoregressive Generation Framework over Vision, Language, and Speech Data", "year": 2023}, {"arxiv_id": "2006.09199", "title": "AVLnet: Learning Audio-Visual Language Representations from Instructional Videos", "year": 2020}, {"arxiv_id": "2107.00249", "title": "OPT: Omni-Perception Pre-Trainer for Cross-Modal Understanding and Generation", "year": 2021}, {"arxiv_id": "2307.08581", "title": "BuboGPT: Enabling Visual Grounding in Multi-Modal LLMs", "year": 2023}, {"arxiv_id": "2305.14381", "title": "Connecting Multi-modal Contrastive Representations", "year": 2023}, {"arxiv_id": "2409.17692", "title": "MIO: A Foundation Model on Multimodal Tokens", "year": 2024}, {"arxiv_id": "2312.17172", "title": "Unified-IO 2: Scaling Autoregressive Multimodal Models with Vision, Language, Audio, and Action", "year": 2023}, {"arxiv_id": "2212.04408", "title": "OFASys: A Multi-Modal Multi-Task Learning System for Building Generalist Models", "year": 2022}, {"arxiv_id": "2106.13043", "title": "AudioCLIP: Extending CLIP to Image, Text and Audio", "year": 2021}, {"arxiv_id": "2312.06720", "title": "Audio-Visual LLM for Video Understanding", "year": 2023}, {"arxiv_id": "2407.11895", "title": "OmniBind: Large-scale Omni Multimodal Representation via Binding Spaces", "year": 2024}, {"arxiv_id": "2309.05519", "title": "NExT-GPT: Any-to-Any Multimodal LLM", "year": 2023}, {"arxiv_id": "2312.03700", "title": "OneLLM: One Framework to Align All Modalities with Language", "year": 2023}, {"arxiv_id": "2406.07476", "title": "VideoLLaMA 2: Advancing Spatial-Temporal Modeling and Audio Understanding in Video-LLMs", "year": 2024}, {"arxiv_id": "2111.12993", "title": "PolyViT: Co-training Vision Transformers on Images, Videos and Audio", "year": 2021}, {"arxiv_id": "2205.01818", "title": "i-Code: An Integrative and Composable Multimodal Learning Framework", "year": 2022}, {"arxiv_id": "2311.18775", "title": "CoDi-2: In-Context, Interleaved, and Interactive Any-to-Any Generation", "year": 2023}, {"arxiv_id": "2305.11172", "title": "ONE-PEACE: Exploring One General Representation Model Toward Unlimited Modalities", "year": 2023}, {"arxiv_id": "2305.16355", "title": "PandaGPT: One Model To Instruction-Follow Them All", "year": 2023}, {"arxiv_id": "2311.13435", "title": "PG-Video-LLaVA: Pixel Grounding Large Video-Language Models", "year": 2023}, {"arxiv_id": "2405.04883", "title": "FreeBind: Free Lunch in Unified Multimodal Space via Knowledge Fusion", "year": 2024}, {"arxiv_id": "2212.07525", "title": "Efficient Self-supervised Learning with Contextualized Target Representations for Vision, Speech and Language", "year": 2022}, {"arxiv_id": "2310.08884", "title": "Extending Multi-modal Contrastive Representations", "year": 2023}, {"arxiv_id": "2409.19132", "title": "From Vision to Audio and Beyond: A Unified Model for Audio-Visual Representation and Generation", "year": 2024}], "gt_label": [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], "date": "2024-09", "source": "PASA_RealScholar", "qid": "RealScholarQuery_3", "valid": true}
{"query": "Provide me with all papers that discuss reinforcement learning training for Large Language Model agent tasks.", "cited_paper": [{"arxiv_id": "2401.14151", "title": "True Knowledge Comes from Practice: Aligning LLMs with Embodied Environments via Reinforcement Learning", "year": 2024}, {"arxiv_id": "2403.04642", "title": "Teaching Large Language Models to Reason with Reinforcement Learning", "year": 2024}, {"arxiv_id": "2310.18940", "title": "Language Agents with Reinforcement Learning for Strategic Play in the Werewolf Game", "year": 2023}, {"arxiv_id": "2306.03604", "title": "Enabling Intelligent Interactions between an Agent and an LLM: A Reinforcement Learning Approach", "year": 2023}, {"arxiv_id": "2402.19446", "title": "ArCHer: Training Language Model Agents via Hierarchical Multi-Turn RL", "year": 2024}, {"arxiv_id": "2303.11366", "title": "Reflexion: Language Agents with Verbal Reinforcement Learning", "year": 2023}, {"arxiv_id": "2302.06692", "title": "Guiding Pretraining in Reinforcement Learning with Large Language Models", "year": 2023}, {"arxiv_id": "2406.05872", "title": "STARLING: Self-supervised Training of Text-based Reinforcement Learning Agent with Large Language Models", "year": 2024}, {"arxiv_id": "2404.18978", "title": "Towards Generalizable Agents in Text-Based Educational Environments: A Study of Integrating RL with LLMs", "year": 2024}, {"arxiv_id": "2302.00763", "title": "Collaborating with language models for embodied reasoning", "year": 2023}, {"arxiv_id": "2112.09332", "title": "WebGPT: Browser-assisted question-answering with human feedback", "year": 2021}, {"arxiv_id": "2302.02662", "title": "Grounding Large Language Models in Interactive Environments with Online Reinforcement Learning", "year": 2023}, {"arxiv_id": "2401.06603", "title": "Mutual Enhancement of Large Language and Reinforcement Learning Models through Bi-Directional Feedback Mechanisms: A Planning Case Study", "year": 2024}, {"arxiv_id": "2311.05596", "title": "LLM Augmented Hierarchical Agents", "year": 2023}, {"arxiv_id": "2402.06700", "title": "Entropy-Regularized Token-Level Policy Optimization for Language Agent Reinforcement", "year": 2024}, {"arxiv_id": "2401.00006", "title": "Building Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptation", "year": 2024}, {"arxiv_id": "2402.19299", "title": "RL-GPT: Integrating Reinforcement Learning and Code-as-policy", "year": 2024}, {"arxiv_id": "2310.00166", "title": "Motif: Intrinsic Motivation from Artificial Intelligence Feedback", "year": 2023}, {"arxiv_id": "2312.08935", "title": "Math-Shepherd: Verify and Reinforce LLMs Step-by-step without Human Annotations", "year": 2023}, {"arxiv_id": "2307.02157", "title": "Generative Job Recommendations with Large Language Model", "year": 2023}, {"arxiv_id": "2404.18638", "title": "Reinforcement Learning Problem Solving with Large Language Models", "year": 2024}, {"arxiv_id": "2405.15383", "title": "Generating Code World Models with Large Language Models Guided by Monte Carlo Tree Search", "year": 2024}, {"arxiv_id": "2402.02330", "title": "Enhance Reasoning for Large Language Models in the Game Werewolf", "year": 2024}, {"arxiv_id": "2303.00001", "title": "Reward Design with Language Models", "year": 2023}, {"arxiv_id": "2309.17176", "title": "AdaRefiner: Refining Decisions of Language Models with Adaptive Feedback", "year": 2023}, {"arxiv_id": "2310.17722", "title": "Large Language Models as Generalizable Policies for Embodied Tasks", "year": 2023}, {"arxiv_id": "2308.13542", "title": "LaGR-SEQ: Language-Guided Reinforcement Learning with Sample-Efficient Querying", "year": 2023}, {"arxiv_id": "2310.12931", "title": "Eureka: Human-Level Reward Design via Coding Large Language Models", "year": 2023}, {"arxiv_id": "2304.04370", "title": "OpenAGI: When LLM Meets Domain Experts", "year": 2023}, {"arxiv_id": "2308.02151", "title": "Retroformer: Retrospective Large Language Agents with Policy Gradient Optimization", "year": 2023}, {"arxiv_id": "2310.20587", "title": "Unleashing the Power of Pre-trained Language Models for Offline Reinforcement Learning", "year": 2023}, {"arxiv_id": "2309.17382", "title": "Reason for Future, Act for Now: A Principled Framework for Autonomous LLM Agents with Provable Sample Efficiency", "year": 2023}, {"arxiv_id": "2307.09668", "title": "Towards A Unified Agent with Foundation Models", "year": 2023}, {"arxiv_id": "2307.11922", "title": "Selective Perception: Optimizing State Descriptions with Reinforcement Learning for Language Model Actors", "year": 2023}, {"arxiv_id": "2305.14387", "title": "AlpacaFarm: A Simulation Framework for Methods that Learn from Human Feedback", "year": 2023}, {"arxiv_id": "2209.14375", "title": "Improving alignment of dialogue agents via targeted human judgements", "year": 2022}, {"arxiv_id": "2402.12914", "title": "Large Language Model-based Human-Agent Collaboration for Complex Task Solving", "year": 2024}, {"arxiv_id": "2305.08844", "title": "RL4F: Generating Natural Language Feedback with Reinforcement Learning for Repairing Model Outputs", "year": 2023}, {"arxiv_id": "2301.12729", "title": "Response-act Guided Reinforced Dialogue Generation for Mental Health Counseling", "year": 2023}, {"arxiv_id": "2104.07972", "title": "Language Models are Few-Shot Butlers", "year": 2021}, {"arxiv_id": "2402.16181", "title": "How Can LLM Guide RL? A Value-Based Approach", "year": 2024}, {"arxiv_id": "2307.04964", "title": "Secrets of RLHF in Large Language Models Part I: PPO", "year": 2023}, {"arxiv_id": "2405.14751", "title": "AGILE: A Novel Reinforcement Learning Framework of LLM Agents", "year": 2024}], "gt_label": [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], "date": "2024-09", "source": "PASA_RealScholar", "qid": "RealScholarQuery_4", "valid": true}
{"query": "Papers that apply RLHF to address the hallucination problem in image and video description.", "cited_paper": [{"arxiv_id": "2309.14525", "title": "Aligning Large Multimodal Models with Factually Augmented RLHF", "year": 2023}, {"arxiv_id": "2402.11411", "title": "Aligning Modalities in Vision Large Language Models via Preference Fine-tuning", "year": 2024}, {"arxiv_id": "2402.06118", "title": "ViGoR: Improving Visual Grounding of Large Vision Language Models with Fine-Grained Reward Modeling", "year": 2024}, {"arxiv_id": "2312.10665", "title": "Silkie: Preference Distillation for Large Visual Language Models", "year": 2023}, {"arxiv_id": "2311.10081", "title": "DRESS: Instructing Large Vision-Language Models to Align and Interact with Humans via Natural Language Feedback", "year": 2023}, {"arxiv_id": "2312.00849", "title": "RLHF-V: Towards Trustworthy MLLMs via Behavior Alignment from Fine-grained Correctional Human Feedback", "year": 2023}], "gt_label": [1, 1, 1, 1, 1, 1], "date": "2024-03", "source": "PASA_RealScholar", "qid": "RealScholarQuery_5", "valid": true}
{"query": "Papers that propose methods based on large language models and evaluate their performance through experiments on the HotPotQA dataset.", "cited_paper": [{"arxiv_id": "2305.15064", "title": "AutoPlan: Automatic Planning of Interactive Decision-Making Tasks With Large Language Models", "year": 2023}, {"arxiv_id": "2402.11166", "title": "GenDec: A robust generative Question-decomposition method for Multi-hop reasoning", "year": 2024}, {"arxiv_id": "2403.12393", "title": "Dr3: Ask Large Language Models Not to Give Off-Topic Answers in Open Domain Multi-Hop Question Answering", "year": 2024}, {"arxiv_id": "2210.03629", "title": "ReAct: Synergizing Reasoning and Acting in Language Models", "year": 2022}, {"arxiv_id": "2406.15319", "title": "LongRAG: Enhancing Retrieval-Augmented Generation with Long-context LLMs", "year": 2024}, {"arxiv_id": "2310.05915", "title": "FireAct: Toward Language Agent Fine-tuning", "year": 2023}, {"arxiv_id": "2402.11924", "title": "Cofca: A Step-Wise Counterfactual Multi-hop QA benchmark", "year": 2024}, {"arxiv_id": "2404.09129", "title": "When Hindsight is Not 20/20: Testing Limits on Reflective Thinking in Large Language Models", "year": 2024}, {"arxiv_id": "2407.10245", "title": "GenSco: Can Question Decomposition based Passage Alignment improve Question Answering?", "year": 2024}, {"arxiv_id": "2212.10509", "title": "Interleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-Intensive Multi-Step Questions", "year": 2022}, {"arxiv_id": "2405.13021", "title": "IM-RAG: Multi-Round Retrieval-Augmented Generation Through Learning Inner Monologues", "year": 2024}, {"arxiv_id": "2403.10131", "title": "RAFT: Adapting Language Model to Domain Specific RAG", "year": 2024}, {"arxiv_id": "2210.01296", "title": "Recitation-Augmented Language Models", "year": 2022}, {"arxiv_id": "2407.13101", "title": "Retrieve, Summarize, Plan: Advancing Multi-hop Question Answering with an Iterative Approach", "year": 2024}, {"arxiv_id": "2210.16865", "title": "Learning to Decompose: Hypothetical Question Decomposition Based on Comparable Texts", "year": 2022}, {"arxiv_id": "2310.04406", "title": "Language Agent Tree Search Unifies Reasoning Acting and Planning in Language Models", "year": 2023}, {"arxiv_id": "2305.18323", "title": "ReWOO: Decoupling Reasoning from Observations for Efficient Augmented Language Models", "year": 2023}, {"arxiv_id": "2402.19350", "title": "Prompting Explicit and Implicit Knowledge for Multi-hop Question Answering Based on Human Reading Process", "year": 2024}, {"arxiv_id": "2305.03130", "title": "Chain-of-Skills: A Configurable Model for Open-domain Question Answering", "year": 2023}, {"arxiv_id": "2404.03414", "title": "Can Small Language Models Help Large Language Models Reason Better?: LM-Guided Chain-of-Thought", "year": 2024}, {"arxiv_id": "2205.12650", "title": "Few-shot Reranking for Multi-hop QA via Language Model Prompting", "year": 2022}, {"arxiv_id": "2305.14323", "title": "ChatCoT: Tool-Augmented Chain-of-Thought Reasoning on Chat-based Large Language Models", "year": 2023}, {"arxiv_id": "2304.13007", "title": "Answering Questions by Meta-Reasoning over Multiple Chains of Thought", "year": 2023}], "gt_label": [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], "date": "2024-07", "source": "PASA_RealScholar", "qid": "RealScholarQuery_6", "valid": true}
{"query": "Show me research on the long video description. Here, long videos are defined as those with a duration of at least several minutes.", "cited_paper": [{"arxiv_id": "2402.13250", "title": "Video ReCap: Recursive Captioning of Hour-Long Videos", "year": 2024}, {"arxiv_id": "2406.14515", "title": "MMBench-Video: A Long-Form Multi-Shot Benchmark for Holistic Video Understanding", "year": 2024}, {"arxiv_id": "2405.16009", "title": "Streaming Long Video Understanding with Large Language Models", "year": 2024}, {"arxiv_id": "2201.10990", "title": "Learning To Recognize Procedural Activities with Distant Supervision", "year": 2022}, {"arxiv_id": "2310.04900", "title": "HowToCaption: Prompting LLMs to Transform Video Annotations at Scale", "year": 2023}, {"arxiv_id": "2407.15754", "title": "LongVideoBench: A Benchmark for Long-context Interleaved Video-Language Understanding", "year": 2024}, {"arxiv_id": "2204.02968", "title": "Temporal Alignment Networks for Long-term Video", "year": 2022}, {"arxiv_id": "2409.06299", "title": "Enhancing Long Video Understanding via Hierarchical Event-Based Memory", "year": 2024}, {"arxiv_id": "2406.12846", "title": "DrVideo: Document Retrieval Based Long Video Understanding", "year": 2024}, {"arxiv_id": "2210.06031", "title": "Long-Form Video-Language Pre-Training with Multimodal Temporal Contrastive Learning", "year": 2022}, {"arxiv_id": "2402.08268", "title": "World Model on Million-Length Video And Language With Blockwise RingAttention", "year": 2024}, {"arxiv_id": "2404.04346", "title": "Koala: Key frame-conditioned long video-LLM", "year": 2024}, {"arxiv_id": "1705.00754", "title": "Dense-Captioning Events in Videos", "year": 2017}, {"arxiv_id": "2406.17880", "title": "MLLM as Video Narrator: Mitigating Modality Imbalance in Video Moment Retrieval", "year": 2024}, {"arxiv_id": "2312.02051", "title": "TimeChat: A Time-sensitive Multimodal Large Language Model for Long Video Understanding", "year": 2023}, {"arxiv_id": "2404.05726", "title": "MA-LMM: Memory-Augmented Large Multimodal Model for Long-Term Video Understanding", "year": 2024}, {"arxiv_id": "2404.03384", "title": "LongVLM: Efficient Long Video Understanding via Large Language Models", "year": 2024}, {"arxiv_id": "2406.04264", "title": "MLVU: Benchmarking Multi-task Long Video Understanding", "year": 2024}, {"arxiv_id": "2408.15542", "title": "Kangaroo: A Powerful Video-Language Model Supporting Long-context Video Input", "year": 2024}, {"arxiv_id": "2312.05269", "title": "LifelongMemory: Leveraging LLMs for Answering Queries in Long-form Egocentric Videos", "year": 2023}, {"arxiv_id": "2406.04325", "title": "ShareGPT4Video: Improving Video Understanding and Generation with Better Captions", "year": 2024}, {"arxiv_id": "2402.13546", "title": "LLMs Meet Long Video: Advancing Long Video Question Answering with An Interactive Visual Adapter in LLMs", "year": 2024}, {"arxiv_id": "2307.16449", "title": "MovieChat: From Dense Token to Sparse Memory for Long Video Understanding", "year": 2023}, {"arxiv_id": "2403.10517", "title": "VideoAgent: Long-form Video Understanding with Large Language Model as Agent", "year": 2024}, {"arxiv_id": "2301.11507", "title": "Semi-Parametric Video-Grounded Text Generation", "year": 2023}, {"arxiv_id": "2311.17043", "title": "LLaMA-VID: An Image is Worth 2 Tokens in Large Language Models", "year": 2023}, {"arxiv_id": "2312.07395", "title": "A Simple Recipe for Contrastively Pre-training Video-First Encoders Beyond 16 Frames", "year": 2023}, {"arxiv_id": "2310.19773", "title": "MM-VID: Advancing Video Understanding with GPT-4V(ision)", "year": 2023}, {"arxiv_id": "2311.17435", "title": "MM-Narrator: Narrating Long-form Videos with Multimodal In-Context Learning", "year": 2023}, {"arxiv_id": "2303.12060", "title": "VideoXum: Cross-modal Visual and Textural Summarization of Videos", "year": 2023}, {"arxiv_id": "2404.01297", "title": "Streaming Dense Video Captioning", "year": 2024}], "gt_label": [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], "date": "2024-09", "source": "PASA_RealScholar", "qid": "RealScholarQuery_7", "valid": true}
{"query": "Do you know some papers about using reward shaping methods to train large language model agent.", "cited_paper": [{"arxiv_id": "2401.07382", "title": "Beyond Sparse Rewards: Enhancing Reinforcement Learning with Language Model Critique in Text Generation", "year": 2024}, {"arxiv_id": "2308.02151", "title": "Retroformer: Retrospective Large Language Agents with Policy Gradient Optimization", "year": 2023}, {"arxiv_id": "2303.00001", "title": "Reward Design with Language Models", "year": 2023}, {"arxiv_id": "2205.13636", "title": "Quark: Controllable Text Generation with Reinforced Unlearning", "year": 2022}, {"arxiv_id": "2402.00782", "title": "Dense Reward for Free in Reinforcement Learning from Human Feedback", "year": 2024}, {"arxiv_id": "2308.12270", "title": "Language Reward Modulation for Pretraining Reinforcement Learning", "year": 2023}, {"arxiv_id": "2306.01693", "title": "Fine-Grained Human Feedback Gives Better Rewards for Language Model Training", "year": 2023}, {"arxiv_id": "2401.10020", "title": "Self-Rewarding Language Models", "year": 2024}, {"arxiv_id": "2404.11999", "title": "Token-level Direct Preference Optimization", "year": 2024}, {"arxiv_id": "2405.15194", "title": "Extracting Heuristics from Large Language Models for Reward Shaping in Reinforcement Learning", "year": 2024}, {"arxiv_id": "2310.00166", "title": "Motif: Intrinsic Motivation from Artificial Intelligence Feedback", "year": 2023}, {"arxiv_id": "2409.12798", "title": "Assessing the Zero-Shot Capabilities of LLMs for Action Evaluation in RL", "year": 2024}, {"arxiv_id": "2405.14734", "title": "SimPO: Simple Preference Optimization with a Reference-Free Reward", "year": 2024}, {"arxiv_id": "2312.08935", "title": "Math-Shepherd: Verify and Reinforce LLMs Step-by-step without Human Annotations", "year": 2023}, {"arxiv_id": "2302.06692", "title": "Guiding Pretraining in Reinforcement Learning with Large Language Models", "year": 2023}], "gt_label": [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], "date": "2024-09", "source": "PASA_RealScholar", "qid": "RealScholarQuery_8", "valid": true}
{"query": "Give me papers about how to rank search results by the use of LLM.", "cited_paper": [{"arxiv_id": "2311.01555", "title": "Instruction Distillation Makes Large Language Models Efficient Zero-shot Rankers", "year": 2023}, {"arxiv_id": "2310.14122", "title": "Beyond Yes and No: Improving Zero-Shot LLM Rankers via Scoring Fine-Grained Relevance Labels", "year": 2023}, {"arxiv_id": "2306.17563", "title": "Large Language Models are Effective Text Rankers with Pairwise Ranking Prompting", "year": 2023}, {"arxiv_id": "2310.09497", "title": "A Setwise Approach for Effective and Highly Efficient Zero-shot Ranking with Large Language Models", "year": 2023}, {"arxiv_id": "2309.15088", "title": "RankVicuna: Zero-Shot Listwise Document Reranking with Open-Source Large Language Models", "year": 2023}, {"arxiv_id": "2310.14408", "title": "PaRaDe: Passage Ranking using Demonstrations with Large Language Models", "year": 2023}, {"arxiv_id": "2304.09542", "title": "Is ChatGPT Good at Search? Investigating Large Language Models as Re-Ranking Agents", "year": 2023}, {"arxiv_id": "2305.08845", "title": "Large Language Models are Zero-Shot Rankers for Recommender Systems", "year": 2023}, {"arxiv_id": "2406.11678", "title": "TourRank: Utilizing Large Language Models for Documents Ranking with a Tournament-Inspired Strategy", "year": 2024}, {"arxiv_id": "2301.10521", "title": "ExaRanker: Explanation-Augmented Neural Ranker", "year": 2023}, {"arxiv_id": "2407.02485", "title": "RankRAG: Unifying Context Ranking with Retrieval-Augmented Generation in LLMs", "year": 2024}, {"arxiv_id": "2403.19181", "title": "Make Large Language Model a Better Ranker", "year": 2024}, {"arxiv_id": "2406.00231", "title": "LLM-RankFusion: Mitigating Intrinsic Inconsistency in LLM-based Ranking", "year": 2024}, {"arxiv_id": "2406.13331", "title": "Improving Zero-shot LLM Re-Ranker with Risk Minimization", "year": 2024}, {"arxiv_id": "2305.02156", "title": "Zero-Shot Listwise Document Reranking with a Large Language Model", "year": 2023}, {"arxiv_id": "2404.11791", "title": "Consolidating Ranking and Relevance Predictions of Large Language Models through Post-Processing", "year": 2024}, {"arxiv_id": "2406.18740", "title": "Re-Ranking Step by Step: Investigating Pre-Filtering for Re-Ranking with Large Language Models", "year": 2024}, {"arxiv_id": "2406.00247", "title": "Large Language Models for Relevance Judgment in Product Search", "year": 2024}, {"arxiv_id": "2404.18424", "title": "PromptReps: Prompting Large Language Models to Generate Dense and Sparse Representations for Zero-Shot Document Retrieval", "year": 2024}, {"arxiv_id": "2405.20654", "title": "Passage-specific Prompt Tuning for Passage Reranking in Question Answering with Large Language Models", "year": 2024}, {"arxiv_id": "2407.00128", "title": "When Search Engine Services meet Large Language Models: Visions and Challenges", "year": 2024}, {"arxiv_id": "2312.02724", "title": "RankZephyr: Effective and Robust Zero-Shot Listwise Reranking is a Breeze!", "year": 2023}, {"arxiv_id": "2312.02969", "title": "Rank-without-GPT: Building GPT-Independent Listwise Rerankers on Open-Source Large Language Models", "year": 2023}, {"arxiv_id": "2401.06311", "title": "Exploring the Best Practices of Query Expansion with Large Language Models", "year": 2024}, {"arxiv_id": "2305.13729", "title": "Discrete Prompt Optimization via Constrained Generation for Zero-shot Re-ranker", "year": 2023}, {"arxiv_id": "2402.17497", "title": "REAR: A Relevance-Aware Retrieval-Augmented Framework for Open-Domain Question Answering", "year": 2024}, {"arxiv_id": "2312.15450", "title": "Agent4Ranking: Semantic Robust Ranking via Personalized Query Rewriting Using Multi-agent LLM", "year": 2023}, {"arxiv_id": "2406.15657", "title": "FIRST: Faster Improved Listwise Reranking with Single Token Decoding", "year": 2024}, {"arxiv_id": "2402.04853", "title": "Leveraging LLMs for Unsupervised Dense Retriever Ranking", "year": 2024}, {"arxiv_id": "2309.06991", "title": "Unsupervised Contrast-Consistent Ranking with Language Models", "year": 2023}, {"arxiv_id": "2403.18093", "title": "Enhancing Legal Document Retrieval: A Multi-Phase Approach with Large Language Models", "year": 2024}, {"arxiv_id": "2310.07712", "title": "Found in the Middle: Permutation Self-Consistency Improves Listwise Ranking in Large Language Models", "year": 2023}, {"arxiv_id": "2310.08319", "title": "Fine-Tuning LLaMA for Multi-Stage Text Retrieval", "year": 2023}, {"arxiv_id": "2309.07606", "title": "Zero-shot Audio Topic Reranking using Large Language Models", "year": 2023}, {"arxiv_id": "2305.02182", "title": "Uncovering ChatGPT's Capabilities in Recommender Systems", "year": 2023}, {"arxiv_id": "2402.10548", "title": "Cognitive Personalized Search Integrating Large Language Models with an Efficient Memory Mechanism", "year": 2024}, {"arxiv_id": "2409.17460", "title": "Towards More Relevant Product Search Ranking Via Large Language Models: An Empirical Study", "year": 2024}, {"arxiv_id": "2306.01599", "title": "Pretrained Language Model based Web Search Ranking: From Relevance to Satisfaction", "year": 2023}, {"arxiv_id": "2310.13243", "title": "Open-source Large Language Models are Strong Zero-shot Query Likelihood Models for Document Ranking", "year": 2023}], "gt_label": [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], "date": "2024-09", "source": "PASA_RealScholar", "qid": "RealScholarQuery_9", "valid": true}
{"query": "Is there any work that analyzes the scaling law of the multi-module models, such as video-text, image-text models?", "cited_paper": [{"arxiv_id": "2010.14701", "title": "Scaling Laws for Autoregressive Generative Modeling", "year": 2020}, {"arxiv_id": "2301.03728", "title": "Scaling Laws for Generative Mixed-Modal Language Models", "year": 2023}, {"arxiv_id": "2212.07143", "title": "Reproducible scaling laws for contrastive language-image learning", "year": 2022}, {"arxiv_id": "2408.11039", "title": "Transfusion: Predict the Next Token and Diffuse Images with One Multi-Modal Model", "year": 2024}, {"arxiv_id": "2111.12233", "title": "Scaling Up Vision-Language Pre-training for Image Captioning", "year": 2021}, {"arxiv_id": "2211.07636", "title": "EVA: Exploring the Limits of Masked Visual Representation Learning at Scale", "year": 2022}, {"arxiv_id": "2209.06794", "title": "PaLI: A Jointly-Scaled Multilingual Language-Image Model", "year": 2022}, {"arxiv_id": "2409.06754", "title": "Scaling Law Hypothesis for Multimodal Model", "year": 2024}, {"arxiv_id": "2309.09958", "title": "An Empirical Study of Scaling Instruct-Tuned Large Multimodal Models", "year": 2023}, {"arxiv_id": "2402.05935", "title": "SPHINX-X: Scaling Data and Parameters for a Family of Multi-modal Large Language Models", "year": 2024}, {"arxiv_id": "2206.02770", "title": "Multimodal Contrastive Learning with LIMoE: the Language-Image Mixture of Experts", "year": 2022}, {"arxiv_id": "2205.06230", "title": "Simple Open-Vocabulary Object Detection with Vision Transformers", "year": 2022}, {"arxiv_id": "2212.00794", "title": "Scaling Language-Image Pre-training via Masking", "year": 2022}, {"arxiv_id": "2403.03206", "title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "year": 2024}], "gt_label": [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], "date": "2024-09", "source": "PASA_RealScholar", "qid": "RealScholarQuery_10", "valid": true}
{"query": "Give me all visual-LLM models that are MoE architecture", "cited_paper": [{"arxiv_id": "2312.00968", "title": "Omni-SMoLA: Boosting Generalist Multimodal Models with Soft Mixture of Low-rank Experts", "year": 2023}, {"arxiv_id": "2401.15947", "title": "MoE-LLaVA: Mixture of Experts for Large Vision-Language Models", "year": 2024}, {"arxiv_id": "2405.05949", "title": "CuMo: Scaling Multimodal LLM with Co-Upcycled Mixture-of-Experts", "year": 2024}, {"arxiv_id": "2407.14177", "title": "EVLM: An Efficient Vision-Language Model for Visual Understanding", "year": 2024}, {"arxiv_id": "2206.02770", "title": "Multimodal Contrastive Learning with LIMoE: the Language-Image Mixture of Experts", "year": 2022}, {"arxiv_id": "2312.12379", "title": "Mixture of Cluster-conditional LoRA Experts for Vision-language Instruction Tuning", "year": 2023}, {"arxiv_id": "2407.12709", "title": "MoME: Mixture of Multimodal Experts for Generalist Multimodal Large Language Models", "year": 2024}, {"arxiv_id": "2405.11273", "title": "Uni-MoE: Scaling Unified Multimodal LLMs with Mixture of Experts", "year": 2024}, {"arxiv_id": "2406.19905", "title": "Solving Token Gradient Conflict in Mixture-of-Experts for Large Vision-Language Model", "year": 2024}, {"arxiv_id": "2111.02358", "title": "VLMo: Unified Vision-Language Pre-Training with Mixture-of-Modality-Experts", "year": 2021}, {"arxiv_id": "2308.11971", "title": "EVE: Efficient Vision-Language Pre-training with Masked Prediction and Modality-Aware MoE", "year": 2023}, {"arxiv_id": "2403.11549", "title": "Boosting Continual Learning of Vision-Language Models via Mixture-of-Experts Adapters", "year": 2024}, {"arxiv_id": "2407.21770", "title": "MoMa: Efficient Early-Fusion Pre-training with Mixture of Modality-Aware Experts", "year": 2024}, {"arxiv_id": "2401.16160", "title": "LLaVA-MoLE: Sparse Mixture of LoRA Experts for Mitigating Data Conflicts in Instruction Finetuning MLLMs", "year": 2024}, {"arxiv_id": "2403.18814", "title": "Mini-Gemini: Mining the Potential of Multi-modality Vision Language Models", "year": 2024}, {"arxiv_id": "2408.03511", "title": "MoExtend: Tuning New Experts for Modality and Task Extension", "year": 2024}, {"arxiv_id": "2303.07226", "title": "Scaling Vision-Language Models with Sparse Mixture of Experts", "year": 2023}, {"arxiv_id": "2409.07267", "title": "MiniDrive: More Efficient Vision-Language Models with Multi-Level 2D Features as Text Tokens for Autonomous Driving", "year": 2024}, {"arxiv_id": "2402.14891", "title": "LLMBind: A Unified Modality-Task Integration Framework", "year": 2024}, {"arxiv_id": "2403.09611", "title": "MM1: Methods, Analysis & Insights from Multimodal LLM Pre-training", "year": 2024}], "gt_label": [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], "date": "2024-09", "source": "PASA_RealScholar", "qid": "RealScholarQuery_11", "valid": true}
{"query": "What papers discuss the use of transformer architecture in 3d video generation", "cited_paper": [{"arxiv_id": "2104.10157", "title": "VideoGPT: Video Generation using VQ-VAE and Transformers", "year": 2021}, {"arxiv_id": "2305.13311", "title": "VDT: General-purpose Video Diffusion Transformers via Mask Modeling", "year": 2023}, {"arxiv_id": "2209.04066", "title": "TEACH: Temporal Action Composition for 3D Humans", "year": 2022}, {"arxiv_id": "2203.09457", "title": "Look Outside the Room: Synthesizing A Consistent Long-Term 3D Scene Video from A Single Image", "year": 2022}, {"arxiv_id": "2212.05199", "title": "MAGVIT: Masked Generative Video Transformer", "year": 2022}, {"arxiv_id": "2203.13055", "title": "Bailando: 3D Dance Generation by Actor-Critic GPT with Choreographic Memory", "year": 2022}, {"arxiv_id": "2407.21705", "title": "Tora: Trajectory-oriented Diffusion Transformer for Video Generation", "year": 2024}, {"arxiv_id": "2103.10206", "title": "DanceFormer: Music Conditioned 3D Dance Generation with Parametric Motion Transformer", "year": 2021}, {"arxiv_id": "2310.20240", "title": "Breathing Life into Faces: Speech-driven 3D Facial Animation with Natural Head Pose and Detailed Shape", "year": 2023}, {"arxiv_id": "2406.17601", "title": "Director3D: Real-world Camera Trajectory and 3D Scene Generation from Text", "year": 2024}, {"arxiv_id": "2405.05945", "title": "Lumina-T2X: Transforming Text into Any Modality, Resolution, and Duration via Flow-based Large Diffusion Transformers", "year": 2024}, {"arxiv_id": "2405.17405", "title": "Human4DiT: 360-degree Human Video Generation with 4D Diffusion Transformer", "year": 2024}, {"arxiv_id": "2101.08779", "title": "AI Choreographer: Music Conditioned 3D Dance Generation with AIST++", "year": 2021}, {"arxiv_id": "2405.18991", "title": "EasyAnimate: A High-Performance Long Video Generation Method based on Transformer Architecture", "year": 2024}, {"arxiv_id": "2111.12417", "title": "NWA: Visual Synthesis Pre-training for Neural visUal World creAtion", "year": 2021}, {"arxiv_id": "2407.12781", "title": "VD3D: Taming Large Video Diffusion Transformers for 3D Camera Control", "year": 2024}, {"arxiv_id": "2204.03638", "title": "Long Video Generation with Time-Agnostic VQGAN and Time-Sensitive Transformer", "year": 2022}], "gt_label": [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], "date": "2024-07", "source": "PASA_RealScholar", "qid": "RealScholarQuery_12", "valid": true}
{"query": "Provide papers demonstrating that the self-correction of LLMs does not enhance their performance.", "cited_paper": [{"arxiv_id": "2310.08118", "title": "Can Large Language Models Really Improve by Self-critiquing Their Own Plans?", "year": 2023}, {"arxiv_id": "2311.08596", "title": "Are You Sure? Challenging LLMs Leads to Performance Drops in The FlipFlop Experiment", "year": 2023}, {"arxiv_id": "2311.07954", "title": "A Closer Look at the Self-Verification Abilities of Large Language Models in Logical Reasoning", "year": 2023}, {"arxiv_id": "2404.04298", "title": "SELF-[IN]CORRECT: LLMs Struggle with Discriminating Self-Generated Responses", "year": 2024}, {"arxiv_id": "2310.01798", "title": "Large Language Models Cannot Self-Correct Reasoning Yet", "year": 2023}, {"arxiv_id": "2306.09896", "title": "Is Self-Repair a Silver Bullet for Code Generation?", "year": 2023}, {"arxiv_id": "2402.19475", "title": "The Counterfeit Conundrum: Can Code Language Models Grasp the Nuances of Their Incorrect Generations?", "year": 2024}, {"arxiv_id": "2406.01297", "title": "When Can LLMs Actually Correct Their Own Mistakes? A Critical Survey of Self-Correction of LLMs", "year": 2024}, {"arxiv_id": "2406.02378", "title": "On the Intrinsic Self-Correction Capability of LLMs: Uncertainty and Latent Concept", "year": 2024}, {"arxiv_id": "2402.11436", "title": "Pride and Prejudice: LLM Amplifies Self-Bias in Self-Refinement", "year": 2024}], "gt_label": [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], "date": "2024-06", "source": "PASA_RealScholar", "qid": "RealScholarQuery_13", "valid": true}
{"query": "Find papers that use LLMs or LLM-based agents to automatically write surveys or summaries for multiple scholarly documents.", "cited_paper": [{"arxiv_id": "2406.10252", "title": "AutoSurvey: Large Language Models Can Automatically Write Surveys", "year": 2024}, {"arxiv_id": "2408.07884", "title": "Instruct Large Language Models to Generate Scientific Literature Survey Step by Step", "year": 2024}, {"arxiv_id": "2402.01788", "title": "LitLLM: A Toolkit for Scientific Literature Review", "year": 2024}, {"arxiv_id": "2408.13450", "title": "vitaLITy 2: Reviewing Academic Literature Using Large Language Models", "year": 2024}, {"arxiv_id": "2305.06299", "title": "Summarizing, Simplifying, and Synthesizing Medical Evidence Using GPT-3 (with Varying Success)", "year": 2023}, {"arxiv_id": "2403.02574", "title": "ChatCite: LLM Agent with Human Workflow Guidance for Comparative Literature Summary", "year": 2024}, {"arxiv_id": "2403.08399", "title": "System for systematic literature review using multiple AI agents: Concept and an empirical evaluation", "year": 2024}, {"arxiv_id": "2404.08680", "title": "Automating Research Synthesis with Domain-Specific Large Language Model Fine-Tuning", "year": 2024}], "gt_label": [1, 1, 1, 1, 1, 1, 1, 1], "date": "2024-09", "source": "PASA_RealScholar", "qid": "RealScholarQuery_14", "valid": true}
{"query": "Provide papers claiming that reinforcement learning can negatively impact the performance of supervised fine-tuned LLMs.", "cited_paper": [{"arxiv_id": "2310.06452", "title": "Understanding the Effects of RLHF on LLM Generalisation and Diversity", "year": 2023}, {"arxiv_id": "2304.11082", "title": "Fundamental Limitations of Alignment in Large Language Models", "year": 2023}, {"arxiv_id": "2305.17608", "title": "Reward Collapse in Aligning Large Language Models", "year": 2023}, {"arxiv_id": "2212.09251", "title": "Discovering Language Model Behaviors with Model-Written Evaluations", "year": 2022}, {"arxiv_id": "2310.20703", "title": "Vanishing Gradients in Reinforcement Finetuning of Language Models", "year": 2023}], "gt_label": [1, 1, 1, 1, 1], "date": "2023-10", "source": "PASA_RealScholar", "qid": "RealScholarQuery_15", "valid": true}
{"query": "Find papers on trigger-free document-level event extraction methods that do not use human-annotated triggers.", "cited_paper": [{"arxiv_id": "2208.09659", "title": "Trigger-free Event Detection via Derangement Reading Comprehension", "year": 2022}, {"arxiv_id": "2303.14452", "title": "COFFEE: A Contrastive Oracle-Free Framework for Event Extraction", "year": 2023}, {"arxiv_id": "1904.07535", "title": "Doc2EDAG: An End-to-End Document-level Framework for Chinese Financial Event Extraction", "year": 2019}, {"arxiv_id": "2112.06013", "title": "Efficient Document-level Event Extraction via Pseudo-Trigger-aware Pruned Complete Graph", "year": 2021}, {"arxiv_id": "2305.18926", "title": "Document-Level Multi-Event Extraction with Event Proxy Nodes and Hausdorff Distance Minimization", "year": 2023}, {"arxiv_id": "2206.03377", "title": "RAAT: Relation-Augmented Attention Transformer for Relation Modeling in Document-Level Event Extraction", "year": 2022}, {"arxiv_id": "2202.03092", "title": "Document-Level Event Extraction via Human-Like Reading Process", "year": 2022}, {"arxiv_id": "2105.14924", "title": "Document-level Event Extraction via Heterogeneous Graph-based Interaction Model with a Tracker", "year": 2021}], "gt_label": [1, 1, 1, 1, 1, 1, 1, 1], "date": "2023-05", "source": "PASA_RealScholar", "qid": "RealScholarQuery_16", "valid": true}
{"query": "Provide papers explaining why the in-context learning performance of LLMs cannot surpass that of supervised fine-tuned small language models in information extraction tasks, such as NER, RE, and EE.", "cited_paper": [{"arxiv_id": "2203.08410", "title": "Thinking about GPT-3 In-Context Learning for Biomedical IE? Think Again", "year": 2022}, {"arxiv_id": "2311.08993", "title": "When does In-context Learning Fall Short and Why? A Study on Specification-Heavy Tasks", "year": 2023}, {"arxiv_id": "2404.03598", "title": "Intent Detection and Entity Extraction from BioMedical Literature", "year": 2024}, {"arxiv_id": "2404.00457", "title": "MetaIE: Distilling a Meta Model from LLM for All Kinds of Information Extraction Tasks", "year": 2024}, {"arxiv_id": "2303.03836", "title": "Exploring the Feasibility of ChatGPT for Event Extraction", "year": 2023}, {"arxiv_id": "2310.05066", "title": "Guideline Learning for In-context Information Extraction", "year": 2023}, {"arxiv_id": "2306.09719", "title": "Pushing the Limits of ChatGPT on NLP Tasks", "year": 2023}, {"arxiv_id": "2303.08559", "title": "Large Language Model Is Not a Good Few-shot Information Extractor, but a Good Reranker for Hard Samples!", "year": 2023}, {"arxiv_id": "2401.14556", "title": "Looking Right is Sometimes Right: Investigating the Capabilities of Decoder-only LLMs for Sequence Labeling", "year": 2024}], "gt_label": [1, 1, 1, 1, 1, 1, 1, 1, 1], "date": "2024-04", "source": "PASA_RealScholar", "qid": "RealScholarQuery_17", "valid": true}
{"query": "Can LLMs detect LLM-generated text in a zero-shot manner? Do they perform better than supervised fine-tuned small classification models? Provide related papers.", "cited_paper": [{"arxiv_id": "2310.13606", "title": "MULTITuDE: Large-Scale Multilingual Machine-Generated Text Detection Benchmark", "year": 2023}, {"arxiv_id": "2405.04286", "title": "Who Wrote This? The Key to Zero-Shot LLM-Generated Text Detection Is GECScore", "year": 2024}, {"arxiv_id": "2409.16914", "title": "Zero-Shot Detection of LLM-Generated Text using Token Cohesiveness", "year": 2024}, {"arxiv_id": "2310.14479", "title": "DetectGPT-SC: Improving Detection of Text Generated by Large Language Models through Self-Consistency with Masked Predictions", "year": 2023}, {"arxiv_id": "2310.15515", "title": "Fighting Fire with Fire: The Dual Role of LLMs in Crafting and Detecting Elusive Disinformation", "year": 2023}], "gt_label": [1, 1, 1, 1, 1], "date": "2024-09", "source": "PASA_RealScholar", "qid": "RealScholarQuery_18", "valid": true}
{"query": "Provide papers on methods that protect the generation quality of LLMs under vocabulary watermarking settings.", "cited_paper": [{"arxiv_id": "2310.07710", "title": "A Resilient and Accessible Distribution-Preserving Watermark for Large Language Models", "year": 2023}, {"arxiv_id": "2310.12362", "title": "REMARK-LLM: A Robust and Efficient Watermarking Framework for Generative Large Language Models", "year": 2023}, {"arxiv_id": "2403.19548", "title": "WaterJudge: Quality-Detection Trade-off when Watermarking Large Language Models", "year": 2024}, {"arxiv_id": "2402.18059", "title": "Token-Specific Watermarking with Enhanced Detectability and Semantic Coherence for Large Language Models", "year": 2024}, {"arxiv_id": "2209.08773", "title": "CATER: Intellectual Property Protection on Text Generation APIs via Conditional Watermarks", "year": 2022}, {"arxiv_id": "2112.02701", "title": "Protecting Intellectual Property of Language Generation APIs with Lexical Watermark", "year": 2021}, {"arxiv_id": "2311.09832", "title": "WatME: Towards Lossless Watermarking Through Lexical Redundancy", "year": 2023}, {"arxiv_id": "2306.17439", "title": "Provable Robust Watermarking for AI-Generated Text", "year": 2023}, {"arxiv_id": "2401.13927", "title": "Adaptive Text Watermark for Large Language Models", "year": 2024}, {"arxiv_id": "2301.10226", "title": "A Watermark for Large Language Models", "year": 2023}, {"arxiv_id": "2407.13803", "title": "Less is More: Sparse Watermarking in LLMs with Enhanced Text Quality", "year": 2024}, {"arxiv_id": "2312.17295", "title": "Optimizing watermarks for large language models", "year": 2023}, {"arxiv_id": "2406.14517", "title": "PostMark: A Robust Blackbox Watermark for Large Language Models", "year": 2024}, {"arxiv_id": "2310.00833", "title": "Necessary and Sufficient Watermark for Large Language Models", "year": 2023}, {"arxiv_id": "2306.09194", "title": "Undetectable Watermarks for Language Models", "year": 2023}, {"arxiv_id": "2302.03162", "title": "Protecting Language Generation Models via Invisible Watermarking", "year": 2023}, {"arxiv_id": "2310.08920", "title": "Embarrassingly Simple Text Watermarks", "year": 2023}, {"arxiv_id": "2405.14604", "title": "Watermarking Low-entropy Generation for Large Language Models: An Unbiased and Low-risk Method", "year": 2024}, {"arxiv_id": "2312.00273", "title": "Mark My Words: Analyzing and Evaluating Language Model Watermarks", "year": 2023}, {"arxiv_id": "2404.02138", "title": "Topic-Based Watermarks for Large Language Models", "year": 2024}, {"arxiv_id": "2406.10281", "title": "Watermarking Language Models with Error Correcting Codes", "year": 2024}, {"arxiv_id": "2409.09739", "title": "PersonaMark: Personalized LLM watermarking for model protection and user attribution", "year": 2024}, {"arxiv_id": "2009.03015", "title": "Adversarial Watermarking Transformer: Towards Tracing Text Provenance with Data Hiding", "year": 2020}, {"arxiv_id": "2405.02365", "title": "ModelShield: Adaptive and Robust Watermark against Model Extraction Attack", "year": 2024}, {"arxiv_id": "2311.09668", "title": "Improving the Generation Quality of Watermarked Large Language Models via Word Importance Scoring", "year": 2023}, {"arxiv_id": "2403.04808", "title": "WaterMax: breaking the LLM watermark detectability-robustness-quality trade-off", "year": 2024}, {"arxiv_id": "2307.13808", "title": "Watermarking Conditional Text Generation for AI Detection: Unveiling Challenges and a Semantic-Aware Watermark Remedy", "year": 2023}, {"arxiv_id": "2307.15992", "title": "Towards Codable Watermarking for Injecting Multi-bits Information to LLMs", "year": 2023}, {"arxiv_id": "2310.06356", "title": "A Semantic Invariant Robust Watermark for Large Language Models", "year": 2023}, {"arxiv_id": "2401.16820", "title": "Provably Robust Multi-bit Watermarking for AI-generated Text", "year": 2024}, {"arxiv_id": "2308.00221", "title": "Advancing Beyond Identification: Multi-bit Watermark for Large Language Models", "year": 2023}, {"arxiv_id": "2401.06829", "title": "Cross-Attention Watermarking of Large Language Models", "year": 2024}, {"arxiv_id": "2310.03991", "title": "SemStamp: A Semantic Watermark with Paraphrastic Robustness for Text Generation", "year": 2023}, {"arxiv_id": "2305.15060", "title": "Who Wrote this Code? Watermarking for Code Generation", "year": 2023}], "gt_label": [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], "date": "2024-09", "source": "PASA_RealScholar", "qid": "RealScholarQuery_19", "valid": true}
{"query": "Find papers supporting the claim that knowledgeable LLMs have sufficient inductive capacity to analyze the relationships between multiple papers and systematically write a survey on them.", "cited_paper": [{"arxiv_id": "2403.08399", "title": "System for systematic literature review using multiple AI agents: Concept and an empirical evaluation", "year": 2024}, {"arxiv_id": "2402.13426", "title": "Explaining Relationships Among Research Papers", "year": 2024}, {"arxiv_id": "2408.07884", "title": "Instruct Large Language Models to Generate Scientific Literature Survey Step by Step", "year": 2024}, {"arxiv_id": "2408.13450", "title": "vitaLITy 2: Reviewing Academic Literature Using Large Language Models", "year": 2024}], "gt_label": [1, 1, 1, 1], "date": "2024-08", "source": "PASA_RealScholar", "qid": "RealScholarQuery_20", "valid": true}
{"query": "Search for papers related to large language models that demonstrate how the same prompt with different responses can improve the performance of the SFT model.", "cited_paper": [{"arxiv_id": "2403.07230", "title": "Curry-DPO: Enhancing Alignment using Curriculum Learning & Ranked Preferences", "year": 2024}, {"arxiv_id": "2402.10958", "title": "Relative Preference Optimization: Enhancing LLM Alignment through Contrasting Responses across Identical and Diverse Prompts", "year": 2024}], "gt_label": [1, 1], "date": "2024-03", "source": "PASA_RealScholar", "qid": "RealScholarQuery_21", "valid": true}
{"query": "Papers on solving common sense problems in machine translation.", "cited_paper": [{"arxiv_id": "2402.10699", "title": "Thinker-DDM: Modeling Deliberation for Machine Translation with a Drift-Diffusion Process", "year": 2024}, {"arxiv_id": "2305.19118", "title": "Encouraging Divergent Thinking in Large Language Models through Multi-Agent Debate", "year": 2023}, {"arxiv_id": "2112.10668", "title": "Few-shot Learning with Multilingual Language Models", "year": 2021}], "gt_label": [1, 1, 1], "date": "2024-02", "source": "PASA_RealScholar", "qid": "RealScholarQuery_22", "valid": true}
{"query": "Show me papers utilizing reinforcement learning to optimize diffusion models for video generation.", "cited_paper": [{"arxiv_id": "2407.08737", "title": "Video Diffusion Alignment via Reward Gradients", "year": 2024}, {"arxiv_id": "2312.12490", "title": "InstructVideo: Instructing Video Diffusion Models with Human Feedback", "year": 2023}], "gt_label": [1, 1], "date": "2024-07", "source": "PASA_RealScholar", "qid": "RealScholarQuery_23", "valid": true}
{"query": "Show me all research papers on machine translation agents.", "cited_paper": [{"arxiv_id": "2407.12126", "title": "LLMs-in-the-loop Part-1: Expert Small AI Models for Bio-Medical Text Translation", "year": 2024}, {"arxiv_id": "2407.21646", "title": "Towards Achieving Human Parity on End-to-end Simultaneous Speech Translation via LLM Agent", "year": 2024}, {"arxiv_id": "2402.13036", "title": "SiLLM: Large Language Models for Simultaneous Machine Translation", "year": 2024}, {"arxiv_id": "1611.00179", "title": "Dual Learning for Machine Translation", "year": 2016}, {"arxiv_id": "1610.00388", "title": "Learning to Translate in Real-time with Neural Machine Translation", "year": 2016}, {"arxiv_id": "2406.06910", "title": "Agent-SiMT: Agent-assisted Simultaneous Machine Translation with Large Language Models", "year": 2024}, {"arxiv_id": "1805.01553", "title": "A Reinforcement Learning Approach to Interactive-Predictive Neural Machine Translation", "year": 2018}, {"arxiv_id": "2304.02426", "title": "ParroT: Translating during Chat using Large Language Models tuned with Human Translation and Feedback", "year": 2023}, {"arxiv_id": "1802.03116", "title": "Zero-Resource Neural Machine Translation with Multi-Agent Communication Game", "year": 2018}, {"arxiv_id": "1806.03661", "title": "Incremental Decoding and Training Methods for Simultaneous Translation in Neural Machine Translation", "year": 2018}], "gt_label": [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], "date": "2024-07", "source": "PASA_RealScholar", "qid": "RealScholarQuery_24", "valid": true}
{"query": "Video aesthetics score, using multimodal large models.", "cited_paper": [{"arxiv_id": "2312.17090", "title": "Q-Align: Teaching LMMs for Visual Scoring via Discrete Text-Defined Levels", "year": 2023}], "gt_label": [1], "date": "2023-12", "source": "PASA_RealScholar", "qid": "RealScholarQuery_25", "valid": true}
{"query": "Scaling Laws for Fine-Grained Mixture of Experts.", "cited_paper": [{"arxiv_id": "2402.07871", "title": "Scaling Laws for Fine-Grained Mixture of Experts", "year": 2024}, {"arxiv_id": "2407.04153", "title": "Mixture of A Million Experts", "year": 2024}], "gt_label": [1, 1], "date": "2024-07", "source": "PASA_RealScholar", "qid": "RealScholarQuery_26", "valid": true}
{"query": "Show me research on rejection sampling finetuning.", "cited_paper": [{"arxiv_id": "2309.06657", "title": "Statistical Rejection Sampling Improves Preference Optimization", "year": 2023}, {"arxiv_id": "2308.01825", "title": "Scaling Relationship on Learning Mathematical Reasoning with Large Language Models", "year": 2023}, {"arxiv_id": "2402.10038", "title": "RS-DPO: A Hybrid Rejection Sampling and Direct Preference Optimization Method for Alignment of Large Language Models", "year": 2024}, {"arxiv_id": "2406.13542", "title": "Self-play with Execution Feedback: Improving Instruction-following Capabilities of Large Language Models", "year": 2024}, {"arxiv_id": "2312.12457", "title": "Let AI Entertain You: Increasing User Engagement with Generative AI and Rejection Sampling", "year": 2023}, {"arxiv_id": "2405.20335", "title": "Xwin-LM: Strong and Scalable Alignment Practice for LLMs", "year": 2024}, {"arxiv_id": "2407.13690", "title": "DART-Math: Difficulty-Aware Rejection Tuning for Mathematical Problem-Solving", "year": 2024}, {"arxiv_id": "2310.20246", "title": "Breaking Language Barriers in Multilingual Mathematical Reasoning: Insights and Observations", "year": 2023}, {"arxiv_id": "2402.18571", "title": "Arithmetic Control of LLMs for Diverse User Preferences: Directional Preference Alignment with Multi-Objective Rewards", "year": 2024}], "gt_label": [1, 1, 1, 1, 1, 1, 1, 1, 1], "date": "2024-07", "source": "PASA_RealScholar", "qid": "RealScholarQuery_27", "valid": true}
{"query": "Show me code evaluation datasets with a mid-level hardness. It show be harder than HumanEval and MBPP, but easier than code_contests.", "cited_paper": [{"arxiv_id": "2401.03855", "title": "PythonSaga: Redefining the Benchmark to Evaluate Code Generating LLMs", "year": 2024}, {"arxiv_id": "2310.06770", "title": "SWE-bench: Can Language Models Resolve Real-World GitHub Issues?", "year": 2023}, {"arxiv_id": "2401.03065", "title": "CRUXEval: A Benchmark for Code Reasoning, Understanding and Execution", "year": 2024}, {"arxiv_id": "2405.04520", "title": "NaturalCodeBench: Examining Coding Performance Mismatch on HumanEval and Natural User Prompts", "year": 2024}], "gt_label": [1, 1, 1, 1], "date": "2024-05", "source": "PASA_RealScholar", "qid": "RealScholarQuery_28", "valid": true}
{"query": "Research on teaching llms to do math prove and solve IMO level math problems.", "cited_paper": [{"arxiv_id": "2409.17433", "title": "HDFlow: Enhancing LLM Complex Problem-Solving with Hybrid Thinking and Dynamic Workflows", "year": 2024}, {"arxiv_id": "2402.00157", "title": "Large Language Models for Mathematical Reasoning: Progresses and Challenges", "year": 2024}, {"arxiv_id": "2406.14219", "title": "Proving Olympiad Algebraic Inequalities without Human Demonstrations", "year": 2024}, {"arxiv_id": "2405.14333", "title": "DeepSeek-Prover: Advancing Theorem Proving in LLMs through Large-Scale Synthetic Data", "year": 2024}, {"arxiv_id": "2406.07394", "title": "Accessing GPT-4 level Mathematical Olympiad Solutions via Monte Carlo Tree Self-refine with LLaMa-3 8B", "year": 2024}, {"arxiv_id": "2309.15806", "title": "Lyra: Orchestrating Dual Correction in Automated Theorem Proving", "year": 2023}, {"arxiv_id": "2409.12568", "title": "InfiMM-WebMath-40B: Advancing Multimodal Pre-Training for Enhanced Mathematical Reasoning", "year": 2024}, {"arxiv_id": "2407.10040", "title": "Lean-STaR: Learning to Interleave Thinking and Proving", "year": 2024}], "gt_label": [1, 1, 1, 1, 1, 1, 1, 1], "date": "2024-09", "source": "PASA_RealScholar", "qid": "RealScholarQuery_29", "valid": true}
{"query": "I would like to find some research papers about test time training topic, in LLM research area.", "cited_paper": [{"arxiv_id": "2305.18466", "title": "Test-Time Training on Nearest Neighbors for Large Language Models", "year": 2023}, {"arxiv_id": "2404.13571", "title": "Test-Time Training on Graphs with Large Language Models (LLMs)", "year": 2024}, {"arxiv_id": "2403.18293", "title": "Efficient Test-Time Adaptation of Vision-Language Models", "year": 2024}, {"arxiv_id": "2405.02266", "title": "On the test-time zero-shot generalization of vision-language models: Do we really need prompt learning?", "year": 2024}, {"arxiv_id": "2405.03000", "title": "MedAdapter: Efficient Test-Time Adaptation of Large Language Models towards Medical Reasoning", "year": 2024}, {"arxiv_id": "2303.17651", "title": "Self-Refine: Iterative Refinement with Self-Feedback", "year": 2023}], "gt_label": [1, 1, 1, 1, 1, 1], "date": "2024-05", "source": "PASA_RealScholar", "qid": "RealScholarQuery_30", "valid": true}
{"query": "DPO training for large-scale vision-language models.", "cited_paper": [{"arxiv_id": "2406.11839", "title": "mDPO: Conditional Preference Optimization for Multimodal Large Language Models", "year": 2024}, {"arxiv_id": "2311.16839", "title": "Beyond Hallucinations: Enhancing LVLMs through Hallucination-Aware Direct Preference Optimization", "year": 2023}, {"arxiv_id": "2408.00550", "title": "Mitigating Multilingual Hallucination in Large Vision-Language Models", "year": 2024}, {"arxiv_id": "2308.06394", "title": "Detecting and Preventing Hallucinations in Large Vision Language Models", "year": 2023}, {"arxiv_id": "2312.10665", "title": "Silkie: Preference Distillation for Large Visual Language Models", "year": 2023}, {"arxiv_id": "2404.14233", "title": "Detecting and Mitigating Hallucination in Large Vision Language Models via Fine-Grained AI Feedback", "year": 2024}, {"arxiv_id": "2402.10884", "title": "Multi-modal Preference Alignment Remedies Degradation of Visual Instruction Tuning on Language Models", "year": 2024}, {"arxiv_id": "2402.11411", "title": "Aligning Modalities in Vision Large Language Models via Preference Fine-tuning", "year": 2024}, {"arxiv_id": "2405.19716", "title": "Enhancing Large Vision Language Models with Self-Training on Image Comprehension", "year": 2024}, {"arxiv_id": "2404.01258", "title": "Direct Preference Optimization of Video Large Multimodal Models from Language Model Reward", "year": 2024}, {"arxiv_id": "2406.19973", "title": "STLLaVA-Med: Self-Training Large Language and Vision Assistant for Medical Question-Answering", "year": 2024}, {"arxiv_id": "2408.10433", "title": "CLIP-DPO: Vision-Language Models as a Source of Preference for Fixing Hallucinations in LVLMs", "year": 2024}, {"arxiv_id": "2311.12908", "title": "Diffusion Model Alignment Using Direct Preference Optimization", "year": 2023}, {"arxiv_id": "2403.14003", "title": "Multi-Modal Hallucination Control by Visual Information Grounding", "year": 2024}, {"arxiv_id": "2405.11165", "title": "Automated Multi-level Preference for MLLMs", "year": 2024}], "gt_label": [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], "date": "2024-08", "source": "PASA_RealScholar", "qid": "RealScholarQuery_31", "valid": true}
{"query": "Show me cutting edge research works on neural network based quantum Monte Carlo.", "cited_paper": [{"arxiv_id": "2308.09709", "title": "Neural-network quantum state study of the long-range antiferromagnetic Ising chain", "year": 2023}, {"arxiv_id": "2409.01306", "title": "Highly Accurate Real-space Electron Densities with Neural Networks", "year": 2024}], "gt_label": [1, 1], "date": "2024-09", "source": "PASA_RealScholar", "qid": "RealScholarQuery_32", "valid": true}
{"query": "Show me some popular papers on generating textual adversarial examples for machine translation.", "cited_paper": [{"arxiv_id": "2308.15246", "title": "A Classification-Guided Approach for Adversarial Attacks against Neural Machine Translation", "year": 2023}, {"arxiv_id": "1911.03677", "title": "A Reinforced Generation of Adversarial Examples for Neural Machine Translation", "year": 2019}, {"arxiv_id": "1806.09030", "title": "On Adversarial Examples for Character-Level Neural Machine Translation", "year": 2018}, {"arxiv_id": "2011.00675", "title": "A Targeted Attack on Black-Box Neural Machine Translation with Parallel Data Poisoning", "year": 2020}, {"arxiv_id": "2305.01437", "title": "Sentiment Perception Adversarial Attacks on Neural Machine Translation Systems", "year": 2023}, {"arxiv_id": "2201.02009", "title": "PAEG: Phrase-level Adversarial Example Generation for Neural Machine Translation", "year": 2022}, {"arxiv_id": "2303.01068", "title": "Targeted Adversarial Attacks against Neural Machine Translation", "year": 2023}, {"arxiv_id": "2204.08689", "title": "Generating Authentic Adversarial Examples beyond Meaning-preserving with Doubly Round-trip Translation", "year": 2022}, {"arxiv_id": "1803.01128", "title": "Seq2Sick: Evaluating the Robustness of Sequence-to-Sequence Models with Adversarial Examples", "year": 2018}, {"arxiv_id": "2302.00944", "title": "TransFool: An Adversarial Attack against Neural Machine Translation Models", "year": 2023}, {"arxiv_id": "2407.05319", "title": "Rethinking Targeted Adversarial Attacks For Neural Machine Translation", "year": 2024}, {"arxiv_id": "2409.05021", "title": "Vision-fused Attack: Advancing Aggressive and Stealthy Adversarial Text against Neural Machine Translation", "year": 2024}, {"arxiv_id": "1906.02443", "title": "Robust Neural Machine Translation with Doubly Adversarial Inputs", "year": 2019}], "gt_label": [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], "date": "2024-09", "source": "PASA_RealScholar", "qid": "RealScholarQuery_33", "valid": true}
{"query": "Show me research on 3d scene understanding leveraging progress on 3D AIGC foundation models.", "cited_paper": [{"arxiv_id": "2403.09631", "title": "3D-VLA: A 3D Vision-Language-Action Generative World Model", "year": 2024}, {"arxiv_id": "2409.03757", "title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene Understanding", "year": 2024}, {"arxiv_id": "2302.07241", "title": "ConceptFusion: Open-set Multimodal 3D Mapping", "year": 2023}, {"arxiv_id": "2401.09340", "title": "SceneVerse: Scaling 3D Vision-Language Learning for Grounded Scene Understanding", "year": 2024}, {"arxiv_id": "2401.01970", "title": "FMGS: Foundation Model Embedded 3D Gaussian Splatting for Holistic 3D Scene Understanding", "year": 2024}, {"arxiv_id": "2305.08776", "title": "Bridging the Domain Gap: Self-Supervised 3D Scene Understanding with Foundation Models", "year": 2023}], "gt_label": [1, 1, 1, 1, 1, 1], "date": "2024-09", "source": "PASA_RealScholar", "qid": "RealScholarQuery_34", "valid": true}
{"query": "Give me papers about LLM quantized pretraining.", "cited_paper": [{"arxiv_id": "2402.17764", "title": "The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits", "year": 2024}, {"arxiv_id": "2310.18313", "title": "FP8-LM: Training FP8 Large Language Models", "year": 2023}, {"arxiv_id": "2405.16528", "title": "LoQT: Low-Rank Adapters for Quantized Pretraining", "year": 2024}, {"arxiv_id": "2407.11722", "title": "Exploring Quantization for Efficient Pre-Training of Transformer Language Models", "year": 2024}, {"arxiv_id": "2407.08296", "title": "Q-GaLore: Quantized GaLore with INT4 Projection and Layer-Adaptive Low-Rank Gradients", "year": 2024}, {"arxiv_id": "2403.12422", "title": "Jetfire: Efficient and Accurate Transformer Pretraining with INT8 Data Flow and Per-Block Quantization", "year": 2024}, {"arxiv_id": "2309.17224", "title": "Training and inference of large language models using 8-bit floating point", "year": 2023}], "gt_label": [1, 1, 1, 1, 1, 1, 1], "date": "2024-07", "source": "PASA_RealScholar", "qid": "RealScholarQuery_35", "valid": true}
{"query": "Show me research on identity preservation video generation.", "cited_paper": [{"arxiv_id": "2012.08261", "title": "HeadGAN: One-shot Neural Head Synthesis and Editing", "year": 2020}, {"arxiv_id": "2311.12052", "title": "MagicPose: Realistic Human Poses and Facial Expressions Retargeting with Identity-aware Diffusion", "year": 2023}, {"arxiv_id": "2301.03396", "title": "Diffused Heads: Diffusion Models Beat GANs on Talking-Face Generation", "year": 2023}, {"arxiv_id": "2001.05201", "title": "Everybody's Talkin': Let Me Talk as You Want", "year": 2020}, {"arxiv_id": "2403.08764", "title": "VLOGGER: Multimodal Diffusion for Embodied Avatar Synthesis", "year": 2024}, {"arxiv_id": "2106.11895", "title": "A Latent Transformer for Disentangled Face Editing in Images and Videos", "year": 2021}, {"arxiv_id": "2401.09962", "title": "CustomVideo: Customizing Text-to-Video Generation with Multiple Subjects", "year": 2024}, {"arxiv_id": "2208.02210", "title": "Free-HeadGAN: Neural Talking Head Synthesis with Explicit Gaze Control", "year": 2022}, {"arxiv_id": "2407.15153", "title": "Anchored Diffusion for Video Face Reenactment", "year": 2024}, {"arxiv_id": "2405.18326", "title": "VITON-DiT: Learning In-the-Wild Video Try-On from Human Dance Videos via Diffusion Transformers", "year": 2024}, {"arxiv_id": "2402.17485", "title": "EMO: Emote Portrait Alive -- Generating Expressive Portrait Videos with Audio2Video Diffusion Model under Weak Conditions", "year": 2024}, {"arxiv_id": "2004.12452", "title": "One-Shot Identity-Preserving Portrait Reenactment", "year": 2020}, {"arxiv_id": "2201.06260", "title": "Towards Realistic Visual Dubbing with Heterogeneous Sources", "year": 2022}, {"arxiv_id": "2204.06862", "title": "An Identity-Preserved Framework for Human Motion Transfer", "year": 2022}, {"arxiv_id": "2403.11781", "title": "Infinite-ID: Identity-preserved Personalization via ID-semantics Decoupling Paradigm", "year": 2024}, {"arxiv_id": "2405.01434", "title": "StoryDiffusion: Consistent Self-Attention for Long-Range Image and Video Generation", "year": 2024}, {"arxiv_id": "1909.02518", "title": "Neural Style-Preserving Visual Dubbing", "year": 2019}, {"arxiv_id": "2305.08293", "title": "Identity-Preserving Talking Face Generation with Landmark and Appearance Priors", "year": 2023}, {"arxiv_id": "2404.15275", "title": "ID-Animator: Zero-Shot Identity-Preserving Human Video Generation", "year": 2024}, {"arxiv_id": "1807.10550", "title": "X2Face: A network for controlling face generation by using images, audio, and pose codes", "year": 2018}, {"arxiv_id": "1805.11714", "title": "Deep Video Portraits", "year": 2018}, {"arxiv_id": "1911.08139", "title": "MarioNETte: Few-shot Face Reenactment Preserving Identity of Unseen Targets", "year": 2019}, {"arxiv_id": "2210.11182", "title": "Facial Expression Video Generation Based-On Spatio-temporal Convolutional GAN: FEV-GAN", "year": 2022}, {"arxiv_id": "2409.15179", "title": "MIMAFace: Face Animation via Motion-Identity Modulated Appearance Feature Learning", "year": 2024}, {"arxiv_id": "2312.05107", "title": "DreaMoving: A Human Video Generation Framework based on Diffusion Models", "year": 2023}, {"arxiv_id": "2303.13439", "title": "Text2Video-Zero: Text-to-Image Diffusion Models are Zero-Shot Video Generators", "year": 2023}, {"arxiv_id": "2402.09368", "title": "Magic-Me: Identity-Specific Video Customized Diffusion", "year": 2024}, {"arxiv_id": "2407.05577", "title": "Audio-driven High-resolution Seamless Talking Head Video Editing via StyleGAN", "year": 2024}, {"arxiv_id": "1602.02651", "title": "Automatic Face Reenactment", "year": 2016}, {"arxiv_id": "2404.08111", "title": "S3Editor: A Sparse Semantic-Disentangled Self-Training Framework for Face Video Editing", "year": 2024}, {"arxiv_id": "2311.17338", "title": "MagDiff: Multi-Alignment Diffusion for High-Fidelity Video Generation and Editing", "year": 2023}, {"arxiv_id": "2311.16498", "title": "MagicAnimate: Temporally Consistent Human Image Animation using Diffusion Model", "year": 2023}, {"arxiv_id": "2304.05097", "title": "One-Shot High-Fidelity Talking-Head Synthesis with Deformable Neural Radiance Field", "year": 2023}], "gt_label": [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], "date": "2024-09", "source": "PASA_RealScholar", "qid": "RealScholarQuery_36", "valid": true}
{"query": "Give me some papers showing that LLM agents can do schedule planning.", "cited_paper": [{"arxiv_id": "2311.15649", "title": "RoboGPT: an intelligent agent of making embodied long-term decisions for daily instruction tasks", "year": 2023}, {"arxiv_id": "2407.00476", "title": "Large Language Models for Power Scheduling: A User-Centric Approach", "year": 2024}, {"arxiv_id": "2407.19667", "title": "Smart Language Agents in Real-World Planning", "year": 2024}, {"arxiv_id": "2408.06318", "title": "Can We Rely on LLM Agents to Draft Long-Horizon Plans? Let's Take TravelPlanner as an Example", "year": 2024}, {"arxiv_id": "2406.11132", "title": "RePrompt: Planning by Automatic Prompt Engineering for Large Language Models Agents", "year": 2024}, {"arxiv_id": "2403.16971", "title": "AIOS: LLM Agent Operating System", "year": 2024}, {"arxiv_id": "2304.03442", "title": "Generative Agents: Interactive Simulacra of Human Behavior", "year": 2023}, {"arxiv_id": "2408.06993", "title": "LLMs can Schedule", "year": 2024}, {"arxiv_id": "2305.14078", "title": "Large Language Models as Commonsense Knowledge for Large-Scale Task Planning", "year": 2023}], "gt_label": [1, 1, 1, 1, 1, 1, 1, 1, 1], "date": "2024-08", "source": "PASA_RealScholar", "qid": "RealScholarQuery_37", "valid": true}
{"query": "Show me research on image encoding distributions.", "cited_paper": [{"arxiv_id": "1902.02102", "title": "BIVA: A Very Deep Hierarchy of Latent Variables for Generative Modeling", "year": 2019}, {"arxiv_id": "2201.11795", "title": "Neural JPEG: End-to-End Image Compression Leveraging a Standard JPEG Encoder-Decoder", "year": 2022}, {"arxiv_id": "1711.01558", "title": "Wasserstein Auto-Encoders", "year": 2017}, {"arxiv_id": "2009.12927", "title": "Learning to Improve Image Compression without Changing the Standard Decoder", "year": 2020}, {"arxiv_id": "2111.09172", "title": "End-to-end optimized image compression with competition of prior distributions", "year": 2021}, {"arxiv_id": "2407.07052", "title": "Latent Space Imaging", "year": 2024}, {"arxiv_id": "2107.06463", "title": "Learned Image Compression with Gaussian-Laplacian-Logistic Mixture Model and Concatenated Residual Modules", "year": 2021}, {"arxiv_id": "1611.05013", "title": "PixelVAE: A Latent Variable Model for Natural Images", "year": 2016}, {"arxiv_id": "2406.13059", "title": "Learned Compression of Encoding Distributions", "year": 2024}, {"arxiv_id": "1410.8516", "title": "NICE: Non-linear Independent Components Estimation", "year": 2014}, {"arxiv_id": "2306.00927", "title": "Second Sight: Using brain-optimized encoding models to align image distributions with human brain activity", "year": 2023}, {"arxiv_id": "1805.11057", "title": "Deep Generative Models for Distribution-Preserving Lossy Compression", "year": 2018}, {"arxiv_id": "2406.07699", "title": "CUPID: Contextual Understanding of Prompt-conditioned Image Distributions", "year": 2024}, {"arxiv_id": "2310.10517", "title": "Distribution prediction for image compression: An experimental re-compressor for JPEG images", "year": 2023}, {"arxiv_id": "2308.15667", "title": "Bridging Distribution Learning and Image Clustering in High-dimensional Space", "year": 2023}, {"arxiv_id": "2010.01185", "title": "Compressing Images by Encoding Their Latent Representations with Relative Entropy Coding", "year": 2020}, {"arxiv_id": "2409.08376", "title": "Learned Compression for Images and Point Clouds", "year": 2024}], "gt_label": [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], "date": "2024-09", "source": "PASA_RealScholar", "qid": "RealScholarQuery_38", "valid": true}
{"query": "Help me search for the work related to the synthetic data of large language models. I want to know how to automatically generate large-scale, high-quality, diverse, difficult, and valuable long thought data for learning.", "cited_paper": [{"arxiv_id": "2402.08957", "title": "MUSTARD: Mastering Uniform Synthesis of Theorem and Proof Data", "year": 2024}, {"arxiv_id": "2405.14333", "title": "DeepSeek-Prover: Advancing Theorem Proving in LLMs through Large-Scale Synthetic Data", "year": 2024}], "gt_label": [1, 1], "date": "2024-05", "source": "PASA_RealScholar", "qid": "RealScholarQuery_39", "valid": true}
{"query": "Could you list research that demonstrates the advantages of Quantization-Aware Training (QAT), which can enable the model to learn better representations for low-bit weights?.", "cited_paper": [{"arxiv_id": "1806.08342", "title": "Quantizing deep convolutional networks for efficient inference: A whitepaper", "year": 2018}, {"arxiv_id": "2406.06385", "title": "Low-Rank Quantization-Aware Training for LLMs", "year": 2024}, {"arxiv_id": "2402.10787", "title": "Squat: Quant Small Language Models on the Edge", "year": 2024}, {"arxiv_id": "2407.11062", "title": "EfficientQAT: Efficient Quantization-Aware Training for Large Language Models", "year": 2024}], "gt_label": [1, 1, 1, 1], "date": "2024-07", "source": "PASA_RealScholar", "qid": "RealScholarQuery_40", "valid": true}
{"query": "Using synthesis data for scaling up sft data.", "cited_paper": [{"arxiv_id": "2403.04706", "title": "Common 7B Language Models Already Possess Strong Math Capabilities", "year": 2024}, {"arxiv_id": "2408.08343", "title": "API-guided Dataset Synthesis to Finetune Large Code Models", "year": 2024}, {"arxiv_id": "2406.08464", "title": "Magpie: Alignment Data Synthesis from Scratch by Prompting Aligned LLMs with Nothing", "year": 2024}, {"arxiv_id": "2407.08348", "title": "Skywork-Math: Data Scaling Laws for Mathematical Reasoning in Large Language Models -- The Story Goes On", "year": 2024}, {"arxiv_id": "2409.13540", "title": "FullAnno: A Data Engine for Enhancing Image Comprehension of MLLMs", "year": 2024}], "gt_label": [1, 1, 1, 1, 1], "date": "2024-09", "source": "PASA_RealScholar", "qid": "RealScholarQuery_41", "valid": true}
{"query": "Show me research on how to select frames when doing video understanding.", "cited_paper": [{"arxiv_id": "1907.00193", "title": "Frame attention networks for facial expression recognition in videos", "year": 2019}, {"arxiv_id": "1907.13369", "title": "Multi-Agent Reinforcement Learning Based Frame Sampling for Effective Untrimmed Video Recognition", "year": 2019}, {"arxiv_id": "1903.11779", "title": "BubbleNets: Learning to Select the Guidance Frame in Video Object Segmentation by Deep Sorting Frames", "year": 2019}, {"arxiv_id": "2407.03104", "title": "KeyVideoLLM: Towards Large-scale Video Keyframe Selection", "year": 2024}, {"arxiv_id": "1910.04792", "title": "Unsupervised video summarization framework using keyframe extraction and video skimming", "year": 2019}, {"arxiv_id": "1811.12432", "title": "AdaFrame: Adaptive Frame Selection for Fast Video Recognition", "year": 2018}, {"arxiv_id": "2404.04346", "title": "Koala: Key frame-conditioned long video-LLM", "year": 2024}, {"arxiv_id": "2407.15047", "title": "End-to-End Video Question Answering with Frame Scoring Mechanisms and Adaptive Sampling", "year": 2024}, {"arxiv_id": "2009.12434", "title": "Online Learnable Keyframe Extraction in Videos and its Application with Semantic Word Vector in Action Recognition", "year": 2020}], "gt_label": [1, 1, 1, 1, 1, 1, 1, 1, 1], "date": "2024-07", "source": "PASA_RealScholar", "qid": "RealScholarQuery_42", "valid": true}
{"query": "AI for Science papers, especially protein design and DPO of antibody design.", "cited_paper": [{"arxiv_id": "2402.10516", "title": "Generative AI for Controllable Protein Sequence Design: A Survey", "year": 2024}, {"arxiv_id": "2306.16819", "title": "Graph Denoising Diffusion for Inverse Protein Folding", "year": 2023}, {"arxiv_id": "2305.20009", "title": "Protein Design with Guided Discrete Diffusion", "year": 2023}, {"arxiv_id": "2106.13058", "title": "Fold2Seq: A Joint Sequence(1D)-Fold(3D) Embedding-based Generative Model for Protein Design", "year": 2021}, {"arxiv_id": "2402.18567", "title": "Diffusion Language Models Are Versatile Protein Learners", "year": 2024}, {"arxiv_id": "2209.15611", "title": "Protein structure generation via folding diffusion", "year": 2022}, {"arxiv_id": "2302.04611", "title": "A Text-guided Protein Design Framework", "year": 2023}, {"arxiv_id": "2205.15019", "title": "Protein Structure and Sequence Generation with Equivariant Denoising Diffusion Probabilistic Models", "year": 2022}, {"arxiv_id": "2104.04457", "title": "Protein sequence design with deep generative models", "year": 2021}, {"arxiv_id": "2209.12643", "title": "PiFold: Toward effective and efficient protein inverse folding", "year": 2022}, {"arxiv_id": "2004.03497", "title": "ProGen: Language Modeling for Protein Generation", "year": 2020}, {"arxiv_id": "2407.07177", "title": "Protein Design by Integrating Machine Learning with Quantum Annealing and Quantum-inspired Optimization", "year": 2024}, {"arxiv_id": "2407.13981", "title": "Decomposed Direct Preference Optimization for Structure-Based Drug Design", "year": 2024}, {"arxiv_id": "2302.02277", "title": "SE(3) diffusion model with application to protein backbone generation", "year": 2023}, {"arxiv_id": "2202.01079", "title": "AlphaDesign: A graph protein design method and benchmark on AlphaFoldDB", "year": 2022}, {"arxiv_id": "2302.01649", "title": "Structure-informed Language Models Are Protein Designers", "year": 2023}, {"arxiv_id": "2109.13754", "title": "Deep Generative Modeling for Protein Design", "year": 2021}, {"arxiv_id": "2403.16576", "title": "Antigen-Specific Antibody Design via Direct Energy-based Preference Optimization", "year": 2024}, {"arxiv_id": "2403.14088", "title": "Protein Conformation Generation via Force-Guided SE(3) Diffusion Models", "year": 2024}, {"arxiv_id": "2204.10673", "title": "Generative De Novo Protein Design with Global Context", "year": 2022}, {"arxiv_id": "2312.00080", "title": "PDB-Struct: A Comprehensive Benchmark for Structure-based Protein Design", "year": 2023}], "gt_label": [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], "date": "2024-08", "source": "PASA_RealScholar", "qid": "RealScholarQuery_43", "valid": true}
{"query": "What are the researches that have explored the application of Crypto-based Private Learning in privacy-preserving machine learning?.", "cited_paper": [{"arxiv_id": "1711.05189", "title": "CryptoDL: Deep Neural Networks over Encrypted Data", "year": 2017}, {"arxiv_id": "2106.07229", "title": "Privacy-Preserving Machine Learning with Fully Homomorphic Encryption for Deep Neural Network", "year": 2021}, {"arxiv_id": "1811.09953", "title": "Faster CryptoNets: Leveraging Sparsity for Real-World Encrypted Inference", "year": 2018}, {"arxiv_id": "2012.13552", "title": "Neural Network Training With Homomorphic Encryption", "year": 2020}, {"arxiv_id": "1901.08755", "title": "SecureBoost: A Lossless Federated Learning Framework", "year": 2019}, {"arxiv_id": "1911.07101", "title": "Glyph: Fast and Accurately Training Deep Neural Networks on Encrypted Data", "year": 2019}, {"arxiv_id": "1811.00778", "title": "Towards the AlexNet Moment for Homomorphic Encryption: HCNN, theFirst Homomorphic CNN on Encrypted Data with GPUs", "year": 2018}, {"arxiv_id": "2009.00349", "title": "POSEIDON: Privacy-Preserving Federated Neural Network Learning", "year": 2020}, {"arxiv_id": "2204.05136", "title": "SoK: Privacy Preserving Machine Learning using Functional Encryption: Opportunities and Challenges", "year": 2022}, {"arxiv_id": "2409.07751", "title": "Efficient Privacy-Preserving KAN Inference Using Homomorphic Encryption", "year": 2024}, {"arxiv_id": "1806.03461", "title": "TAPAS: Tricks to Accelerate (encrypted) Prediction As a Service", "year": 2018}, {"arxiv_id": "1904.07303", "title": "CryptoNN: Training Neural Networks over Encrypted Data", "year": 2019}, {"arxiv_id": "2107.14338", "title": "Blind Faith: Privacy-Preserving Machine Learning using Function Approximation", "year": 2021}, {"arxiv_id": "2406.13221", "title": "Privacy-Preserving Logistic Regression Training on Large Datasets", "year": 2024}, {"arxiv_id": "2209.11904", "title": "CryptoGCN: Fast and Scalable Homomorphically Encrypted Graph Convolutional Network Inference", "year": 2022}, {"arxiv_id": "2402.00205", "title": "Decentralised, Collaborative, and Privacy-preserving Machine Learning for Multi-Hospital Data", "year": 2024}, {"arxiv_id": "2108.04417", "title": "Privacy-Preserving Machine Learning: Methods, Challenges and Directions", "year": 2021}, {"arxiv_id": "2204.07752", "title": "Homomorphic Encryption and Federated Learning based Privacy-Preserving CNN Training: COVID-19 Detection Use-Case", "year": 2022}, {"arxiv_id": "2409.06422", "title": "A Pervasive, Efficient and Private Future: Realizing Privacy-Preserving Machine Learning Through Hybrid Homomorphic Encryption", "year": 2024}, {"arxiv_id": "1412.6181", "title": "Crypto-Nets: Neural Networks over Encrypted Data", "year": 2014}, {"arxiv_id": "1906.00148", "title": "SHE: A Fast and Accurate Deep Neural Network for Encrypted Data", "year": 2019}, {"arxiv_id": "2210.15614", "title": "Private and Reliable Neural Network Inference", "year": 2022}, {"arxiv_id": "2309.08190", "title": "Learning in the Dark: Privacy-Preserving Machine Learning using Function Approximation", "year": 2023}, {"arxiv_id": "2403.17296", "title": "Hawk: Accurate and Fast Privacy-Preserving Machine Learning Using Secure Lookup Table Computation", "year": 2024}, {"arxiv_id": "1905.10214", "title": "Partially Encrypted Machine Learning using Functional Encryption", "year": 2019}], "gt_label": [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], "date": "2024-09", "source": "PASA_RealScholar", "qid": "RealScholarQuery_44", "valid": true}
{"query": "All papers about controllability of video generation.", "cited_paper": [{"arxiv_id": "2304.14404", "title": "Motion-Conditioned Diffusion Model for Controllable Video Synthesis", "year": 2023}, {"arxiv_id": "2408.07605", "title": "Panacea+: Panoramic and Controllable Video Generation for Autonomous Driving", "year": 2024}, {"arxiv_id": "2407.15642", "title": "Cinemo: Consistent and Controllable Image Animation with Motion Diffusion Models", "year": 2024}, {"arxiv_id": "2402.09368", "title": "Magic-Me: Identity-Specific Video Customized Diffusion", "year": 2024}, {"arxiv_id": "2402.00769", "title": "AnimateLCM: Computation-Efficient Personalized Style Video Generation without Personalized Video Data", "year": 2024}, {"arxiv_id": "2310.07771", "title": "DrivingDiffusion: Layout-Guided multi-view driving scene video generation with latent diffusion model", "year": 2023}, {"arxiv_id": "2401.00896", "title": "TrailBlazer: Trajectory Control for Diffusion-Based Video Generation", "year": 2024}, {"arxiv_id": "1707.04993", "title": "MoCoGAN: Decomposing Motion and Content for Video Generation", "year": 2017}, {"arxiv_id": "2402.15391", "title": "Genie: Generative Interactive Environments", "year": 2024}, {"arxiv_id": "2402.03162", "title": "Direct-a-Video: Customized Video Generation with User-Directed Camera Movement and Object Motion", "year": 2024}, {"arxiv_id": "2108.08815", "title": "Click to Move: Controlling Video Generation with Sparse Motion", "year": 2021}, {"arxiv_id": "2304.11603", "title": "LaMD: Latent Motion Diffusion for Image-Conditional Video Generation", "year": 2023}, {"arxiv_id": "2302.03011", "title": "Structure and Content-Guided Video Synthesis with Diffusion Models", "year": 2023}, {"arxiv_id": "2405.17414", "title": "Collaborative Video Diffusion: Consistent Multi-video Generation with Camera Control", "year": 2024}, {"arxiv_id": "2305.13840", "title": "Control-A-Video: Controllable Text-to-Video Diffusion Models with Motion Prior and Reward Feedback Learning", "year": 2023}, {"arxiv_id": "2406.17777", "title": "Text-Animator: Controllable Visual Text Video Generation", "year": 2024}, {"arxiv_id": "2409.01595", "title": "DiVE: DiT-based Video Generation with Enhanced Control", "year": 2024}, {"arxiv_id": "2106.11303", "title": "Understanding Object Dynamics for Interactive Image-to-Video Synthesis", "year": 2021}, {"arxiv_id": "2305.13077", "title": "ControlVideo: Training-free Controllable Text-to-Video Generation", "year": 2023}, {"arxiv_id": "2311.17117", "title": "Animate Anyone: Consistent and Controllable Image-to-Video Synthesis for Character Animation", "year": 2023}, {"arxiv_id": "2409.05463", "title": "DriveScape: Towards High-Resolution Controllable Multi-View Driving Video Generation", "year": 2024}, {"arxiv_id": "2406.02509", "title": "CamCo: Camera-Controllable 3D-Consistent Image-to-Video Generation", "year": 2024}, {"arxiv_id": "2312.07509", "title": "PEEKABOO: Interactive Video Generation via Masked-Diffusion", "year": 2023}, {"arxiv_id": "2404.02101", "title": "CameraCtrl: Enabling Camera Control for Text-to-Video Generation", "year": 2024}, {"arxiv_id": "2304.07483", "title": "Video Generation Beyond a Single Clip", "year": 2023}, {"arxiv_id": "2306.02018", "title": "VideoComposer: Compositional Video Synthesis with Motion Controllability", "year": 2023}, {"arxiv_id": "2403.07420", "title": "DragAnything: Motion Control for Anything using Entity Representation", "year": 2024}, {"arxiv_id": "2405.17661", "title": "RefDrop: Controllable Consistency in Image or Video Generation via Reference Feature Guidance", "year": 2024}, {"arxiv_id": "2306.00943", "title": "Make-Your-Video: Customized Video Generation Using Textual and Structural Guidance", "year": 2023}, {"arxiv_id": "2312.03018", "title": "DreamVideo: High-Fidelity Image-to-Video Generation with Image Retention and Text Guidance", "year": 2023}, {"arxiv_id": "2308.08089", "title": "DragNUWA: Fine-grained Control in Video Generation by Integrating Text, Image, and Trajectory", "year": 2023}, {"arxiv_id": "2310.02601", "title": "MagicDrive: Street View Generation with Diverse 3D Geometry Control", "year": 2023}, {"arxiv_id": "2406.16863", "title": "FreeTraj: Tuning-Free Trajectory Control in Video Diffusion Models", "year": 2024}, {"arxiv_id": "2312.02919", "title": "Fine-grained Controllable Video Generation via Object Appearance and Context", "year": 2023}, {"arxiv_id": "2311.16813", "title": "Panacea: Panoramic and Controllable Video Generation for Autonomous Driving", "year": 2023}, {"arxiv_id": "2406.05630", "title": "Ctrl-V: Higher Fidelity Video Generation with Bounding-Box Controlled Object Motion", "year": 2024}, {"arxiv_id": "2309.17444", "title": "LLM-grounded Video Diffusion Models", "year": 2023}, {"arxiv_id": "2402.01566", "title": "Boximator: Generating Rich and Controllable Motions for Video Synthesis", "year": 2024}, {"arxiv_id": "2408.11475", "title": "TrackGo: A Flexible and Efficient Method for Controllable Video Generation", "year": 2024}, {"arxiv_id": "2112.02815", "title": "Make It Move: Controllable Image-to-Video Generation with Text Descriptions", "year": 2021}, {"arxiv_id": "2312.03641", "title": "MotionCtrl: A Unified and Flexible Motion Controller for Video Generation", "year": 2023}, {"arxiv_id": "2409.01502", "title": "AMG: Avatar Motion Guided Video Generation", "year": 2024}, {"arxiv_id": "2210.02303", "title": "Imagen Video: High Definition Video Generation with Diffusion Models", "year": 2022}, {"arxiv_id": "2307.14073", "title": "VideoControlNet: A Motion-Guided Video-to-Video Translation Framework by Using Diffusion Model with ControlNet", "year": 2023}, {"arxiv_id": "2408.06070", "title": "ControlNeXt: Powerful and Efficient Control for Image and Video Generation", "year": 2024}, {"arxiv_id": "2409.06189", "title": "MyGo: Consistent and Controllable Multi-View Driving Video Generation with Camera Control", "year": 2024}, {"arxiv_id": "2304.01186", "title": "Follow Your Pose: Pose-Guided Text-to-Video Generation using Pose-Free Videos", "year": 2023}, {"arxiv_id": "2401.01827", "title": "Moonshot: Towards Controllable Video Generation and Editing with Multimodal Conditions", "year": 2024}, {"arxiv_id": "2406.10126", "title": "Training-free Camera Control for Video Generation", "year": 2024}, {"arxiv_id": "2310.08465", "title": "MotionDirector: Motion Customization of Text-to-Video Diffusion Models", "year": 2023}, {"arxiv_id": "2105.04551", "title": "Stochastic Image-to-Video Synthesis using cINNs", "year": 2021}, {"arxiv_id": "2311.16933", "title": "SparseCtrl: Adding Sparse Controls to Text-to-Video Diffusion Models", "year": 2023}, {"arxiv_id": "2406.05338", "title": "MotionClone: Training-Free Motion Cloning for Controllable Video Generation", "year": 2024}, {"arxiv_id": "2312.03047", "title": "MagicStick: Controllable Video Editing via Control Handle Transformations", "year": 2023}, {"arxiv_id": "2312.00845", "title": "VMC: Video Motion Customization using Temporal Attention Adaption for Text-to-Video Diffusion Models", "year": 2023}, {"arxiv_id": "2312.04433", "title": "DreamVideo: Composing Your Dream Videos with Customized Subject and Motion", "year": 2023}, {"arxiv_id": "2401.15977", "title": "Motion-I2V: Consistent and Controllable Image-to-Video Generation with Explicit Motion Modeling", "year": 2024}, {"arxiv_id": "2405.20222", "title": "MOFA-Video: Controllable Image Animation via Generative Motion Field Adaptions in Frozen Image-to-Video Diffusion Model", "year": 2024}], "gt_label": [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], "date": "2024-09", "source": "PASA_RealScholar", "qid": "RealScholarQuery_45", "valid": true}
{"query": "Show me research on robot decision making and task planning, especially relevant datasets and benchmarks.", "cited_paper": [{"arxiv_id": "2103.16544", "title": "Visual Room Rearrangement", "year": 2021}, {"arxiv_id": "1712.05474", "title": "AI2-THOR: An Interactive 3D Environment for Visual AI", "year": 2017}, {"arxiv_id": "2302.01560", "title": "Describe, Explain, Plan and Select: Interactive Planning with Large Language Models Enables Open-World Multi-Task Agents", "year": 2023}, {"arxiv_id": "2309.10062", "title": "SMART-LLM: Smart Multi-Agent Robot Task Planning using Large Language Models", "year": 2023}, {"arxiv_id": "2108.03332", "title": "BEHAVIOR: Benchmark for Everyday Household Activities in Virtual, Interactive, and Ecological Environments", "year": 2021}, {"arxiv_id": "2310.08864", "title": "Open X-Embodiment: Robotic Learning Datasets and RT-X Models", "year": 2023}, {"arxiv_id": "2402.10885", "title": "3D Diffuser Actor: Policy Diffusion with 3D Scene Representations", "year": 2024}, {"arxiv_id": "1910.10897", "title": "Meta-World: A Benchmark and Evaluation for Multi-Task and Meta Reinforcement Learning", "year": 2019}, {"arxiv_id": "2311.17842", "title": "Look Before You Leap: Unveiling the Power of GPT-4V in Robotic Vision-Language Planning", "year": 2023}, {"arxiv_id": "1809.00786", "title": "Mapping Instructions to Actions in 3D Environments with Visual Goal Prediction", "year": 2018}, {"arxiv_id": "2310.12020", "title": "LoHoRavens: A Long-Horizon Language-Conditioned Benchmark for Robotic Tabletop Manipulation", "year": 2023}, {"arxiv_id": "2202.10667", "title": "Visually Grounded Task and Motion Planning for Mobile Manipulation", "year": 2022}, {"arxiv_id": "2311.15649", "title": "RoboGPT: an intelligent agent of making embodied long-term decisions for daily instruction tasks", "year": 2023}, {"arxiv_id": "1912.01734", "title": "ALFRED: A Benchmark for Interpreting Grounded Instructions for Everyday Tasks", "year": 2019}, {"arxiv_id": "2405.01534", "title": "Plan-Seq-Learn: Language Model Guided RL for Solving Long Horizon Robotics Tasks", "year": 2024}, {"arxiv_id": "2210.03094", "title": "VIMA: General Robot Manipulation with Multimodal Prompts", "year": 2022}, {"arxiv_id": "2406.02523", "title": "RoboCasa: Large-Scale Simulation of Everyday Tasks for Generalist Robots", "year": 2024}, {"arxiv_id": "2407.00278", "title": "PerAct2: Benchmarking and Learning for Robotic Bimanual Manipulation Tasks", "year": 2024}, {"arxiv_id": "2312.12036", "title": "LHManip: A Dataset for Long-Horizon Language-Grounded Manipulation Tasks in Cluttered Tabletop Environments", "year": 2023}, {"arxiv_id": "1802.08705", "title": "PDDLStream: Integrating Symbolic Planners and Blackbox Samplers via Optimistic Adaptive Planning", "year": 2018}, {"arxiv_id": "2306.00942", "title": "Train Offline, Test Online: A Real Robot Learning Benchmark", "year": 2023}, {"arxiv_id": "2009.14259", "title": "Visually-Grounded Planning without Vision: Language Models Infer Detailed Plans from High-level Instructions", "year": 2020}, {"arxiv_id": "2403.19622", "title": "RH20T-P: A Primitive-Level Robotic Dataset Towards Composable Generalization Agents", "year": 2024}, {"arxiv_id": "2305.12821", "title": "FurnitureBench: Reproducible Real-World Benchmark for Long-Horizon Complex Manipulation", "year": 2023}, {"arxiv_id": "2403.18760", "title": "MLDT: Multi-Level Decomposition for Complex Long-Horizon Robotic Task Planning with Open-Source Large Language Model", "year": 2024}, {"arxiv_id": "2407.06951", "title": "RoboCAS: A Benchmark for Robotic Manipulation in Complex Object Arrangement Scenarios", "year": 2024}, {"arxiv_id": "2304.11477", "title": "LLM+P: Empowering Large Language Models with Optimal Planning Proficiency", "year": 2023}, {"arxiv_id": "2303.04137", "title": "Diffusion Policy: Visuomotor Policy Learning via Action Diffusion", "year": 2023}, {"arxiv_id": "2401.04157", "title": "RePLan: Robotic Replanning with Perception and Language Models", "year": 2024}, {"arxiv_id": "2112.03227", "title": "CALVIN: A Benchmark for Language-Conditioned Policy Learning for Long-Horizon Robot Manipulation Tasks", "year": 2021}, {"arxiv_id": "2307.14535", "title": "Scaling Up and Distilling Down: Language-Guided Robot Skill Acquisition", "year": 2023}, {"arxiv_id": "2007.13202", "title": "CAMPs: Learning Context-Specific Abstractions for Efficient Planning in Factored MDPs", "year": 2020}, {"arxiv_id": "2409.13998", "title": "Relevance-driven Decision Making for Safer and More Efficient Human Robot Collaboration", "year": 2024}, {"arxiv_id": "2002.03671", "title": "Autonomous Planning Based on Spatial Concepts to Tidy Up Home Environments with Service Robots", "year": 2020}, {"arxiv_id": "2307.05973", "title": "VoxPoser: Composable 3D Value Maps for Robotic Manipulation with Language Models", "year": 2023}, {"arxiv_id": "2205.10712", "title": "Housekeep: Tidying Virtual Households using Commonsense Reasoning", "year": 2022}, {"arxiv_id": "1909.12271", "title": "RLBench: The Robot Learning Benchmark & Learning Environment", "year": 2019}, {"arxiv_id": "2406.03641", "title": "Task and Motion Planning for Execution in the Real", "year": 2024}, {"arxiv_id": "2009.12293", "title": "robosuite: A Modular Simulation Framework and Benchmark for Robot Learning", "year": 2020}, {"arxiv_id": "2402.08546", "title": "Grounding LLMs For Robot Task Planning Using Closed-loop State Feedback", "year": 2024}, {"arxiv_id": "1702.03920", "title": "Cognitive Mapping and Planning for Visual Navigation", "year": 2017}, {"arxiv_id": "2210.01287", "title": "Robot Task Planning and Situation Handling in Open Worlds", "year": 2022}, {"arxiv_id": "2106.14405", "title": "Habitat 2.0: Training Home Assistants to Rearrange their Habitat", "year": 2021}, {"arxiv_id": "2305.05658", "title": "TidyBot: Personalized Robot Assistance with Large Language Models", "year": 2023}, {"arxiv_id": "2303.12153", "title": "Text2Motion: From Natural Language Instructions to Feasible Plans", "year": 2023}, {"arxiv_id": "1804.07779", "title": "PEORL: Integrating Symbolic Planning and Hierarchical Reinforcement Learning for Robust Decision-Making", "year": 2018}, {"arxiv_id": "2006.05398", "title": "Deep Visual Reasoning: Learning to Predict Action Sequences for Task and Motion Planning from an Initial Scene Image", "year": 2020}, {"arxiv_id": "2010.04296", "title": "CausalWorld: A Robotic Manipulation Benchmark for Causal Structure and Transfer Learning", "year": 2020}, {"arxiv_id": "2408.16844", "title": "A framework for training and benchmarking algorithms that schedule robot tasks", "year": 2024}, {"arxiv_id": "2406.11793", "title": "FetchBench: A Simulation Benchmark for Robot Fetching", "year": 2024}, {"arxiv_id": "2310.15127", "title": "Open-Ended Instructable Embodied Agents with Memory-Augmented Large Language Models", "year": 2023}, {"arxiv_id": "2401.04181", "title": "Language-Conditioned Robotic Manipulation with Fast and Slow Thinking", "year": 2024}, {"arxiv_id": "2308.07241", "title": "Context-Aware Planning and Environment-Aware Memory for Instruction Following Embodied Agents", "year": 2023}, {"arxiv_id": "2408.05478", "title": "Multi-Agent Planning Using Visual Language Models", "year": 2024}, {"arxiv_id": "2301.04195", "title": "Orbit: A Unified Simulation Framework for Interactive Robot Learning Environments", "year": 2023}, {"arxiv_id": "2401.12975", "title": "HAZARD Challenge: Embodied Decision Making in Dynamically Changing Environments", "year": 2024}, {"arxiv_id": "2403.10506", "title": "HumanoidBench: Simulated Humanoid Benchmark for Whole-Body Locomotion and Manipulation", "year": 2024}, {"arxiv_id": "2402.08178", "title": "LoTa-Bench: Benchmarking Language-oriented Task Planners for Embodied Agents", "year": 2024}, {"arxiv_id": "2307.04738", "title": "RoCo: Dialectic Multi-Robot Collaboration with Large Language Models", "year": 2023}, {"arxiv_id": "2306.11565", "title": "HomeRobot: Open-Vocabulary Mobile Manipulation", "year": 2023}, {"arxiv_id": "2305.05706", "title": "DexArt: Benchmarking Generalizable Dexterous Manipulation with Articulated Objects", "year": 2023}, {"arxiv_id": "2310.02071", "title": "Towards End-to-End Embodied Decision Making via Multi-modal Large Language Model: Explorations with GPT4-Vision and Beyond", "year": 2023}], "gt_label": [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], "date": "2024-09", "source": "PASA_RealScholar", "qid": "RealScholarQuery_46", "valid": true}
{"query": "How can LLM agents be evaluated and benchmarked for financial tasks? Note that I am referring to agents.", "cited_paper": [{"arxiv_id": "2402.12659", "title": "FinBen: A Holistic Financial Benchmark for Large Language Models", "year": 2024}, {"arxiv_id": "2407.00365", "title": "Financial Knowledge Large Language Model", "year": 2024}, {"arxiv_id": "2409.14913", "title": "Towards a Realistic Long-Term Benchmark for Open-Web Research Agents", "year": 2024}, {"arxiv_id": "2308.09975", "title": "FinEval: A Chinese Financial Domain Knowledge Evaluation Benchmark for Large Language Models", "year": 2023}], "gt_label": [1, 1, 1, 1], "date": "2024-09", "source": "PASA_RealScholar", "qid": "RealScholarQuery_47", "valid": true}
{"query": "Papers that explore using large language models for mining factors in stock exchange analysis.", "cited_paper": [{"arxiv_id": "2306.14222", "title": "Unveiling the Potential of Sentiment: Can Large Language Models Predict Chinese Stock Price Movements?", "year": 2023}, {"arxiv_id": "2409.06289", "title": "Automate Strategy Finding with LLM in Quant Investment", "year": 2024}, {"arxiv_id": "2406.10811", "title": "LLMFactor: Extracting Profitable Factors through Prompts for Explainable Stock Movement Prediction", "year": 2024}, {"arxiv_id": "2304.07619", "title": "Can ChatGPT Forecast Stock Price Movements? Return Predictability and Large Language Models", "year": 2023}, {"arxiv_id": "2308.00016", "title": "Alpha-GPT: Human-AI Interactive Alpha Mining for Quantitative Investment", "year": 2023}, {"arxiv_id": "2403.12285", "title": "FinLlama: Financial Sentiment Classification for Algorithmic Trading Applications", "year": 2024}, {"arxiv_id": "2407.00904", "title": "Background-aware Multi-source Fusion Financial Trend Forecasting Mechanism", "year": 2024}], "gt_label": [1, 1, 1, 1, 1, 1, 1], "date": "2024-09", "source": "PASA_RealScholar", "qid": "RealScholarQuery_48", "valid": true}
{"query": "Can you help me find research papers that explore the use of large vision-language models as agents to automatically play PC games?", "cited_paper": [{"arxiv_id": "2409.12889", "title": "Can VLMs Play Action Role-Playing Games? Take Black Myth Wukong as a Study Case", "year": 2024}, {"arxiv_id": "2403.03186", "title": "Cradle: Empowering Foundation Agents Towards General Computer Control", "year": 2024}, {"arxiv_id": "2310.08588", "title": "Octopus: Embodied Vision-Language Programmer from Environmental Feedback", "year": 2023}, {"arxiv_id": "2311.05997", "title": "JARVIS-1: Open-World Multi-task Agents with Memory-Augmented Multimodal Language Models", "year": 2023}, {"arxiv_id": "2306.00937", "title": "STEVE-1: A Generative Model for Text-to-Behavior in Minecraft", "year": 2023}, {"arxiv_id": "2403.05468", "title": "Will GPT-4 Run DOOM?", "year": 2024}, {"arxiv_id": "2310.08235", "title": "GROOT: Learning to Follow Instructions by Watching Gameplay Videos", "year": 2023}], "gt_label": [1, 1, 1, 1, 1, 1, 1], "date": "2024-09", "source": "PASA_RealScholar", "qid": "RealScholarQuery_49", "valid": true}
{"query": "Are there any research papers on methods to compress large-scale language models using task-agnostic knowledge distillation techniques?", "cited_paper": [{"arxiv_id": "1909.10351", "title": "TinyBERT: Distilling BERT for Natural Language Understanding", "year": 2019}], "gt_label": [1], "date": "2019-09", "source": "LitSearch", "qid": "LitSearch_0", "valid": true}
{"query": "Are there any studies that explore post-hoc techniques for hallucination detection at both the token- and sentence-level in neural sequence generation tasks?", "cited_paper": [{"arxiv_id": "2011.02593", "title": "Detecting Hallucinated Content in Conditional Neural Sequence Generation", "year": 2020}, {"arxiv_id": "1910.12840", "title": "Evaluating the Factual Consistency of Abstractive Text Summarization", "year": 2019}], "gt_label": [1, 1], "date": "2020-11", "source": "LitSearch", "qid": "LitSearch_2", "valid": true}
{"query": "Are there papers that propose contextualized calibration for the probability of answers in language models?", "cited_paper": [{"arxiv_id": "2104.08315", "title": "Surface Form Competition: Why the Highest Probability Answer Isn't Always Right", "year": 2021}], "gt_label": [1], "date": "2021-04", "source": "LitSearch", "qid": "LitSearch_4", "valid": true}
{"query": "Are there studies that combine convolutional and recurrent neural network approaches to extract multiple types of features for relation extraction? If so, could you point me to one of them?", "cited_paper": [{"arxiv_id": "1605.07333", "title": "Combining Recurrent and Convolutional Neural Networks for Relation Classification", "year": 2016}], "gt_label": [1], "date": "2016-05", "source": "LitSearch", "qid": "LitSearch_5", "valid": true}
{"query": "Can you direct me to research that explores methods for transforming multi-hop questions into single-hop sub-questions to leverage existing single-hop answer models?", "cited_paper": [{"arxiv_id": "1906.02916", "title": "Multi-hop Reading Comprehension through Question Decomposition and Rescoring", "year": 2019}], "gt_label": [1], "date": "2019-06", "source": "LitSearch", "qid": "LitSearch_6", "valid": true}
{"query": "Can you direct me to studies that explore techniques like question answering and passage retrieval for mitigating the effects of clickbait headlines?", "cited_paper": [{"arxiv_id": "2203.10282", "title": "Clickbait Spoiling via Question Answering and Passage Retrieval", "year": 2022}], "gt_label": [1], "date": "2022-03", "source": "LitSearch", "qid": "LitSearch_7", "valid": true}
{"query": "Can you point me to a paper that discussed transformer-based sentence embeddings?", "cited_paper": [{"arxiv_id": "1908.10084", "title": "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks", "year": 2019}], "gt_label": [1], "date": "2019-08", "source": "LitSearch", "qid": "LitSearch_9", "valid": true}
{"query": "Can you point me to studies discussing methods for evaluating text generation models on various dimensions? I'm particularly interested in models like T5 and FLAN-T5, and how to assess their performance on summary-level and turn-level tasks.", "cited_paper": [{"arxiv_id": "2210.07197", "title": "Towards a Unified Multi-Dimensional Evaluator for Text Generation", "year": 2022}], "gt_label": [1], "date": "2022-10", "source": "LitSearch", "qid": "LitSearch_11", "valid": true}
{"query": "Can you point me to studies that explore the impact of different data augmentation strategies, such as feature/token/span cutoff or dropout, in the context of contrastive learning for sentence representations?", "cited_paper": [{"arxiv_id": "2105.11741", "title": "ConSERT: A Contrastive Framework for Self-Supervised Sentence Representation Transfer", "year": 2021}, {"arxiv_id": "2104.08821", "title": "SimCSE: Simple Contrastive Learning of Sentence Embeddings", "year": 2021}], "gt_label": [1, 1], "date": "2021-05", "source": "LitSearch", "qid": "LitSearch_12", "valid": true}
{"query": "Can you point me towards research on contrastive learning methods used for fine-tuning sentence representations, where in-batch negatives may sometimes unintentionally be similar to the positive examples?", "cited_paper": [{"arxiv_id": "2105.11741", "title": "ConSERT: A Contrastive Framework for Self-Supervised Sentence Representation Transfer", "year": 2021}, {"arxiv_id": "2104.08821", "title": "SimCSE: Simple Contrastive Learning of Sentence Embeddings", "year": 2021}], "gt_label": [1, 1], "date": "2021-05", "source": "LitSearch", "qid": "LitSearch_13", "valid": true}
{"query": "Can you recommend a conversational QA dataset where the human questioner does not have access to the evidence passage to simulate a more real-world information-seeking environment?", "cited_paper": [{"arxiv_id": "1808.07036", "title": "QuAC : Question Answering in Context", "year": 2018}], "gt_label": [1], "date": "2018-08", "source": "LitSearch", "qid": "LitSearch_15", "valid": true}
{"query": "Can you recommend a foundational paper that provides a scalable framework for generating English sentences with controllable semantic and syntactic attributes for the purpose of augmenting datasets in NLP tasks?", "cited_paper": [{"arxiv_id": "2004.14983", "title": "Control, Generate, Augment: A Scalable Framework for Multi-Attribute Text Generation", "year": 2020}], "gt_label": [1], "date": "2020-04", "source": "LitSearch", "qid": "LitSearch_16", "valid": true}
{"query": "Can you recommend a paper that uses an NLI model for sentence-level relation extraction using hypothesis generation and verification with entity-type constraints?", "cited_paper": [{"arxiv_id": "2109.03659", "title": "Label Verbalization and Entailment for Effective Zeroand Few-Shot Relation Extraction", "year": 2021}], "gt_label": [1], "date": "2021-09", "source": "LitSearch", "qid": "LitSearch_17", "valid": true}
{"query": "Can you suggest a corpus that contains French encyclopedia documents with semantic annotations and includes a test set of manually written question/answer triplets that align with the constraints of FrameNet semantic analysis?", "cited_paper": [{"arxiv_id": "1812.08039", "title": "Semantic Frame Parsing for Information Extraction : the CALOR corpus", "year": 2018}], "gt_label": [1], "date": "2018-12", "source": "LitSearch", "qid": "LitSearch_20", "valid": true}
{"query": "Can you suggest any literature that explores the idea of training neural networks to translate text passages into related questions?", "cited_paper": [{"arxiv_id": "1705.00106", "title": "Learning to Ask: Neural Question Generation for Reading Comprehension", "year": 2017}], "gt_label": [1], "date": "2017-05", "source": "LitSearch", "qid": "LitSearch_21", "valid": true}
{"query": "Can you suggest recent studies that have integrated prompt fine-tuning into semi-supervised learning workflows for natural language understanding tasks?", "cited_paper": [{"arxiv_id": "2001.07676", "title": "Exploiting Cloze Questions for Few Shot Text Classification and Natural Language Inference", "year": 2020}], "gt_label": [1], "date": "2020-01", "source": "LitSearch", "qid": "LitSearch_24", "valid": true}
{"query": "Can you suggest some literature that evaluates the ability of context-aware machine translation systems to handle discourse phenomena such as deixis and lexical cohesion?", "cited_paper": [{"arxiv_id": "1905.05979", "title": "When a Good Translation is Wrong in Context: Context-Aware Machine Translation Improves on Deixis, Ellipsis, and Lexical Cohesion", "year": 2019}], "gt_label": [1], "date": "2019-05", "source": "LitSearch", "qid": "LitSearch_25", "valid": true}
{"query": "Could you direct me towards a study that explores the potential to predict a reader's native language based on their eye movement patterns while reading English texts?", "cited_paper": [{"arxiv_id": "1704.07398", "title": "Predicting Native Language from Gaze", "year": 2017}], "gt_label": [1], "date": "2017-04", "source": "LitSearch", "qid": "LitSearch_27", "valid": true}
{"query": "Could you point me toward some large-scale multilingual Amazon customer review data?", "cited_paper": [{"arxiv_id": "2010.02573", "title": "The Multilingual Amazon Reviews Corpus", "year": 2020}], "gt_label": [1], "date": "2020-10", "source": "LitSearch", "qid": "LitSearch_31", "valid": true}
{"query": "Could you provide me with a reference that discusses the development of classifiers for suicide risk detection in a low-resource language, with a specific focus on using explicit suicide-related terminology?", "cited_paper": [{"arxiv_id": "2209.04830", "title": "Detecting Suicide Risk in Online Counseling Services: A Study in a Low-Resource Language", "year": 2022}], "gt_label": [1], "date": "2022-09", "source": "LitSearch", "qid": "LitSearch_32", "valid": true}
{"query": "Could you recommend datasets that include SQL annotations over WikiTQ?", "cited_paper": [{"arxiv_id": "2010.11246", "title": "On the Potential of Lexico-logical Alignments for Semantic Parsing to SQL Queries", "year": 2020}], "gt_label": [1], "date": "2020-10", "source": "LitSearch", "qid": "LitSearch_33", "valid": true}
{"query": "Could you recommend studies that provide a baseline for experiments using supervised constituency parsers with a focus on few-shot learning settings and have also reported on the use of pre-training and data augmentation techniques for parser performance improvement?", "cited_paper": [{"arxiv_id": "2010.02423", "title": "On the Role of Supervision in Unsupervised Constituency Parsing", "year": 2020}], "gt_label": [1], "date": "2020-10", "source": "LitSearch", "qid": "LitSearch_34", "valid": true}
{"query": "Could you suggest a paper that introduces an approach to relation extraction that involves learning syntax dependency structures using a tree LSTM model?", "cited_paper": [{"arxiv_id": "1503.00075", "title": "Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks", "year": 2015}], "gt_label": [1], "date": "2015-03", "source": "LitSearch", "qid": "LitSearch_35", "valid": true}
{"query": "Could you suggest studies that employ novel methods for capturing data, specifically in the context of sarcasm detection on social media platforms like Twitter?", "cited_paper": [{"arxiv_id": "2009.13080", "title": "Reactive Supervision: A New Method for Collecting Sarcasm Data", "year": 2020}], "gt_label": [1], "date": "2020-09", "source": "LitSearch", "qid": "LitSearch_36", "valid": true}
{"query": "I am exploring state-of-the-art techniques in language representation models that are trained to understand context from both the preceding and succeeding text. Where can I find foundational research on this topic, including information about the Transformer architecture, and the specific tasks such models are pre-trained on?", "cited_paper": [{"arxiv_id": "1810.04805", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "year": 2018}], "gt_label": [1], "date": "2018-10", "source": "LitSearch", "qid": "LitSearch_38", "valid": true}
{"query": "I am looking for research that has explored topic and frame conditioning in transformer language models to enhance the quality of generated argument claims. Is there a paper discussing this approach?", "cited_paper": [{"arxiv_id": "2005.00084", "title": "Aspect-Controlled Neural Argument Generation", "year": 2020}], "gt_label": [1], "date": "2020-05", "source": "LitSearch", "qid": "LitSearch_39", "valid": true}
{"query": "I am looking to understand more about sequence-to-sequence pre-training and its applications in natural language tasks. Can you suggest a significant paper that describes the denoising process for such models?", "cited_paper": [{"arxiv_id": "1910.13461", "title": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension", "year": 2019}], "gt_label": [1], "date": "2019-10", "source": "LitSearch", "qid": "LitSearch_40", "valid": true}
{"query": "I would like to understand the theoretical basis for using the nuclear norm of a weight matrix as a measure of complexity in linear models for probing tasks. Which paper should I refer to?", "cited_paper": [{"arxiv_id": "2004.03061", "title": "Information-Theoretic Probing for Linguistic Structure", "year": 2020}], "gt_label": [1], "date": "2020-04", "source": "LitSearch", "qid": "LitSearch_41", "valid": true}
{"query": "I'm exploring efficient transformer architectures for language embeddings and came across some work that utilizes advanced pre-trained models. Which paper should I reference to learn more about the use of XLM-R for multilingual representation learning in a transformer-based setting?", "cited_paper": [{"arxiv_id": "1911.02116", "title": "Unsupervised Cross-lingual Representation Learning at Scale", "year": 2019}], "gt_label": [1], "date": "2019-11", "source": "LitSearch", "qid": "LitSearch_43", "valid": true}
{"query": "I'm exploring research that utilizes large datasets for the task of sentence simplification. Are there any prominent datasets sourced from Wikipedia that I could look into?", "cited_paper": [{"arxiv_id": "1703.10931", "title": "Sentence Simplification with Deep Reinforcement Learning", "year": 2017}], "gt_label": [1], "date": "2017-03", "source": "LitSearch", "qid": "LitSearch_44", "valid": true}
{"query": "I'm exploring ways to enhance question answering systems through domain adaptation. Could you point me towards research that specifically focuses on synthetic data generation for this purpose?", "cited_paper": [{"arxiv_id": "2010.06028", "title": "End-to-End Synthetic Data Generation for Domain Adaptation of Question Answering Systems", "year": 2020}], "gt_label": [1], "date": "2020-10", "source": "LitSearch", "qid": "LitSearch_45", "valid": true}
{"query": "I'm interested in understanding how perplexity is utilized in identifying misinformation or fact-checking. Are there studies discussing this application of perplexity?", "cited_paper": [{"arxiv_id": "2103.09535", "title": "Towards Few-Shot Fact-Checking via Perplexity", "year": 2021}], "gt_label": [1], "date": "2021-03", "source": "LitSearch", "qid": "LitSearch_46", "valid": true}
{"query": "I'm looking for a comprehensive dataset that has been influential in fact verification research", "cited_paper": [{"arxiv_id": "1803.05355", "title": "FEVER: a large-scale dataset for Fact Extraction and VERification", "year": 2018}], "gt_label": [1], "date": "2018-03", "source": "LitSearch", "qid": "LitSearch_47", "valid": true}
{"query": "I'm looking for a paper that discusses improvements in constituency parsing performance by applying a partition strategy for content embedding and positional embedding within self-attention and label attention layers.", "cited_paper": [{"arxiv_id": "1805.01052", "title": "Constituency Parsing with a Self-Attentive Encoder", "year": 2018}], "gt_label": [1], "date": "2018-05", "source": "LitSearch", "qid": "LitSearch_48", "valid": true}
{"query": "I'm looking for innovative approaches to data annotation on platforms like Amazon Mechanical Turk that focus on maximizing document coverage. Is there any research discussing strategies that balance the trade-off between full annotation and broader document coverage?", "cited_paper": [{"arxiv_id": "1906.04937", "title": "Partial Or Complete, That's The Question", "year": 2019}], "gt_label": [1], "date": "2019-06", "source": "LitSearch", "qid": "LitSearch_49", "valid": true}
{"query": "I'm looking into morphological embedding algorithms that build upon the word2vec model by utilizing character n-grams. Which papers should I read to learn more about this approach?", "cited_paper": [{"arxiv_id": "1607.04606", "title": "Enriching Word Vectors with Subword Information", "year": 2016}], "gt_label": [1], "date": "2016-07", "source": "LitSearch", "qid": "LitSearch_50", "valid": true}
{"query": "I'm looking into the distillation process of language models and would like to examine studies that specifically discuss the attention mechanism alignment in the teacher-student model architecture. Are there any papers you can suggest?", "cited_paper": [{"arxiv_id": "2012.15828", "title": "MiniLMv2: Multi-Head Self-Attention Relation Distillation for Compressing Pretrained Transformers", "year": 2020}], "gt_label": [1], "date": "2020-12", "source": "LitSearch", "qid": "LitSearch_51", "valid": true}
{"query": "I'm researching insertion-based decoding methods for semantic parsing and language modeling, and I'm looking for works that discuss alternatives to traditional loss functions such as cross-entropy, particularly those using KullbackLeibler divergence in this context. Could you point me to some studies on this?", "cited_paper": [{"arxiv_id": "2010.03714", "title": "Don't Parse, Insert: Multilingual Semantic Parsing with Insertion Based Decoding", "year": 2020}], "gt_label": [1], "date": "2020-10", "source": "LitSearch", "qid": "LitSearch_52", "valid": true}
{"query": "I'm searching for studies that explore advancements in dependency parsing, particularly using graph-to-graph transformers with iterative refinement processes. Which publications should I look into?", "cited_paper": [{"arxiv_id": "2003.13118", "title": "Recursive Non-Autoregressive Graph-to-Graph Transformer for Dependency Parsing with Iterative Refinement", "year": 2020}], "gt_label": [1], "date": "2020-03", "source": "LitSearch", "qid": "LitSearch_54", "valid": true}
{"query": "In discourse parsing literature, which works have explored parser performance by adopting the original Parseval procedure and reporting micro-averaged F1 scores?", "cited_paper": [{"arxiv_id": "2005.02680", "title": "A Top-Down Neural Architecture towards Text-Level Parsing of Discourse Rhetorical Structure", "year": 2020}], "gt_label": [1], "date": "2020-05", "source": "LitSearch", "qid": "LitSearch_55", "valid": true}
{"query": "In the context of Named Entity Recognition tasks across multiple languages, which work highlights the necessity of retrieving related knowledge to aid in the annotation of ambiguous named entities?", "cited_paper": [{"arxiv_id": "1909.01441", "title": "CrossWeigh: Training Named Entity Tagger from Imperfect Annotations", "year": 2019}], "gt_label": [1], "date": "2019-09", "source": "LitSearch", "qid": "LitSearch_58", "valid": true}
{"query": "In the context of machine translation, can you point me towards literature discussing the specifications for setting up encoder/decoder layers, attention heads, and other hyperparameters for a neural network model?", "cited_paper": [{"arxiv_id": "2004.11867", "title": "Improving Massively Multilingual Neural Machine Translation and Zero-Shot Translation", "year": 2020}], "gt_label": [1], "date": "2020-04", "source": "LitSearch", "qid": "LitSearch_59", "valid": true}
{"query": "In the context of natural language processing, I am looking for research that explores the relationship between a model's prediction entropy and its tendencies to copy existing text versus generating novel content. Can you recommend a paper?", "cited_paper": [{"arxiv_id": "2010.07882", "title": "Understanding Neural Abstractive Summarization Models via Uncertainty", "year": 2020}], "gt_label": [1], "date": "2020-10", "source": "LitSearch", "qid": "LitSearch_60", "valid": true}
{"query": "In the field of reinforcement learning models for multi-hop reasoning, what issue involves an agent erroneously correlating a successful outcome with irrelevant or coincidental actions, and are there any papers discussing this phenomenon?", "cited_paper": [{"arxiv_id": "1704.07926", "title": "From Language to Programs: Bridging Reinforcement Learning and Maximum Marginal Likelihood", "year": 2017}], "gt_label": [1], "date": "2017-04", "source": "LitSearch", "qid": "LitSearch_62", "valid": true}
{"query": "What are some approaches to generating sports news reports from event data and what models have been used for this task in Finnish language NLP?", "cited_paper": [{"arxiv_id": "1910.01863", "title": "Template-free Data-to-Text Generation of Finnish Sports News", "year": 2019}], "gt_label": [1], "date": "2019-10", "source": "LitSearch", "qid": "LitSearch_64", "valid": true}
{"query": "What are some good datasets for conversational question answering?", "cited_paper": [{"arxiv_id": "1808.07036", "title": "QuAC : Question Answering in Context", "year": 2018}, {"arxiv_id": "1808.07042", "title": "CoQA: A Conversational Question Answering Challenge", "year": 2018}], "gt_label": [1, 1], "date": "2018-08", "source": "LitSearch", "qid": "LitSearch_65", "valid": true}
{"query": "What are some recent advancements in training systems to parse complex multi-hop questions into a sequence of simpler query steps for improved question answering?", "cited_paper": [{"arxiv_id": "2001.11770", "title": "Break It Down: A Question Understanding Benchmark", "year": 2020}], "gt_label": [1], "date": "2020-01", "source": "LitSearch", "qid": "LitSearch_67", "valid": true}
{"query": "What are some techniques or tools used in machine learning for matching and grounding annotated data to an existing knowledge base, particularly using sentence embedding-based cosine similarity or clustering algorithms?", "cited_paper": [{"arxiv_id": "1908.10084", "title": "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks", "year": 2019}], "gt_label": [1], "date": "2019-08", "source": "LitSearch", "qid": "LitSearch_70", "valid": true}
{"query": "What are the recent developments in evaluating the flow or 'streaming degree' of the translation processes in simultaneous machine translation (SiMT), and which metric has proven useful for this purpose?", "cited_paper": [{"arxiv_id": "1610.00388", "title": "Learning to Translate in Real-time with Neural Machine Translation", "year": 2016}], "gt_label": [1], "date": "2016-10", "source": "LitSearch", "qid": "LitSearch_71", "valid": true}
{"query": "What paper should I look at if I am interested in the challenges of compositional generalization in the context of semantic parsing, especially regarding the impact of unseen local structures in program outputs?", "cited_paper": [{"arxiv_id": "2201.05899", "title": "Unobserved Local Structures Make Compositional Generalization Hard", "year": 2022}], "gt_label": [1], "date": "2022-01", "source": "LitSearch", "qid": "LitSearch_72", "valid": true}
{"query": "What papers should I refer to if I want to explore datasets and shared tasks designed for the purpose of fact verification using evidence from Wikipedia?", "cited_paper": [{"arxiv_id": "1803.05355", "title": "FEVER: a large-scale dataset for Fact Extraction and VERification", "year": 2018}], "gt_label": [1], "date": "2018-03", "source": "LitSearch", "qid": "LitSearch_73", "valid": true}
{"query": "What prior works suggested that exposure bias could lead to hallucinations in neural machine translation models?", "cited_paper": [{"arxiv_id": "2005.03642", "title": "On Exposure Bias, Hallucination and Domain Shift in Neural Machine Translation", "year": 2020}, {"arxiv_id": "2010.10907", "title": "Analyzing the Source and Target Contributions to Predictions in Neural Machine Translation", "year": 2020}], "gt_label": [1, 1], "date": "2020-10", "source": "LitSearch", "qid": "LitSearch_74", "valid": true}
{"query": "What research has been done on improving named entity recognition tasks by augmenting them with retrieval of external contexts?", "cited_paper": [{"arxiv_id": "2105.03654", "title": "Improving Named Entity Recognition by External Context Retrieving and Cooperative Learning", "year": 2021}], "gt_label": [1], "date": "2021-05", "source": "LitSearch", "qid": "LitSearch_77", "valid": true}
{"query": "What research should I explore to understand methods for matching word senses in contexts using embeddings without relying on external lexical resources?", "cited_paper": [{"arxiv_id": "1708.03390", "title": "Making Sense of Word Embeddings", "year": 2017}], "gt_label": [1], "date": "2017-08", "source": "LitSearch", "qid": "LitSearch_78", "valid": true}
{"query": "When using pretrained transformer models for generating sentence embeddings, I've heard different strategies such as mean pooling and using the CLS token's embedding. What study shows that mean pooling outperforms CLS in semantic similarity tasks?", "cited_paper": [{"arxiv_id": "1908.10084", "title": "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks", "year": 2019}], "gt_label": [1], "date": "2019-08", "source": "LitSearch", "qid": "LitSearch_79", "valid": true}
{"query": "Where can I find a detailed discussion on automating the assessment of clarifications in instructional text, including tasks for grading these clarifications as plausible, implausible, or neutral and ranking them on a scale?", "cited_paper": [{"arxiv_id": "2309.12102", "title": "SemEval-2022 Task 7: Identifying Plausible Clarifications of Implicit and Underspecified Phrases in Instructional Texts", "year": 2023}], "gt_label": [1], "date": "2023-09", "source": "LitSearch", "qid": "LitSearch_81", "valid": true}
{"query": "Where can I find a large corpus of annotated social media posts concerning a variety of health conditions?", "cited_paper": [{"arxiv_id": "2210.06331", "title": "RedHOT: A Corpus of Annotated Medical Questions, Experiences, and Claims on Social Media", "year": 2022}], "gt_label": [1], "date": "2022-10", "source": "LitSearch", "qid": "LitSearch_83", "valid": true}
{"query": "Where can I find guidelines on standard practices for probing machine learning models to discern what information the models have captured without training them on a new task?", "cited_paper": [{"arxiv_id": "2104.05807", "title": "Does My Representation Capture X? Probe-Ably", "year": 2021}], "gt_label": [1], "date": "2021-04", "source": "LitSearch", "qid": "LitSearch_86", "valid": true}
{"query": "Where can I find information on self-attentive parsers that have been trained in a few-shot learning setting, including their official code and hyperparameters?", "cited_paper": [{"arxiv_id": "1805.01052", "title": "Constituency Parsing with a Self-Attentive Encoder", "year": 2018}], "gt_label": [1], "date": "2018-05", "source": "LitSearch", "qid": "LitSearch_87", "valid": true}
{"query": "Where can I find interdisciplinary research that investigates how creative natural language generation (NLG) systems are evaluated?", "cited_paper": [{"arxiv_id": "2108.00308", "title": "Human Evaluation of Creative NLG Systems: An Interdisciplinary Survey on Recent Papers", "year": 2021}], "gt_label": [1], "date": "2021-08", "source": "LitSearch", "qid": "LitSearch_88", "valid": true}
{"query": "Where can I find research about automatic evaluation metrics in summarization tasks disagree with each other?", "cited_paper": [{"arxiv_id": "2011.04096", "title": "Metrics also Disagree in the Low Scoring Range: Revisiting Summarization Evaluation Metrics", "year": 2020}], "gt_label": [1], "date": "2020-11", "source": "LitSearch", "qid": "LitSearch_90", "valid": true}
{"query": "Where might I find a dataset annotated specifically for patronizing and condescending language to use in computational linguistics research?", "cited_paper": [{"arxiv_id": "2011.08320", "title": "Don't Patronize Me! An Annotated Dataset with Patronizing and Condescending Language towards Vulnerable Communities", "year": 2020}], "gt_label": [1], "date": "2020-11", "source": "LitSearch", "qid": "LitSearch_91", "valid": true}
{"query": "Where might I find research on the evaluation of consistency in generated summaries?", "cited_paper": [{"arxiv_id": "2111.09525", "title": "SummaC: Re-Visiting NLI-based Models for Inconsistency Detection in Summarization", "year": 2021}, {"arxiv_id": "2109.06379", "title": "Compression, Transduction, and Creation: A Unified Framework for Evaluating Natural Language Generation", "year": 2021}], "gt_label": [1, 1], "date": "2021-11", "source": "LitSearch", "qid": "LitSearch_92", "valid": true}
{"query": "Which paper specifies the typical configurations used in fine-tuning deep bidirectional transformers like BERT and RoBERTa for language understanding tasks?", "cited_paper": [{"arxiv_id": "1810.04805", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "year": 2018}], "gt_label": [1], "date": "2018-10", "source": "LitSearch", "qid": "LitSearch_94", "valid": true}
{"query": "*Could you suggest a dataset with legally or ethically contentious content, and labels for acceptable and non-acceptable questions.", "cited_paper": [{"arxiv_id": "2305.17696", "title": "SQuARe: A Large-Scale Dataset of Sensitive Questions and Acceptable Responses Created Through Human-Machine Collaboration", "year": 2023}], "gt_label": [1], "date": "2023-05", "source": "LitSearch", "qid": "LitSearch_98", "valid": true}
{"query": "Are there any papers on training video-language models with contrastive approahes and evaluation on temporal localization tasks?", "cited_paper": [{"arxiv_id": "2210.05039", "title": "Contrastive Video-Language Learning with Fine-grained Frame Sampling", "year": 2022}], "gt_label": [1], "date": "2022-10", "source": "LitSearch", "qid": "LitSearch_99", "valid": true}
{"query": "Are there any recent papers investigating the use of expert and anti-expert models together to guide text generation and mitigate toxic output?", "cited_paper": [{"arxiv_id": "2105.03023", "title": "DExperts: Decoding-Time Controlled Text Generation with Experts and Anti-Experts", "year": 2021}], "gt_label": [1], "date": "2021-05", "source": "LitSearch", "qid": "LitSearch_100", "valid": true}
{"query": "Are there any research papers investigating the improvement of radiology report summarization through the application of graph neural networks in conjunction with biomedical entity extraction?", "cited_paper": [{"arxiv_id": "2112.09925", "title": "Word Graph Guided Summarization for Radiology Findings", "year": 2021}], "gt_label": [1], "date": "2021-12", "source": "LitSearch", "qid": "LitSearch_101", "valid": true}
{"query": "Are there any studies investigating example-based approaches to predict user intent in few-shot learning contexts?", "cited_paper": [{"arxiv_id": "2010.08684", "title": "Example-Driven Intent Prediction with Observers", "year": 2020}, {"arxiv_id": "2109.10126", "title": "ConvFiT: Conversational Fine-Tuning of Pretrained Language Models", "year": 2021}], "gt_label": [1, 1], "date": "2021-09", "source": "LitSearch", "qid": "LitSearch_102", "valid": true}
{"query": "Are there any studies investigating sentiment analysis through text-to-graph conversion models that incorporate contextual embeddings?", "cited_paper": [{"arxiv_id": "2203.13209", "title": "Direct parsing to sentiment graphs", "year": 2022}], "gt_label": [1], "date": "2022-03", "source": "LitSearch", "qid": "LitSearch_103", "valid": true}
{"query": "Are there any studies on incorporating external commonsense knowledge into conversational models to enhance emotional support?", "cited_paper": [{"arxiv_id": "2203.13560", "title": "MISC: A MIxed Strategy-Aware Model Integrating COMET for Emotional Support Conversation", "year": 2022}], "gt_label": [1], "date": "2022-03", "source": "LitSearch", "qid": "LitSearch_104", "valid": true}
{"query": "Are there studies examining how well question answering systems perform on queries that cannot be directly recalled from their training data?", "cited_paper": [{"arxiv_id": "2008.02637", "title": "Question and Answer Test-Train Overlap in Open-Domain Question Answering Datasets", "year": 2020}], "gt_label": [1], "date": "2020-08", "source": "LitSearch", "qid": "LitSearch_105", "valid": true}
{"query": "Can you give me a paper that does self-supervised contrastive learning of sentence embeddings by sampling in-batch negatives?", "cited_paper": [{"arxiv_id": "2104.08821", "title": "SimCSE: Simple Contrastive Learning of Sentence Embeddings", "year": 2021}], "gt_label": [1], "date": "2021-04", "source": "LitSearch", "qid": "LitSearch_107", "valid": true}
{"query": "Can you recommend a dialogue summarization dataset mined from broadcast interviews on the TV or radio?", "cited_paper": [{"arxiv_id": "2103.06410", "title": "MediaSum: A Large-scale Media Interview Dataset for Dialogue Summarization", "year": 2021}], "gt_label": [1], "date": "2021-03", "source": "LitSearch", "qid": "LitSearch_108", "valid": true}
{"query": "Can you recommend research that uses an LLM to generate better prompts/tempates given task input/output?", "cited_paper": [{"arxiv_id": "2012.15723", "title": "Making Pre-trained Language Models Better Few-shot Learners", "year": 2020}], "gt_label": [1], "date": "2020-12", "source": "LitSearch", "qid": "LitSearch_109", "valid": true}
{"query": "Can you show me a paper that built a large structured knowledge base from wikipedia, that can then be used for entity linking and ranking tasks?", "cited_paper": [{"arxiv_id": "2009.02252", "title": "KILT: a Benchmark for Knowledge Intensive Language Tasks", "year": 2020}], "gt_label": [1], "date": "2020-09", "source": "LitSearch", "qid": "LitSearch_110", "valid": true}
{"query": "Could you direct me to research that evaluates few-shot slot tagging model performance by averaging micro-F1 scores across different test episodes?", "cited_paper": [{"arxiv_id": "2204.00885", "title": "Inverse is Better! Fast and Accurate Prompt for Few-shot Slot Tagging", "year": 2022}], "gt_label": [1], "date": "2022-04", "source": "LitSearch", "qid": "LitSearch_112", "valid": true}
{"query": "Could you recommend a contemporary research paper that has advanced natural language watermarking quality through algorithmic methods?", "cited_paper": [{"arxiv_id": "2305.01904", "title": "Robust Multi-bit Natural Language Watermarking through Invariant Features", "year": 2023}], "gt_label": [1], "date": "2023-05", "source": "LitSearch", "qid": "LitSearch_115", "valid": true}
{"query": "Could you recommend a paper that builds a writing assistant with autocomplete capabilities conditioned on user intent?", "cited_paper": [{"arxiv_id": "2104.07000", "title": "IGA : An Intent-Guided Authoring Assistant", "year": 2021}], "gt_label": [1], "date": "2021-04", "source": "LitSearch", "qid": "LitSearch_116", "valid": true}
{"query": "Could you recommend a study that examines how cross project code summarization evaluation methodologies compare to time-segmented eval methodology.", "cited_paper": [{"arxiv_id": "2108.09619", "title": "Impact of Evaluation Methodologies on Code Summarization", "year": 2021}], "gt_label": [1], "date": "2021-08", "source": "LitSearch", "qid": "LitSearch_118", "valid": true}
{"query": "Could you recommend a study that examines how incorporating external commonsense knowledge into conversational agents can better interpret user emotions and refine their response formulation techniques?", "cited_paper": [{"arxiv_id": "2203.13560", "title": "MISC: A MIxed Strategy-Aware Model Integrating COMET for Emotional Support Conversation", "year": 2022}], "gt_label": [1], "date": "2022-03", "source": "LitSearch", "qid": "LitSearch_119", "valid": true}
{"query": "Could you recommend a study that examines the intricacies of few-shot relation extraction challenges and introduces an approach integrating both global and local attributes alongside external descriptions of relations?", "cited_paper": [{"arxiv_id": "2109.05473", "title": "Exploring Task Difficulty for Few-Shot Relation Extraction", "year": 2021}], "gt_label": [1], "date": "2021-09", "source": "LitSearch", "qid": "LitSearch_120", "valid": true}
{"query": "Could you recommend a study that explores a pre-trained multilingual text-to-text transformer applicable to text summarization tasks?", "cited_paper": [{"arxiv_id": "2010.11934", "title": "mT5: A massively multilingual pre-trained text-to-text transformer", "year": 2020}], "gt_label": [1], "date": "2020-10", "source": "LitSearch", "qid": "LitSearch_121", "valid": true}
{"query": "Could you recommend a study that explores employing variational autoencoders to standardize open knowledge graphs?", "cited_paper": [{"arxiv_id": "2012.04780", "title": "Open Knowledge Graphs Canonicalization using Variational Autoencoders", "year": 2020}], "gt_label": [1], "date": "2020-12", "source": "LitSearch", "qid": "LitSearch_122", "valid": true}
{"query": "Could you recommend a study that explores how language models are not robust to the surface form editing when testing commonsense knowledge?", "cited_paper": [{"arxiv_id": "2107.10922", "title": "Did the Cat Drink the Coffee? Challenging Transformers with Generalized Event Knowledge", "year": 2021}], "gt_label": [1], "date": "2021-07", "source": "LitSearch", "qid": "LitSearch_123", "valid": true}
{"query": "Could you recommend a study that explores mitigating bias in natural language understanding via example reweighting?", "cited_paper": [{"arxiv_id": "2109.02071", "title": "End-to-End Self-Debiasing Framework for Robust NLU Training", "year": 2021}], "gt_label": [1], "date": "2021-09", "source": "LitSearch", "qid": "LitSearch_124", "valid": true}
{"query": "Could you recommend a study that explores strategies for improving multi-label text classification by incorporating information about label distribution directly into the loss function?", "cited_paper": [{"arxiv_id": "2109.04712", "title": "Balancing Methods for Multi-label Text Classification with Long-Tailed Class Distribution", "year": 2021}], "gt_label": [1], "date": "2021-09", "source": "LitSearch", "qid": "LitSearch_125", "valid": true}
{"query": "Could you recommend a study that explores the difficulties in creating shared multilingual vocabularies, especially focusing on the problem of over segmentation in low-resource languages?", "cited_paper": [{"arxiv_id": "2103.08490", "title": "Multi-view Subword Regularization", "year": 2021}], "gt_label": [1], "date": "2021-03", "source": "LitSearch", "qid": "LitSearch_126", "valid": true}
{"query": "Could you recommend a study that explores the improvement of Chinese sequence labeling with BERT through the incorporation of lexical data via a character-to-word bilinear attention approach?", "cited_paper": [{"arxiv_id": "2105.07148", "title": "Lexicon Enhanced Chinese Sequence Labeling Using BERT Adapter", "year": 2021}], "gt_label": [1], "date": "2021-05", "source": "LitSearch", "qid": "LitSearch_127", "valid": true}
{"query": "Could you recommend a study that initializes embeddings in multilingual transformer for subwords common with original vocabulary with original embeddings?", "cited_paper": [{"arxiv_id": "2205.06266", "title": "Lifting the Curse of Multilinguality by Pre-training Modular Transformers", "year": 2022}], "gt_label": [1], "date": "2022-05", "source": "LitSearch", "qid": "LitSearch_128", "valid": true}
{"query": "Could you recommend a study that investigates employing graph neural networks to produce replies within multi-party conversational contexts?", "cited_paper": [{"arxiv_id": "2203.08500", "title": "HeterMPC: A Heterogeneous Graph Neural Network for Response Generation in Multi-Party Conversations", "year": 2022}], "gt_label": [1], "date": "2022-03", "source": "LitSearch", "qid": "LitSearch_129", "valid": true}
{"query": "Could you recommend a study that investigates employing prefix vectors for conditional natural language generation?", "cited_paper": [{"arxiv_id": "2202.13257", "title": "Controllable Natural Language Generation with Contrastive Prefixes", "year": 2022}], "gt_label": [1], "date": "2022-02", "source": "LitSearch", "qid": "LitSearch_130", "valid": true}
{"query": "Could you recommend a study that investigates enhancing prompt engineering techniques for generative models using meta-learning strategies?", "cited_paper": [{"arxiv_id": "2209.11486", "title": "MetaPrompting: Learning to Learn Better Prompts", "year": 2022}], "gt_label": [1], "date": "2022-09", "source": "LitSearch", "qid": "LitSearch_131", "valid": true}
{"query": "Could you recommend a study that investigates guiding abstractive summarization through assessing sentence informativeness?", "cited_paper": [{"arxiv_id": "2210.12330", "title": "Salience Allocation as Guidance for Abstractive Summarization", "year": 2022}], "gt_label": [1], "date": "2022-10", "source": "LitSearch", "qid": "LitSearch_134", "valid": true}
{"query": "Could you recommend a study that investigates how a subset with clean, annotated datasets improve denoising methods?", "cited_paper": [{"arxiv_id": "2008.09887", "title": "Semi-Supervised Data Programming with Subset Selection", "year": 2020}], "gt_label": [1], "date": "2020-08", "source": "LitSearch", "qid": "LitSearch_135", "valid": true}
{"query": "Could you recommend a study that investigates how contrastive learning enhances sentence-level embeddings in natural language processing, especially for subsequent applications?", "cited_paper": [{"arxiv_id": "2104.08027", "title": "Fast, Effective, and Self-Supervised: Transforming Masked Language Models into Universal Lexical and Sentence Encoders", "year": 2021}], "gt_label": [1], "date": "2021-04", "source": "LitSearch", "qid": "LitSearch_136", "valid": true}
{"query": "Could you recommend a study that investigates how integrating model quantization with knowledge distillation?", "cited_paper": [{"arxiv_id": "2012.15701", "title": "BinaryBERT: Pushing the Limit of BERT Quantization", "year": 2020}], "gt_label": [1], "date": "2020-12", "source": "LitSearch", "qid": "LitSearch_137", "valid": true}
{"query": "Could you recommend a study that investigates representing entities in knowledge graphs with intricate geometric shapes, emphasizing probabilistic analysis and uncertainty modeling?", "cited_paper": [{"arxiv_id": "2104.04597", "title": "Probabilistic Box Embeddings for Uncertain Knowledge Graph Reasoning", "year": 2021}], "gt_label": [1], "date": "2021-04", "source": "LitSearch", "qid": "LitSearch_140", "valid": true}
{"query": "Could you recommend a study that investigates the implementation of sparsity within attention mechanisms to enhance the performance of models processing extremely lengthy documents?", "cited_paper": [{"arxiv_id": "2105.04241", "title": "ReadTwice: Reading Very Large Documents with Memories", "year": 2021}], "gt_label": [1], "date": "2021-05", "source": "LitSearch", "qid": "LitSearch_142", "valid": true}
{"query": "Could you recommend a study that investigates the use of trainable prompts to enhance the parameter optimization process in machine learning models?", "cited_paper": [{"arxiv_id": "2104.08691", "title": "The Power of Scale for Parameter-Efficient Prompt Tuning", "year": 2021}], "gt_label": [1], "date": "2021-04", "source": "LitSearch", "qid": "LitSearch_144", "valid": true}
{"query": "Could you recommend a study that uses feedback-driven decoding for producing mathematical proofs using language models?", "cited_paper": [{"arxiv_id": "2205.12443", "title": "Generating Natural Language Proofs with Verifier-Guided Search", "year": 2022}], "gt_label": [1], "date": "2022-05", "source": "LitSearch", "qid": "LitSearch_145", "valid": true}
{"query": "Could you recommend articles that explore the role of late interaction in dense retrieval systems and its influence on the performance of information retrieval?", "cited_paper": [{"arxiv_id": "2112.01488", "title": "ColBERTv2: Effective and Efficient Retrieval via Lightweight Late Interaction", "year": 2021}], "gt_label": [1], "date": "2021-12", "source": "LitSearch", "qid": "LitSearch_146", "valid": true}
{"query": "Could you recommend research articles that explore the application of contrastive learning methods to improve sentence embedding efficacy in natural language processing?", "cited_paper": [{"arxiv_id": "2105.11741", "title": "ConSERT: A Contrastive Framework for Self-Supervised Sentence Representation Transfer", "year": 2021}], "gt_label": [1], "date": "2021-05", "source": "LitSearch", "qid": "LitSearch_147", "valid": true}
{"query": "Could you recommend research papers that investigate employing Transformer-based architectures for completing knowledge graphs?", "cited_paper": [{"arxiv_id": "2009.07058", "title": "MLMLM: Link Prediction with Mean Likelihood Masked Language Model", "year": 2020}], "gt_label": [1], "date": "2020-09", "source": "LitSearch", "qid": "LitSearch_149", "valid": true}
{"query": "Could you recommend research that analyses prompt tuning as a method to improve the generalizability of pre-trained models while avoiding catastrophic forgetting?", "cited_paper": [{"arxiv_id": "2104.08691", "title": "The Power of Scale for Parameter-Efficient Prompt Tuning", "year": 2021}, {"arxiv_id": "2101.00190", "title": "Prefix-Tuning: Optimizing Continuous Prompts for Generation", "year": 2021}], "gt_label": [1, 1], "date": "2021-04", "source": "LitSearch", "qid": "LitSearch_150", "valid": true}
{"query": "Could you recommend research that assesses how well large language models, such as GPT-3, perform at coreference resolution when tested in a few-shot learning context?", "cited_paper": [{"arxiv_id": "2205.12689", "title": "Large Language Models are Few-Shot Clinical Information Extractors", "year": 2022}], "gt_label": [1], "date": "2022-05", "source": "LitSearch", "qid": "LitSearch_152", "valid": true}
{"query": "Could you recommend research that assesses techniques to mitigate intersectional biases within Transformer-based models?", "cited_paper": [{"arxiv_id": "2109.10441", "title": "Evaluating Debiasing Techniques for Intersectional Biases", "year": 2021}], "gt_label": [1], "date": "2021-09", "source": "LitSearch", "qid": "LitSearch_153", "valid": true}
{"query": "Could you recommend research that employs a relaxed l0 regularization for structured pruning to downsize language models with transformer architectures?", "cited_paper": [{"arxiv_id": "2204.00408", "title": "Structured Pruning Learns Compact and Accurate Models", "year": 2022}], "gt_label": [1], "date": "2022-04", "source": "LitSearch", "qid": "LitSearch_154", "valid": true}
{"query": "Could you recommend research that evaluates the performance decline in various language models, like BLOOM, under 4-bit integer columnar weight-only quantization?", "cited_paper": [{"arxiv_id": "2103.10360", "title": "GLM: General Language Model Pretraining with Autoregressive Blank Infilling", "year": 2021}], "gt_label": [1], "date": "2021-03", "source": "LitSearch", "qid": "LitSearch_155", "valid": true}
{"query": "Could you recommend research that examines how an annotator's individual attributes, like their gender, ethnicity, and political views, influence their judgment of content deemed offensive?", "cited_paper": [{"arxiv_id": "2110.05699", "title": "On Releasing Annotator-Level Labels and Information in Datasets", "year": 2021}, {"arxiv_id": "2111.07997", "title": "Annotators with Attitudes: How Annotator Beliefs And Identities Bias Toxic Language Detection", "year": 2021}], "gt_label": [1, 1], "date": "2021-11", "source": "LitSearch", "qid": "LitSearch_156", "valid": true}
{"query": "Could you recommend research that examines how decoding strategies like top-k impact hallucinatory in generated text?", "cited_paper": [{"arxiv_id": "2104.08455", "title": "Neural Path Hunter: Reducing Hallucination in Dialogue Systems via Path Grounding", "year": 2021}], "gt_label": [1], "date": "2021-04", "source": "LitSearch", "qid": "LitSearch_157", "valid": true}
{"query": "Could you recommend research that examines how optimized continuous prompts perform against discrete prompts in relational tasks?", "cited_paper": [{"arxiv_id": "2104.05240", "title": "Factual Probing Is [MASK]: Learning vs. Learning to Recall", "year": 2021}], "gt_label": [1], "date": "2021-04", "source": "LitSearch", "qid": "LitSearch_159", "valid": true}
{"query": "Could you recommend research that examines how syntactic configurations affect aspect-level sentiment analysis when employing a pretrained model such as RoBERTa?", "cited_paper": [{"arxiv_id": "2104.04986", "title": "Does syntax matter? A strong baseline for Aspect-based Sentiment Analysis with RoBERTa", "year": 2021}], "gt_label": [1], "date": "2021-04", "source": "LitSearch", "qid": "LitSearch_160", "valid": true}
{"query": "Could you recommend research that examines the effect of example sequencing on machine learning model efficacy in few-shot learning scenarios?", "cited_paper": [{"arxiv_id": "2104.08786", "title": "Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot Prompt Order Sensitivity", "year": 2021}], "gt_label": [1], "date": "2021-04", "source": "LitSearch", "qid": "LitSearch_162", "valid": true}
{"query": "Could you recommend research that explores how the loss of spatial information impacts the effectiveness of global features in visual tasks?", "cited_paper": [{"arxiv_id": "2204.05991", "title": "ReCLIP: A Strong Zero-Shot Baseline for Referring Expression Comprehension", "year": 2022}], "gt_label": [1], "date": "2022-04", "source": "LitSearch", "qid": "LitSearch_163", "valid": true}
{"query": "Could you recommend research that explores identifying excess and insufficient translations in evaluating machine translation, especially regarding resolving label discrepancies in cases of omitted content?", "cited_paper": [{"arxiv_id": "2203.01927", "title": "As Little as Possible, as Much as Necessary: Detecting Overand Undertranslations with Contrastive Conditioning", "year": 2022}], "gt_label": [1], "date": "2022-03", "source": "LitSearch", "qid": "LitSearch_164", "valid": true}
{"query": "Could you recommend research that improves knowledge base generation using non-differentiable evaluation metrics like BLEU, METEOR, and chrF++?", "cited_paper": [{"arxiv_id": "2108.12472", "title": "ReGen: Reinforcement Learning for Text and Knowledge Base Generation using Pretrained Language Models", "year": 2021}], "gt_label": [1], "date": "2021-08", "source": "LitSearch", "qid": "LitSearch_166", "valid": true}
{"query": "Could you recommend research that introduces a metric for assessing Text-to-Image synthesis, emphasizing the semantic congruence between the text and the produced image rather than solely the visual quality?", "cited_paper": [{"arxiv_id": "2104.08718", "title": "CLIPScore: A Reference-free Evaluation Metric for Image Captioning", "year": 2021}], "gt_label": [1], "date": "2021-04", "source": "LitSearch", "qid": "LitSearch_167", "valid": true}
{"query": "Could you recommend research that investigates generative modeling approaches for event extraction, specifically focusing on leveraging large pre-trained language models to reduce the complexity of template engineering?", "cited_paper": [{"arxiv_id": "2106.09232", "title": "Text2Event: Controllable Sequence-to-Structure Generation for End-to-end Event Extraction", "year": 2021}], "gt_label": [1], "date": "2021-06", "source": "LitSearch", "qid": "LitSearch_169", "valid": true}
{"query": "Could you recommend research that investigates how to use contrastive learning for improving logical reasoning over text?", "cited_paper": [{"arxiv_id": "2203.00357", "title": "MERIt: Meta-Path Guided Contrastive Learning for Logical Reasoning", "year": 2022}], "gt_label": [1], "date": "2022-03", "source": "LitSearch", "qid": "LitSearch_170", "valid": true}
{"query": "Could you recommend research that investigates how to use data augmentation for improving logical reasoning over text?", "cited_paper": [{"arxiv_id": "2105.03659", "title": "Logic-Driven Context Extension and Data Augmentation for Logical Reasoning of Text", "year": 2021}, {"arxiv_id": "2203.00357", "title": "MERIt: Meta-Path Guided Contrastive Learning for Logical Reasoning", "year": 2022}], "gt_label": [1, 1], "date": "2022-03", "source": "LitSearch", "qid": "LitSearch_171", "valid": true}
{"query": "Could you recommend research that investigates merging speech and text modalities in a unified representation space for processing spoken language through encoder-decoder models?", "cited_paper": [{"arxiv_id": "2110.07205", "title": "SpeechT5: Unified-Modal Encoder-Decoder Pre-Training for Spoken Language Processing", "year": 2021}], "gt_label": [1], "date": "2021-10", "source": "LitSearch", "qid": "LitSearch_172", "valid": true}
{"query": "Could you recommend research that investigates methods for fine-tuning generative language models with a focus on parameter efficiency to reduce computational demands?", "cited_paper": [{"arxiv_id": "2101.00190", "title": "Prefix-Tuning: Optimizing Continuous Prompts for Generation", "year": 2021}], "gt_label": [1], "date": "2021-01", "source": "LitSearch", "qid": "LitSearch_173", "valid": true}
{"query": "Could you recommend research that investigates techniques for creating counterfactual examples to enhance question-answering systems, such as training a T5 model augmented with retrieval?", "cited_paper": [{"arxiv_id": "2110.07596", "title": "Retrieval-guided Counterfactual Generation for QA", "year": 2021}], "gt_label": [1], "date": "2021-10", "source": "LitSearch", "qid": "LitSearch_174", "valid": true}
{"query": "Could you recommend research that investigates the impact of randomly removing words from sentences as a data augmentation strategy to mitigate overfitting in NLP models?", "cited_paper": [{"arxiv_id": "2204.10298", "title": "DiffCSE: Difference-based Contrastive Learning for Sentence Embeddings", "year": 2022}], "gt_label": [1], "date": "2022-04", "source": "LitSearch", "qid": "LitSearch_176", "valid": true}
{"query": "Could you recommend research that investigates the influence of cognitive biases on human interpretation of AI-generated explanations, specifically within the realm of explainable natural language processing?", "cited_paper": [{"arxiv_id": "2106.15355", "title": "On the Interaction of Belief Bias and Explanations", "year": 2021}], "gt_label": [1], "date": "2021-06", "source": "LitSearch", "qid": "LitSearch_177", "valid": true}
{"query": "Could you recommend scholarly articles that investigate the practice of refining language models through the exclusive modification of bias parameters in their linear components?", "cited_paper": [{"arxiv_id": "2106.10199", "title": "BitFit: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models", "year": 2021}], "gt_label": [1], "date": "2021-06", "source": "LitSearch", "qid": "LitSearch_180", "valid": true}
{"query": "Could you recommend studies on hierarchical modeling of user interests for tailoring news recommendation systems?", "cited_paper": [{"arxiv_id": "2106.04408", "title": "HieRec: Hierarchical User Interest Modeling for Personalized News Recommendation", "year": 2021}], "gt_label": [1], "date": "2021-06", "source": "LitSearch", "qid": "LitSearch_181", "valid": true}
{"query": "Could you recommend studies that concentrate on analyzing and constructing models for discourse organization in conversations involving multiple turns and parties, aimed at separating dialogues?", "cited_paper": [{"arxiv_id": "2110.08018", "title": "Structural Characterization for Dialogue Disentanglement", "year": 2021}], "gt_label": [1], "date": "2021-10", "source": "LitSearch", "qid": "LitSearch_182", "valid": true}
{"query": "Could you recommend studies that investigate fine-tuning pre-trained language models using weakly supervised learning, especially those employing techniques like contrastive regularization or self-training?", "cited_paper": [{"arxiv_id": "2010.07835", "title": "Fine-Tuning Pre-trained Language Model with Weak Supervision: A Contrastive-Regularized Self-Training Approach", "year": 2020}], "gt_label": [1], "date": "2020-10", "source": "LitSearch", "qid": "LitSearch_183", "valid": true}
{"query": "Could you recommend studies that tackle the issue of popularity bias within news recommendation engines and offer techniques to distinguish between user interests and the popularity of news items?", "cited_paper": [{"arxiv_id": "2106.01300", "title": "PP-Rec: News Recommendation with Personalized User Interest and Time-aware News Popularity", "year": 2021}], "gt_label": [1], "date": "2021-06", "source": "LitSearch", "qid": "LitSearch_184", "valid": true}
{"query": "Could you recommend studies which explore how to optimally select demonstrations for few-shot in-context learning?", "cited_paper": [{"arxiv_id": "2101.06804", "title": "What Makes Good In-Context Examples for GPT-$3$?", "year": 2021}], "gt_label": [1], "date": "2021-01", "source": "LitSearch", "qid": "LitSearch_185", "valid": true}
{"query": "Could you suggest a dataset containing diverse, intricate natural language queries that necessitate multi-step reasoning, comparing attributes, and performing set operations for answering questions from a knowledge base, without depending on entity linking?", "cited_paper": [{"arxiv_id": "2007.03875", "title": "KQA Pro: A Dataset with Explicit Compositional Programs for Complex Question Answering over Knowledge Base", "year": 2020}], "gt_label": [1], "date": "2020-07", "source": "LitSearch", "qid": "LitSearch_186", "valid": true}
{"query": "Could you suggest a dataset for question-answering frameworks utilizing temporal knowledge graphs with broad coverage?", "cited_paper": [{"arxiv_id": "2106.01515", "title": "Question Answering Over Temporal Knowledge Graphs", "year": 2021}], "gt_label": [1], "date": "2021-06", "source": "LitSearch", "qid": "LitSearch_187", "valid": true}
{"query": "Could you suggest a research article that explores generative methods for extracting information, specifically one that covers the generation of surface forms, labeling of entities, and classification of entity types?", "cited_paper": [{"arxiv_id": "2112.08340", "title": "GenIE: Generative Information Extraction", "year": 2021}], "gt_label": [1], "date": "2021-12", "source": "LitSearch", "qid": "LitSearch_188", "valid": true}
{"query": "Could you suggest a study examining how transformer models utilize feed-forward neural networks (FFNs) to encode factual information?", "cited_paper": [{"arxiv_id": "2012.14913", "title": "Transformer Feed-Forward Layers Are Key-Value Memories", "year": 2020}], "gt_label": [1], "date": "2020-12", "source": "LitSearch", "qid": "LitSearch_189", "valid": true}
{"query": "Could you suggest a study that evaluates cross-encoder BERT rankers?", "cited_paper": [{"arxiv_id": "2010.06467", "title": "Pretrained Transformers for Text Ranking: BERT and Beyond", "year": 2020}], "gt_label": [1], "date": "2020-10", "source": "LitSearch", "qid": "LitSearch_190", "valid": true}
{"query": "Could you suggest a study that examines how well contrastive learning performs in unimodal representation learning, specifically for sentence embeddings?", "cited_paper": [{"arxiv_id": "2104.08821", "title": "SimCSE: Simple Contrastive Learning of Sentence Embeddings", "year": 2021}], "gt_label": [1], "date": "2021-04", "source": "LitSearch", "qid": "LitSearch_191", "valid": true}
{"query": "Could you suggest a study that explores a cohesive pre-training method for code representation learning across different modalities?", "cited_paper": [{"arxiv_id": "2203.03850", "title": "UniXcoder: Unified Cross-Modal Pre-training for Code Representation", "year": 2022}], "gt_label": [1], "date": "2022-03", "source": "LitSearch", "qid": "LitSearch_192", "valid": true}
{"query": "Could you suggest a study that explores a compression method that merges product quantization with integer quantization for token-level retrieval?", "cited_paper": [{"arxiv_id": "2112.01488", "title": "ColBERTv2: Effective and Efficient Retrieval via Lightweight Late Interaction", "year": 2021}], "gt_label": [1], "date": "2021-12", "source": "LitSearch", "qid": "LitSearch_193", "valid": true}
{"query": "Could you suggest a study that explores data annotation paradigms that help assuage concerns in lack of annotator expertise when using crowdsourcing?", "cited_paper": [{"arxiv_id": "2112.07475", "title": "Two Contrasting Data Annotation Paradigms for Subjective NLP Tasks", "year": 2021}], "gt_label": [1], "date": "2021-12", "source": "LitSearch", "qid": "LitSearch_194", "valid": true}
{"query": "Could you suggest a study that explores employing a beta distribution to sample span sizes within unsupervised learning frameworks for text representation?", "cited_paper": [{"arxiv_id": "2006.03659", "title": "DeCLUTR: Deep Contrastive Learning for Unsupervised Textual Representations", "year": 2020}], "gt_label": [1], "date": "2020-06", "source": "LitSearch", "qid": "LitSearch_195", "valid": true}
{"query": "Could you suggest a study that explores improved training methods for dense passage retrieval within open-domain question answering systems?", "cited_paper": [{"arxiv_id": "2010.08191", "title": "RocketQA: An Optimized Training Approach to Dense Passage Retrieval for Open-Domain Question Answering", "year": 2020}], "gt_label": [1], "date": "2020-10", "source": "LitSearch", "qid": "LitSearch_196", "valid": true}
{"query": "Could you suggest a study that explores the idea of using prompts to fine-tune language models, potentially providing an alternative viewpoint to standard optimization and regularization methods?", "cited_paper": [{"arxiv_id": "2012.15723", "title": "Making Pre-trained Language Models Better Few-shot Learners", "year": 2020}], "gt_label": [1], "date": "2020-12", "source": "LitSearch", "qid": "LitSearch_197", "valid": true}
{"query": "Could you suggest a study that explores the use of multi-modal pre-training techniques to improve the comprehension of documents with a high visual content?", "cited_paper": [{"arxiv_id": "2012.14740", "title": "LayoutLMv2: Multi-modal Pre-training for Visually-Rich Document Understanding", "year": 2020}], "gt_label": [1], "date": "2020-12", "source": "LitSearch", "qid": "LitSearch_198", "valid": true}
{"query": "Could you suggest a study that investigates the incorporation of human intervention in generating adversarial examples to attack conversational agents, with the aim of improving classifier efficacy", "cited_paper": [{"arxiv_id": "2004.13637", "title": "Recipes for building an open-domain chatbot", "year": 2020}], "gt_label": [1], "date": "2020-04", "source": "LitSearch", "qid": "LitSearch_199", "valid": true}
{"query": "Could you suggest a study that proposes high-parameter efficeint fine-tuning techinque that only trains the bias terms?", "cited_paper": [{"arxiv_id": "2106.10199", "title": "BitFit: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models", "year": 2021}], "gt_label": [1], "date": "2021-06", "source": "LitSearch", "qid": "LitSearch_200", "valid": true}
{"query": "Could you suggest a thorough comparative analysis or review of the performance of different pretrained transformer architectures, such as BERT, in text ranking applications?", "cited_paper": [{"arxiv_id": "2010.06467", "title": "Pretrained Transformers for Text Ranking: BERT and Beyond", "year": 2020}], "gt_label": [1], "date": "2020-10", "source": "LitSearch", "qid": "LitSearch_201", "valid": true}
{"query": "Could you suggest a triplet-formatted structured dataset suitable for training table-to-text generation models?", "cited_paper": [{"arxiv_id": "2007.02871", "title": "DART: Open-Domain Structured Data Record to Text Generation", "year": 2020}], "gt_label": [1], "date": "2020-07", "source": "LitSearch", "qid": "LitSearch_202", "valid": true}
{"query": "Could you suggest an article that leverages the spatial information available in documents for multi-modal LMs by using a spatially-aware attention mechanism?", "cited_paper": [{"arxiv_id": "2012.14740", "title": "LayoutLMv2: Multi-modal Pre-training for Visually-Rich Document Understanding", "year": 2020}], "gt_label": [1], "date": "2020-12", "source": "LitSearch", "qid": "LitSearch_203", "valid": true}
{"query": "Could you suggest datasets that can benchmark LLM performance in achieving conversational continuity and recall over long multi-session conversations?", "cited_paper": [{"arxiv_id": "2107.07567", "title": "Beyond Goldfish Memory: Long-Term Open-Domain Conversation", "year": 2021}], "gt_label": [1], "date": "2021-07", "source": "LitSearch", "qid": "LitSearch_204", "valid": true}
{"query": "Could you suggest research on detecting common errors like additions and omissions in machine translation?", "cited_paper": [{"arxiv_id": "2203.01927", "title": "As Little as Possible, as Much as Necessary: Detecting Overand Undertranslations with Contrastive Conditioning", "year": 2022}], "gt_label": [1], "date": "2022-03", "source": "LitSearch", "qid": "LitSearch_206", "valid": true}
{"query": "Could you suggest research that assesses if language models use extended contextual information, with experiments on the book dataset from Project Gutenberg?", "cited_paper": [{"arxiv_id": "2109.09115", "title": "Do Long-Range Language Models Actually Use Long-Range Context?", "year": 2021}], "gt_label": [1], "date": "2021-09", "source": "LitSearch", "qid": "LitSearch_207", "valid": true}
{"query": "Could you suggest research that examines a system for multimodal annotation intended to assist people in analyzing dialogues within video content?", "cited_paper": [{"arxiv_id": "2101.07339", "title": "MONAH: Multi-Modal Narratives for Humans to analyze conversations", "year": 2021}], "gt_label": [1], "date": "2021-01", "source": "LitSearch", "qid": "LitSearch_209", "valid": true}
{"query": "Could you suggest research that examines how coreference resolution affects dialogue summarization quality?", "cited_paper": [{"arxiv_id": "2106.08556", "title": "Coreference-Aware Dialogue Summarization", "year": 2021}], "gt_label": [1], "date": "2021-06", "source": "LitSearch", "qid": "LitSearch_210", "valid": true}
{"query": "Could you suggest research that examines how prompt tuning can be used for domain transfer?", "cited_paper": [{"arxiv_id": "2104.08691", "title": "The Power of Scale for Parameter-Efficient Prompt Tuning", "year": 2021}], "gt_label": [1], "date": "2021-04", "source": "LitSearch", "qid": "LitSearch_211", "valid": true}
{"query": "Could you suggest research that examines how prompt tuning efficacy in language model training is affected by scaling?", "cited_paper": [{"arxiv_id": "2104.08691", "title": "The Power of Scale for Parameter-Efficient Prompt Tuning", "year": 2021}], "gt_label": [1], "date": "2021-04", "source": "LitSearch", "qid": "LitSearch_212", "valid": true}
{"query": "Could you suggest research that examines how strangers exchange information during discussions, specifically concentrating on the kinds of information typically shared during first encounters?", "cited_paper": [{"arxiv_id": "2107.07567", "title": "Beyond Goldfish Memory: Long-Term Open-Domain Conversation", "year": 2021}], "gt_label": [1], "date": "2021-07", "source": "LitSearch", "qid": "LitSearch_213", "valid": true}
{"query": "Could you suggest research that examines how the order of in-context examples influences the efficacy of in-context learning in language models?", "cited_paper": [{"arxiv_id": "2104.08786", "title": "Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot Prompt Order Sensitivity", "year": 2021}], "gt_label": [1], "date": "2021-04", "source": "LitSearch", "qid": "LitSearch_214", "valid": true}
{"query": "Could you suggest research that examines how well language models work with creole languages, particularly in relation to their effectiveness with Nigerian Pidgin, given its close linguistic relationship with English?", "cited_paper": [{"arxiv_id": "2109.06074", "title": "On Language Models for Creoles", "year": 2021}], "gt_label": [1], "date": "2021-09", "source": "LitSearch", "qid": "LitSearch_215", "valid": true}
{"query": "Could you suggest research that examines how well prompt tuning using soft embeddings works for utilizing pretrained language models in downstream applications with minimal finetuning?", "cited_paper": [{"arxiv_id": "2104.08691", "title": "The Power of Scale for Parameter-Efficient Prompt Tuning", "year": 2021}], "gt_label": [1], "date": "2021-04", "source": "LitSearch", "qid": "LitSearch_216", "valid": true}
{"query": "Could you suggest research that examines how well structured pruning techniques perform in developing both small and precise models in natural language processing?", "cited_paper": [{"arxiv_id": "2204.00408", "title": "Structured Pruning Learns Compact and Accurate Models", "year": 2022}], "gt_label": [1], "date": "2022-04", "source": "LitSearch", "qid": "LitSearch_217", "valid": true}
{"query": "Could you suggest research that examines the application of specialized architecture in pre-trained language models to enhance text-to-SQL tasks?", "cited_paper": [{"arxiv_id": "2010.12773", "title": "Structure-Grounded Pretraining for Text-to-SQL", "year": 2020}], "gt_label": [1], "date": "2020-10", "source": "LitSearch", "qid": "LitSearch_218", "valid": true}
{"query": "Could you suggest research that examines the challenges faced by neural networks in discerning causation from correlation?", "cited_paper": [{"arxiv_id": "2005.13407", "title": "CausaLM: Causal Model Explanation Through Counterfactual Language Models", "year": 2020}], "gt_label": [1], "date": "2020-05", "source": "LitSearch", "qid": "LitSearch_219", "valid": true}
{"query": "Could you suggest research that examines the difficulties in employing weakly labeled datasets for named entity recognition and offers techniques to mitigate the associated data noise?", "cited_paper": [{"arxiv_id": "2106.08977", "title": "Named Entity Recognition with Small Strongly Labeled and Large Weakly Labeled Data", "year": 2021}], "gt_label": [1], "date": "2021-06", "source": "LitSearch", "qid": "LitSearch_220", "valid": true}
{"query": "Could you suggest research that examines the effects of starting language models with weights from pretrained nondiffusion models on the convergence behavior of diffusion losses?", "cited_paper": [{"arxiv_id": "2210.17432", "title": "SSD-LM: Semi-autoregressive Simplex-based Diffusion Language Model for Text Generation and Modular Control", "year": 2022}], "gt_label": [1], "date": "2022-10", "source": "LitSearch", "qid": "LitSearch_221", "valid": true}
{"query": "Could you suggest research that explores a pre-trained encoder-decoder architecture aimed at comprehending and generating code, potentially beneficial for enhancing automated code repair systems?", "cited_paper": [{"arxiv_id": "2109.00859", "title": "CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation", "year": 2021}], "gt_label": [1], "date": "2021-09", "source": "LitSearch", "qid": "LitSearch_222", "valid": true}
{"query": "Could you suggest research that explores employing independently sampled dropout masks to generate positive pairs in contrastive learning for sentence embeddings?", "cited_paper": [{"arxiv_id": "2104.08821", "title": "SimCSE: Simple Contrastive Learning of Sentence Embeddings", "year": 2021}], "gt_label": [1], "date": "2021-04", "source": "LitSearch", "qid": "LitSearch_223", "valid": true}
{"query": "Could you suggest research that explores generating synthetic labels for sentences within a vast text collection to pre-train models for few-shot learning applications?", "cited_paper": [{"arxiv_id": "2109.04332", "title": "PPT: Pre-trained Prompt Tuning for Few-shot Learning", "year": 2021}], "gt_label": [1], "date": "2021-09", "source": "LitSearch", "qid": "LitSearch_224", "valid": true}
{"query": "Could you suggest research that explores the drawbacks of dense retrieval systems especially with large-scale indices?", "cited_paper": [{"arxiv_id": "2012.14210", "title": "The Curse of Dense Low-Dimensional Information Retrieval for Large Index Sizes", "year": 2020}], "gt_label": [1], "date": "2020-12", "source": "LitSearch", "qid": "LitSearch_225", "valid": true}
{"query": "Could you suggest research that explores the idea of training soft prompts rather than identifying fixed ones within the realm of prompt tuning?", "cited_paper": [{"arxiv_id": "2104.05240", "title": "Factual Probing Is [MASK]: Learning vs. Learning to Recall", "year": 2021}], "gt_label": [1], "date": "2021-04", "source": "LitSearch", "qid": "LitSearch_226", "valid": true}
{"query": "Could you suggest research that includes an online community dataset used to examine machine learning models for hate speech identification?", "cited_paper": [{"arxiv_id": "2103.01664", "title": "Hate Towards the Political Opponent: A Twitter Corpus Study of the 2020 US Elections on the Basis of Offensive Speech and Stance Detection", "year": 2021}], "gt_label": [1], "date": "2021-03", "source": "LitSearch", "qid": "LitSearch_228", "valid": true}
{"query": "Could you suggest research that investigates a clustering-based efficient attention mechanism within Transformer models?", "cited_paper": [{"arxiv_id": "2003.05997", "title": "Efficient Content-Based Sparse Attention with Routing Transformers", "year": 2020}], "gt_label": [1], "date": "2020-03", "source": "LitSearch", "qid": "LitSearch_229", "valid": true}
{"query": "Could you suggest research that investigates across multiple languages how dictionary definitions and word embedding models compare?", "cited_paper": [{"arxiv_id": "2205.13858", "title": "Semeval-2022 Task 1: CODWOE -- Comparing Dictionaries and Word Embeddings", "year": 2022}], "gt_label": [1], "date": "2022-05", "source": "LitSearch", "qid": "LitSearch_230", "valid": true}
{"query": "Could you suggest research that investigates applying combinatorial optimization techniques in unsupervised entity matching?", "cited_paper": [{"arxiv_id": "2109.02363", "title": "From Alignment to Assignment: Frustratingly Simple Unsupervised Entity Alignment", "year": 2021}], "gt_label": [1], "date": "2021-09", "source": "LitSearch", "qid": "LitSearch_231", "valid": true}
{"query": "Could you suggest research that investigates efficient finetuning methods that only trains very very few parameters in language models?", "cited_paper": [{"arxiv_id": "2106.10199", "title": "BitFit: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models", "year": 2021}], "gt_label": [1], "date": "2021-06", "source": "LitSearch", "qid": "LitSearch_233", "valid": true}
{"query": "Could you suggest research that investigates employing graph attention techniques for integrating multiple modalities for identifying emotions?", "cited_paper": [{"arxiv_id": "2205.02455", "title": "COGMEN: COntextualized GNN based Multimodal Emotion recognitioN", "year": 2022}], "gt_label": [1], "date": "2022-05", "source": "LitSearch", "qid": "LitSearch_234", "valid": true}
{"query": "Could you suggest research that investigates expanding keyword collections through embedding similarity in weakly-supervised document categorization?", "cited_paper": [{"arxiv_id": "2010.12794", "title": "X-Class: Text Classification with Extremely Weak Supervision", "year": 2020}], "gt_label": [1], "date": "2020-10", "source": "LitSearch", "qid": "LitSearch_236", "valid": true}
{"query": "Could you suggest research that investigates graph-based methods for predicting connections in knowledge graphs, focusing on n-ary relational facts rather than just simple triple structures?", "cited_paper": [{"arxiv_id": "2105.08476", "title": "Link Prediction on N-ary Relational Facts: A Graph-based Approach", "year": 2021}], "gt_label": [1], "date": "2021-05", "source": "LitSearch", "qid": "LitSearch_237", "valid": true}
{"query": "Could you suggest research that investigates how hierarchical structures within transformers enhance task-oriented dialogue systems?", "cited_paper": [{"arxiv_id": "2011.08067", "title": "Hierarchical Transformer for Task Oriented Dialog Systems", "year": 2020}], "gt_label": [1], "date": "2020-11", "source": "LitSearch", "qid": "LitSearch_239", "valid": true}
{"query": "Could you suggest research that investigates how many evidence sentences are needed for document-level RE?", "cited_paper": [{"arxiv_id": "2106.01793", "title": "Three Sentences Are All You Need: Local Path Enhanced Document Relation Extraction", "year": 2021}], "gt_label": [1], "date": "2021-06", "source": "LitSearch", "qid": "LitSearch_240", "valid": true}
{"query": "Could you suggest research that investigates how neural language models' forecasts correlate with human linguistic processing, especially in terms of syntactic surprisal?", "cited_paper": [{"arxiv_id": "2210.12187", "title": "Syntactic Surprisal From Neural Models Predicts, But Underestimates, Human Processing Difficulty From Syntactic Ambiguities", "year": 2022}], "gt_label": [1], "date": "2022-10", "source": "LitSearch", "qid": "LitSearch_241", "valid": true}
{"query": "Could you suggest research that investigates the use of past dialogues for enhancing query expansion in conversational search systems?", "cited_paper": [{"arxiv_id": "2010.04898", "title": "Open-Domain Question Answering Goes Conversational via Question Rewriting", "year": 2020}], "gt_label": [1], "date": "2020-10", "source": "LitSearch", "qid": "LitSearch_244", "valid": true}
{"query": "Could you suggest research that investigates training BERT-based classifiers with Wikipedia data for zero-shot text classification in open domains?", "cited_paper": [{"arxiv_id": "2306.17290", "title": "Towards Open-Domain Topic Classification", "year": 2023}], "gt_label": [1], "date": "2023-06", "source": "LitSearch", "qid": "LitSearch_245", "valid": true}
{"query": "Could you suggest research that trains language models specifically on mental health-related social media data and the model is helpful for identifying mental health issues?", "cited_paper": [{"arxiv_id": "2110.15621", "title": "MentalBERT: Publicly Available Pretrained Language Models for Mental Healthcare", "year": 2021}], "gt_label": [1], "date": "2021-10", "source": "LitSearch", "qid": "LitSearch_248", "valid": true}
{"query": "Could you suggest research that tries to interpret how bi-directional RNNs manage to carry out Named Entity Recognition (NER) tasks?", "cited_paper": [{"arxiv_id": "2004.04564", "title": "Interpretability Analysis for Named Entity Recognition to Understand System Predictions and How They Can Improve", "year": 2020}], "gt_label": [1], "date": "2020-04", "source": "LitSearch", "qid": "LitSearch_249", "valid": true}
{"query": "Could you suggest some work that develops multimodal models with contrastive learning approaches?", "cited_paper": [{"arxiv_id": "2012.15409", "title": "UNIMO: Towards Unified-Modal Understanding and Generation via Cross-Modal Contrastive Learning", "year": 2020}], "gt_label": [1], "date": "2020-12", "source": "LitSearch", "qid": "LitSearch_250", "valid": true}
{"query": "Has any research explored using other off-the-shelf summarization techniques to improve neural abstractive summarization?", "cited_paper": [{"arxiv_id": "2010.08014", "title": "GSum: A General Framework for Guided Neural Abstractive Summarization", "year": 2020}], "gt_label": [1], "date": "2020-10", "source": "LitSearch", "qid": "LitSearch_252", "valid": true}
{"query": "Has any research tried to mitigate overfitting in weakly-supervised settings by introducing an adversarial framework where the influence of the labeling function is a hyperparameter for the feature representation?", "cited_paper": [{"arxiv_id": "2109.07994", "title": "KnowMAN: Weakly Supervised Multinomial Adversarial Networks", "year": 2021}], "gt_label": [1], "date": "2021-09", "source": "LitSearch", "qid": "LitSearch_253", "valid": true}
{"query": "Has there been any research that uses multiple models to learn the preferences of individual annotators, and then ensembles these models to obtain majority-vote preference scores while also having an uncertainty measure?", "cited_paper": [{"arxiv_id": "2110.05719", "title": "Dealing with Disagreements: Looking Beyond the Majority Vote in Subjective Annotations", "year": 2021}], "gt_label": [1], "date": "2021-10", "source": "LitSearch", "qid": "LitSearch_255", "valid": true}
{"query": "Has there been any work that improves the work on integrated gradients by leveraging interpolation strategies to improve gradient accuracies?", "cited_paper": [{"arxiv_id": "2108.13654", "title": "Discretized Integrated Gradients for Explaining Language Models", "year": 2021}], "gt_label": [1], "date": "2021-08", "source": "LitSearch", "qid": "LitSearch_256", "valid": true}
{"query": "Have any new metrics been developed to assess the factual alignment of machine-generated summaries with their original source texts?", "cited_paper": [{"arxiv_id": "2111.09525", "title": "SummaC: Re-Visiting NLI-based Models for Inconsistency Detection in Summarization", "year": 2021}, {"arxiv_id": "2103.12693", "title": "QuestEval: Summarization Asks for Fact-based Evaluation", "year": 2021}], "gt_label": [1, 1], "date": "2021-11", "source": "LitSearch", "qid": "LitSearch_257", "valid": true}
{"query": "Have any papers tried to address the background-shift problem in named entity recognition by identifying non-entity type tokens belonging to old entity types through knowledge distillation from an old model?", "cited_paper": [{"arxiv_id": "2210.03980", "title": "Distilling Causal Effect from Miscellaneous Other-Class for Continual Named Entity Recognition", "year": 2022}], "gt_label": [1], "date": "2022-10", "source": "LitSearch", "qid": "LitSearch_258", "valid": true}
{"query": "Have any recent publications explored the use of neural network methods, like transformer architectures, in creating novel readability metrics?", "cited_paper": [{"arxiv_id": "1907.11779", "title": "Supervised and Unsupervised Neural Approaches to Text Readability", "year": 2019}], "gt_label": [1], "date": "2019-07", "source": "LitSearch", "qid": "LitSearch_259", "valid": true}
{"query": "Have any research efforts been made to gather dialogue data via crowdworkers to enhance conversational information retrieval systems?", "cited_paper": [{"arxiv_id": "2109.08877", "title": "DuRecDial 2.0: A Bilingual Parallel Corpus for Conversational Recommendation", "year": 2021}], "gt_label": [1], "date": "2021-09", "source": "LitSearch", "qid": "LitSearch_260", "valid": true}
{"query": "Have any research papers been published on models for representing sentences in under-resourced languages like Slovenian or Romanian?", "cited_paper": [{"arxiv_id": "2007.01852", "title": "Language-agnostic BERT Sentence Embedding", "year": 2020}], "gt_label": [1], "date": "2020-07", "source": "LitSearch", "qid": "LitSearch_261", "valid": true}
{"query": "Have any research papers collected feedback from real users who were using LLMs for scientific writing?", "cited_paper": [{"arxiv_id": "2110.07640", "title": "Sparks: Inspiration for Science Writing using Language Models", "year": 2021}], "gt_label": [1], "date": "2021-10", "source": "LitSearch", "qid": "LitSearch_262", "valid": true}
{"query": "Have any research papers critically analyzed the performance speed of non-autoregressive translation models compared to autoregressive models", "cited_paper": [{"arxiv_id": "2205.01966", "title": "Non-Autoregressive Machine Translation: It's Not as Fast as it Seems", "year": 2022}], "gt_label": [1], "date": "2022-05", "source": "LitSearch", "qid": "LitSearch_263", "valid": true}
{"query": "Have any research papers examined the efficacy of multilingual text-to-text transformers across various languages, particularly those less represented in pretraining corpora?", "cited_paper": [{"arxiv_id": "2010.11934", "title": "mT5: A massively multilingual pre-trained text-to-text transformer", "year": 2020}], "gt_label": [1], "date": "2020-10", "source": "LitSearch", "qid": "LitSearch_264", "valid": true}
{"query": "Have any research papers examined whether using language models for providing evidence in fact-checking systems risks propagating biases?", "cited_paper": [{"arxiv_id": "2103.09535", "title": "Towards Few-Shot Fact-Checking via Perplexity", "year": 2021}], "gt_label": [1], "date": "2021-03", "source": "LitSearch", "qid": "LitSearch_265", "valid": true}
{"query": "Have any research papers introduced a dedicated pre-training architecture designed to improve dense retrieval system efficacy?", "cited_paper": [{"arxiv_id": "2104.08253", "title": "Condenser: a Pre-training Architecture for Dense Retrieval", "year": 2021}], "gt_label": [1], "date": "2021-04", "source": "LitSearch", "qid": "LitSearch_267", "valid": true}
{"query": "Have any research papers investigated human capacity to distinguish AI-generated text from human-authored text?", "cited_paper": [{"arxiv_id": "2107.00061", "title": "All That's 'Human' Is Not Gold: Evaluating Human Evaluation of Generated Text", "year": 2021}], "gt_label": [1], "date": "2021-07", "source": "LitSearch", "qid": "LitSearch_268", "valid": true}
{"query": "Have any research papers investigated the creation of datasets through model-generated data for annotators to identify hallucinations in the results, specifically for developing diagnostic evaluation datasets?", "cited_paper": [{"arxiv_id": "2104.08202", "title": "$Q^2$: Evaluating Factual Consistency in Knowledge-Grounded Dialogues via Question Generation and Question Answering", "year": 2021}], "gt_label": [1], "date": "2021-04", "source": "LitSearch", "qid": "LitSearch_269", "valid": true}
{"query": "Have any research papers suggested techniques for automatically choosing in-context examples?", "cited_paper": [{"arxiv_id": "2211.04486", "title": "Active Example Selection for In-Context Learning", "year": 2022}], "gt_label": [1], "date": "2022-11", "source": "LitSearch", "qid": "LitSearch_271", "valid": true}
{"query": "Have any research papers tried to create conversational agents with inner states represented by a knowledge graph that can be continually updated based on the agents environment?", "cited_paper": [{"arxiv_id": "2103.07011", "title": "Towards Socially Intelligent Agents with Mental State Transition and Human Utility", "year": 2021}], "gt_label": [1], "date": "2021-03", "source": "LitSearch", "qid": "LitSearch_272", "valid": true}
{"query": "Have any studies explored the creation of memory management systems in AI to improve sustained conversational capabilities and tackle the challenge of contextual retention over extended periods?", "cited_paper": [{"arxiv_id": "2203.05797", "title": "Long Time No See! Open-Domain Conversation with Long-Term Persona Memory", "year": 2022}], "gt_label": [1], "date": "2022-03", "source": "LitSearch", "qid": "LitSearch_273", "valid": true}
{"query": "Have there been any advancements in language models that operate without tokenization and emphasize encoding at the character level, and what benefits might they have compared to conventional methods using subword tokenization?", "cited_paper": [{"arxiv_id": "2103.06874", "title": "CANINE: Pre-training an Efficient Tokenization-Free Encoder for Language Representation", "year": 2021}], "gt_label": [1], "date": "2021-03", "source": "LitSearch", "qid": "LitSearch_274", "valid": true}
{"query": "How can SQL-to-text be utilized to improve text-to-SQL parsing through data augmentation techniques?", "cited_paper": [{"arxiv_id": "2103.02227", "title": "Data Augmentation with Hierarchical SQL-to-Question Generation for Cross-domain Text-to-SQL Parsing", "year": 2021}], "gt_label": [1], "date": "2021-03", "source": "LitSearch", "qid": "LitSearch_276", "valid": true}
{"query": "How can dense retrieval models for open-domain question answering be improved, specifically through the use of hard negative mining techniques?", "cited_paper": [{"arxiv_id": "2010.08191", "title": "RocketQA: An Optimized Training Approach to Dense Passage Retrieval for Open-Domain Question Answering", "year": 2020}], "gt_label": [1], "date": "2020-10", "source": "LitSearch", "qid": "LitSearch_277", "valid": true}
{"query": "I know about prompt tuning, but have any works tried learning embeddings that are inputted to every transformer layer in a language model?", "cited_paper": [{"arxiv_id": "2101.00190", "title": "Prefix-Tuning: Optimizing Continuous Prompts for Generation", "year": 2021}], "gt_label": [1], "date": "2021-01", "source": "LitSearch", "qid": "LitSearch_278", "valid": true}
{"query": "In multi-hop question answering, is there a paper that explores \"per-hop\" retrieval evaluation that treats each hop of retrieval independently?", "cited_paper": [{"arxiv_id": "2106.08433", "title": "Combining Lexical and Dense Retrieval for Computationally Efficient Multi-hop Question Answering", "year": 2021}], "gt_label": [1], "date": "2021-06", "source": "LitSearch", "qid": "LitSearch_279", "valid": true}
{"query": "Is there a benchmark designed to assess language models' social biases within question answering frameworks, specifically one encompassing diverse categories of social bias and offering instances of ambiguity and its resolution?", "cited_paper": [{"arxiv_id": "2110.08193", "title": "BBQ: A Hand-Built Bias Benchmark for Question Answering", "year": 2021}], "gt_label": [1], "date": "2021-10", "source": "LitSearch", "qid": "LitSearch_281", "valid": true}
{"query": "Is there a comprehensive dataset available for summarizing broad-spectrum conversational dialogues?", "cited_paper": [{"arxiv_id": "2105.06762", "title": "DialogSum: A Real-Life Scenario Dialogue Summarization Dataset", "year": 2021}], "gt_label": [1], "date": "2021-05", "source": "LitSearch", "qid": "LitSearch_282", "valid": true}
{"query": "Is there a dataset available for open-domain targeted sentiment analysis containing user reviews from platforms like Yelp and Amazon?", "cited_paper": [{"arxiv_id": "2012.14541", "title": "YASO: A Targeted Sentiment Analysis Evaluation Dataset for Open-Domain Reviews", "year": 2020}], "gt_label": [1], "date": "2020-12", "source": "LitSearch", "qid": "LitSearch_283", "valid": true}
{"query": "Is there a dataset containing question-answer pairs used in psychological counseling available for research?", "cited_paper": [{"arxiv_id": "2106.01702", "title": "PsyQA: A Chinese Dataset for Generating Long Counseling Text for Mental Health Support", "year": 2021}], "gt_label": [1], "date": "2021-06", "source": "LitSearch", "qid": "LitSearch_284", "valid": true}
{"query": "Is there a research paper that has developed a customer service conversation dataset aimed at forecasting customer intentions, taking into account the limitations imposed by agent protocols?", "cited_paper": [{"arxiv_id": "2104.00783", "title": "Action-Based Conversations Dataset: A Corpus for Building More In-Depth Task-Oriented Dialogue Systems", "year": 2021}], "gt_label": [1], "date": "2021-04", "source": "LitSearch", "qid": "LitSearch_285", "valid": true}
{"query": "Is there a specialized question answering dataset that concentrates on intricate tables within certain sectors, like the airline industry?", "cited_paper": [{"arxiv_id": "2106.12944", "title": "AIT-QA: Question Answering Dataset over Complex Tables in the Airline Industry", "year": 2021}], "gt_label": [1], "date": "2021-06", "source": "LitSearch", "qid": "LitSearch_286", "valid": true}
{"query": "Is there a study that investigates if large language models can assist with generating ideas pertinent to scientific concepts during the scientific writing ideation stage?", "cited_paper": [{"arxiv_id": "2110.07640", "title": "Sparks: Inspiration for Science Writing using Language Models", "year": 2021}], "gt_label": [1], "date": "2021-10", "source": "LitSearch", "qid": "LitSearch_287", "valid": true}
{"query": "Is there any paper that tried fine-tuning mBERT to enhance word-level alignment in a multilingual setting?", "cited_paper": [{"arxiv_id": "2101.08231", "title": "Word Alignment by Fine-tuning Embeddings on Parallel Corpora", "year": 2021}], "gt_label": [1], "date": "2021-01", "source": "LitSearch", "qid": "LitSearch_288", "valid": true}
{"query": "Is there research examining if multilingual pre-trained models utilize identical sets of neural units to encode morphosyntactic features in various languages?", "cited_paper": [{"arxiv_id": "2205.02023", "title": "Same Neurons, Different Languages: Probing Morphosyntax in Multilingual Pre-trained Models", "year": 2022}], "gt_label": [1], "date": "2022-05", "source": "LitSearch", "qid": "LitSearch_290", "valid": true}
{"query": "Is there research on a specialized language model designed to detect mental health issues on social media platforms?", "cited_paper": [{"arxiv_id": "2110.15621", "title": "MentalBERT: Publicly Available Pretrained Language Models for Mental Healthcare", "year": 2021}], "gt_label": [1], "date": "2021-10", "source": "LitSearch", "qid": "LitSearch_291", "valid": true}
{"query": "Is there research that argues for transparency and open-access to the training data of LLMs and demontrates its importance with case studies of existing data?", "cited_paper": [{"arxiv_id": "2104.08758", "title": "Documenting Large Webtext Corpora: A Case Study on the Colossal Clean Crawled Corpus", "year": 2021}], "gt_label": [1], "date": "2021-04", "source": "LitSearch", "qid": "LitSearch_292", "valid": true}
{"query": "Is there research that investigates embedding multi-bit data into watermarks to improve resilience to text corruption, particularly aimed at safeguarding keywords and syntactic elements from modification?", "cited_paper": [{"arxiv_id": "2305.01904", "title": "Robust Multi-bit Natural Language Watermarking through Invariant Features", "year": 2023}], "gt_label": [1], "date": "2023-05", "source": "LitSearch", "qid": "LitSearch_293", "valid": true}
{"query": "What approaches have been suggested to lower the computational demands of basic attention mechanisms in transformers, and is there a study that explores a memory-saving technique through selective key-value pairing for each query?", "cited_paper": [{"arxiv_id": "2106.06899", "title": "Memory-efficient Transformers via Top-$k$ Attention", "year": 2021}], "gt_label": [1], "date": "2021-06", "source": "LitSearch", "qid": "LitSearch_295", "valid": true}
{"query": "What are some scholarly articles that explore scaling laws for parameter-efficient prompt tuning techniques for fine-tuning language models?", "cited_paper": [{"arxiv_id": "2104.08691", "title": "The Power of Scale for Parameter-Efficient Prompt Tuning", "year": 2021}], "gt_label": [1], "date": "2021-04", "source": "LitSearch", "qid": "LitSearch_296", "valid": true}
{"query": "What are some scholarly articles that explore the enhancement of dense retrieval in student models through the application of prediction distributions from teacher models?", "cited_paper": [{"arxiv_id": "2112.01488", "title": "ColBERTv2: Effective and Efficient Retrieval via Lightweight Late Interaction", "year": 2021}], "gt_label": [1], "date": "2021-12", "source": "LitSearch", "qid": "LitSearch_297", "valid": true}
{"query": "What are the latest developments in conversational agents that integrate external knowledge sources and employ diverse tactics to offer emotional support during interactions?", "cited_paper": [{"arxiv_id": "2203.13560", "title": "MISC: A MIxed Strategy-Aware Model Integrating COMET for Emotional Support Conversation", "year": 2022}], "gt_label": [1], "date": "2022-03", "source": "LitSearch", "qid": "LitSearch_300", "valid": true}
{"query": "What benchmarks have prior research utilized to assess models performing knowledge-rich language tasks?", "cited_paper": [{"arxiv_id": "2009.02252", "title": "KILT: a Benchmark for Knowledge Intensive Language Tasks", "year": 2020}], "gt_label": [1], "date": "2020-09", "source": "LitSearch", "qid": "LitSearch_301", "valid": true}
{"query": "What concerns or key points have been highlighted in scholarly articles about employing random divisions in machine learning datasets, especially with respect to contamination of the test set?", "cited_paper": [{"arxiv_id": "2005.00636", "title": "We Need to Talk About Random Splits", "year": 2020}], "gt_label": [1], "date": "2020-05", "source": "LitSearch", "qid": "LitSearch_302", "valid": true}
{"query": "What difficulties do neural conversational models face, particularly concerning the decoder's ability to produce precise and fact-based replies?", "cited_paper": [{"arxiv_id": "2104.08455", "title": "Neural Path Hunter: Reducing Hallucination in Dialogue Systems via Path Grounding", "year": 2021}], "gt_label": [1], "date": "2021-04", "source": "LitSearch", "qid": "LitSearch_303", "valid": true}
{"query": "What literature is available on training semantic parsers with deep learning for knowledge base question answering systems, especially those employing tree-structured representations of queries?", "cited_paper": [{"arxiv_id": "2012.01707", "title": "Leveraging Abstract Meaning Representation for Knowledge Base Question Answering", "year": 2020}], "gt_label": [1], "date": "2020-12", "source": "LitSearch", "qid": "LitSearch_304", "valid": true}
{"query": "What methods exist for tailoring news suggestions that consider a user's preferences as well as the current popularity of news stories?", "cited_paper": [{"arxiv_id": "2106.01300", "title": "PP-Rec: News Recommendation with Personalized User Interest and Time-aware News Popularity", "year": 2021}], "gt_label": [1], "date": "2021-06", "source": "LitSearch", "qid": "LitSearch_305", "valid": true}
{"query": "What papers discuss the effect of false negatives among hard negatives in dense retriever training?", "cited_paper": [{"arxiv_id": "2205.00656", "title": "Debiased Contrastive Learning of Unsupervised Sentence Representations", "year": 2022}], "gt_label": [1], "date": "2022-05", "source": "LitSearch", "qid": "LitSearch_306", "valid": true}
{"query": "What papers explore replacing schema linking with human annotations to study the maximum potential benefit of schema linking for text-to-SQL tasks?", "cited_paper": [{"arxiv_id": "2109.10540", "title": "Awakening Latent Grounding from Pretrained Language Models for Semantic Parsing", "year": 2021}], "gt_label": [1], "date": "2021-09", "source": "LitSearch", "qid": "LitSearch_307", "valid": true}
{"query": "What recent developments in transformer architecture aim to improve the multi-head self-attention mechanism for better transmission of unprocessed attention scores and more stable training?", "cited_paper": [{"arxiv_id": "2012.11747", "title": "RealFormer: Transformer Likes Residual Attention", "year": 2020}], "gt_label": [1], "date": "2020-12", "source": "LitSearch", "qid": "LitSearch_308", "valid": true}
{"query": "What recent research has been conducted on improving few-shot learning in pre-trained language models through the use of prompt-based fine tuning techniques?", "cited_paper": [{"arxiv_id": "2012.15723", "title": "Making Pre-trained Language Models Better Few-shot Learners", "year": 2020}], "gt_label": [1], "date": "2020-12", "source": "LitSearch", "qid": "LitSearch_309", "valid": true}
{"query": "What research articles should I consult to understand a method for quantitatively assessing how successful neuron interventions are at altering a model's predictions?", "cited_paper": [{"arxiv_id": "2112.06837", "title": "Sparse Interventions in Language Models with Differentiable Masking", "year": 2021}], "gt_label": [1], "date": "2021-12", "source": "LitSearch", "qid": "LitSearch_310", "valid": true}
{"query": "What research exists comparing adapter-based tuning and full fine-tuning efficacy in limited data contexts?", "cited_paper": [{"arxiv_id": "2101.00190", "title": "Prefix-Tuning: Optimizing Continuous Prompts for Generation", "year": 2021}, {"arxiv_id": "2110.07577", "title": "UniPELT: A Unified Framework for Parameter-Efficient Language Model Tuning", "year": 2021}], "gt_label": [1, 1], "date": "2021-10", "source": "LitSearch", "qid": "LitSearch_311", "valid": true}
{"query": "What research exists on employing generative models with latent variable to capture semantic dependencies in conversational systems?", "cited_paper": [{"arxiv_id": "2106.03635", "title": "GTM: A Generative Triple-Wise Model for Conversational Question Generation", "year": 2021}], "gt_label": [1], "date": "2021-06", "source": "LitSearch", "qid": "LitSearch_312", "valid": true}
{"query": "What research exists on leveraging syntactic roles and semantic interpretations for backdoor attacks on natural language processing systems?", "cited_paper": [{"arxiv_id": "2105.12400", "title": "Hidden Killer: Invisible Textual Backdoor Attacks with Syntactic Trigger", "year": 2021}, {"arxiv_id": "2106.06361", "title": "Turn the Combination Lock: Learnable Textual Backdoor Attacks via Word Substitution", "year": 2021}], "gt_label": [1, 1], "date": "2021-06", "source": "LitSearch", "qid": "LitSearch_314", "valid": true}
{"query": "What research exists on the impact of scaling on prompt tuning efficiency in pre-trained language models?", "cited_paper": [{"arxiv_id": "2104.08691", "title": "The Power of Scale for Parameter-Efficient Prompt Tuning", "year": 2021}], "gt_label": [1], "date": "2021-04", "source": "LitSearch", "qid": "LitSearch_315", "valid": true}
{"query": "What research exists on using reinforcement learning methods for event prediction in temporal knowledge graphs?", "cited_paper": [{"arxiv_id": "2109.04101", "title": "TimeTraveler: Reinforcement Learning for Temporal Knowledge Graph Forecasting", "year": 2021}], "gt_label": [1], "date": "2021-09", "source": "LitSearch", "qid": "LitSearch_316", "valid": true}
{"query": "What research has been conducted on applying contrastive techniques to distinguish normal from abnormal imagery for the creation of radiology reports?", "cited_paper": [{"arxiv_id": "2106.06965", "title": "Contrastive Attention for Automatic Chest X-ray Report Generation", "year": 2021}], "gt_label": [1], "date": "2021-06", "source": "LitSearch", "qid": "LitSearch_317", "valid": true}
{"query": "What research has been conducted on creating neural network frameworks for parsing text into SQL?", "cited_paper": [{"arxiv_id": "2104.04689", "title": "ShadowGNN: Graph Projection Neural Network for Text-to-SQL Parser", "year": 2021}], "gt_label": [1], "date": "2021-04", "source": "LitSearch", "qid": "LitSearch_318", "valid": true}
{"query": "What research has been conducted on determining the ideal segment length for unsupervised keyphrase extraction?", "cited_paper": [{"arxiv_id": "2109.07293", "title": "Unsupervised Keyphrase Extraction by Jointly Modeling Local and Global Context", "year": 2021}], "gt_label": [1], "date": "2021-09", "source": "LitSearch", "qid": "LitSearch_319", "valid": true}
{"query": "What research has been conducted on enhancing conversational generation models using knowledge sourced from the internet?", "cited_paper": [{"arxiv_id": "2107.07566", "title": "Internet-Augmented Dialogue Generation", "year": 2021}], "gt_label": [1], "date": "2021-07", "source": "LitSearch", "qid": "LitSearch_320", "valid": true}
{"query": "What research has been conducted on incorporating visual data into the text summarization process?", "cited_paper": [{"arxiv_id": "2212.07672", "title": "Summary-Oriented Vision Modeling for Multimodal Abstractive Summarization", "year": 2022}], "gt_label": [1], "date": "2022-12", "source": "LitSearch", "qid": "LitSearch_321", "valid": true}
{"query": "What research has been conducted on news recommendation engines that consider individual user preferences as well as the time-sensitive popularity of news content?", "cited_paper": [{"arxiv_id": "2106.01300", "title": "PP-Rec: News Recommendation with Personalized User Interest and Time-aware News Popularity", "year": 2021}], "gt_label": [1], "date": "2021-06", "source": "LitSearch", "qid": "LitSearch_322", "valid": true}
{"query": "What research has been conducted on the impact of intervening at intermediate layers in pretrained language models to alter the resulting text?", "cited_paper": [{"arxiv_id": "2205.05124", "title": "Extracting Latent Steering Vectors from Pretrained Language Models", "year": 2022}], "gt_label": [1], "date": "2022-05", "source": "LitSearch", "qid": "LitSearch_324", "valid": true}
{"query": "What research is available on acquiring sentence embeddings through unsupervised approaches, possibly employing contrastive learning methods?", "cited_paper": [{"arxiv_id": "2109.04321", "title": "Smoothed Contrastive Learning for Unsupervised Sentence Embedding", "year": 2021}], "gt_label": [1], "date": "2021-09", "source": "LitSearch", "qid": "LitSearch_325", "valid": true}
{"query": "What research is available on hybrid approaches that combine extractive and abstractive methods for summarizing extensive texts?", "cited_paper": [{"arxiv_id": "2103.00751", "title": "Long Document Summarization in a Low Resource Setting using Pretrained Language Models", "year": 2021}], "gt_label": [1], "date": "2021-03", "source": "LitSearch", "qid": "LitSearch_326", "valid": true}
{"query": "What research is available on the concept of using prefix tokens as a parameter-efficient method for fine-tuning language models?", "cited_paper": [{"arxiv_id": "2101.00190", "title": "Prefix-Tuning: Optimizing Continuous Prompts for Generation", "year": 2021}], "gt_label": [1], "date": "2021-01", "source": "LitSearch", "qid": "LitSearch_327", "valid": true}
{"query": "What resources or toolkits are available to facilitate prompt-based learning model development in PyTorch?", "cited_paper": [{"arxiv_id": "2111.01998", "title": "OpenPrompt: An Open-source Framework for Prompt-learning", "year": 2021}], "gt_label": [1], "date": "2021-11", "source": "LitSearch", "qid": "LitSearch_329", "valid": true}
{"query": "What sources offer research on maintaining factual accuracy at the entity level in abstractive summary generation?", "cited_paper": [{"arxiv_id": "2102.09130", "title": "Entity-level Factual Consistency of Abstractive Text Summarization", "year": 2021}], "gt_label": [1], "date": "2021-02", "source": "LitSearch", "qid": "LitSearch_330", "valid": true}
{"query": "What techniques and frameworks have been suggested for summarizing extensive texts under resource-constrained conditions?", "cited_paper": [{"arxiv_id": "2103.00751", "title": "Long Document Summarization in a Low Resource Setting using Pretrained Language Models", "year": 2021}], "gt_label": [1], "date": "2021-03", "source": "LitSearch", "qid": "LitSearch_331", "valid": true}
{"query": "What techniques exist for efficiently fine-tuning transformer language models by adjusting a limited set of parameters?", "cited_paper": [{"arxiv_id": "2101.00190", "title": "Prefix-Tuning: Optimizing Continuous Prompts for Generation", "year": 2021}], "gt_label": [1], "date": "2021-01", "source": "LitSearch", "qid": "LitSearch_332", "valid": true}
{"query": "What techniques exist for incorporating context in detecting emotions within dialogues by leveraging pre-trained language models?", "cited_paper": [{"arxiv_id": "2304.08216", "title": "Context-Dependent Embedding Utterance Representations for Emotion Recognition in Conversations", "year": 2023}], "gt_label": [1], "date": "2023-04", "source": "LitSearch", "qid": "LitSearch_333", "valid": true}
{"query": "What techniques exist to enhance the few-shot fine-tuning performance in small pre-trained language models?", "cited_paper": [{"arxiv_id": "2012.15723", "title": "Making Pre-trained Language Models Better Few-shot Learners", "year": 2020}, {"arxiv_id": "2001.07676", "title": "Exploiting Cloze Questions for Few Shot Text Classification and Natural Language Inference", "year": 2020}], "gt_label": [1, 1], "date": "2020-12", "source": "LitSearch", "qid": "LitSearch_334", "valid": true}
{"query": "Where can I find a database of good prompts to use for prompting language models for in-context learning?", "cited_paper": [{"arxiv_id": "2111.01998", "title": "OpenPrompt: An Open-source Framework for Prompt-learning", "year": 2021}], "gt_label": [1], "date": "2021-11", "source": "LitSearch", "qid": "LitSearch_336", "valid": true}
{"query": "Where can I read about the using soft embeddings to elicit knowledge from large pre-trained models, at small tuning cost?", "cited_paper": [{"arxiv_id": "2104.08691", "title": "The Power of Scale for Parameter-Efficient Prompt Tuning", "year": 2021}], "gt_label": [1], "date": "2021-04", "source": "LitSearch", "qid": "LitSearch_337", "valid": true}
{"query": "Which method involves training additional prompt tokens for every layer during the fine-tuning of language models, specifically evaluating their performance on generation tasks?", "cited_paper": [{"arxiv_id": "2101.00190", "title": "Prefix-Tuning: Optimizing Continuous Prompts for Generation", "year": 2021}], "gt_label": [1], "date": "2021-01", "source": "LitSearch", "qid": "LitSearch_338", "valid": true}
{"query": "Which paper has conducted a thorough analysis of how language models of different architectures generate text that either aligns with or deviates from the properties of natural human language?", "cited_paper": [{"arxiv_id": "2106.00085", "title": "Language Model Evaluation Beyond Perplexity", "year": 2021}], "gt_label": [1], "date": "2021-06", "source": "LitSearch", "qid": "LitSearch_339", "valid": true}
{"query": "Which paper introduced the task of creating extended, coherent dialogues from brief summaries?", "cited_paper": [{"arxiv_id": "2106.03337", "title": "Summary Grounded Conversation Generation", "year": 2021}], "gt_label": [1], "date": "2021-06", "source": "LitSearch", "qid": "LitSearch_340", "valid": true}
{"query": "Which paper presents a platform that emphasizes evaluating the robustness of models on benchmarks?", "cited_paper": [{"arxiv_id": "2104.14337", "title": "Dynabench: Rethinking Benchmarking in NLP", "year": 2021}], "gt_label": [1], "date": "2021-04", "source": "LitSearch", "qid": "LitSearch_341", "valid": true}
{"query": "Which paper shows that generated captions of models are still worse than human written ones?", "cited_paper": [{"arxiv_id": "2111.08940", "title": "Transparent Human Evaluation for Image Captioning", "year": 2021}], "gt_label": [1], "date": "2021-11", "source": "LitSearch", "qid": "LitSearch_342", "valid": true}
{"query": "Which paper shows that human experts and non-experts focus on very different aspects when identifying AI=generated texts?", "cited_paper": [{"arxiv_id": "2107.00061", "title": "All That's 'Human' Is Not Gold: Evaluating Human Evaluation of Generated Text", "year": 2021}], "gt_label": [1], "date": "2021-07", "source": "LitSearch", "qid": "LitSearch_343", "valid": true}
{"query": "Which work introduces sparse attention modules and evaluate specifically on summarization?", "cited_paper": [{"arxiv_id": "2104.02112", "title": "Efficient Attentions for Long Document Summarization", "year": 2021}], "gt_label": [1], "date": "2021-04", "source": "LitSearch", "qid": "LitSearch_344", "valid": true}
{"query": "Which work pushes the limit of model quantization in BERT models by introducing a ternary network?", "cited_paper": [{"arxiv_id": "2012.15701", "title": "BinaryBERT: Pushing the Limit of BERT Quantization", "year": 2020}], "gt_label": [1], "date": "2020-12", "source": "LitSearch", "qid": "LitSearch_345", "valid": true}
{"query": "Which work suggests that machine translation models might get too confident and generate coherent but inadequant translations?", "cited_paper": [{"arxiv_id": "2105.11098", "title": "Prevent the Language Model from being Overconfident in Neural Machine Translation", "year": 2021}], "gt_label": [1], "date": "2021-05", "source": "LitSearch", "qid": "LitSearch_348", "valid": true}
{"query": "Which works shows that training large language models with purely mathematical and structural data can exhibit emergence of causal reasoning faster?", "cited_paper": [{"arxiv_id": "2306.01707", "title": "Learning Multi-Step Reasoning by Solving Arithmetic Tasks", "year": 2023}], "gt_label": [1], "date": "2023-06", "source": "LitSearch", "qid": "LitSearch_349", "valid": true}
{"query": "ould you direct me to research that shows that the transfer of specialized knowledge between various ABSA tasks if trained under the same paradigm?", "cited_paper": [{"arxiv_id": "2110.00796", "title": "Aspect Sentiment Quad Prediction as Paraphrase Generation", "year": 2021}], "gt_label": [1], "date": "2021-10", "source": "LitSearch", "qid": "LitSearch_350", "valid": true}
{"query": "Are there any examples of using dense phrase retrieval systems in the automatic curation of entity dictionaries?", "cited_paper": [{"arxiv_id": "2210.07586", "title": "Automatic Creation of Named Entity Recognition Datasets by Querying Phrase Representations", "year": 2022}], "gt_label": [1], "date": "2022-10", "source": "LitSearch", "qid": "LitSearch_351", "valid": true}
{"query": "Are there any large-scale and open-source text simplification datasets dealing with long passages?", "cited_paper": [{"arxiv_id": "2305.19204", "title": "SWiPE: A Dataset for Document-Level Simplification of Wikipedia Pages", "year": 2023}], "gt_label": [1], "date": "2023-05", "source": "LitSearch", "qid": "LitSearch_352", "valid": true}
{"query": "Are there any papers that build dense retrievers with mixture-of-experts architecture where each expert is responsible for different types of queries?", "cited_paper": [{"arxiv_id": "2305.03130", "title": "Chain-of-Skills: A Configurable Model for Open-domain Question Answering", "year": 2023}], "gt_label": [1], "date": "2023-05", "source": "LitSearch", "qid": "LitSearch_353", "valid": true}
{"query": "Could you recommend a dataset paper which presents relation extraction performance on translated data and compare it to English data?", "cited_paper": [{"arxiv_id": "2305.04582", "title": "MultiTACRED: A Multilingual Version of the TAC Relation Extraction Dataset", "year": 2023}], "gt_label": [1], "date": "2023-05", "source": "LitSearch", "qid": "LitSearch_354", "valid": true}
{"query": "Find the NLP paper that focuses on dialogue generation and introduces advancements in the augmentation of one-to-many or one-to-one dialogue data by conducting augmentation within the semantic space.", "cited_paper": [{"arxiv_id": "2306.16770", "title": "DialoGPS: Dialogue Path Sampling in Continuous Semantic Space for Data Augmentation in Multi-Turn Conversations", "year": 2023}], "gt_label": [1], "date": "2023-06", "source": "LitSearch", "qid": "LitSearch_355", "valid": true}
{"query": "Give me a paper proposing to circumvent a single-truth target in training generative language models.", "cited_paper": [{"arxiv_id": "2211.16550", "title": "Soft Alignment Objectives for Robust Adaptation of Language Generation", "year": 2022}], "gt_label": [1], "date": "2022-11", "source": "LitSearch", "qid": "LitSearch_356", "valid": true}
{"query": "How to achieve zero-shot lip reading?", "cited_paper": [{"arxiv_id": "2306.06410", "title": "OpenSR: Open-Modality Speech Recognition via Maintaining Multi-Modality Alignment", "year": 2023}], "gt_label": [1], "date": "2023-06", "source": "LitSearch", "qid": "LitSearch_357", "valid": true}
{"query": "How to faithfully and explicitly measure the helpfulness of human explanations to language models during finetuning and inference?", "cited_paper": [{"arxiv_id": "2305.03117", "title": "Are Human Explanations Always Helpful? Towards Objective Evaluation of Human Natural Language Explanations", "year": 2023}], "gt_label": [1], "date": "2023-05", "source": "LitSearch", "qid": "LitSearch_359", "valid": true}
{"query": "In multimodal (multilingual) abstractive summarization field, is there any paper that propose target-oriented vision modeling method to improve the quality of summaries?", "cited_paper": [{"arxiv_id": "2212.07672", "title": "Summary-Oriented Vision Modeling for Multimodal Abstractive Summarization", "year": 2022}], "gt_label": [1], "date": "2022-12", "source": "LitSearch", "qid": "LitSearch_361", "valid": true}
{"query": "Is there a Chinese hate speech paper that constructs an insulting lexicon while building the dataset?", "cited_paper": [{"arxiv_id": "2305.04446", "title": "Facilitating Fine-grained Detection of Chinese Toxic Language: Hierarchical Taxonomy, Resources, and Benchmarks", "year": 2023}], "gt_label": [1], "date": "2023-05", "source": "LitSearch", "qid": "LitSearch_362", "valid": true}
{"query": "Is there a decoder-only language model that does not use a tokenizer and operates on raw text bytes?", "cited_paper": [{"arxiv_id": "2212.10474", "title": "ByGPT5: End-to-End Style-conditioned Poetry Generation with Token-free Language Models", "year": 2022}], "gt_label": [1], "date": "2022-12", "source": "LitSearch", "qid": "LitSearch_364", "valid": true}
{"query": "Is there a dialogue dataset where a speaker's utterance is grounded in their persona, consisting of image-text pairs representing their episodic memories?", "cited_paper": [{"arxiv_id": "2305.17388", "title": "MPCHAT: Towards Multimodal Persona-Grounded Conversation", "year": 2023}], "gt_label": [1], "date": "2023-05", "source": "LitSearch", "qid": "LitSearch_365", "valid": true}
{"query": "Is there a method for measuring the critical errors that a dialogue system makes in its responses?", "cited_paper": [{"arxiv_id": "2212.09180", "title": "Don't Forget Your ABC's: Evaluating the State-of-the-Art in Chat-Oriented Dialogue Systems", "year": 2022}], "gt_label": [1], "date": "2022-12", "source": "LitSearch", "qid": "LitSearch_366", "valid": true}
{"query": "Is there a method that measures the information provided in a (model generated) rationale beyond what the original context provided?", "cited_paper": [{"arxiv_id": "2210.04982", "title": "REV: Information-Theoretic Evaluation of Free-Text Rationales", "year": 2022}], "gt_label": [1], "date": "2022-10", "source": "LitSearch", "qid": "LitSearch_367", "valid": true}
{"query": "Is there a paper comparing knowledge distillation and human annotation in terms of cost efficiency?", "cited_paper": [{"arxiv_id": "2305.01645", "title": "Distill or Annotate? Cost-Efficient Fine-Tuning of Compact Models", "year": 2023}], "gt_label": [1], "date": "2023-05", "source": "LitSearch", "qid": "LitSearch_368", "valid": true}
{"query": "Is there a paper exploring the curse of multilinguality for similar languages?", "cited_paper": [{"arxiv_id": "2305.12182", "title": "Glot500: Scaling Multilingual Corpora and Language Models to 500 Languages", "year": 2023}], "gt_label": [1], "date": "2023-05", "source": "LitSearch", "qid": "LitSearch_369", "valid": true}
{"query": "Is there a paper that applies large language models to visual Ravens Progressive Matrices?", "cited_paper": [{"arxiv_id": "2305.17626", "title": "In-Context Analogical Reasoning with Pre-Trained Language Models", "year": 2023}], "gt_label": [1], "date": "2023-05", "source": "LitSearch", "qid": "LitSearch_370", "valid": true}
{"query": "Is there a paper that connects the basic elements of storytelling with biased or imbalanced media reporting?", "cited_paper": [{"arxiv_id": "2306.02052", "title": "Conflicts, Villains, Resolutions: Towards models of Narrative Media Framing", "year": 2023}], "gt_label": [1], "date": "2023-06", "source": "LitSearch", "qid": "LitSearch_371", "valid": true}
{"query": "Is there a paper that links exposure bias to distillation?", "cited_paper": [{"arxiv_id": "2305.02031", "title": "A Systematic Study of Knowledge Distillation for Natural Language Generation with Pseudo-Target Training", "year": 2023}], "gt_label": [1], "date": "2023-05", "source": "LitSearch", "qid": "LitSearch_372", "valid": true}
{"query": "Is there a paper that uses an app for a popular tabletop game to gather real transcripts of gameplay with concrete values for players' and monsters' health?", "cited_paper": [{"arxiv_id": "2305.01528", "title": "FIREBALL: A Dataset of Dungeons and Dragons Actual-Play with Structured Game State Information", "year": 2023}], "gt_label": [1], "date": "2023-05", "source": "LitSearch", "qid": "LitSearch_376", "valid": true}
{"query": "Is there a paper that uses evolutionary algorithms and neural MT metrics to produce translations?", "cited_paper": [{"arxiv_id": "2305.19330", "title": "Breeding Machine Translations: Evolutionary approach to survive and thrive in the world of automated evaluation", "year": 2023}], "gt_label": [1], "date": "2023-05", "source": "LitSearch", "qid": "LitSearch_377", "valid": true}
{"query": "Is there a paper that uses similarity scores to check knowledge in diffusion models", "cited_paper": [{"arxiv_id": "2306.01735", "title": "Multilingual Conceptual Coverage in Text-to-Image Models", "year": 2023}], "gt_label": [1], "date": "2023-06", "source": "LitSearch", "qid": "LitSearch_378", "valid": true}
{"query": "Is there a paper that uses the tree structure of math equations in autoregressive language models?", "cited_paper": [{"arxiv_id": "2302.07974", "title": "Tree-Based Representation and Generation of Natural and Mathematical Language", "year": 2023}], "gt_label": [1], "date": "2023-02", "source": "LitSearch", "qid": "LitSearch_379", "valid": true}
{"query": "Is there a paper that utilizes the characteristics of human evolutionary knowledge to guide language models in generating scientific ideas?", "cited_paper": [{"arxiv_id": "2306.02282", "title": "Exploring and Verbalizing Academic Ideas by Concept Co-occurrence", "year": 2023}], "gt_label": [1], "date": "2023-06", "source": "LitSearch", "qid": "LitSearch_380", "valid": true}
{"query": "Is there a study that shows how to help the demonstration retriever better integrate feedback from LLMs?", "cited_paper": [{"arxiv_id": "2305.04320", "title": "Unified Demonstration Retriever for In-Context Learning", "year": 2023}], "gt_label": [1], "date": "2023-05", "source": "LitSearch", "qid": "LitSearch_381", "valid": true}
{"query": "Is there a tool that can automatically segment speech and the corresponding text transcriptions, to obtain a finer grained alignment?", "cited_paper": [{"arxiv_id": "2305.14635", "title": "CMOT: Cross-modal Mixup via Optimal Transport for Speech Translation", "year": 2023}], "gt_label": [1], "date": "2023-05", "source": "LitSearch", "qid": "LitSearch_382", "valid": true}
{"query": "Is there an evaluation metric for natural language generation that predicts the factual consistency score through a mean-max aggregation method?", "cited_paper": [{"arxiv_id": "2305.16739", "title": "AlignScore: Evaluating Factual Consistency with a Unified Alignment Function", "year": 2023}], "gt_label": [1], "date": "2023-05", "source": "LitSearch", "qid": "LitSearch_383", "valid": true}
{"query": "Is there any dataset that contains minimally-contrasting social situations that lead to different decisions about which behaviors are appropriate in that situation?", "cited_paper": [{"arxiv_id": "2305.17008", "title": "NormBank: A Knowledge Bank of Situational Social Norms", "year": 2023}], "gt_label": [1], "date": "2023-05", "source": "LitSearch", "qid": "LitSearch_384", "valid": true}
{"query": "Is there any paper about style transfer for stories?", "cited_paper": [{"arxiv_id": "2208.13423", "title": "StoryTrans: Non-Parallel Story Author-Style Transfer with Discourse Representations and Content Enhancing", "year": 2022}], "gt_label": [1], "date": "2022-08", "source": "LitSearch", "qid": "LitSearch_385", "valid": true}
{"query": "Is there any paper leverages knowledge distillation of language models for textual out-of-distribution detection or anomaly detection?", "cited_paper": [{"arxiv_id": "2211.11300", "title": "Multi-Level Knowledge Distillation for Out-of-Distribution Detection in Text", "year": 2022}], "gt_label": [1], "date": "2022-11", "source": "LitSearch", "qid": "LitSearch_387", "valid": true}
{"query": "Is there any paper that aligns speech and text embeddings better than CTC training?", "cited_paper": [{"arxiv_id": "2212.09359", "title": "WACO: Word-Aligned Contrastive Learning for Speech Translation", "year": 2022}], "gt_label": [1], "date": "2022-12", "source": "LitSearch", "qid": "LitSearch_389", "valid": true}
{"query": "Is there any paper that applies curriculum learning to various NLG tasks without depending on specific metrics?", "cited_paper": [{"arxiv_id": "2211.11297", "title": "In-sample Curriculum Learning by Sequence Completion for Natural Language Generation", "year": 2022}], "gt_label": [1], "date": "2022-11", "source": "LitSearch", "qid": "LitSearch_390", "valid": true}
{"query": "Is there any paper that applies symbolic distillation on black-box generalist language models to harvest high-quality counterfactual data for out-of-distribution generalization?", "cited_paper": [{"arxiv_id": "2212.10534", "title": "DISCO: Distilling Counterfactuals with Large Language Models", "year": 2022}], "gt_label": [1], "date": "2022-12", "source": "LitSearch", "qid": "LitSearch_391", "valid": true}
{"query": "Is there any paper that automatically creates a dataset for summarizing text from one language to another for a large collection of languages?", "cited_paper": [{"arxiv_id": "2112.08804", "title": "CrossSum: Beyond English-Centric Cross-Lingual Summarization for 1,500+ Language Pairs", "year": 2021}], "gt_label": [1], "date": "2021-12", "source": "LitSearch", "qid": "LitSearch_393", "valid": true}
{"query": "Is there any paper that combines causal inference and finetuning for language models?", "cited_paper": [{"arxiv_id": "2306.10790", "title": "Preserving Commonsense Knowledge from Pre-trained Language Models via Causal Inference", "year": 2023}], "gt_label": [1], "date": "2023-06", "source": "LitSearch", "qid": "LitSearch_394", "valid": true}
{"query": "Is there any paper that constructs augmented training data based on the entity-to-entity correlations?", "cited_paper": [{"arxiv_id": "2210.08855", "title": "PeerDA: Data Augmentation via Modeling Peer Relation for Span Identification Tasks", "year": 2022}], "gt_label": [1], "date": "2022-10", "source": "LitSearch", "qid": "LitSearch_395", "valid": true}
{"query": "Is there any paper that employs code LLMs to iteratively generate and refine code with execution results to improve the performance?", "cited_paper": [{"arxiv_id": "2305.04087", "title": "Self-Edit: Fault-Aware Code Editor for Code Generation", "year": 2023}], "gt_label": [1], "date": "2023-05", "source": "LitSearch", "qid": "LitSearch_396", "valid": true}
{"query": "Is there any paper that investigates backdoor attacks across various types of tasks, not limited to classification, in language models?", "cited_paper": [{"arxiv_id": "2306.08350", "title": "Multi-target Backdoor Attacks for Code Pre-trained Models", "year": 2023}], "gt_label": [1], "date": "2023-06", "source": "LitSearch", "qid": "LitSearch_399", "valid": true}
{"query": "Is there any paper that leverages syntactic rules to explicitly guide text generation?", "cited_paper": [{"arxiv_id": "2306.11485", "title": "Explicit Syntactic Guidance for Neural Text Generation", "year": 2023}], "gt_label": [1], "date": "2023-06", "source": "LitSearch", "qid": "LitSearch_401", "valid": true}
{"query": "Is there any paper that performs adversarial training on frame level for audio-visual representation learning?", "cited_paper": [{"arxiv_id": "2306.10567", "title": "MIR-GAN: Refining Frame-Level Modality-Invariant Representations with Adversarial Network for Audio-Visual Speech Recognition", "year": 2023}], "gt_label": [1], "date": "2023-06", "source": "LitSearch", "qid": "LitSearch_402", "valid": true}
{"query": "Is there any paper that proposes a new multimodal video dataset that image-level multimodal models do not work well?", "cited_paper": [{"arxiv_id": "2206.03428", "title": "Revealing Single Frame Bias for Video-and-Language Learning", "year": 2022}], "gt_label": [1], "date": "2022-06", "source": "LitSearch", "qid": "LitSearch_403", "valid": true}
{"query": "Is there any paper that proposes a set of criteria to comprehensively evaluate generated conversations?", "cited_paper": [{"arxiv_id": "2305.03088", "title": "Modeling What-to-ask and How-to-ask for Answer-unaware Conversational Question Generation", "year": 2023}], "gt_label": [1], "date": "2023-05", "source": "LitSearch", "qid": "LitSearch_404", "valid": true}
{"query": "Is there any paper that reveals annotation problems in cross-lingual summarization caused by decomposing the task into translation and summarization?", "cited_paper": [{"arxiv_id": "2307.04018", "title": "Revisiting Cross-Lingual Summarization: A Corpus-based Study and A New Benchmark with Improved Annotation", "year": 2023}], "gt_label": [1], "date": "2023-07", "source": "LitSearch", "qid": "LitSearch_405", "valid": true}
{"query": "Is there any paper that studies a teacher AI inferring mental states of a student role in a role-playing game setup using reinforcement learning?", "cited_paper": [{"arxiv_id": "2212.10060", "title": "I Cast Detect Thoughts: Learning to Converse and Guide with Intents and Theory-of-Mind in Dungeons and Dragons", "year": 2022}], "gt_label": [1], "date": "2022-12", "source": "LitSearch", "qid": "LitSearch_406", "valid": true}
{"query": "Is there any paper that uses data collected from the Dark Web, specifically onion domains, to pretrain a language model?", "cited_paper": [{"arxiv_id": "2305.08596", "title": "DarkBERT: A Language Model for the Dark Side of the Internet", "year": 2023}], "gt_label": [1], "date": "2023-05", "source": "LitSearch", "qid": "LitSearch_408", "valid": true}
{"query": "Is there any paper that uses prompt tuning in multi-answer QA?", "cited_paper": [{"arxiv_id": "2307.03897", "title": "Answering Ambiguous Questions via Iterative Prompting", "year": 2023}], "gt_label": [1], "date": "2023-07", "source": "LitSearch", "qid": "LitSearch_409", "valid": true}
{"query": "Is there any paper that uses token-level loss to enhance sentence-level embedding learning?", "cited_paper": [{"arxiv_id": "2305.09148", "title": "Dual-Alignment Pre-training for Cross-lingual Sentence Embedding", "year": 2023}], "gt_label": [1], "date": "2023-05", "source": "LitSearch", "qid": "LitSearch_410", "valid": true}
{"query": "Is there any paper that utilizes Gaussian processes to analyze the vulnerability of text-conditioned generative models?", "cited_paper": [{"arxiv_id": "2305.17444", "title": "Query-Efficient Black-Box Red Teaming via Bayesian Optimization", "year": 2023}], "gt_label": [1], "date": "2023-05", "source": "LitSearch", "qid": "LitSearch_411", "valid": true}
{"query": "Is there any paper that utilizes graph structure to model conversation history?", "cited_paper": [{"arxiv_id": "2306.06872", "title": "History Semantic Graph Enhanced Conversational KBQA with Temporal Information Modeling", "year": 2023}], "gt_label": [1], "date": "2023-06", "source": "LitSearch", "qid": "LitSearch_412", "valid": true}
{"query": "Is there any work that allows large numbers of model outputs to be encoded and compared by causal language models in a single forward pass?", "cited_paper": [{"arxiv_id": "2306.00947", "title": "EEL: Efficiently Encoding Lattices for Reranking", "year": 2023}], "gt_label": [1], "date": "2023-06", "source": "LitSearch", "qid": "LitSearch_415", "valid": true}
{"query": "Is there any work that attacks language models in dialogue generation?", "cited_paper": [{"arxiv_id": "2305.03655", "title": "White-Box Multi-Objective Adversarial Attack on Dialogue Generation", "year": 2023}], "gt_label": [1], "date": "2023-05", "source": "LitSearch", "qid": "LitSearch_416", "valid": true}
{"query": "Is there commonsense reasoning dataset which generates diverse sentences to describe the relation between concepts?", "cited_paper": [{"arxiv_id": "2212.10545", "title": "DimonGen: Diversified Generative Commonsense Reasoning for Explaining Concept Relationships", "year": 2022}], "gt_label": [1], "date": "2022-12", "source": "LitSearch", "qid": "LitSearch_418", "valid": true}
{"query": "Is there such a factuality evaluation dataset that can be used to evaluate the performance of fact-checking models on summaries generated by latest summarization models?", "cited_paper": [{"arxiv_id": "2205.12854", "title": "Understanding Factual Errors in Summarization: Errors, Summarizers, Datasets, Error Detectors", "year": 2022}], "gt_label": [1], "date": "2022-05", "source": "LitSearch", "qid": "LitSearch_419", "valid": true}
{"query": "Is there such a reading comprehension dataset in understanding a snippet from a long story book, while it requires to integrate the necessary long history texts before the snippet to full understand it?", "cited_paper": [{"arxiv_id": "2305.10156", "title": "Personality Understanding of Fictional Characters during Book Reading", "year": 2023}], "gt_label": [1], "date": "2023-05", "source": "LitSearch", "qid": "LitSearch_420", "valid": true}
{"query": "Provide an example of a paper which proposes a method to learn a dynamic (conditioned on the input) sequence tokenizer (segmenter) via standard gradient backpropagation.", "cited_paper": [{"arxiv_id": "2211.09761", "title": "Efficient Transformers with Dynamic Token Pooling", "year": 2022}], "gt_label": [1], "date": "2022-11", "source": "LitSearch", "qid": "LitSearch_421", "valid": true}
{"query": "What are some data-efficient ways to learn text embeddings thru contrastive learning?", "cited_paper": [{"arxiv_id": "2307.07380", "title": "Composition-contrastive Learning for Sentence Embeddings", "year": 2023}], "gt_label": [1], "date": "2023-07", "source": "LitSearch", "qid": "LitSearch_422", "valid": true}
{"query": "What is a large event-coverage general-domain event argument extraction dataset?", "cited_paper": [{"arxiv_id": "2205.12505", "title": "GENEVA: Benchmarking Generalizability for Event Argument Extraction with Hundreds of Event Types and Argument Roles", "year": 2022}], "gt_label": [1], "date": "2022-05", "source": "LitSearch", "qid": "LitSearch_424", "valid": true}
{"query": "What is the performance of large language models in text summarization under reference-based and reference-free human evaluations?", "cited_paper": [{"arxiv_id": "2212.07981", "title": "Revisiting the Gold Standard: Grounding Summarization Evaluation with Robust Human Evaluation", "year": 2022}], "gt_label": [1], "date": "2022-12", "source": "LitSearch", "qid": "LitSearch_426", "valid": true}
{"query": "What limitations do large language models have in evaluating information-seeking question answering?", "cited_paper": [{"arxiv_id": "2305.06984", "title": "Evaluating Open-Domain Question Answering in the Era of Large Language Models", "year": 2023}], "gt_label": [1], "date": "2023-05", "source": "LitSearch", "qid": "LitSearch_427", "valid": true}
{"query": "What paper compares humans' and language models' non-literal interpretations of utterances featuring phenomena like deceit, irony, and humor?", "cited_paper": [{"arxiv_id": "2212.06801", "title": "A fine-grained comparison of pragmatic language understanding in humans and language models", "year": 2022}], "gt_label": [1], "date": "2022-12", "source": "LitSearch", "qid": "LitSearch_428", "valid": true}
{"query": "What work attempts to explore multi-hop reasoning by densifying commonsense knowledge graphs?", "cited_paper": [{"arxiv_id": "2210.07621", "title": "Dense-ATOMIC: Towards Densely-connected ATOMIC with High Knowledge Coverage and Massive Multi-hop Paths", "year": 2022}], "gt_label": [1], "date": "2022-10", "source": "LitSearch", "qid": "LitSearch_429", "valid": true}
{"query": "Which dataset supports narration generation and temporal localization tasks in Chinese movies?", "cited_paper": [{"arxiv_id": "2305.12140", "title": "Movie101: A New Movie Understanding Benchmark", "year": 2023}], "gt_label": [1], "date": "2023-05", "source": "LitSearch", "qid": "LitSearch_431", "valid": true}
{"query": "Which family of model generally perform the best for the event conceptualization task", "cited_paper": [{"arxiv_id": "2305.04808", "title": "CAT: A Contextualized Conceptualization and Instantiation Framework for Commonsense Reasoning", "year": 2023}], "gt_label": [1], "date": "2023-05", "source": "LitSearch", "qid": "LitSearch_432", "valid": true}
{"query": "Which is the first multimodal model combining text and speech transformers trained without labelled text-speech pairs?", "cited_paper": [{"arxiv_id": "2211.08402", "title": "Introducing Semantics into Speech Encoders", "year": 2022}], "gt_label": [1], "date": "2022-11", "source": "LitSearch", "qid": "LitSearch_433", "valid": true}
{"query": "Which knowledge graph completion method focuses on reducing memory usage by pruning features?", "cited_paper": [{"arxiv_id": "2208.09137", "title": "GreenKGC: A Lightweight Knowledge Graph Completion Method", "year": 2022}], "gt_label": [1], "date": "2022-08", "source": "LitSearch", "qid": "LitSearch_434", "valid": true}
{"query": "Which language model distillation paper that first identified the capacity gap in distillation and used the MoE student model to counter the curse of capacity gap?", "cited_paper": [{"arxiv_id": "2305.12129", "title": "Lifting the Curse of Capacity Gap in Distilling Language Models", "year": 2023}], "gt_label": [1], "date": "2023-05", "source": "LitSearch", "qid": "LitSearch_435", "valid": true}
{"query": "Which numerical reasoning paper first published a dataset that considers different types of size of numbers and their representations in arithmetic questions?", "cited_paper": [{"arxiv_id": "2305.17491", "title": "FERMAT: An Alternative to Accuracy for Numerical Reasoning", "year": 2023}], "gt_label": [1], "date": "2023-05", "source": "LitSearch", "qid": "LitSearch_436", "valid": true}
{"query": "Which paper about parameter-efficient finetuning first proposes to feed the pretrained weight instead of the activation to an adapter?", "cited_paper": [{"arxiv_id": "2305.16742", "title": "Parameter-Efficient Fine-Tuning without Introducing New Latency", "year": 2023}], "gt_label": [1], "date": "2023-05", "source": "LitSearch", "qid": "LitSearch_437", "valid": true}
{"query": "Which paper did a comprehensive survey of the code large language model (code LLMs)?", "cited_paper": [{"arxiv_id": "2212.09420", "title": "Large Language Models Meet NL2Code: A Survey", "year": 2022}], "gt_label": [1], "date": "2022-12", "source": "LitSearch", "qid": "LitSearch_439", "valid": true}
{"query": "Which paper employs a two-stage approach in generative models to tackle ABSA tasks across various domains?", "cited_paper": [{"arxiv_id": "2305.09509", "title": "Bidirectional Generative Framework for Cross-domain Aspect-based Sentiment Analysis", "year": 2023}], "gt_label": [1], "date": "2023-05", "source": "LitSearch", "qid": "LitSearch_440", "valid": true}
{"query": "Which paper enables interactive semantic parsing by training an error correction model with simulated human feedback instead of human annotations?", "cited_paper": [{"arxiv_id": "2305.08195", "title": "Learning to Simulate Natural Language Feedback for Interactive Semantic Parsing", "year": 2023}], "gt_label": [1], "date": "2023-05", "source": "LitSearch", "qid": "LitSearch_441", "valid": true}
{"query": "Which paper explored training a GPT-2 for automatic diagnosis, emphasizing efficient data augmentation for symptom prediction and disease identification?", "cited_paper": [{"arxiv_id": "2307.08290", "title": "CoAD: Automatic Diagnosis through Symptom and Disease Collaborative Generation", "year": 2023}], "gt_label": [1], "date": "2023-07", "source": "LitSearch", "qid": "LitSearch_442", "valid": true}
{"query": "Which paper first applied the chain-of-thought technique in the text summarization field?", "cited_paper": [{"arxiv_id": "2305.13412", "title": "Element-aware Summarization with Large Language Models: Expert-aligned Evaluation and Chain-of-Thought Method", "year": 2023}], "gt_label": [1], "date": "2023-05", "source": "LitSearch", "qid": "LitSearch_444", "valid": true}
{"query": "Which paper first apply mixture of experts idea to large language models for domain adaptation?", "cited_paper": [{"arxiv_id": "2306.05406", "title": "Mixture-of-Domain-Adapters: Decoupling and Injecting Domain Knowledge to Pre-trained Language Models Memories", "year": 2023}], "gt_label": [1], "date": "2023-06", "source": "LitSearch", "qid": "LitSearch_445", "valid": true}
{"query": "Which paper first combines rewriting and expansion methods to reformulate a query for conversational search?", "cited_paper": [{"arxiv_id": "2305.15645", "title": "ConvGQR: Generative Query Reformulation for Conversational Search", "year": 2023}], "gt_label": [1], "date": "2023-05", "source": "LitSearch", "qid": "LitSearch_448", "valid": true}
{"query": "Which paper first conducted the positioned error test for the MAUVE metric?", "cited_paper": [{"arxiv_id": "2212.10020", "title": "On the Blind Spots of Model-Based Evaluation Metrics for Text Generation", "year": 2022}], "gt_label": [1], "date": "2022-12", "source": "LitSearch", "qid": "LitSearch_449", "valid": true}
{"query": "Which paper first construct large-scale corpus to improve in-context learning of large language models in the pre-training stage?", "cited_paper": [{"arxiv_id": "2305.09137", "title": "Pre-Training to Learn in Context", "year": 2023}], "gt_label": [1], "date": "2023-05", "source": "LitSearch", "qid": "LitSearch_450", "valid": true}
{"query": "Which paper first constructed a structured knowledge base to interconnect different human social roles and attributes?", "cited_paper": [{"arxiv_id": "2305.02364", "title": "PeaCoK: Persona Commonsense Knowledge for Consistent and Engaging Narratives", "year": 2023}], "gt_label": [1], "date": "2023-05", "source": "LitSearch", "qid": "LitSearch_451", "valid": true}
{"query": "Which paper first explored In-context learning in a cross lingual setup and made use of alignment to better it's performance?", "cited_paper": [{"arxiv_id": "2305.05940", "title": "Multilingual LLMs are Better Cross-lingual In-context Learners with Alignment", "year": 2023}], "gt_label": [1], "date": "2023-05", "source": "LitSearch", "qid": "LitSearch_452", "valid": true}
{"query": "Which paper first introduced document content as an intermediate generation target and utilized textual document identifiers in generative retrieval?", "cited_paper": [{"arxiv_id": "2305.11161", "title": "TOME: A Two-stage Approach for Model-based Retrieval", "year": 2023}], "gt_label": [1], "date": "2023-05", "source": "LitSearch", "qid": "LitSearch_454", "valid": true}
{"query": "Which paper first propose to mask positions to pre-train multi-modal document transformer", "cited_paper": [{"arxiv_id": "2305.18721", "title": "LayoutMask: Enhance Text-Layout Interaction in Multi-modal Pre-training for Document Understanding", "year": 2023}], "gt_label": [1], "date": "2023-05", "source": "LitSearch", "qid": "LitSearch_455", "valid": true}
{"query": "Which paper first proposed shared adapter module across layers?", "cited_paper": [{"arxiv_id": "2305.17682", "title": "One Network, Many Masks: Towards More Parameter-Efficient Transfer Learning", "year": 2023}], "gt_label": [1], "date": "2023-05", "source": "LitSearch", "qid": "LitSearch_458", "valid": true}
{"query": "Which paper first proposed to combine pretrained masked language models (BERT) and discrete diffusion language models?", "cited_paper": [{"arxiv_id": "2211.15029", "title": "DiffusionBERT: Improving Generative Masked Language Models with Diffusion Models", "year": 2022}], "gt_label": [1], "date": "2022-11", "source": "LitSearch", "qid": "LitSearch_459", "valid": true}
{"query": "Which paper first proposed to only update some original weights of self-attention layers in parameter-efficient fine-tuning?", "cited_paper": [{"arxiv_id": "2305.04573", "title": "HiFi: High-Information Attention Heads Hold for Parameter-Efficient Model Adaptation", "year": 2023}], "gt_label": [1], "date": "2023-05", "source": "LitSearch", "qid": "LitSearch_460", "valid": true}
{"query": "Which paper first published a real-world Chinese-English text image translation dataset?", "cited_paper": [{"arxiv_id": "2305.17415", "title": "Exploring Better Text Image Translation with Multimodal Codebook", "year": 2023}], "gt_label": [1], "date": "2023-05", "source": "LitSearch", "qid": "LitSearch_461", "valid": true}
{"query": "Which paper first shows that it is possible to maintain high LLM reasoning performance with in-context examples that are absurdly wrong?", "cited_paper": [{"arxiv_id": "2212.10001", "title": "Towards Understanding Chain-of-Thought Prompting: An Empirical Study of What Matters", "year": 2022}], "gt_label": [1], "date": "2022-12", "source": "LitSearch", "qid": "LitSearch_462", "valid": true}
{"query": "Which paper first shows that large language models can be prompted to act like professional annotators to evaluate text generation quality?", "cited_paper": [{"arxiv_id": "2305.01937", "title": "Can Large Language Models Be an Alternative to Human Evaluations?", "year": 2023}], "gt_label": [1], "date": "2023-05", "source": "LitSearch", "qid": "LitSearch_463", "valid": true}
{"query": "Which paper first studied the efficiency robustness of multi-exit language models?", "cited_paper": [{"arxiv_id": "2305.12228", "title": "Dynamic Transformers Provide a False Sense of Efficiency", "year": 2023}], "gt_label": [1], "date": "2023-05", "source": "LitSearch", "qid": "LitSearch_464", "valid": true}
{"query": "Which paper first use the attention weights to guide the simultaneous inference of speech translation models?", "cited_paper": [{"arxiv_id": "2212.07850", "title": "Attention as a Guide for Simultaneous Speech Translation", "year": 2022}], "gt_label": [1], "date": "2022-12", "source": "LitSearch", "qid": "LitSearch_465", "valid": true}
{"query": "Which paper first used structural information for coherence modeling?", "cited_paper": [{"arxiv_id": "2306.06472", "title": "Modeling Structural Similarities between Documents for Coherence Assessment with Graph Convolutional Networks", "year": 2023}], "gt_label": [1], "date": "2023-06", "source": "LitSearch", "qid": "LitSearch_466", "valid": true}
{"query": "Which paper found that mutual learning benefits multlingual models?", "cited_paper": [{"arxiv_id": "2305.15718", "title": "Towards Higher Pareto Frontier in Multilingual Machine Translation", "year": 2023}], "gt_label": [1], "date": "2023-05", "source": "LitSearch", "qid": "LitSearch_467", "valid": true}
{"query": "Which paper highlights the need for leveraging all available resources, including dictionaries, machine translation systems, and language learners, to construct NLP data in low-resource languages?", "cited_paper": [{"arxiv_id": "2210.06828", "title": "Rethinking Annotation: Can Language Learners Contribute?", "year": 2022}], "gt_label": [1], "date": "2022-10", "source": "LitSearch", "qid": "LitSearch_468", "valid": true}
{"query": "Which paper introduce a DRO (distribution robust optimization) like training objective for doing adversarial training without constructing adversarial samples.", "cited_paper": [{"arxiv_id": "2306.15164", "title": "DSRM: Boost Textual Adversarial Training with Distribution Shift Risk Minimization", "year": 2023}], "gt_label": [1], "date": "2023-06", "source": "LitSearch", "qid": "LitSearch_469", "valid": true}
{"query": "Which paper introduced the human-evaluated timeliness metric for misinformation detection?", "cited_paper": [{"arxiv_id": "2212.09683", "title": "Human-in-the-loop Evaluation for Early Misinformation Detection: A Case Study of COVID-19 Treatments", "year": 2022}], "gt_label": [1], "date": "2022-12", "source": "LitSearch", "qid": "LitSearch_470", "valid": true}
{"query": "Which paper investigates the influence of the diversity of source tasks on the performance of target tasks in prompt tuning using CrossFit?", "cited_paper": [{"arxiv_id": "2302.08143", "title": "Learning to Initialize: Can Meta Learning Improve Cross-task Generalization in Prompt Tuning?", "year": 2023}], "gt_label": [1], "date": "2023-02", "source": "LitSearch", "qid": "LitSearch_472", "valid": true}
{"query": "Which paper is among the earliest to train on extensive collection of signing video and subtitle pairs available from online platforms?", "cited_paper": [{"arxiv_id": "2305.12876", "title": "Gloss-Free End-to-End Sign Language Translation", "year": 2023}], "gt_label": [1], "date": "2023-05", "source": "LitSearch", "qid": "LitSearch_473", "valid": true}
{"query": "Which paper is the first to comprehensively review the progress of deep learning in mathematical reasoning?", "cited_paper": [{"arxiv_id": "2212.10535", "title": "A Survey of Deep Learning for Mathematical Reasoning", "year": 2022}], "gt_label": [1], "date": "2022-12", "source": "LitSearch", "qid": "LitSearch_474", "valid": true}
{"query": "Which paper makes sure that the questions used in the paper are all from real users that are genuinely curious about a specific topic or concept?", "cited_paper": [{"arxiv_id": "2211.17257", "title": "CREPE: Open-Domain Question Answering with False Presuppositions", "year": 2022}], "gt_label": [1], "date": "2022-11", "source": "LitSearch", "qid": "LitSearch_475", "valid": true}
{"query": "Which paper measured how well the source-translation contribution by the translation model can be used to detect its own hallucinations?", "cited_paper": [{"arxiv_id": "2212.08597", "title": "Detecting and Mitigating Hallucinations in Machine Translation: Model Internal Workings Alone Do Well, Sentence Similarity Even Better", "year": 2022}], "gt_label": [1], "date": "2022-12", "source": "LitSearch", "qid": "LitSearch_476", "valid": true}
{"query": "Which paper presents an easy to implement and high performing method for OOD detection with language models?", "cited_paper": [{"arxiv_id": "2305.13282", "title": "Is Fine-tuning Needed? Pre-trained Language Models Are Near Perfect for Out-of-Domain Detection", "year": 2023}], "gt_label": [1], "date": "2023-05", "source": "LitSearch", "qid": "LitSearch_477", "valid": true}
{"query": "Which paper produces a dataset for text simplification in over 12 languages and evaluates both finetuning and in context learning approaches to text simplification in those languages?", "cited_paper": [{"arxiv_id": "2305.15678", "title": "Revisiting non-English Text Simplification: A Unified Multilingual Benchmark", "year": 2023}], "gt_label": [1], "date": "2023-05", "source": "LitSearch", "qid": "LitSearch_478", "valid": true}
{"query": "Which paper proposed a learning-based data augmentation method for improving compositional generalization of language models?", "cited_paper": [{"arxiv_id": "2306.02840", "title": "Learning to Substitute Spans towards Improving Compositional Generalization", "year": 2023}], "gt_label": [1], "date": "2023-06", "source": "LitSearch", "qid": "LitSearch_479", "valid": true}
{"query": "Which paper proposed decomposing the logit update of each of the attention blocks inputs to analyze how the context influences the prediction?", "cited_paper": [{"arxiv_id": "2305.12535", "title": "Explaining How Transformers Use Context to Build Predictions", "year": 2023}], "gt_label": [1], "date": "2023-05", "source": "LitSearch", "qid": "LitSearch_480", "valid": true}
{"query": "Which paper proposed dictionary-based Bayesian inference to improve the performance of image text matching model?", "cited_paper": [{"arxiv_id": "2305.01788", "title": "Vision Meets Definitions: Unsupervised Visual Word Sense Disambiguation Incorporating Gloss Information", "year": 2023}], "gt_label": [1], "date": "2023-05", "source": "LitSearch", "qid": "LitSearch_481", "valid": true}
{"query": "Which paper proposed the integration of human translators' considerations, such as length control, rhyme type control and suggestion, and enhancing compatibility between translation output and unseen melodies, into the design of machine translation models when translating lyrics?", "cited_paper": [{"arxiv_id": "2305.16816", "title": "Songs Across Borders: Singable and Controllable Neural Lyric Translation", "year": 2023}], "gt_label": [1], "date": "2023-05", "source": "LitSearch", "qid": "LitSearch_482", "valid": true}
{"query": "Which paper proposes a memory-efficient optimizer considering the confidence of each update during the optimization?", "cited_paper": [{"arxiv_id": "2307.02047", "title": "CAME: Confidence-guided Adaptive Memory Efficient Optimization", "year": 2023}], "gt_label": [1], "date": "2023-07", "source": "LitSearch", "qid": "LitSearch_483", "valid": true}
{"query": "Which paper proposes the two-stage training method, i.e., task-specific fine-tuning and cross-domain pre-training, to train an open-domain dialogue evaluator using the self-collected dataset.", "cited_paper": [{"arxiv_id": "2309.08156", "title": "RADE: Reference-Assisted Dialogue Evaluation for Open-Domain Dialogue", "year": 2023}], "gt_label": [1], "date": "2023-09", "source": "LitSearch", "qid": "LitSearch_484", "valid": true}
{"query": "Which paper proposes to use rewriting based approaches to defending against adversarial attacks in text classification?", "cited_paper": [{"arxiv_id": "2305.16444", "title": "Don't Retrain, Just Rewrite: Countering Adversarial Perturbations by Rewriting Text", "year": 2023}], "gt_label": [1], "date": "2023-05", "source": "LitSearch", "qid": "LitSearch_485", "valid": true}
{"query": "Which paper showed that social relationships were helpful for identifying inappropriate messages?", "cited_paper": [{"arxiv_id": "2307.02763", "title": "Your spouse needs professional help: Determining the Contextual Appropriateness of Messages through Modeling Social Relationships", "year": 2023}], "gt_label": [1], "date": "2023-07", "source": "LitSearch", "qid": "LitSearch_486", "valid": true}
{"query": "Which paper shows assessment of training instabilities at different levels for language models?", "cited_paper": [{"arxiv_id": "2302.07778", "title": "Measuring the Instability of Fine-Tuning", "year": 2023}], "gt_label": [1], "date": "2023-02", "source": "LitSearch", "qid": "LitSearch_487", "valid": true}
{"query": "Which paper shows that in instruction tuning, the instructions can be compressed to small supporting sets of words that provide useful information?", "cited_paper": [{"arxiv_id": "2306.01150", "title": "Did You Read the Instructions? Rethinking the Effectiveness of Task Definitions in Instruction Learning", "year": 2023}], "gt_label": [1], "date": "2023-06", "source": "LitSearch", "qid": "LitSearch_488", "valid": true}
{"query": "Which paper studies how current retrieval systems handle queries which contain multiple constraints?", "cited_paper": [{"arxiv_id": "2305.11694", "title": "QUEST: A Retrieval Dataset of Entity-Seeking Queries with Implicit Set Operations", "year": 2023}], "gt_label": [1], "date": "2023-05", "source": "LitSearch", "qid": "LitSearch_489", "valid": true}
{"query": "Which paper studies the concept of enhancing the coverage of a selective prediction system by re-attempting the questions on which it was not sufficiently confident.", "cited_paper": [{"arxiv_id": "2305.01812", "title": "Post-Abstention: Towards Reliably Re-Attempting the Abstained Instances in QA", "year": 2023}], "gt_label": [1], "date": "2023-05", "source": "LitSearch", "qid": "LitSearch_490", "valid": true}
{"query": "Which paper surveyed the datasets and tasks of asking clarification questions in conversational systems??", "cited_paper": [{"arxiv_id": "2305.15933", "title": "A Survey on Asking Clarification Questions Datasets in Conversational Systems", "year": 2023}], "gt_label": [1], "date": "2023-05", "source": "LitSearch", "qid": "LitSearch_491", "valid": true}
{"query": "Which paper used both automatically generated and manual templates with word tuples to adapt language models from one timestamp to another?", "cited_paper": [{"arxiv_id": "2208.10734", "title": "Learning Dynamic Contextualised Word Embeddings via Template-based Temporal Adaptation", "year": 2022}], "gt_label": [1], "date": "2022-08", "source": "LitSearch", "qid": "LitSearch_492", "valid": true}
{"query": "Which paper utilizes language models to generate singable lyrics that can go well with a predefined melody?", "cited_paper": [{"arxiv_id": "2305.19228", "title": "Unsupervised Melody-to-Lyric Generation", "year": 2023}], "gt_label": [1], "date": "2023-05", "source": "LitSearch", "qid": "LitSearch_493", "valid": true}
{"query": "Which papers were among the first to explore the task of targeted training data extraction?", "cited_paper": [{"arxiv_id": "2307.04401", "title": "Ethicist: Targeted Training Data Extraction Through Loss Smoothed Soft Prompting and Calibrated Confidence Estimation", "year": 2023}], "gt_label": [1], "date": "2023-07", "source": "LitSearch", "qid": "LitSearch_496", "valid": true}
{"query": "Which pre-trained model is specifically designed for low-resource dialogue summarization tasks?", "cited_paper": [{"arxiv_id": "2212.10018", "title": "DIONYSUS: A Pre-trained Model for Low-Resource Dialogue Summarization", "year": 2022}], "gt_label": [1], "date": "2022-12", "source": "LitSearch", "qid": "LitSearch_497", "valid": true}
{"query": "Which research paper leverages event structure information from Abstract Meaning Representation (AMR) graphs to aid in recognizing causal relations between events?", "cited_paper": [{"arxiv_id": "2305.12792", "title": "Semantic Structure Enhanced Event Causality Identification", "year": 2023}], "gt_label": [1], "date": "2023-05", "source": "LitSearch", "qid": "LitSearch_498", "valid": true}
{"query": "Which vision-language model can demonstrate that visual grounding could facilitate efficient language acquisition? (OctoBERT)", "cited_paper": [{"arxiv_id": "2306.08685", "title": "World-to-Words: Grounded Open Vocabulary Acquisition through Fast Mapping in Vision-Language Models", "year": 2023}], "gt_label": [1], "date": "2023-06", "source": "LitSearch", "qid": "LitSearch_499", "valid": true}
{"query": "Which vision-language model paper in 2023 developed techniques that reduce input tokens to improve model inference speed?", "cited_paper": [{"arxiv_id": "2305.17530", "title": "PuMer: Pruning and Merging Tokens for Efficient Vision Language Models", "year": 2023}], "gt_label": [1], "date": "2023-05", "source": "LitSearch", "qid": "LitSearch_500", "valid": true}
{"query": "Which work proposes an approach to improve candidate responses in the smart reply task by directly optimizing the metric to ensure that a response is selected by the user?", "cited_paper": [{"arxiv_id": "2305.16852", "title": "Model-Based Simulation for Optimising Smart Reply", "year": 2023}], "gt_label": [1], "date": "2023-05", "source": "LitSearch", "qid": "LitSearch_503", "valid": true}
{"query": "what's the first paper that manages to handle KBQA using LLMs without fine-tuning?", "cited_paper": [{"arxiv_id": "2212.09736", "title": "Don't Generate, Discriminate: A Proposal for Grounding Language Models to Real-World Environments", "year": 2022}], "gt_label": [1], "date": "2022-12", "source": "LitSearch", "qid": "LitSearch_504", "valid": true}
{"query": "which paper first focuses on addressing the over-smoothing issue for sentence embedding?", "cited_paper": [{"arxiv_id": "2305.06154", "title": "Alleviating Over-smoothing for Unsupervised Sentence Representation", "year": 2023}], "gt_label": [1], "date": "2023-05", "source": "LitSearch", "qid": "LitSearch_505", "valid": true}
{"query": "Can we reduce visual tokens in vision transformers right from the beginning?", "cited_paper": [{"arxiv_id": "2304.03768", "title": "SparseFormer: Sparse Visual Recognition via Limited Latent Tokens", "year": 2023}], "gt_label": [1], "date": "2023-04", "source": "LitSearch", "qid": "LitSearch_506", "valid": true}
{"query": "Can we learn to represent an image with arbitary numbers of tokens?", "cited_paper": [{"arxiv_id": "2304.03768", "title": "SparseFormer: Sparse Visual Recognition via Limited Latent Tokens", "year": 2023}], "gt_label": [1], "date": "2023-04", "source": "LitSearch", "qid": "LitSearch_507", "valid": true}
{"query": "Are there any papers that construct convolutional networks which are equivariant with respect to non-compact/non-abelian Lie groups?", "cited_paper": [{"arxiv_id": "2310.11366", "title": "Lie Group Decompositions for Equivariant Neural Networks", "year": 2023}], "gt_label": [1], "date": "2023-10", "source": "LitSearch", "qid": "LitSearch_508", "valid": true}
{"query": "Are there any papers that study whether you can identify if a LLM has been instructed to hide some information?", "cited_paper": [{"arxiv_id": "2309.15840", "title": "How to Catch an AI Liar: Lie Detection in Black-Box LLMs by Asking Unrelated Questions", "year": 2023}], "gt_label": [1], "date": "2023-09", "source": "LitSearch", "qid": "LitSearch_509", "valid": true}
{"query": "Are there any papers that use a world model for planning to ensure that decisions meet constraints?", "cited_paper": [{"arxiv_id": "2307.07176", "title": "SafeDreamer: Safe Reinforcement Learning with World Models", "year": 2023}], "gt_label": [1], "date": "2023-07", "source": "LitSearch", "qid": "LitSearch_510", "valid": true}
{"query": "Are there datasets and benchmarks available for measuring LLM graph reasoning abilities?", "cited_paper": [{"arxiv_id": "2310.04560", "title": "Talk like a Graph: Encoding Graphs for Large Language Models", "year": 2023}], "gt_label": [1], "date": "2023-10", "source": "LitSearch", "qid": "LitSearch_511", "valid": true}
{"query": "Are there sequential learning guarantees for configuring a linear system solver under a distributional assumption on the systems' target vectors?", "cited_paper": [{"arxiv_id": "2310.02246", "title": "Learning to Relax: Setting Solver Parameters Across a Sequence of Linear System Instances", "year": 2023}], "gt_label": [1], "date": "2023-10", "source": "LitSearch", "qid": "LitSearch_512", "valid": true}
{"query": "Can we find the solution of the Bilevel optimization when the lower-level problem is nonconvex?", "cited_paper": [{"arxiv_id": "2309.01753", "title": "On Penalty Methods for Nonconvex Bilevel Optimization and First-Order Stochastic Approximation", "year": 2023}], "gt_label": [1], "date": "2023-09", "source": "LitSearch", "qid": "LitSearch_513", "valid": true}
{"query": "Can you find a dataset that shows LLM-based evaluation may not be reliable enough?", "cited_paper": [{"arxiv_id": "2310.07641", "title": "Evaluating Large Language Models at Evaluating Instruction Following", "year": 2023}], "gt_label": [1], "date": "2023-10", "source": "LitSearch", "qid": "LitSearch_514", "valid": true}
{"query": "Can you find a research paper that discusses using structured pruning techniques to scale down language models, where the original model being pruned has billions of parameters?", "cited_paper": [{"arxiv_id": "2310.06694", "title": "Sheared LLaMA: Accelerating Language Model Pre-training via Structured Pruning", "year": 2023}], "gt_label": [1], "date": "2023-10", "source": "LitSearch", "qid": "LitSearch_515", "valid": true}
{"query": "I'm using Local SGD with a decaying learning rate for distributed training. Which paper offers guidance on setting the synchronization period in Local SGD to optimize test accuracy?", "cited_paper": [{"arxiv_id": "2310.14423", "title": "A Quadratic Synchronization Rule for Distributed Deep Learning", "year": 2023}], "gt_label": [1], "date": "2023-10", "source": "LitSearch", "qid": "LitSearch_516", "valid": true}
{"query": "In video diffusion models, is there any paper that tried decomposing video instruction into sub instructions of different time?", "cited_paper": [{"arxiv_id": "2303.14897", "title": "Seer: Language Instructed Video Prediction with Latent Diffusion Models", "year": 2023}], "gt_label": [1], "date": "2023-03", "source": "LitSearch", "qid": "LitSearch_517", "valid": true}
{"query": "Is there a paper illustrating that pre-trained transformers from LLMs can be used to encode visual information in a wide range of scenarios?", "cited_paper": [{"arxiv_id": "2310.12973", "title": "Frozen Transformers in Language Models Are Effective Visual Encoder Layers", "year": 2023}], "gt_label": [1], "date": "2023-10", "source": "LitSearch", "qid": "LitSearch_518", "valid": true}
{"query": "Is there a paper that takes a mixed machine learning and solver based approach to code translation?", "cited_paper": [{"arxiv_id": "2309.14396", "title": "Guess & Sketch: Language Model Guided Transpilation", "year": 2023}], "gt_label": [1], "date": "2023-09", "source": "LitSearch", "qid": "LitSearch_519", "valid": true}
{"query": "Is there a paper which applies Bayesian optimization to modular continual learning?", "cited_paper": [{"arxiv_id": "2306.06545", "title": "A Probabilistic Framework for Modular Continual Learning", "year": 2023}], "gt_label": [1], "date": "2023-06", "source": "LitSearch", "qid": "LitSearch_520", "valid": true}
{"query": "Is there a paper which proposes a general data selection method based on information theory?", "cited_paper": [{"arxiv_id": "2306.11670", "title": "GIO: Gradient Information Optimization for Training Dataset Selection", "year": 2023}], "gt_label": [1], "date": "2023-06", "source": "LitSearch", "qid": "LitSearch_521", "valid": true}
{"query": "Is there a parameter-efficient fine-tuning method (PEFT) that allows adjusting the number of optimized parameters to any value, irrespective of the model's architecture or choice of the adapter?", "cited_paper": [{"arxiv_id": "2310.02556", "title": "NOLA: Compressing LoRA using Linear Combination of Random Basis", "year": 2023}], "gt_label": [1], "date": "2023-10", "source": "LitSearch", "qid": "LitSearch_522", "valid": true}
{"query": "Is there a single GNN model that can inductively generalize to any knowledge graph?;What is the method to generalize knowledge graph reasoning to graphs with new entities and relations?;Is there a foundation model for knowledge graphs that does not learn embeddings for each node and relation type?", "cited_paper": [{"arxiv_id": "2310.04562", "title": "Towards Foundation Models for Knowledge Graph Reasoning", "year": 2023}], "gt_label": [1], "date": "2023-10", "source": "LitSearch", "qid": "LitSearch_523", "valid": true}
{"query": "Is there a theory paper that explains why sometimes tuning momentum does not boost performance for training a neural network?", "cited_paper": [{"arxiv_id": "2307.15196", "title": "The Marginal Value of Momentum for Small Learning Rate SGD", "year": 2023}], "gt_label": [1], "date": "2023-07", "source": "LitSearch", "qid": "LitSearch_524", "valid": true}
{"query": "Is there an existing dataset of images with alt-text that also includes the text the image was originally posted with?", "cited_paper": [{"arxiv_id": "2305.14779", "title": "Alt-Text with Context: Improving Accessibility for Images on Twitter", "year": 2023}], "gt_label": [1], "date": "2023-05", "source": "LitSearch", "qid": "LitSearch_525", "valid": true}
{"query": "Is there any generalizable NeRF paper that disentangles texture and shape?", "cited_paper": [{"arxiv_id": "2305.03040", "title": "TUVF: Learning Generalizable Texture UV Radiance Fields", "year": 2023}], "gt_label": [1], "date": "2023-05", "source": "LitSearch", "qid": "LitSearch_526", "valid": true}
{"query": "Is there any paper applies off-shelf GPT-2 model in D4RL tasks, using PEFT techniques?", "cited_paper": [{"arxiv_id": "2310.20587", "title": "Unleashing the Power of Pre-trained Language Models for Offline Reinforcement Learning", "year": 2023}], "gt_label": [1], "date": "2023-10", "source": "LitSearch", "qid": "LitSearch_527", "valid": true}
{"query": "Is there any paper improves adversarial training by forming semantic aware label without extra pre-train time or data?", "cited_paper": [{"arxiv_id": "2305.12118", "title": "Annealing Self-Distillation Rectification Improves Adversarial Training", "year": 2023}], "gt_label": [1], "date": "2023-05", "source": "LitSearch", "qid": "LitSearch_528", "valid": true}
{"query": "Is there any paper that explores ways to parameterize neural networks as proximal operators?", "cited_paper": [{"arxiv_id": "2310.14344", "title": "What's in a Prior? Learned Proximal Networks for Inverse Problems", "year": 2023}], "gt_label": [1], "date": "2023-10", "source": "LitSearch", "qid": "LitSearch_529", "valid": true}
{"query": "Is there any paper that previously proposed to control a risk using prediction sets, based on the literature in conformal prediction?", "cited_paper": [{"arxiv_id": "2208.02814", "title": "Conformal Risk Control", "year": 2022}], "gt_label": [1], "date": "2022-08", "source": "LitSearch", "qid": "LitSearch_530", "valid": true}
{"query": "Is there any paper that seamlessly integrates the multigrid structure in operator learning for solving partial differential equations (PDEs)?", "cited_paper": [{"arxiv_id": "2310.19809", "title": "MgNO: Efficient Parameterization of Linear Operators via Multigrid", "year": 2023}], "gt_label": [1], "date": "2023-10", "source": "LitSearch", "qid": "LitSearch_531", "valid": true}
{"query": "Is there any paper that theoretically explains why in-context reinforcement learning works?", "cited_paper": [{"arxiv_id": "2310.08566", "title": "Transformers as Decision Makers: Provable In-Context Reinforcement Learning via Supervised Pretraining", "year": 2023}], "gt_label": [1], "date": "2023-10", "source": "LitSearch", "qid": "LitSearch_532", "valid": true}
{"query": "Is there any paper that uses Lipschitz continuity in learning a dynamics model?", "cited_paper": [{"arxiv_id": "2310.12972", "title": "CCIL: Continuity-based Data Augmentation for Corrective Imitation Learning", "year": 2023}], "gt_label": [1], "date": "2023-10", "source": "LitSearch", "qid": "LitSearch_533", "valid": true}
{"query": "Is there any paper trying to improve MLE for auto-regressive language modeling through the lens of optimal transport?", "cited_paper": [{"arxiv_id": "2310.04691", "title": "EMO: Earth Mover Distance Optimization for Auto-Regressive Language Modeling", "year": 2023}], "gt_label": [1], "date": "2023-10", "source": "LitSearch", "qid": "LitSearch_534", "valid": true}
{"query": "Name a paper which proposes a probabilsitic formulation of retrosynthesis.", "cited_paper": [{"arxiv_id": "2310.09270", "title": "Retro-fallback: retrosynthetic planning in an uncertain world", "year": 2023}], "gt_label": [1], "date": "2023-10", "source": "LitSearch", "qid": "LitSearch_535", "valid": true}
{"query": "What are some evaluation benchmarks for LLM privacy at inference time, targeted towards model input and NOT the training data.", "cited_paper": [{"arxiv_id": "2310.17884", "title": "Can LLMs Keep a Secret? Testing Privacy Implications of Language Models via Contextual Integrity Theory", "year": 2023}], "gt_label": [1], "date": "2023-10", "source": "LitSearch", "qid": "LitSearch_536", "valid": true}
{"query": "What are the key advantages of coupling neural SDEs with neural CDEs for treatment effect estimation over existing baselines?", "cited_paper": [{"arxiv_id": "2310.17463", "title": "Bayesian Neural Controlled Differential Equations for Treatment Effect Estimation", "year": 2023}], "gt_label": [1], "date": "2023-10", "source": "LitSearch", "qid": "LitSearch_537", "valid": true}
{"query": "What is a paper studying data being collected in bundles in reinforcement learning ?", "cited_paper": [{"arxiv_id": "2310.01616", "title": "Sample-Efficiency in Multi-Batch Reinforcement Learning: The Need for Dimension-Dependent Adaptivity", "year": 2023}], "gt_label": [1], "date": "2023-10", "source": "LitSearch", "qid": "LitSearch_538", "valid": true}
{"query": "What is the first paper that theoretically studies training neural networks under small initialization?", "cited_paper": [{"arxiv_id": "2307.12851", "title": "Early Neuron Alignment in Two-layer ReLU Networks with Small Initialization", "year": 2023}], "gt_label": [1], "date": "2023-07", "source": "LitSearch", "qid": "LitSearch_539", "valid": true}
{"query": "What is the first paper that uses the generalized linear model to analyze multi-neural spike train data?", "cited_paper": [{"arxiv_id": "2310.15263", "title": "One-hot Generalized Linear Model for Switching Brain State Discovery", "year": 2023}], "gt_label": [1], "date": "2023-10", "source": "LitSearch", "qid": "LitSearch_540", "valid": true}
{"query": "What molecular representation learning paper introduced a benchmark that focuses on learning over thermodynamically-accessible conformer ensembles across diverse molecular properties and chemical reactions?", "cited_paper": [{"arxiv_id": "2310.00115", "title": "Learning Over Molecular Conformer Ensembles: Datasets and Benchmarks", "year": 2023}], "gt_label": [1], "date": "2023-10", "source": "LitSearch", "qid": "LitSearch_541", "valid": true}
{"query": "What open-source dataset combined knowledge retrieval with constraint satisfaction queries?", "cited_paper": [{"arxiv_id": "2310.15511", "title": "KITAB: Evaluating LLMs on Constraint Satisfaction for Information Retrieval", "year": 2023}], "gt_label": [1], "date": "2023-10", "source": "LitSearch", "qid": "LitSearch_542", "valid": true}
{"query": "What paper considers sensitive data issue when prompting large language model APIs?", "cited_paper": [{"arxiv_id": "2305.12723", "title": "Enhancing Small Medical Learners with Privacy-preserving Contextual Prompting", "year": 2023}], "gt_label": [1], "date": "2023-05", "source": "LitSearch", "qid": "LitSearch_543", "valid": true}
{"query": "What paper evaluated the ability of visual few-shot learning models to do in-context learning?", "cited_paper": [{"arxiv_id": "2310.10971", "title": "Context-Aware Meta-Learning", "year": 2023}], "gt_label": [1], "date": "2023-10", "source": "LitSearch", "qid": "LitSearch_544", "valid": true}
{"query": "What paper first adapted ControlNet to generate continuous videos in a training-free manner?", "cited_paper": [{"arxiv_id": "2305.13077", "title": "ControlVideo: Training-free Controllable Text-to-Video Generation", "year": 2023}], "gt_label": [1], "date": "2023-05", "source": "LitSearch", "qid": "LitSearch_545", "valid": true}
{"query": "What paper first associate the modeling frequency with input human skeletons under the NeRF framework?", "cited_paper": [{"arxiv_id": "2308.11951", "title": "Pose Modulated Avatars from Video", "year": 2023}], "gt_label": [1], "date": "2023-08", "source": "LitSearch", "qid": "LitSearch_546", "valid": true}
{"query": "What paper first extends rotary positional encoding (RoPE) for camera-geometry encoding in multi-view transformers?", "cited_paper": [{"arxiv_id": "2310.10375", "title": "GTA: A Geometry-Aware Attention Mechanism for Multi-View Transformers", "year": 2023}], "gt_label": [1], "date": "2023-10", "source": "LitSearch", "qid": "LitSearch_547", "valid": true}
{"query": "What paper first proposed a robust perceptual similarity metric with certificates?", "cited_paper": [{"arxiv_id": "2310.18274", "title": "LipSim: A Provably Robust Perceptual Similarity Metric", "year": 2023}], "gt_label": [1], "date": "2023-10", "source": "LitSearch", "qid": "LitSearch_548", "valid": true}
{"query": "What paper first proposes that simply reversing the output can significantly enhance the sample efficiency and the performance of the arithmetic capability of a decoder-only Transformer model?", "cited_paper": [{"arxiv_id": "2307.03381", "title": "Teaching Arithmetic to Small Transformers", "year": 2023}], "gt_label": [1], "date": "2023-07", "source": "LitSearch", "qid": "LitSearch_549", "valid": true}
{"query": "What paper first showed that you can score the code explanations using the pass@k metric?", "cited_paper": [{"arxiv_id": "2308.07124", "title": "OctoPack: Instruction Tuning Code Large Language Models", "year": 2023}], "gt_label": [1], "date": "2023-08", "source": "LitSearch", "qid": "LitSearch_550", "valid": true}
{"query": "What paper first used the technique of prompt engineering to generate adversarial prompts that can fool LLMs into making wrong predictions in prompt-based learning?", "cited_paper": [{"arxiv_id": "2310.13345", "title": "An LLM can Fool Itself: A Prompt-Based Adversarial Attack", "year": 2023}], "gt_label": [1], "date": "2023-10", "source": "LitSearch", "qid": "LitSearch_551", "valid": true}
{"query": "What paper first uses decoupled workers in distributed RL system?", "cited_paper": [{"arxiv_id": "2306.16688", "title": "SRL: Scaling Distributed Reinforcement Learning to Over Ten Thousand Cores", "year": 2023}], "gt_label": [1], "date": "2023-06", "source": "LitSearch", "qid": "LitSearch_552", "valid": true}
{"query": "What paper investigated the effect of the relative position (closer or further away) of the most pertinent retrieved code snippets on repository-level code completion performance?", "cited_paper": [{"arxiv_id": "2306.03091", "title": "RepoBench: Benchmarking Repository-Level Code Auto-Completion Systems", "year": 2023}], "gt_label": [1], "date": "2023-06", "source": "LitSearch", "qid": "LitSearch_553", "valid": true}
{"query": "What paper is the first to prove finetuned LLM can be a reliable judge?", "cited_paper": [{"arxiv_id": "2306.05087", "title": "PandaLM: An Automatic Evaluation Benchmark for LLM Instruction Tuning Optimization", "year": 2023}], "gt_label": [1], "date": "2023-06", "source": "LitSearch", "qid": "LitSearch_554", "valid": true}
{"query": "What paper mitigates language model sampling errors due to the softmax bottleneck?", "cited_paper": [{"arxiv_id": "2310.01693", "title": "Closing the Curious Case of Neural Text Degeneration", "year": 2023}], "gt_label": [1], "date": "2023-10", "source": "LitSearch", "qid": "LitSearch_555", "valid": true}
{"query": "What paper mitigates the vocabulary size limitation when pretraining multilingual masked language models using a contrastive loss?", "cited_paper": [{"arxiv_id": "2309.08351", "title": "Headless Language Models: Learning without Predicting with Contrastive Weight Tying", "year": 2023}], "gt_label": [1], "date": "2023-09", "source": "LitSearch", "qid": "LitSearch_556", "valid": true}
{"query": "What paper proposes breaking down programming problems by predicting the objects that a solution would create?", "cited_paper": [{"arxiv_id": "2307.13883", "title": "ExeDec: Execution Decomposition for Compositional Generalization in Neural Program Synthesis", "year": 2023}], "gt_label": [1], "date": "2023-07", "source": "LitSearch", "qid": "LitSearch_557", "valid": true}
{"query": "What paper showed first that one can build a fully differentiable mixture of experts layer with no increase in time complexity?", "cited_paper": [{"arxiv_id": "2308.00951", "title": "From Sparse to Soft Mixtures of Experts", "year": 2023}], "gt_label": [1], "date": "2023-08", "source": "LitSearch", "qid": "LitSearch_558", "valid": true}
{"query": "What paper shows that RLAIF can fully replace RLHF to align language models from scratch?", "cited_paper": [{"arxiv_id": "2310.05910", "title": "SALMON: Self-Alignment with Instructable Reward Models", "year": 2023}], "gt_label": [1], "date": "2023-10", "source": "LitSearch", "qid": "LitSearch_559", "valid": true}
{"query": "What research first proposed a new kind of cascaded diffusion of a Markov process?", "cited_paper": [{"arxiv_id": "2309.03350", "title": "Relay Diffusion: Unifying diffusion process across resolutions for image synthesis", "year": 2023}], "gt_label": [1], "date": "2023-09", "source": "LitSearch", "qid": "LitSearch_560", "valid": true}
{"query": "What work first uses LLM to code robotic simulation tasks and show sim-to-real benefits with policy pre-training in simulation?", "cited_paper": [{"arxiv_id": "2310.01361", "title": "GenSim: Generating Robotic Simulation Tasks via Large Language Models", "year": 2023}], "gt_label": [1], "date": "2023-10", "source": "LitSearch", "qid": "LitSearch_561", "valid": true}
{"query": "What work proposes a model to learn a latent regular cell complex from data?", "cited_paper": [{"arxiv_id": "2305.16174", "title": "From Latent Graph to Latent Topology Inference: Differentiable Cell Complex Module", "year": 2023}], "gt_label": [1], "date": "2023-05", "source": "LitSearch", "qid": "LitSearch_562", "valid": true}
{"query": "What work proposes to combine video foundation models with vision language models to effective high dimensional robot planning?", "cited_paper": [{"arxiv_id": "2310.10625", "title": "Video Language Planning", "year": 2023}], "gt_label": [1], "date": "2023-10", "source": "LitSearch", "qid": "LitSearch_563", "valid": true}
{"query": "Which backdoor paper first used the CLIP to suppress benign features and enhance poisoning features to design triggers?", "cited_paper": [{"arxiv_id": "2306.08386", "title": "Efficient Backdoor Attacks for Deep Neural Networks in Real-world Scenarios", "year": 2023}], "gt_label": [1], "date": "2023-06", "source": "LitSearch", "qid": "LitSearch_564", "valid": true}
{"query": "Which foundation model paper first proposed a time series model with proposed financial time series and text data?", "cited_paper": [{"arxiv_id": "2310.04948", "title": "TEMPO: Prompt-based Generative Pre-trained Transformer for Time Series Forecasting", "year": 2023}], "gt_label": [1], "date": "2023-10", "source": "LitSearch", "qid": "LitSearch_565", "valid": true}
{"query": "Which is one of the first papers to highlight and resolve the distribution shift in RLHF?", "cited_paper": [{"arxiv_id": "2308.02585", "title": "PARL: A Unified Framework for Policy Alignment in Reinforcement Learning from Human Feedback", "year": 2023}], "gt_label": [1], "date": "2023-08", "source": "LitSearch", "qid": "LitSearch_566", "valid": true}
{"query": "Which machine learning paper proposed certified robustness in the malware detection domain?", "cited_paper": [{"arxiv_id": "2303.13372", "title": "DRSM: De-Randomized Smoothing on Malware Classifier Providing Certified Robustness", "year": 2023}], "gt_label": [1], "date": "2023-03", "source": "LitSearch", "qid": "LitSearch_567", "valid": true}
{"query": "Which multimodal large language model represents visual data as the discrete tokens like text and training with the unified next-token prediction objective?", "cited_paper": [{"arxiv_id": "2309.04669", "title": "Unified Language-Vision Pretraining in LLM with Dynamic Discrete Visual Tokenization", "year": 2023}, {"arxiv_id": "2309.04669", "title": "Unified Language-Vision Pretraining in LLM with Dynamic Discrete Visual Tokenization", "year": 2023}], "gt_label": [1, 1], "date": "2023-09", "source": "LitSearch", "qid": "LitSearch_568", "valid": true}
{"query": "Which neural theorem proving paper first attempted to prove theorems in a block-by-block manner?", "cited_paper": [{"arxiv_id": "2310.00656", "title": "LEGO-Prover: Neural Theorem Proving with Growing Libraries", "year": 2023}], "gt_label": [1], "date": "2023-10", "source": "LitSearch", "qid": "LitSearch_569", "valid": true}
{"query": "Which paper considers both weights and activations when pruning large language models?", "cited_paper": [{"arxiv_id": "2306.11695", "title": "A Simple and Effective Pruning Approach for Large Language Models", "year": 2023}], "gt_label": [1], "date": "2023-06", "source": "LitSearch", "qid": "LitSearch_570", "valid": true}
{"query": "Which paper contains quantitative results demonstrating taking VQ tokens as inputs is inferior to pixel images for dense recognition tasks?", "cited_paper": [{"arxiv_id": "2306.05423", "title": "ADDP: Learning General Representations for Image Recognition and Generation with Alternating Denoising Diffusion Process", "year": 2023}], "gt_label": [1], "date": "2023-06", "source": "LitSearch", "qid": "LitSearch_571", "valid": true}
{"query": "Which paper examined the scalability of instruction-tuning with respect to Mixture of Expert models?", "cited_paper": [{"arxiv_id": "2305.14705", "title": "Mixture-of-Experts Meets Instruction Tuning:A Winning Combination for Large Language Models", "year": 2023}], "gt_label": [1], "date": "2023-05", "source": "LitSearch", "qid": "LitSearch_572", "valid": true}
{"query": "Which paper first applied the chain of thought concepts in 3D localization problem?", "cited_paper": [{"arxiv_id": "2310.06214", "title": "CoT3DRef: Chain-of-Thoughts Data-Efficient 3D Visual Grounding", "year": 2023}], "gt_label": [1], "date": "2023-10", "source": "LitSearch", "qid": "LitSearch_573", "valid": true}
{"query": "Which paper first derived online occupany estimation technique to get sqrt(T) bound for reinforcement learning in adversarial linear MDP?", "cited_paper": [{"arxiv_id": "2310.11550", "title": "Towards Optimal Regret in Adversarial Linear MDPs with Bandit Feedback", "year": 2023}], "gt_label": [1], "date": "2023-10", "source": "LitSearch", "qid": "LitSearch_574", "valid": true}
{"query": "Which paper first found that REINFORCE works better than actor critic algorithms like PPO for RL finetuning of pretrained chemistry language models (Transformers and RNNs)?", "cited_paper": [{"arxiv_id": "2310.02902", "title": "Searching for High-Value Molecules Using Reinforcement Learning and Transformers", "year": 2023}], "gt_label": [1], "date": "2023-10", "source": "LitSearch", "qid": "LitSearch_575", "valid": true}
{"query": "Which paper first found that when transformers are trained to in-context learn function classes, they might exhibit generalization followed by memorization, in certain settings?", "cited_paper": [{"arxiv_id": "2306.04891", "title": "In-Context Learning through the Bayesian Prism", "year": 2023}], "gt_label": [1], "date": "2023-06", "source": "LitSearch", "qid": "LitSearch_576", "valid": true}
{"query": "Which paper first investigates the knowledge preferences of LLMs when there are conflicts between the context and the parametric memory?", "cited_paper": [{"arxiv_id": "2305.13300", "title": "Adaptive Chameleon or Stubborn Sloth: Revealing the Behavior of Large Language Models in Knowledge Conflicts", "year": 2023}], "gt_label": [1], "date": "2023-05", "source": "LitSearch", "qid": "LitSearch_577", "valid": true}
{"query": "Which paper first proposes a unified framework for black-box and white-box detection of AI-written text with explanations?", "cited_paper": [{"arxiv_id": "2305.17359", "title": "DNA-GPT: Divergent N-Gram Analysis for Training-Free Detection of GPT-Generated Text", "year": 2023}], "gt_label": [1], "date": "2023-05", "source": "LitSearch", "qid": "LitSearch_578", "valid": true}
{"query": "Which paper first proved that wide-enough transformer architectures trained with gradient methods on enough data would learn to solve relational reasoning tasks?", "cited_paper": [{"arxiv_id": "2310.09753", "title": "When can transformers reason with abstract symbols?", "year": 2023}], "gt_label": [1], "date": "2023-10", "source": "LitSearch", "qid": "LitSearch_579", "valid": true}
{"query": "Which paper first showed that task-specific knowledge embedded in parameters can be extracted from one LLM using seed samples and transferred to another via parameter-efficient fine-tuning?", "cited_paper": [{"arxiv_id": "2310.11451", "title": "Seeking Neural Nuggets: Knowledge Transfer in Large Language Models from a Parametric Perspective", "year": 2023}], "gt_label": [1], "date": "2023-10", "source": "LitSearch", "qid": "LitSearch_580", "valid": true}
{"query": "Which paper first studied differential privacy for in-context learning to prevent prompt leakage attacks?", "cited_paper": [{"arxiv_id": "2305.01639", "title": "Privacy-Preserving In-Context Learning for Large Language Models", "year": 2023}], "gt_label": [1], "date": "2023-05", "source": "LitSearch", "qid": "LitSearch_581", "valid": true}
{"query": "Which paper first study POMDP with enhanced feedback on observations?", "cited_paper": [{"arxiv_id": "2307.02884", "title": "Sample-Efficient Learning of POMDPs with Multiple Observations In Hindsight", "year": 2023}], "gt_label": [1], "date": "2023-07", "source": "LitSearch", "qid": "LitSearch_582", "valid": true}
{"query": "Which paper first tried to fine-tune LLMs with chain-of-thoughts and program-of-thoughts for math reasoning?", "cited_paper": [{"arxiv_id": "2309.05653", "title": "MAmmoTH: Building Math Generalist Models through Hybrid Instruction Tuning", "year": 2023}], "gt_label": [1], "date": "2023-09", "source": "LitSearch", "qid": "LitSearch_583", "valid": true}
{"query": "Which paper first used language models to emulate tool executions for studying the risks of language model agents?", "cited_paper": [{"arxiv_id": "2309.15817", "title": "Identifying the Risks of LM Agents with an LM-Emulated Sandbox", "year": 2023}], "gt_label": [1], "date": "2023-09", "source": "LitSearch", "qid": "LitSearch_584", "valid": true}
{"query": "Which paper formally defines the problem of model selection in llm agent for multi-modal reasoning?", "cited_paper": [{"arxiv_id": "2310.08446", "title": "Towards Robust Multi-Modal Reasoning via Model Selection", "year": 2023}], "gt_label": [1], "date": "2023-10", "source": "LitSearch", "qid": "LitSearch_585", "valid": true}
{"query": "Which paper found that using common character encodings and ciphers, or even just convincing the model that it is not communicating in natural language, can bypass the safety guardrails of large models?", "cited_paper": [{"arxiv_id": "2308.06463", "title": "GPT-4 Is Too Smart To Be Safe: Stealthy Chat with LLMs via Cipher", "year": 2023}], "gt_label": [1], "date": "2023-08", "source": "LitSearch", "qid": "LitSearch_586", "valid": true}
{"query": "Which paper in human motion generation can control the spatial location of any joints of the human with either dense or sparse 3D points?", "cited_paper": [{"arxiv_id": "2310.08580", "title": "OmniControl: Control Any Joint at Any Time for Human Motion Generation", "year": 2023}], "gt_label": [1], "date": "2023-10", "source": "LitSearch", "qid": "LitSearch_587", "valid": true}
{"query": "Which paper is the first to model the helpfulness and harmlessness alignment of LLMs as a Constrained MDP problem?", "cited_paper": [{"arxiv_id": "2310.12773", "title": "Safe RLHF: Safe Reinforcement Learning from Human Feedback", "year": 2023}], "gt_label": [1], "date": "2023-10", "source": "LitSearch", "qid": "LitSearch_588", "valid": true}
{"query": "Which paper proposes an alignment framework that steers language models to preferences of individual groups in a few-shot manner through augmenting the LLM with a transformer module?", "cited_paper": [{"arxiv_id": "2310.11523", "title": "Group Preference Optimization: Few-Shot Alignment of Large Language Models", "year": 2023}], "gt_label": [1], "date": "2023-10", "source": "LitSearch", "qid": "LitSearch_589", "valid": true}
{"query": "Which paper proposes to integrate black-box LLMs with a pool of smaller but specialized language models?", "cited_paper": [{"arxiv_id": "2305.09955", "title": "Knowledge Card: Filling LLMs' Knowledge Gaps with Plug-in Specialized Language Models", "year": 2023}], "gt_label": [1], "date": "2023-05", "source": "LitSearch", "qid": "LitSearch_590", "valid": true}
{"query": "Which paper studies how difficult is a policy learning problem under non-additive rewards in terms of theoretical lower bounds, and what could be a potential strategy to solve it empirically while recovering some specialized guarantees?", "cited_paper": [{"arxiv_id": "2307.13372", "title": "Submodular Reinforcement Learning", "year": 2023}], "gt_label": [1], "date": "2023-07", "source": "LitSearch", "qid": "LitSearch_591", "valid": true}
{"query": "Which paper trains on linear regression to hypothesize how fine-tuning affects language models?", "cited_paper": [{"arxiv_id": "2309.10105", "title": "Understanding Catastrophic Forgetting in Language Models via Implicit Inference", "year": 2023}], "gt_label": [1], "date": "2023-09", "source": "LitSearch", "qid": "LitSearch_592", "valid": true}
{"query": "Which paper uses the latent diffusion model for the first time to solve offline reinforcement learning problems based on the sequential modeling framework?", "cited_paper": [{"arxiv_id": "2310.00311", "title": "Efficient Planning with Latent Diffusion", "year": 2023}], "gt_label": [1], "date": "2023-10", "source": "LitSearch", "qid": "LitSearch_593", "valid": true}
{"query": "Which paper utilized MMD flows with Riesz kernels to solve Bayesian inverse problems?", "cited_paper": [{"arxiv_id": "2310.03054", "title": "Posterior Sampling Based on Gradient Flows of the MMD with Negative Distance Kernel", "year": 2023}], "gt_label": [1], "date": "2023-10", "source": "LitSearch", "qid": "LitSearch_594", "valid": true}
{"query": "What paper provides generalization bounds for self supervised learning models eg. CLIP", "cited_paper": [{"arxiv_id": "2310.03957", "title": "Understanding prompt engineering may not require rethinking generalization", "year": 2023}], "gt_label": [1], "date": "2023-10", "source": "LitSearch", "qid": "LitSearch_595", "valid": true}
{"query": "Which paper systematically examed the input mismatch between training and sampling in diffusion models", "cited_paper": [{"arxiv_id": "2308.15321", "title": "Elucidating the Exposure Bias in Diffusion Models", "year": 2023}], "gt_label": [1], "date": "2023-08", "source": "LitSearch", "qid": "LitSearch_596", "valid": true}
